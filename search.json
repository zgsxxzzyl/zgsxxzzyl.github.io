[{"title":"ANTLR参考","url":"/2020/06/18/ANTLR%E5%8F%82%E8%80%83/","content":"概述ANTLR（ANother Tool for Language Recognition 另一种语言识别工具）是功能强大的解析器生成器，用于读取，处理，执行或翻译结构化文本或二进制文件。它被广泛用于构建语言，工具和框架。ANTLR通过语法生成可以构建和遍历语法树的语法分析器。（转自官方）\n下载：https://www.antlr.org/download.html\n文档：https://github.com/antlr/antlr4/blob/master/doc/index.md\nIDE插件：https://www.antlr.org/tools.html\n官方教程：https://github.com/antlr/antlr4/blob/master/doc/getting-started.md\n流程\nANTLR定义一个g4的文件类型，编写语法规则文件。\ng4文件是ANTLR生成词法解析规则和语法解析规则的基础。\n\n使用工具生成解析代码。\n\n依赖生成的解析代码实现自己的功能。\n\n\n一. 创建g4文件简单例子在一个临时目录下创建Hello.g4文件。在文件中写入以下内容：\n&#x2F;&#x2F; Define a grammar called Hellogrammar Hello;r  : &#39;hello&#39; ID ;         &#x2F;&#x2F; match keyword hello followed by an identifierID : [a-z]+ ;             &#x2F;&#x2F; match lower-case identifiersWS : [ \\t\\r\\n]+ -&gt; skip ; &#x2F;&#x2F; skip spaces, tabs, newlines\n\n注意：定义语法名称需要首字母大写，因为生成的文件和类命名会使用到\n二. 生成解析器1. 安装工具2. 执行命令antlr4 Hello.g4\n\n三. 解析器源码依赖&lt;dependency&gt;  &lt;groupId&gt;org.antlr&lt;/groupId&gt;  &lt;artifactId&gt;antlr4&lt;/artifactId&gt;  &lt;version&gt;4.8&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt;  &lt;groupId&gt;org.antlr&lt;/groupId&gt;  &lt;artifactId&gt;antlr4-runtime&lt;/artifactId&gt;  &lt;version&gt;4.8&lt;/version&gt;&lt;/dependency&gt;\n\n代码\n&lt;Grammar&gt;Lexer.java: 词法分析器源码；\n&lt;Grammar&gt;Parser.java: 语法分析器源码；\n&lt;Grammar&gt;Listener.java: Listener 接口；\n&lt;Grammar&gt;BaseListener.java: Listener 默认实现；\n&lt;Grammar&gt;Visitor.java: Visitor 接口；\n&lt;Grammar&gt;BaseVisitor.java: Visitor 默认实现；\n\n如何使用：\npublic static void main(String[] args) throws Exception &#123;    //代码流    ANTLRInputStream input = new ANTLRInputStream(\"int a = 12; \");    //使用词法分析器生成Token序列    Java8Lexer lexer = new Java8Lexer(input);    CommonTokenStream tokens = new CommonTokenStream(lexer);    //使用语法分析器将Token序列串联成AST    Java8Parser parser = new Java8Parser(tokens);    ParseTree tree = parser.expressionName();    //Visito模式或者Listener模式遍历AST    System.out.println(\"Visitor:\");    Java8Visitor evalByVisitor = new Java8BaseVisitor();    evalByVisitor.visit(tree);    //Listener模式遍历AST    System.out.println(\"Listener:\");    ParseTreeWalker walker = new ParseTreeWalker();    Java8Listener evalByListener = new Java8BaseListener();    walker.walk(evalByListener, tree);&#125;\n\n辅助工具","tags":["Java"]},{"title":"Docker中安装部署MySQL","url":"/2019/10/24/Docker%E4%B8%AD%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2MySQL/","content":"Docker中安装部署MySQL安装启动\n参考文档\n安装前要选择安装版本，访问官方链接，找到适合的版本，这里我选择5.7版本\n\n# 拉去mysql镜像 docker pull mysql/mysql-server:tagdocker pull mysql/mysql-server:5.7# 查看是否全部镜像docker images# 选择镜像，启动容器 docker run --name=container_name -d image_name:tag# -d 后台启动 -p 容器3306端口映射外部端口3308 --name 给容器实例命名mysql5.7# docker run -p 3346:3306 --name mysql-slave4 -e MYSQL_ROOT_PASSWORD=123456 -d mysql:5.7docker run --name=mysql5.7 -p 3308:3306 -d mysql/mysql-server:5.7# 容器启动过程中会重新初始化数据库，生成随机密码，查看容器启动日志docker logs mysql5.7 2&gt;&amp;1 | grep GENERATED# 登录数据库docker exec -it mysql5.7 mysql -uroot -p# 改密码ALTER USER 'root'@'localhost' IDENTIFIED BY 'password';# 默认mysql的root用户不支持远程访问，开启访问权限GRANT ALL ON *.* TO root@'%' IDENTIFIED BY 'password' WITH GRANT OPTION;flush privileges;# 进入dockerdocker exec -it mysql5.7 bash# 容器mysql部署路径cd /var/lib/mysql#启动、重启、停止、删除容器docker start mysql5.7docker restart mysql5.7docker stop mysql5.7docker rm mysql5.7\n\n容器挂载本地配置和数据目录docker run --name=mysql5.7 \\--mount type=bind,src=/opt/mysql.docker/my.cnf,dst=/etc/my.cnf \\--mount type=bind,src=/opt/mysql.docker/datadir,dst=/var/lib/mysql \\-p 3308:3306 \\-d mysql/mysql-server:5.7\n","tags":["Docker","MySQL","数据库","部署文档"]},{"title":"Docker安装部署","url":"/2019/10/23/Docker%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/","content":"Docker安装部署\n官方文档\n\nLinux环境下部署\n卸载旧版本\n\nsudo yum remove docker \\                  docker-client \\                  docker-client-latest \\                  docker-common \\                  docker-latest \\                  docker-latest-logrotate \\                  docker-logrotate \\                  docker-engine\n\n\n在新主机上首次安装Docker Engine-Community之前，需要设置Docker存储库。\n\nsudo yum install -y yum-utils \\  device-mapper-persistent-data \\  lvm2\n\nsudo yum-config-manager \\    --add-repo \\    https://download.docker.com/linux/centos/docker-ce.repo\n\n\n安装\n\nsudo yum install -y docker-ce docker-ce-cli containerd.io\n\n\n启动\n\nsudo systemctl start docker\n\n\n卸载\n\n# 卸载Docker软件包：sudo yum remove docker-ce# 主机上的映像，容器，卷或自定义配置文件不会自动删除。要删除所有图像，容器和卷：sudo rm -rf &#x2F;var&#x2F;lib&#x2F;docker\n","tags":["Docker","部署文档"]},{"title":"Fork/Join框架详细介绍","url":"/2020/04/13/Fork-Join%E6%A1%86%E6%9E%B6%E8%AF%A6%E7%BB%86%E4%BB%8B%E7%BB%8D/","content":"\nhttps://docs.oracle.com/javase/tutorial/essential/concurrency/forkjoin.html\n\n前言理解这个框架前，我们需要知道这个框架设计的目标是为了解决什么问题？\n为什么不可以用已有的框架来解决？\n举例说明：对一个数组进行求和操作，如果这个数组非常大，我们会选择分段求和最后汇总\n\n上图所示，0-999表示数组的下标，如果使用ThreadPoolExecutor创建线程池，可以考虑一下分割成上面的任务需要多少个线程。这里需要用到7个线程来实现求和，如果再分割多次，最后需要的线程会很多，显然不是我们所想要的。\n为了满足我们的需求，Java从JDK7开始引入了Fork/Join框架来解决这类问题，利用多个处理器，是为可以递归分解为较小部分的工作而设计的。目标是使用所有可用的处理能力来增强应用程序的性能。\nFork/Join是ExecutorService接口的实现（这个接口用来实现线程池的，譬如我们常用的ThreadPoolExecutor），与其他实现ExecutorService的线程池一样，Fork/Join框架将任务分配给线程池中的工作线程。\n与其他线程池不同的是，它使用了工作窃取算法（work-stealing），工作用尽的工作线程可以从其他仍很忙的线程中窃取任务。举个例子，0-499这个线程的子线程都计算完了，500-999子线程还在计算500-749，如果0-499这个线程等待500-999这个线程返回结果再去汇总这样会浪费资源，工作窃取算法会使执行0-499这个线程执行750-999，这样可以提高效率。\nFork/Join核心类\nForkJoinPool\n\nForkJoinTask\n\nRecursiveAction\n\nRecursiveTask\n\n\n","tags":["Java"]},{"title":"Docker常用命令","url":"/2019/10/22/Docker%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/","content":"Docker常用命令\n官方文档\n\n启动docker进程systemctl start docker\n\n搜索可用镜像docker search xxNAME           # 镜像仓库DESCRIPTION    # 镜像描述信息STARS          # 镜像收藏数OFFICIAL       # 是否为docker官方发布的镜像AUTOMATED      # 是否为自动化构建的镜像，关于自动化构建，可以查看官方文档：https://docs.docker.com/docker-hub/builds/#how-automated-builds-work\n拉去镜像docker pull [OPTIONS] NAME[:TAG|@DIGEST]\n列出本地全部镜像docker images\n查看镜像的详情docker inspect [OPTIONS] NAME|ID [NAME|ID...]docker image inspect nginx:latest | grep -i version\n\n删除镜像docker rmi [OPTIONS] IMAGE [IMAGE...]\n基于镜像创建并启动容器docker run [OPTIONS] IMAGE [COMMAND] [ARG...]-d, --detach                         Run container in background and print container ID-t, --tty                            Allocate a pseudo-TTY-p, --publish list                   Publish a container's port(s) to the host-P, --publish-all                    Publish all exposed ports to random ports#基于镜像启动容器实例docker run -t -i IMAGE /bin/bash\n删除容器docker rm [OPTIONS] CONTAINER [CONTAINER...]\n启动容器docker start [OPTIONS] CONTAINER [CONTAINER...]\n查看启动容器日志Usage:  docker logs [OPTIONS] CONTAINERFetch the logs of a containerOptions:      --details        Show extra details provided to logs  -f, --follow         Follow log output      --since string   Show logs since timestamp (e.g. 2013-01-02T13:23:37) or relative (e.g. 42m for 42 minutes)      --tail string    Number of lines to show from the end of the logs (default \"all\")  -t, --timestamps     Show timestamps      --until string   Show logs before a timestamp (e.g. 2013-01-02T13:23:37) or relative (e.g. 42m for 42 minutes)\n# 查看容器近30分钟日志docker logs --since 30m CONTAINER_ID# 查看某时间之后的日志docker logs -t --since=\"2019-08-02T13:23:37\" CONTAINER_ID# 查看某时间段日志：docker logs -t --since=\"2019-08-02T13:23:37\" --until \"2019-08-03T12:23:37\" CONTAINER_ID\n\n查看启动的实例docker ps [OPTIONS]-a, --all             Show all containers (default shows just running)\n修改容器端口映射# 直接修改配置文件/var/lib/docker/containers/[CONTAINER ID]/config.v2.json# 容器3306端口映射到外部3310端口docker run -dit --privileged -p 3310:3306 --name=CONTAINNAME IMAGE /usr/sbin/init\n进入容器docker exec -it [CONTAINER ID] bashdocker exec -it [CONTAINER ID] /bin/sh\n导入导出容器docker export [imagename] &gt; [url]docker import [url] &gt; [imagename]\n\n拷贝docker cp [OPTIONS] CONTAINER:SRC_PATH DEST_PATHdocker cp [OPTIONS] SRC_PATH CONTAINER:DEST_PATH\n\n基于容器创建镜像docker commit containid imagename\n\n挂载目录docker run -v &lt;host&gt;:&lt;container&gt;:[rw|ro]\n\n历史记录docker run -di \\--name=tomcat8081 \\-p 8081:8080 \\-v /opt/tomcat.docker/tomcat8081/logs:/usr/local/tomcat/logs \\-v /opt/tomcat.docker/tomcat8081/conf:/usr/local/tomcat/conf \\-v /opt/tomcat.docker/tomcat8081/webapps:/usr/local/tomcat/webapps \\tomcat:8.5-jdk8\n","tags":["Docker"]},{"title":"GitLab配置启动HTTPS","url":"/2019/10/22/GitLab%E9%85%8D%E7%BD%AE%E5%90%AF%E5%8A%A8HTTPS/","content":"#gitlab配置启用httpsopenssl genrsa -des3 -out /etc/gitlab/ssl/gitlab.com.key 2048openssl rsa -in certificate_before.key -out certificate_after.keyopenssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout /etc/gitlab/ssl/gitlab.com.key -out /etc/gitlab/ssl/gitlab.com.crtfirewall-cmd --permanent --add-service=httpssystemctl reload firewalldgitlab-ctl reconfigure\n","tags":["部署文档","GitLab"]},{"title":"Git常用命令","url":"/2019/10/22/Git-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/","content":"Git配置SSH key\n相关：https://blog.csdn.net/hao495430759/article/details/80673568\n\n(1)生成并部署SSH key安装好Git客户端后，打开git bash，输入以下命令生成user1的SSH Key：\nssh-keygen -t rsa -C \"user1@email.com\"\n\n在当前用户的.ssh目录下会生成id_rsa私钥文件和id_rsa.pub公钥文件，将id_rsa.pub中的内容添加至user1的github中。然后在git bash中输入以下命令测试该用户的SSH密钥是否生效：\nssh -T git@github.com\n\n若连接成功则提示Hi user1! You’ve successfully authenticated, but GitHub does not provide shell access.\n注：该命令仅限于文件名为id_rsa的密钥。\n接着生成user2的密钥，注意不能再使用默认的文件名id_rsa，否则会覆盖之前密钥文件：\nssh-keygen -t rsa -f ~/.ssh/id_rsa2 -C \"user2@email.com\"\n\n再将该用户的公钥文件添加至github中。测试user2的ssh连接时需要指定密钥文件：\nssh -T git@github.com -i ~/.ssh/id_rsa2\n\n也可以使用ssh agent添加密钥后进行测试。因为系统默认只读取id_rsa，为了让ssh识别新的私钥，可以使用ssh-agent手动添加私钥：\nssh-agent bashssh-add ~/.ssh/id_rsa2\n\n注：该方法仅限当前窗口有效，打开新的窗口则ssh连接失败。\n(2)配置config文件在.ssh目录下创建一个config文本文件，每个账号配置一个Host节点。主要配置项说明：\nHost    主机别名HostName    服务器真实地址IdentityFile    私钥文件路径PreferredAuthentications    认证方式User    用户名\n\n配置文件内容\n# 配置user1Host u1.github.comHostName github.comIdentityFile C:\\\\Users\\\\Administrator\\\\.ssh\\\\id_rsaPreferredAuthentications publickeyUser user1# 配置user2Host u2.github.comHostName github.comIdentityFile C:\\\\Users\\\\Administrator\\\\.ssh\\\\id_rsa2PreferredAuthentications publickeyUser user2\n\n再通过终端测试SSH Key是否生效\nssh -T git@u1.github.comssh -T git@u2.github.com\n\n(3)配置用户名和邮箱如果之前配置过全局的用户名和邮箱，需要取消相关配置，再在各仓库下配置相应的用户名和邮箱。\ngit config --global --unset user.namegit config --global --unset user.email\n\n为各仓库单独配置用户名和邮箱\ngit config user.name \"user1\"git config user.email \"user1@email.com\"\n\n如果原先使用HTTPS通信，则需要修改远程仓库地址\ngit remote rm origingit remote add origin git@u1.github.com:xxx/xxxxx.git\n\n操作命令\n从远端合并代码到本地\n\n# 1.配置上游地址(只需要一次)git remote add upstream 你上游项目的地址# 2.获取上游更新git fetch upstream# 3.合并到本地分支git merge upstream/master# 4.提交推送git push origin master\n\n\n首次配置git时候测试连接\n\nssh -T git@192.168.99.168\n\n\n生成密钥\n\ngit config –global user.name ‘username’git config –global user.email ‘example@email.com'ssh-keygen -t rsa -C 'example@email.com'\n\n\n撤销上次commit\n\n# 注意，仅仅是撤回commit操作，写的代码仍然保留。git reset --soft HEAD^\nGit中文乱码解决git status不能显示中文\n现象status查看有改动但未提交的文件时总只显示数字串，显示不出中文文件名，非常不方便。如下图：\n\n原因在默认设置下，中文文件名在工作区状态输出，中文名不能正确显示，而是显示为八进制的字符编码。\n\n解决办法将git 配置文件 core.quotepath项设置为false。 quotepath表示引用路径，加上–global表示全局配置\n\n\ngit bash 终端输入命令：\ngit config --global core.quotepath false\n\n通过修改配置文件来解决中文乱码[gui]encoding = utf-8# 代码库统一使用utf-8[i18n]commitencoding = utf-8# log编码[svn]pathnameencoding = utf-8# 支持中文路径[core]quotepath = false# status引用路径不再是八进制（反过来说就是允许显示中文了）\n","tags":["Git"]},{"title":"JDK安装部署","url":"/2019/10/22/JDK%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/","content":"下载Oracle JRE 8 下载链接\n选择jdk-8u211-linux-x64.rpm下载\n# 卸载全部包含java的包rpm -e --nodeps `rpm -qa|grep java`# 查看安装的路径rpm -qpl jdk-8u211-linux-x64.rpm# 安装rpm -i jdk-8u211-linux-x64.rpm\n\n配置环境变量export JAVA_HOME&#x3D;&quot;&#x2F;usr&#x2F;java&#x2F;jdk1.8.0_211-amd64&quot;export PATH&#x3D;&quot;$JAVA_HOME&#x2F;bin:$PATH&quot;export CLASSPATH&#x3D;&quot;.:$JAVA_HOME&#x2F;lib&quot;\n","tags":["Java","部署文档"]},{"title":"JVM参数文档","url":"/2020/03/16/JVM%E5%8F%82%E6%95%B0%E6%96%87%E6%A1%A3/","content":"\nhttps://docs.oracle.com/javase/8/docs/technotes/guides/vm/gctuning/index.html\nhttps://docs.oracle.com/javase/8/docs/technotes/tools/unix/java.html\n\n\n\n\n参数名称 \n含义\n默认值\n说明\n\n\n\n-Xms\n设置堆的初始大小（以字节为单位）\n初始大小将设置为为老一代和年轻一代分配的大小之和\n此值必须是1024的倍数且大于1 MB。在字母后面加上k或K表示千字节，m或M表示兆字节，g或G表示千兆字节。\n\n\n-Xmx\n指定内存分配池的最大大小（以字节为单位）\n默认值是在运行时根据系统配置选择的\n此值必须是1024的倍数且大于2 MB\n\n\nexport JAVA_OPTIONS=\"$&#123;JAVA_OPTIONS&#125; -server -Xms6144m -Xmx6144m -Xmn2048m -Xss256k -XX:PermSize=512m -XX:MaxPermSize=1024m  -XX:SurvivorRatio=2 -XX:+UseConcMarkSweepGC         -XX:+CMSParallelRemarkEnabled         -XX:+UseCMSCompactAtFullCollection         -XX:CMSFullGCsBeforeCompaction=3         -XX:+UseFastAccessorMethods         -XX:+UseCMSInitiatingOccupancyOnly         -XX:CMSInitiatingOccupancyFraction=80         -XX:+DoEscapeAnalysis         -XX:+EliminateAllocations         -XX:+HeapDumpOnOutOfMemoryError         -XX:-UseGCOverheadLimit         -XX:+TraceClassLoading         -XX:+CMSClassUnloadingEnabled         -XX:+PrintClassHistogram         -Djava.awt.headless=true         -XX:+UseParNewGC         -XX:ParallelGCThreads=4         -Doracle.jdbc.useThreadLocalBufferCache=false         -Doracle.jdbc.maxCachedBufferSize=0         -Dfile.encoding=utf-8i         -Dsun.zip.disableMemoryMapping=true        -Djava.rmi.server.hostname=192.168.99.67 -Dcom.sun.management.jmxremote.port=8989 -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false -Djavax.management.builder.initial=weblogic.management.jmx.mbeanserver.WLSMBeanServerBuilder\"\n","tags":["Java"]},{"title":"JDK原生工具介绍","url":"/2019/12/18/JDK%E5%8E%9F%E7%94%9F%E5%B7%A5%E5%85%B7%E4%BB%8B%E7%BB%8D/","content":"native2ascii\nnative2ascii.exe 是Java的一个文件转码工具，是将特殊各异的内容转为用指定的编码标准文体形式统一的表现出来，它通常位于JDK_home\\bin目录下，安装好Java SE后，可在命令行直接使用 native2ascii命令进行转码。\n\nnative2ascii -[options] [inputfile] [outputfile]\n国际化resources.properties文件，中文字符转换为Unicode字符：native2ascii resources.properties tmp.properties或者 native2ascii -encoding Unicode resources.properties tmp.properties 注意：Unicode首字母必须大写国际化resources.properties文件，Unicode字符转换为中文字符： native2ascii -reverse -encoding GB2312 resources.properties tmp.properties\n","tags":["Java"]},{"title":"JavaScript实用操作指南","url":"/2020/08/05/JavaScript%E5%AE%9E%E7%94%A8%E6%93%8D%E4%BD%9C%E6%8C%87%E5%8D%97/","content":"一、JavaScript参考https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Reference\n","tags":["JavaScript"]},{"title":"GitLab安装部署","url":"/2019/12/31/GitLab%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/","content":"安装部署\nhttps://docs.gitlab.com/omnibus/manual_install.html\n\n\n配置ssh免密之后发现还是需要输入密码\n网上的说法是：rpm安装过程当中会创建git用户，但是这个用户处在锁定状态，用下面这个命令解锁\n# 解锁sudo passwd -u -f git# 查看所有账号cat /etc/passwd# 查看xxx账户状态，是否被锁定passwd -S xxx# 锁定xxx账号usermod -L xxx\n\nGitLab钩子\n\nhttps://docs.gitlab.com/ee/administration/custom_hooks.html\n\n/var/opt/gitlab/git-data/repositories//opt/gitlab/embedded/service/gitlab-shell/hooks\n\n汉化\n\nhttps://gitlab.com/xhang/gitlab/-/wikis/home\n\n# 获取当前版本，版本可能不一致gitlab_version=$(sudo cat /opt/gitlab/embedded/service/gitlab-rails/VERSION)# 克隆汉化版本库git clone https://gitlab.com/xhang/gitlab.git# 如果已经克隆过，则进行更新git fetch# 导出gitlab_version版本的汉化补丁git diff v$&#123;gitlab_version&#125; v$&#123;gitlab_version&#125;-zh &gt; ../$&#123;gitlab_version&#125;-zh.diff# 停止 gitlabsudo gitlab-ctl stop# 如果patch命令不存在 yum install -y patchsudo patch -d /opt/gitlab/embedded/service/gitlab-rails -p1 &lt; $&#123;gitlab_version&#125;-zh.diff# 停止gitlabsudo gitlab-ctl stopsudo gitlab-ctl startsudo gitlab-ctl reconfigure\n\n\n","tags":["部署文档","GitLab"]},{"title":"IDEA实用技能","url":"/2020/07/17/IDEA%E5%AE%9E%E7%94%A8%E6%8A%80%E8%83%BD/","content":"查询依赖关系\n近期处理一个模块比较多的项目，存在多个模块引用不同版本的依赖，抱着精简的心态，我们来处理一下\n\n查看依赖有很多种方式：\n\n查看项目的全部依赖\n\n\n\n查看指定模块的依赖\n\n\n但是在第一种的情况下，我们想知道是哪个模块引用的该怎么办？\n\n在指定模块右键或F12打开设置\n\n\n\n通过查看依赖设置我们可以知道这个依赖的具体保存位置\n\n\n\n在我们查询的依赖上右键，Find Usages或者直接Ctrl+G，我们就看到了这个依赖是哪个项目引用的\n\n\n\n\n接下来我们看一下引用关系，在maven找到这个模块，Show Dependencies\n\n\n\n我们就看到了\n\n\n\n在这个地方我们可以方便的解决依赖冲突，找了一个存在依赖的模块\n\n\n\n我们看到Jackson-annotations这个依赖冲突，只需要在指定的依赖后右键Exclude\n\n\n\n我们排除的是jackson-module-jaxb-annotations里面的依赖，所以选定jackson-module-jaxb-annotations这个依赖，F12，我们看到IDEA已经在pom文件里替我们做了修改\n\n\n","tags":["Java","IDEA"]},{"title":"Java二进制操作方法","url":"/2020/03/16/Java%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%93%8D%E4%BD%9C%E6%96%B9%E6%B3%95/","content":"定义二进制\n二进制用0b开头\n\nint i &#x3D; 0b11;\n\n二进制转换//二进制字符串String b = Integer.toBinaryString(3);//同理String b = Integer.toString(3,2);//二进制字符串转intint i = Integer.parseInt(b, 2);\n\n移位intint i1 = 0b11, i2 = 0b10;System.out.println(Integer.toBinaryString(i1 &lt;&lt; 1));System.out.println(Integer.toBinaryString(i1 &gt;&gt; 1));System.out.println(Integer.toBinaryString(i1 &amp; i2));System.out.println(Integer.toBinaryString(~i1));System.out.println(Integer.toBinaryString(i1 | i2));System.out.println(Integer.toBinaryString(i1 ^ i2));\nBigInteger//11    00BigInteger bi1 = new BigInteger(\"3\"),bi2 = new BigInteger(\"0\");//左移System.out.println(bi1.shiftLeft(1).toString(2));//右移System.out.println(bi1.shiftRight(1).toString(2));//与System.out.println(bi1.and(bi2).toString(2));//非System.out.println(bi1.not().toString(2));//或System.out.println(bi1.or(bi2).toString(2));//异或System.out.println(bi1.xor(bi2).toString(2));\n\n除基倒取余法public static void binaryToDecimal(int n) &#123;    int t = 0; // 用来记录位数    int bin = 0; // 用来记录最后的二进制数    int r = 0; // 用来存储余数    while (n != 0) &#123;        r = n % 2;        n = n / 2;        bin += r * Math.pow(10, t);        t++;    &#125;    System.out.println(bin + \"\\n\");&#125;\n","tags":["Java","算法相关"]},{"title":"Java中如何格式化内容","url":"/2020/03/24/Java%E4%B8%AD%E5%A6%82%E4%BD%95%E6%A0%BC%E5%BC%8F%E5%8C%96%E5%86%85%E5%AE%B9/","content":"\nJava中经常会用到格式化内容输出，简单写个文档，看看自己还有哪些遗漏\nhttps://docs.oracle.com/javase/8/docs/api/\n\nString.format常规类型、字符类型和数值类型的格式说明符的语法如下：  %[argument_index$][flags][width][.precision]conversion\n\n\n可选的 argument_index 是一个十进制整数，用于表明参数在参数列表中的位置。第一个参数由 “1$“  引用，第二个参数由 “2$“ 引用，依此类推。\n\n可选 flags 是修改输出格式的字符集。有效标志集取决于转换类型。\n\n\n\nFlag\nGeneral\nCharacter\nIntegral\nFloating Point\nDate/Time\nDescription\n\n\n\n‘-‘\ny\ny\ny\ny\ny\nThe result will be left-justified.\n\n\n‘#’\ny1\n-\ny3\ny\n-\nThe result should use a conversion-dependent alternate form\n\n\n‘+’\n-\n-\ny4\ny\n-\nThe result will always include a sign\n\n\n‘  ‘\n-\n-\ny4\ny\n-\nThe result will include a leading space for positive values\n\n\n‘0’\n-\n-\ny\ny\n-\nThe result will be zero-padded\n\n\n‘,’\n-\n-\ny2\ny5\n-\nThe result will include locale-specific grouping separators\n\n\n‘(‘\n-\n-\ny4\ny5\n-\nThe result will enclose negative numbers in parentheses\n\n\n\n可选 width 是一个非负十进制整数，表明要向输出中写入的最少字符数。\n\n可选 precision 是一个非负十进制整数，通常用来限制字符数。特定行为取决于转换类型。\n\n必须 conversion 是一个表明应该如何格式化参数的字符。给定参数的有效转换集取决于参数的数据类型。\n\n\n\n\n\n%\n[argument_index$]（下标）\nFlags（标志）\nWidth（宽度）\nPrecision（精度）\nConversions（转换格式）\n\n\n\n\n%\n1$\n0\n8\n\nx\n%1$08x\n\n\n","tags":["Java"]},{"title":"HttpComponents组件介绍","url":"/2020/01/16/HttpComponents%E7%BB%84%E4%BB%B6%E4%BB%8B%E7%BB%8D/","content":"HttpComponents概述超文本传输协议（HTTP）可能是当今Internet上使用的最重要的协议。Web服务，具有网络功能的设备以及网络计算的增长继续将HTTP协议的作用扩展到用户驱动的Web浏览器之外，同时增加了需要HTTP支持的应用程序的数量。\nHttpComponents是为扩展而设计的，同时提供了对基本HTTP协议的强大支持，对于构建HTTP感知的客户端和服务器应用程序（例如Web浏览器，Web Spider，HTTP代理，Web服务传输库或利用或扩展HTTP协议以进行分布式通信。\nHttpCore\n是一组低级HTTP传输组件，可用于以最小的占用空间构建自定义客户端和服务器端HTTP服务。HttpCore支持两种I/O模型：基于经典Java I/O的阻塞I/O模型和基于Java NIO的非阻塞事件驱动的I/O模型。\n阻塞I/O模型可能更适合于数据密集型低延迟方案，而非阻塞模型可能更适合于高延迟方案，在原始数据吞吐量中，原始数据吞吐量的重要性不如处理数千个同时HTTP连接的能力。资源高效的方式。\n\n\nHttpCore教程HTML / PDF\nHttpCore 示例\n\nHttpClient\n是基于HttpCore的HTTP/1.1兼容HTTP代理实现。它还为客户端身份验证，HTTP状态管理和HTTP连接管理提供了可重用的组件。HttpComponents Client是Commons HttpClient 3.x的继承者和替代者。强烈建议Commons HttpClient用户进行升级。\n\n\nHttpClient教程HTML / PDF\nHttpClient 示例\n\nAsynch HttpClient\n是基于HttpCore NIO和HttpClient组件的HTTP/1.1兼容HTTP代理实现。它是Apache HttpClient的补充模块，适用于特殊情况，在特殊情况下，就原始数据吞吐量而言，处理大量并发连接的能力比性能更为重要。\n\n\nHttpAsyncClient 示例\n\n","tags":["Java"]},{"title":"Java函数式接口","url":"/2020/06/04/Java%E5%87%BD%E6%95%B0%E5%BC%8F%E6%8E%A5%E5%8F%A3/","content":"一、简介1.1 @FunctionalInterface在JDK 8中引入了FunctionalInterface接口，其源代码定义如下：\n@Documented@Retention(RetentionPolicy.RUNTIME)@Target(ElementType.TYPE)public @interface FunctionalInterface &#123;&#125;\n\n1.2 特征\n有且仅有一个抽象方法\n允许定义静态方法\n允许定义默认方法\n允许java.lang.Object中的public方法\n该注解不是必须的，如果一个接口符合”函数式接口”定义，那么加不加该注解都没有影响。加上该注解能够更好地让编译器进行检查。如果编写的不是函数式接口，但是加上了@FunctionalInterface，那么编译器会报错\n\n@FunctionalInterfacepublic interface TestInterface &#123;    // 抽象方法    public void sub();    // java.lang.Object中的方法不是抽象方法    public boolean equals(Object var1);    // default不是抽象方法    public default void defaultMethod()&#123;    &#125;    // static不是抽象方法    public static void staticMethod()&#123;    &#125;&#125;\n\n二、示例2.1 定义接口/**多参数无返回*/@FunctionalInterfacepublic interface NoReturnMultiParam &#123;    void method(int a, int b);&#125;/**无参无返回值*/@FunctionalInterfacepublic interface NoReturnNoParam &#123;    void method();&#125;/**一个参数无返回*/@FunctionalInterfacepublic interface NoReturnOneParam &#123;    void method(int a);&#125;/**多个参数有返回值*/@FunctionalInterfacepublic interface ReturnMultiParam &#123;    int method(int a, int b);&#125;/*** 无参有返回*/@FunctionalInterfacepublic interface ReturnNoParam &#123;    int method();&#125;/**一个参数有返回值*/@FunctionalInterfacepublic interface ReturnOneParam &#123;    int method(int a);&#125;\n\n2.2 使用示例语法形式为 () -&gt; {}，其中 () 用来描述参数列表，{} 用来描述方法体，-&gt; 为 lambda运算符 ，读作(goes to)。\n//无参无返回NoReturnNoParam noReturnNoParam = () -&gt; &#123;    System.out.println(\"NoReturnNoParam\");&#125;;noReturnNoParam.method();//一个参数无返回NoReturnOneParam noReturnOneParam = (int a) -&gt; &#123;    System.out.println(\"NoReturnOneParam param:\" + a);&#125;;noReturnOneParam.method(6);//多个参数无返回NoReturnMultiParam noReturnMultiParam = (int a, int b) -&gt; &#123;    System.out.println(\"NoReturnMultiParam param:\" + \"&#123;\" + a +\",\" + + b +\"&#125;\");&#125;;noReturnMultiParam.method(6, 8);//无参有返回值ReturnNoParam returnNoParam = () -&gt; &#123;    System.out.print(\"ReturnNoParam\");    return 1;&#125;;int res = returnNoParam.method();System.out.println(\"return:\" + res);//一个参数有返回值ReturnOneParam returnOneParam = (int a) -&gt; &#123;    System.out.println(\"ReturnOneParam param:\" + a);    return 1;&#125;;int res2 = returnOneParam.method(6);System.out.println(\"return:\" + res2);//多个参数有返回值ReturnMultiParam returnMultiParam = (int a, int b) -&gt; &#123;    System.out.println(\"ReturnMultiParam param:\" + \"&#123;\" + a + \",\" + b +\"&#125;\");    return 1;&#125;;int res3 = returnMultiParam.method(6, 8);System.out.println(\"return:\" + res3);\n2.3 简化示例//1.简化参数类型，可以不写参数类型，但是必须所有参数都不写NoReturnMultiParam lamdba1 = (a, b) -&gt; &#123;    System.out.println(\"简化参数类型\");&#125;;lamdba1.method(1, 2);//2.简化参数小括号，如果只有一个参数则可以省略参数小括号NoReturnOneParam lambda2 = a -&gt; &#123;    System.out.println(\"简化参数小括号\");&#125;;lambda2.method(1);//3.简化方法体大括号，如果方法条只有一条语句，则可以胜率方法体大括号NoReturnNoParam lambda3 = () -&gt; System.out.println(\"简化方法体大括号\");lambda3.method();//4.如果方法体只有一条语句，并且是 return 语句，则可以省略方法体大括号ReturnOneParam lambda4 = a -&gt; a+3;System.out.println(lambda4.method(5));ReturnMultiParam lambda5 = (a, b) -&gt; a+b;System.out.println(lambda5.method(1, 1));\n\n2.4 表达式引用方法有时候我们不是必须要自己重写某个匿名内部类的方法，我们可以可以利用 lambda表达式的接口快速指向一个已经被实现的方法。\n语法\n方法归属者::方法名 静态方法的归属者为类名，普通方法归属者为对象\npublic class Exe1 &#123;    public static void main(String[] args) &#123;        ReturnOneParam lambda1 = a -&gt; doubleNum(a);        System.out.println(lambda1.method(3));        //lambda2 引用了已经实现的 doubleNum 方法        ReturnOneParam lambda2 = Exe1::doubleNum;        System.out.println(lambda2.method(3));        Exe1 exe = new Exe1();        //lambda4 引用了已经实现的 addTwo 方法        ReturnOneParam lambda4 = exe::addTwo;        System.out.println(lambda4.method(2));    &#125;    /**     * 要求     * 1.参数数量和类型要与接口中定义的一致     * 2.返回值类型要与接口中定义的一致     */    public static int doubleNum(int a) &#123;        return a * 2;    &#125;    public int addTwo(int a) &#123;        return a + 2;    &#125;&#125;\n\n2.5 构造方法的引用一般我们需要声明接口，该接口作为对象的生成器，通过 类名::new 的方式来实例化对象，然后调用方法返回对象。\ninterface ItemCreatorBlankConstruct &#123;    Item getItem();&#125;interface ItemCreatorParamContruct &#123;    Item getItem(int id, String name, double price);&#125;public class Exe2 &#123;    public static void main(String[] args) &#123;        ItemCreatorBlankConstruct creator = () -&gt; new Item();        Item item = creator.getItem();        ItemCreatorBlankConstruct creator2 = Item::new;        Item item2 = creator2.getItem();        ItemCreatorParamContruct creator3 = Item::new;        Item item3 = creator3.getItem(112, \"鼠标\", 135.99);    &#125;\n\n三、常用函数式接口jdk默认的接口定义在java.util.function包中，常用的有：\n\nPredicate\n\nboolean test(T t);\n\n\nSupplier\n\nT get();\n\n\nConsumer\n\nvoid accept(T t);\n\n\nFunction\n\nR apply(T t);\n\n四、参考https://www.cnblogs.com/haixiang/p/11029639.html\n","tags":["Java"]},{"title":"Java中的引用","url":"/2020/01/09/Java%E4%B8%AD%E7%9A%84%E5%BC%95%E7%94%A8/","content":"Reference\n在Java中提供了四个级别的引用：强引用，软引用，弱引用和虚引用。在这四个引用类型中，只有强引用FinalReference类是包内可见，其他三种引用类型均为public，可以在应用程序中直接使用。引用类型的类结构如图所示。\n\n强引用 FinalReference\nJava中的引用，类似C语言中最难的指针。（我是C语言入门编程，指针的概念还是很深入我心。）通过引用，可以对堆中的对象进行操作。如：\n\nStringBuffer stringBuffer = new StringBuffer(\"Helloword\");\n变量str指向StringBuffer实例所在的堆空间，通过str可以操作该对象。\n\n强引用的特点：\n强引用可以直接访问目标对象。\n强引用所指向的对象在任何时候都不会被系统回收。JVM宁愿抛出OOM异常，也不会回收强引用所指向的对象。\n强引用可能导致内存泄漏。\n\n\n\n软引用 SoftReference\n软引用是除了强引用外，最强的引用类型。可以通过java.lang.ref.SoftReference使用软引用。一个持有软引用的对象，不会被JVM很快回收，JVM会根据当前堆的使用情况来判断何时回收。当堆使用率临近阈值时，才会去回收软引用的对象。因此，软引用可以用于实现对内存敏感的高速缓存。\nSoftReference的特点是它的一个实例保存对一个Java对象的软引用， 该软引用的存在不妨碍垃圾收集线程对该Java对象的回收。也就是说，一旦SoftReference保存了对一个Java对象的软引用后，在垃圾线程对 这个Java对象回收前，SoftReference类所提供的get()方法返回Java对象的强引用。一旦垃圾线程回收该Java对象之后，get()方法将返回null。\n\n下面举一个例子说明软引用的使用方法。\n在你的IDE设置参数 -Xmx2m -Xms2m规定堆内存大小为2m。\n@Testpublic void test3()&#123;    MyObject obj = new myObject();    SoftReference sf = new SoftReference&lt;&gt;(obj);    obj = null;    System.gc();    //        byte[] bytes = new byte[1024*100];    //        System.gc();    System.out.println(\"是否被回收\"+sf.get());&#125;\n运行结果：\n是否被回收cn.zyzpp.MyObject@42110406\n打开被注释掉的new byte[1024*100]语句，这条语句请求一块大的堆空间，使堆内存使用紧张。并显式的再调用一次GC，结果如下：\n是否被回收null\n说明在系统内存紧张的情况下，软引用被回收。\n弱引用 PhantomReference\n弱引用是一种比软引用较弱的引用类型。在系统GC时，只要发现弱引用，不管系统堆空间是否足够，都会将对象进行回收。在java中，可以用java.lang.ref.WeakReference实例来保存对一个Java对象的弱引用。\n\npublic void test3()&#123;    MyObject obj = new MyObject();    WeakReference sf = new WeakReference(obj);    obj = null;    System.out.println(\"是否被回收\"+sf.get());    System.gc();    System.out.println(\"是否被回收\"+sf.get());&#125;\n运行结果：\n是否被回收cn.zyzpp.MyObject@42110406是否被回收null\n软引用，弱引用都非常适合来保存那些可有可无的缓存数据，如果这么做，当系统内存不足时，这些缓存数据会被回收，不会导致内存溢出。而当内存资源充足时，这些缓存数据又可以存在相当长的时间，从而起到加速系统的作用。\n弱引用WeakHashMap\nWeakHashMap类在java.util包内，它实现了Map接口，是HashMap的一种实现，它使用弱引用作为内部数据的存储方案。WeakHashMap是弱引用的一种典型应用，它可以作为简单的缓存表解决方案。\n\n以下两段代码分别使用WeakHashMap和HashMap保存大量的数据：\n@Testpublic void test4()&#123;    Map map;    map = new WeakHashMap&lt;String,Object&gt;();    for (int i =0;i&lt;10000;i++)&#123;        map.put(\"key\"+i,new byte[i]);    &#125;    //        map = new HashMap&lt;String,Object&gt;();    //        for (int i =0;i&lt;10000;i++)&#123;    //            map.put(\"key\"+i,new byte[i]);    //        &#125;&#125;\n使用-Xmx2M限定堆内存，使用WeakHashMap的代码正常运行结束，而使用HashMap的代码段抛出异常\njava.lang.OutOfMemoryError: Java heap space\n由此可见，WeakHashMap会在系统内存紧张时使用弱引用，自动释放掉持有弱引用的内存数据。\n但如果WeakHashMap的key都在系统内持有强引用，那么WeakHashMap就退化为普通的HashMap，因为所有的表项都无法被自动清理。\n虚引用 WeakReference\n虚引用是所有类型中最弱的一个。一个持有虚引用的对象，和没有引用几乎是一样的，随时可能被垃圾回收器回收。当试图通过虚引用的get()方法取得强引用时，总是会失败。并且，虚引用必须和引用队列一起使用，它的作用在于跟踪垃圾回收过程。\n当垃圾回收器准备回收一个对象时，如果发现它还有虚引用，就会在垃圾回收后，销毁这个对象，将这个虚引用加入引用队列。程序可以通过判断引用队列中是否已经加入了虚引用，来了解被引用的对象是否将要被垃圾回收。如果程序发现某个虚引用已经被加入到引用队列，那么就可以在所引用的对象的内存被回收之前采取必要的行动。\n\npublic void test3()&#123;    MyObject obj = new MyObject();    ReferenceQueue&lt;Object&gt; referenceQueue = new ReferenceQueue&lt;&gt;();    PhantomReference sf = new PhantomReference&lt;&gt;(obj,referenceQueue);    obj = null;    System.out.println(\"是否被回收\"+sf.get());    System.gc();    System.out.println(\"是否被回收\"+sf.get());&#125;\n运行结果：\n是否被回收null是否被回收null\n对虚引用的get()操作，总是返回null，因为sf.get()方法的实现如下：\npublic T get() &#123;    return null;&#125;\n","tags":["Java"]},{"title":"Java中常用却存在性能的问题","url":"/2020/01/10/Java%E4%B8%AD%E5%B8%B8%E7%94%A8%E5%8D%B4%E5%AD%98%E5%9C%A8%E6%80%A7%E8%83%BD%E7%9A%84%E9%97%AE%E9%A2%98/","content":"System.currentTimeMillis()测试代码public static void main(String[] args) &#123;    long start = System.nanoTime();    singleThread(100);    System.out.println(\"single: \" + (System.nanoTime() - start));    start = System.nanoTime();    multiThread(100);    System.out.println(\"multi: \" + (System.nanoTime() - start));&#125;public static void singleThread(int size) &#123;    for (int i = 0; i &lt; size; i++) &#123;        System.currentTimeMillis();    &#125;&#125;public static void multiThread(int size) &#123;    CountDownLatch latch = new CountDownLatch(size);    for (int i = 0; i &lt; size; i++) &#123;        new Thread(() -&gt; &#123;            try &#123;                System.currentTimeMillis();            &#125; finally &#123;                latch.countDown();            &#125;        &#125;).start();    &#125;    try &#123;        latch.await();    &#125; catch (InterruptedException e) &#123;        e.printStackTrace();    &#125;&#125;\n分析System.currentTimeMillis()调用会与系统交互，频繁访问或者高并发会造成严重的争用\n结果single: 17727multi: 58580004\n解决思路\n创建一个类，在类中维护一个线程，定时去同步\npublic class CurrentTimeMillisClock &#123;    private volatile long now;    private CurrentTimeMillisClock() &#123;        this.now = System.currentTimeMillis();        scheduleTick();    &#125;    private void scheduleTick() &#123;        new ScheduledThreadPoolExecutor(1, runnable -&gt; &#123;            Thread thread = new Thread(runnable, \"current-time-millis\");            thread.setDaemon(true);            return thread;        &#125;).scheduleAtFixedRate(() -&gt; &#123;            now = System.currentTimeMillis();        &#125;, 1, 1, TimeUnit.MILLISECONDS);    &#125;    public long now() &#123;        return now;    &#125;    public static CurrentTimeMillisClock getInstance() &#123;        return SingletonHolder.INSTANCE;    &#125;    private static class SingletonHolder &#123;        private static final CurrentTimeMillisClock INSTANCE = new                CurrentTimeMillisClock();    &#125;&#125;\n\n\n","tags":["Java"]},{"title":"Java图像相关操作","url":"/2019/12/19/Java%E5%9B%BE%E5%83%8F%E7%9B%B8%E5%85%B3%E6%93%8D%E4%BD%9C/","content":"\nhttps://www.cnblogs.com/donghb/p/7637990.html\n\nBufferedImageBufferedImage image = new BufferedImage(IMG_WIDTH, IMG_HEIGHT, BufferedImage.TYPE_INT_RGB);image = image.getSubimage(50, 50, 20, 20);\n\nImageIOImageIO.write(image, \"jpg\", new File(\"C:\\\\Users\\\\lenovo\\\\Desktop\\\\image.jpg\"));\n\nGraphics，Graphics2D\nGraphics类提供基本绘图方法，Graphics2D类提供更强大的绘图能力。\nGraphics类提供基本的几何图形绘制方法，主要有：画线段、画矩形、画圆、画带颜色的图形、画椭圆、画圆弧、画多边形等。\n\nGraphicspublic class TransparentImage &#123;    private static int IMG_WIDTH = 100;    private static int IMG_HEIGHT = 100;    public static void main(String[] args) throws IOException &#123;        BufferedImage image = new BufferedImage(IMG_WIDTH, IMG_HEIGHT, BufferedImage.TYPE_INT_RGB);        Graphics g = image.getGraphics();        g.drawLine(0, 0, 100, 100);        ImageIO.write(image, \"jpg\", new File(\"C:\\\\Users\\\\lenovo\\\\Desktop\\\\image.jpg\"));                        // 向页面输出图像    &#125;&#125;\n\n\n画一条线\n\ng.drawLine(0, 0, 100, 100);\n\n\n画矩形\n\ng.drawRect(0,0,50,50);g.setColor(Color.lightGray);g.fillRect(50,50,20,20);\n\n\n画圆\n\ng.drawRoundRect(5, 5, 30, 30, 30, 30);g.setColor(Color.lightGray);g.fillRoundRect(30, 30, 30, 30, 30, 30);\n\nGraphics2Dpublic class RenderingHintCase &#123;    public static void main(String[] args) throws IOException &#123;        BufferedImage image = new BufferedImage(260, 80, BufferedImage.TYPE_INT_BGR);        //获取Graphics2D对象        Graphics2D graphics = image.createGraphics();        //开启文字抗锯齿        graphics.setRenderingHint(RenderingHints.KEY_TEXT_ANTIALIASING, RenderingHints.VALUE_TEXT_ANTIALIAS_ON);        //设置字体        Font font = new Font(\"Algerian\", Font.ITALIC, 40);        graphics.setFont(font);        //向画板上写字        graphics.drawString(\"This is test!\", 10, 60);        graphics.dispose();        ImageIO.write(image, \"jpg\", new File(\"C:\\\\Users\\\\lenovo\\\\Desktop\\\\demo.jpg\"));    &#125;&#125;\n","tags":["Java"]},{"title":"Java文件复制","url":"/2020/03/24/Java%E6%96%87%E4%BB%B6%E5%A4%8D%E5%88%B6/","content":"一、使用FileStreams复制private static void copyFileUsingFileStreams(File source, File dest)    throws IOException &#123;    InputStream input = null;    OutputStream output = null;    try &#123;        input = new FileInputStream(source);        output = new FileOutputStream(dest);        byte[] buf = new byte[1024];        int bytesRead;        while ((bytesRead = input.read(buf)) ！= -1) &#123;            output.write(buf, 0, bytesRead);        &#125;    &#125; finally &#123;        input.close();        output.close();    &#125;&#125;\n\n二、使用FileChannel复制private static void copyFileUsingFileChannels(File source, File dest) throws IOException &#123;    FileChannel inputChannel = null;    FileChannel outputChannel = null;    try &#123;        inputChannel = new FileInputStream(source).getChannel();        outputChannel = new FileOutputStream(dest).getChannel();        outputChannel.transferFrom(inputChannel, 0, inputChannel.size());    &#125; finally &#123;        inputChannel.close();        outputChannel.close();    &#125;&#125;\n\n三、使用Commons IO复制private static void copyFileUsingApacheCommonsIO(File source, File dest)    throws IOException &#123;    FileUtils.copyFile(source, dest);&#125;\n\n源码：\nprivate static void doCopyFile(final File srcFile, final File destFile, final boolean preserveFileDate)    throws IOException &#123;    if (destFile.exists() &amp;&amp; destFile.isDirectory()) &#123;        throw new IOException(\"Destination '\" + destFile + \"' exists but is a directory\");    &#125;    try (FileInputStream fis = new FileInputStream(srcFile);         FileChannel input = fis.getChannel();         FileOutputStream fos = new FileOutputStream(destFile);         FileChannel output = fos.getChannel()) &#123;        final long size = input.size(); // TODO See IO-386        long pos = 0;        long count = 0;        while (pos &lt; size) &#123;            final long remain = size - pos;            count = remain &gt; FILE_COPY_BUFFER_SIZE ? FILE_COPY_BUFFER_SIZE : remain;            final long bytesCopied = output.transferFrom(input, pos, count);            if (bytesCopied == 0) &#123; // IO-385 - can happen if file is truncated after caching the size                break; // ensure we don't loop forever            &#125;            pos += bytesCopied;        &#125;    &#125;    final long srcLen = srcFile.length(); // TODO See IO-386    final long dstLen = destFile.length(); // TODO See IO-386    if (srcLen != dstLen) &#123;        throw new IOException(\"Failed to copy full contents from '\" +                              srcFile + \"' to '\" + destFile + \"' Expected length: \" + srcLen + \" Actual: \" + dstLen);    &#125;    if (preserveFileDate) &#123;        destFile.setLastModified(srcFile.lastModified());    &#125;&#125;\n\n四、使用Java7的Files类复制private static void copyFileUsingJava7Files(File source, File dest)    throws IOException &#123;    Files.copy(source.toPath(), dest.toPath());&#125;\n\n五、测试package snippet;import org.apache.commons.io.FileUtils;import java.io.*;import java.nio.channels.FileChannel;import java.nio.file.Files;public class CopyFilesExample &#123;    public static void main(String[] args) throws InterruptedException,            IOException &#123;        File source = new File(\"F:\\\\INSTALL_HISTORY\\\\ideaIU-2019.1.1.exe\");        File dest = new File(\"F:\\\\TEST\\\\ideaIU-2019.1.1_1.exe\");        // copy file using FileStreams        long start = System.nanoTime();        long end;        copyFileUsingFileStreams(source, dest);        System.out.printf(\"%-50s = %20d\\n\", \"Time taken by FileStreams Copy\", (System.nanoTime() - start));        // copy files using java.nio.FileChannelsource = new File(\"C:\\\\Users\\\\nikos7\\\\Desktop\\\\files\\\\sourcefile2.txt\");        dest = new File(\"F:\\\\TEST\\\\ideaIU-2019.1.1_2.exe\");        start = System.nanoTime();        copyFileUsingFileChannels(source, dest);        end = System.nanoTime();        System.out.printf(\"%-50s = %20d\\n\", \"Time taken by FileChannels Copy\", (System.nanoTime() - start));        // copy file using Java 7 Files classsource = new File(\"C:\\\\Users\\\\nikos7\\\\Desktop\\\\files\\\\sourcefile3.txt\");        dest = new File(\"F:\\\\TEST\\\\ideaIU-2019.1.1_3.exe\");        start = System.nanoTime();        copyFileUsingJava7Files(source, dest);        end = System.nanoTime();        System.out.printf(\"%-50s = %20d\\n\", \"Time taken by Java7 Files Copy\", (System.nanoTime() - start));        // copy files using apache commons iosource = new File(\"C:\\\\Users\\\\nikos7\\\\Desktop\\\\files\\\\sourcefile4.txt\");        dest = new File(\"F:\\\\TEST\\\\ideaIU-2019.1.1_4.exe\");        start = System.nanoTime();        copyFileUsingApacheCommonsIO(source, dest);        end = System.nanoTime();        System.out.printf(\"%-50s = %20d\\n\", \"Time taken by Apache Commons IO Copy\", (System.nanoTime() - start));    &#125;    private static void copyFileUsingFileStreams(File source, File dest)            throws IOException &#123;        InputStream input = null;        OutputStream output = null;        try &#123;            input = new FileInputStream(source);            output = new FileOutputStream(dest);            byte[] buf = new byte[1024];            int bytesRead;            while ((bytesRead = input.read(buf)) &gt; 0) &#123;                output.write(buf, 0, bytesRead);            &#125;        &#125; finally &#123;            input.close();            output.close();        &#125;    &#125;    private static void copyFileUsingFileChannels(File source, File dest)            throws IOException &#123;        FileChannel inputChannel = null;        FileChannel outputChannel = null;        try &#123;            inputChannel = new FileInputStream(source).getChannel();            outputChannel = new FileOutputStream(dest).getChannel();            outputChannel.transferFrom(inputChannel, 0, inputChannel.size());        &#125; finally &#123;            inputChannel.close();            outputChannel.close();        &#125;    &#125;    private static void copyFileUsingJava7Files(File source, File dest)            throws IOException &#123;        Files.copy(source.toPath(), dest.toPath());    &#125;    private static void copyFileUsingApacheCommonsIO(File source, File dest)            throws IOException &#123;        FileUtils.copyFile(source, dest);    &#125;&#125;\n\n结果：\nTime taken by FileStreams Copy                     &#x3D;           3307186847Time taken by FileChannels Copy                    &#x3D;            355407223Time taken by Java7 Files Copy                     &#x3D;            434544647Time taken by Apache Commons IO Copy               &#x3D;           2018366048\n","tags":["Java"]},{"title":"Java断点续传","url":"/2020/01/20/Java%E6%96%AD%E7%82%B9%E7%BB%AD%E4%BC%A0/","content":"原理\n所谓断点续传，就是指从文件已经下载好的地方开始继续下载。所以下载端传给Web服务器的时候要多加一条信息，那就是从哪个字节开始下载。了解到这些，我们就可以开发了。\n\nRange\nThe Range 是一个请求首部，告知服务器返回文件的哪一部分。在一个  Range 首部中，可以一次性请求多个部分，服务器会以 multipart 文件的形式将其返回。如果服务器返回的是范围响应，需要使用 206 Partial Content 状态码。假如所请求的范围不合法，那么服务器会返回  416 Range Not Satisfiable 状态码，表示客户端错误。服务器允许忽略  Range  首部，从而返回整个文件，状态码用 200 。\n\nContent-Range\n在HTTP协议中，响应首部 Content-Range 显示的是一个数据片段在整个文件中的位置。\n\n示例代码","tags":["Java"]},{"title":"Java泛型的协变与逆变","url":"/2019/11/14/Java%E6%B3%9B%E5%9E%8B%E7%9A%84%E5%8D%8F%E5%8F%98%E4%B8%8E%E9%80%86%E5%8F%98/","content":"泛型擦除Java的泛型本质上不是真正的泛型，而是利用了类型擦除（type erasure），比如下面的代码就会出现错误：\n// 这里会编译错误public void hello(List&lt;String&gt; list)&#123;&#125;public void hello(List&lt;Integer&gt; list)&#123;&#125;\n\n报的错误是：both methods  have same erasure\n原因是java在编译的时候会把泛型，上面的&lt;String&gt;和&lt;Integer&gt;都给擦除掉（其实并没有真正的被擦除，javap -l -p -v -c可以看到LocalVariableTypeTable里面有方法参数类型的签名）。\n协变与逆变理解了类型擦除有助于我们理解泛型的协变与逆变，现有几个类如下：\nPlant　　Fruit　　Apple　　Banana　　Orange\n其中Apple、Banana、Orange是Fruit的子类，Fruit是Plant的子类。我们来看下下面的代码：\npublic static void main(String[] args) &#123;    List&lt;Fruit&gt; list = new ArrayList&lt;Apple&gt;();\t\t//编译错误&#125;\n\n泛型没有内建的协变类型，无法将List&lt;Fruit&gt;和ArrayList&lt;Apple&gt;关联起来，所以在编译阶段就会出现错误。\n协变于是我们可以利用通配符实现泛型的协变：&lt;? extends T&gt;子类通配符；这个通配符定义了?继承自T，可以帮助我们实现向上转换：\npublic static void main(String[] args) &#123;    List&lt;? extends Fruit&gt; list = new ArrayList&lt;Apple&gt;();&#125;\n\n这里我们要理解当转换之后list中的数据类型是什么。虽然将Apple类型赋值给了list，但是list的类型是? extends Fruit，把? extends Fruit看成一个整体，我们能确定list的具体类型肯定是Fruit或者Fruit的父类(因为一个类只能有一个直接父类，所以确定了Fruit，那么Fruit的父类则都是可以确定的)，而不能确定list的类型是Fruit的子类当中具体的哪一个？（有多个类都继承自Fruit），所以这也就直接导致了一旦使用了&lt;? extends T&gt;向上转换之后，不能再向list中添加任何类型的对象了，这个时候只能选择从list当中get数据而不能add。\npublic static void main(String[] args) &#123;    List&lt;? extends Fruit&gt; list = new ArrayList&lt;Apple&gt;();    list.add(new Apple());\t//编译错误    list.add(new Banana());\t//编译错误    list.add(new Orange());\t//编译错误    list.add(new Fruit());\t//编译错误&#125;\n\n另外还需要注意的是，这个时候从list当中get出来的数据不再是Apple，而是Fruit或者Fruit的父类：\npublic static void main(String[] args) &#123;    List&lt;? extends Fruit&gt; list = new ArrayList&lt;Apple&gt;();    Fruit fruit = list.get(0);    Plant plant = list.get(0);    Apple apple = list.get(0);\t//编译错误&#125;\n\n逆变逆变则和协变相反，它是向下转换：\npublic static void main(String[] args) &#123;    List&lt;? super Fruit&gt; list = new ArrayList&lt;Fruit&gt;();    list.add(new Apple());    list.add(new Fruit());    list.add(new Plant());\t//编译错误&#125;\n\n逆变使用通配符? super T（超类通配符），如上面代码，Fruit是Apple的超类，则这个时候对于JVM来说，它能确定list的类型的超类肯定是Apple或者Apple的父类，换言之该类型就是Apple或者Apple的子类，所以和上面的协变一样，既然确定了类型的范围，那么list能够add的类型也就是Apple或者Apple的子类了。\nPECS是指Producer Extends, Consumer Super\n总结 ? extends T和 ? super T 通配符的特征，我们可以得出以下结论：\n\n如果你想从一个数据类型里获取数据，使用 ? extends T通配符\n如果你想把对象写入一个数据结构里，使用? super T通配符\n如果你既想存，又想取，那就别用通配符。\n\n","tags":["Java"]},{"title":"JavaSPI介绍","url":"/2019/10/23/JavaSPI%E4%BB%8B%E7%BB%8D/","content":"Service Provider Interface\n在classpath下创建META-INF/services/\n在services文件夹下创建接口名字的文件，并在文件中添加实现的类列表ServiceLoader&lt;DriverService&gt; serviceLoader = ServiceLoader.load(DriverService.class);for (DriverService ds : serviceLoader) &#123;    System.out.println(ds.getClass().getName());    ds.onStartUp();&#125;\n\n\n\n源码解读PREFIX就是定义好的文件\npublic final class ServiceLoader&lt;S&gt;    implements Iterable&lt;S&gt;&#123;    private static final String PREFIX = \"META-INF/services/\";\n\nconfigs是Enumeration&lt;URL&gt;类型，可以迭代\nprivate boolean hasNextService() &#123;    if (nextName != null) &#123;        return true;    &#125;    if (configs == null) &#123;        try &#123;            String fullName = PREFIX + service.getName();            if (loader == null)                configs = ClassLoader.getSystemResources(fullName);            else                configs = loader.getResources(fullName);        &#125; catch (IOException x) &#123;            fail(service, \"Error locating configuration files\", x);        &#125;    &#125;    while ((pending == null) || !pending.hasNext()) &#123;        if (!configs.hasMoreElements()) &#123;            return false;        &#125;        pending = parse(service, configs.nextElement());    &#125;    nextName = pending.next();    return true;&#125;\n","tags":["Java"]},{"title":"Java源码解读之Arrays.parallelSort()","url":"/2020/05/28/Java%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB%E4%B9%8BArrays-parallelSort/","content":"\n在学习Fork/Join框架的时候，了解到Arrays.parallelSort()的底层实现用到了Fork/Join，所以看了一下这个方法的源码，记录一下自己的学习过程。\n\npublic static void parallelSort(int[] a) &#123;    int n = a.length, p, g;    if (n &lt;= MIN_ARRAY_SORT_GRAN ||        (p = ForkJoinPool.getCommonPoolParallelism()) == 1)        DualPivotQuicksort.sort(a, 0, n - 1, null, 0, 0);    else        new ArraysParallelSortHelpers.FJInt.Sorter        (null, a, new int[n], 0, n, 0,         ((g = n / (p &lt;&lt; 2)) &lt;= MIN_ARRAY_SORT_GRAN) ?         MIN_ARRAY_SORT_GRAN : g).invoke();&#125;\n\nMIN_ARRAY_SORT_GRAN这个是并行排序的阈值，如果大于这个值采用并行执行，并行用到了Fork/Join。\nForkJoinPool.getCommonPoolParallelism()这个方法的源码，这里看注释说明返回公共线程池的并行度，这里我追了一下源头，应该是根据电脑的CPU核数算出来的，暂时没深究。\n/**  * Returns the targeted parallelism level of the common pool.  *  * @return the targeted parallelism level of the common pool  * @since 1.8  */public static int getCommonPoolParallelism() &#123;    return commonParallelism;&#125;\n\n下面主要看一下DualPivotQuicksort.sort（双轴快速排序）这个方法的实现，\nstatic void sort(int[] a, int left, int right,                 int[] work, int workBase, int workLen) &#123;    // Use Quicksort on small arrays    if (right - left &lt; QUICKSORT_THRESHOLD) &#123;        sort(a, left, right, true);        return;    &#125;\t...&#125;\n\n这个地方判断如果排序的数量小于快速排序的阈值采用快速排序。然后看一下sort(a, left, right, true);这个方法，如果小于INSERTION_SORT_THRESHOLD用插入排序，这个方法不详细说了，了解一下Dual-Pivot Quicksort（新快速排序算法）\n/**  * Sorts the specified range of the array by Dual-Pivot Quicksort.  *  * @param a the array to be sorted  * @param left the index of the first element, inclusive, to be sorted  * @param right the index of the last element, inclusive, to be sorted  * @param leftmost indicates if this part is the leftmost in the range  */private static void sort(int[] a, int left, int right, boolean leftmost) &#123;    int length = right - left + 1;    if (length &lt; INSERTION_SORT_THRESHOLD)\n","tags":["Java"]},{"title":"Java泛型简介","url":"/2020/01/09/Java%E6%B3%9B%E5%9E%8B%E7%AE%80%E4%BB%8B/","content":"泛型(Generics)\n泛型类\n//此处T可以随便写为任意标识，常见的如T、E、K、V等形式的参数常用于表示泛型//在实例化泛型类时，必须指定T的具体类型public class Generic&lt;T&gt;&#123;    //key这个成员变量的类型为T,T的类型由外部指定    private T key;    public Generic(T key) &#123; //泛型构造方法形参key的类型也为T，T的类型由外部指定        this.key = key;    &#125;    public T getKey()&#123; //泛型方法getKey的返回值类型为T，T的类型由外部指定        return key;    &#125;&#125;\n\n泛型接口\n//定义一个泛型接口public interface Generator&lt;T&gt; &#123;    public T next();&#125;\n\n泛型方法\n/** * 泛型方法的基本介绍 * @param tClass 传入的泛型实参 * @return T 返回值为T类型 * 说明： *     1）public 与 返回值中间&lt;T&gt;非常重要，可以理解为声明此方法为泛型方法。 *     2）只有声明了&lt;T&gt;的方法才是泛型方法，泛型类中的使用了泛型的成员方法并不是泛型方法。 *     3）&lt;T&gt;表明该方法将使用泛型类型T，此时才可以在方法中使用泛型类型T。 *     4）与泛型类的定义一样，此处T可以随便写为任意标识，常见的如T、E、K、V等形式的参数常用于表示泛型。 */public &lt;T&gt; T genericMethod(Class&lt;T&gt; tClass)throws InstantiationException ,  IllegalAccessException&#123;        T instance = tClass.newInstance();        return instance;&#125;\n\n\n","tags":["Java"]},{"title":"Java线程池解析","url":"/2020/03/23/Java%E7%BA%BF%E7%A8%8B%E6%B1%A0%E8%A7%A3%E6%9E%90/","content":"线程池概念\n相关文章：https://juejin.im/post/5d1882b1f265da1ba84aa676\n\n线程池： 简单理解，它就是一个管理线程的池子。\n\n它帮我们管理线程，避免增加创建线程和销毁线程的资源损耗。因为线程其实也是一个对象，创建一个对象，需要经过类加载过程，销毁一个对象，需要走GC垃圾回收流程，都是需要资源开销的。\n提高响应速度。 如果任务到达了，相对于从线程池拿线程，重新去创建一条线程执行，速度肯定慢很多。\n重复利用。 线程用完，再放回池子，可以达到重复利用的效果，节省资源。\n\n线程池的创建线程池可以通过ThreadPoolExecutor来创建，我们来看一下它的构造函数：\npublic ThreadPoolExecutor(int corePoolSize,                          int maximumPoolSize,                          long keepAliveTime,TimeUnit unit,                          BlockingQueue&lt;Runnable&gt; workQueue,                          ThreadFactory threadFactory,                          RejectedExecutionHandler handler)\n\n几个核心参数的作用：\n\ncorePoolSize： 线程池核心线程数最大值\nmaximumPoolSize： 线程池最大线程数大小\nkeepAliveTime： 线程池中非核心线程空闲的存活时间大小\nunit： 线程空闲存活时间单位\nworkQueue： 存放任务的阻塞队列\nthreadFactory： 用于设置创建线程的工厂，可以给创建的线程设置有意义的名字，可方便排查问题。\nhandler： 线城池的饱和策略事件，主要有四种类型\n\n","tags":["Java"]},{"title":"Java线程池介绍","url":"/2020/04/13/Java%E7%BA%BF%E7%A8%8B%E6%B1%A0%E4%BB%8B%E7%BB%8D/","content":"参数介绍int corePoolSize\n该线程池中核心线程数最大值\n\n核心线程：线程池新建线程的时候，如果当前线程总数小于corePoolSize，则新建的是核心线程，如果超过corePoolSize，则新建的是非核心线程，核心线程默认情况下会一直存活在线程池中，即使这个核心线程啥也不干(闲置状态)。如果指定ThreadPoolExecutor的allowCoreThreadTimeOut这个属性为true，那么核心线程如果不干活(闲置状态)的话，超过一定时间(时长下面参数决定)，就会被销毁掉。\nint maximumPoolSize\n该线程池中线程总数最大值\n\n线程总数 = 核心线程数 + 非核心线程数。\nlong keepAliveTime\n该线程池中非核心线程闲置超时时长\n\n一个非核心线程，如果不干活(闲置状态)的时长超过这个参数所设定的时长，就会被销毁掉，如果设置allowCoreThreadTimeOut = true，则会作用于核心线程。\nTimeUnit unit\nkeepAliveTime的单位\n\nTimeUnit是一个枚举类型，其包括：\n\nNANOSECONDS : 1微毫秒 = 1微秒 / 1000\nMICROSECONDS : 1微秒 = 1毫秒 / 1000\nMILLISECONDS : 1毫秒 = 1秒 /1000\nSECONDS : 秒\nMINUTES : 分\nHOURS : 小时\nDAYS : 天\n\nBlockingQueue&lt;Runnable&gt; workQueue\n该线程池中的任务队列：维护着等待执行的Runnable对象\n\n当所有的核心线程都在干活时，新添加的任务会被添加到这个队列中等待处理，如果队列满了，则新建非核心线程执行任务。\n常用的workQueue类型：\n\nSynchronousQueue：这个队列接收到任务的时候，会直接提交给线程处理，而不保留它，如果所有线程都在工作怎么办？那就新建一个线程来处理这个任务！所以为了保证不出现&lt;线程数达到了maximumPoolSize而不能新建线程&gt;的错误，使用这个类型队列的时候，maximumPoolSize一般指定成Integer.MAX_VALUE，即无限大\nLinkedBlockingQueue：这个队列接收到任务的时候，如果当前线程数小于核心线程数，则新建线程(核心线程)处理任务；如果当前线程数等于核心线程数，则进入队列等待。由于这个队列没有最大值限制，即所有超过核心线程数的任务都将被添加到队列中，这也就导致了maximumPoolSize的设定失效，因为总线程数永远不会超过corePoolSize\nArrayBlockingQueue：可以限定队列的长度，接收到任务的时候，如果没有达到corePoolSize的值，则新建线程(核心线程)执行任务，如果达到了，则入队等候，如果队列已满，则新建线程(非核心线程)执行任务，又如果总线程数到了maximumPoolSize，并且队列也满了，则发生错误\nDelayQueue：队列内元素必须实现Delayed接口，这就意味着你传进去的任务必须先实现Delayed接口。这个队列接收到任务时，首先先入队，只有达到了指定的延时时间，才会执行任务\n\nThreadFactory threadFactory\n创建线程的工厂，这是一个接口，你new他的时候需要实现他的Thread newThread(Runnable r)方法，一般用不上。\n\nRejectedExecutionHandler handler\n超出maximumPoolSize异常处理类，这玩意儿就是抛出异常专用的\n\n线程池运行机制\n线程数量未达到corePoolSize，则新建一个线程(核心线程)执行任务\n线程数量达到了corePools，则将任务移入队列等待\n队列已满，新建线程(非核心线程)执行任务\n队列已满，总线程数又达到了maximumPoolSize，就会由(RejectedExecutionHandler)抛出异常\n\nJava通过Executors提供四种线程池分别为：\n\nnewCachedThreadPool创建一个可缓存线程池，如果线程池长度超过处理需要，可灵活回收空闲线程，若无可回收，则新建线程。\n\nnewFixedThreadPool 创建一个定长线程池，可控制线程最大并发数，超出的线程会在队列中等待。\n\nnewScheduledThreadPool 创建一个定长线程池，支持定时及周期性任务执行。\n\nnewSingleThreadExecutor 创建一个单线程化的线程池，它只会用唯一的工作线程来执行任务，保证所有任务按照指定顺序(FIFO, LIFO, 优先级)执行。\n\n\n","tags":["Java"]},{"title":"Java序列化","url":"/2019/12/12/Java%E5%BA%8F%E5%88%97%E5%8C%96/","content":"简介Java序列化是指把Java对象转换为字节序列的过程；而Java反序列化是指把字节序列恢复为Java对象的过程。从而达到网络传输、本地存储的效果。\n序列化Serializable\n实现Serializable接口，会自动将非transient修饰的变量序列化\n\npackage org.serial;import java.io.Serializable;public class User implements Serializable &#123;    private static final long serialVersionUID = -184170424740238078L;    private String name;    private String sex;    public String getName() &#123;        return name;    &#125;    public void setName(String name) &#123;        this.name = name;    &#125;    public String getSex() &#123;        return sex;    &#125;    public void setSex(String sex) &#123;        this.sex = sex;    &#125;&#125;\n\n测试类：\npackage org.serial;import java.io.*;public class SerializableUtil &#123;    public static Object deserializable(byte[] bytes) &#123;        try (ObjectInputStream objectInputStream = new ObjectInputStream(new ByteArrayInputStream(bytes))) &#123;            Object o = objectInputStream.readObject();            return o;        &#125; catch (IOException e) &#123;            e.printStackTrace();        &#125; catch (ClassNotFoundException e) &#123;            e.printStackTrace();        &#125;        return null;    &#125;    public static byte[] serializable(Object obj) &#123;        try (ByteArrayOutputStream byteArrayOutputStream = new ByteArrayOutputStream();             ObjectOutputStream objectOutputStream = new ObjectOutputStream(byteArrayOutputStream);) &#123;            objectOutputStream.writeObject(obj);            return byteArrayOutputStream.toByteArray();        &#125; catch (IOException e) &#123;            e.printStackTrace();        &#125;        return null;    &#125;    public static void main(String[] args) &#123;        User user = new User();        user.setName(\"zhang\");        user.setSex(\"man\");        byte[] serializable = SerializableUtil.serializable(user);        User deserializable = (User)SerializableUtil.deserializable(serializable);        System.out.println(deserializable.toString());    &#125;&#125;\n\nExternalizable\n实现Externalizable\n\npackage org.serial;import java.io.Externalizable;import java.io.IOException;import java.io.ObjectInput;import java.io.ObjectOutput;public class Car implements Externalizable &#123;    private String name;    private String color;    @Override    public void writeExternal(ObjectOutput objectOutput) throws IOException &#123;        objectOutput.writeObject(name);        objectOutput.writeObject(color);    &#125;    @Override    public void readExternal(ObjectInput objectInput) throws IOException, ClassNotFoundException &#123;        String name = (String) objectInput.readObject();        String color = (String) objectInput.readObject();    &#125;    public String getName() &#123;        return name;    &#125;    public void setName(String name) &#123;        this.name = name;    &#125;    public String getColor() &#123;        return color;    &#125;    public void setColor(String color) &#123;        this.color = color;    &#125;&#125;\n\n注意Externalizable接口实现方式一定要有默认的无参构造函数\nExternalizable不需要产生序列化ID而Serializable需要\n相比较Serializable,Externalizable序列化、反序列更加快速，占用相比较小的内存\n","tags":["Java"]},{"title":"Java语言解析XML的几种方式","url":"/2020/03/06/Java%E8%AF%AD%E8%A8%80%E8%A7%A3%E6%9E%90XML%E7%9A%84%E5%87%A0%E7%A7%8D%E6%96%B9%E5%BC%8F/","content":"(1)DOM解析\nDOM是html和xml的应用程序接口(API)，以层次结构（类似于树型）来组织节点和信息片段，映射XML文档的结构，允许获取和操作文档的任意部分，是W3C的官方标准\n\n【优点】\n①允许应用程序对数据和结构做出更改。\n②访问是双向的，可以在任何时候在树中上下导航，获取和操作任意部分的数据。\n\n【缺点】①通常需要加载整个XML文档来构造层次结构，消耗资源大。\n【解析详解】\n①构建Document对象：DocumentBuilderFactory dbf = DocumentBuilderFactory.newInstance();DocumentBuilder db = dbf.newDocumentBuilder();InputStream is = Thread.currentThread().getContextClassLoader().getResourceAsStream(xml文件);Document doc = db.parse(is);\n②遍历DOM对象\nDocument：    XML文档对象，由解析器获取\nNodeList：    节点数组\nNode：    节点(包括element、# text)\nElement：    元素，可用于获取属性参数\n\n\n\n(2)SAX(Simple API for XML)解析\n流型中的”推”模型分析方式。通过事件驱动，每发现一个节点就引发一个事件，事件推给事件处理器，通过回调方法完成解析工作，解析XML文档的逻辑需要应用程序完成\n\n【优势】\n①不需要等待所有数据都被处理，分析就能立即开始。\n②只在读取数据时检查数据，不需要保存在内存中。\n③可以在某个条件得到满足时停止解析，不必解析整个文档。\n④效率和性能较高，能解析大于系统内存的文档。\n\n【缺点】\n①需要应用程序自己负责TAG的处理逻辑（例如维护父/子关系等），文档越复杂程序就越复杂。\n②单向导航，无法定位文档层次，很难同时访问同一文档的不同部分数据，不支持XPath。\n\n【原理】简单的说就是对文档进行顺序扫描，当扫描到文档(document)开始与结束、元素(element)开始与结束时通知事件处理函数(回调函数)，进行相应处理，直到文档结束\n【事件处理器类型】\n①访问XML DTD：DTDHandler\n②低级访问解析错误：ErrorHandler\n③访问文档内容：ContextHandler\n\n【DefaultHandler类】SAX事件处理程序的默认基类，实现了DTDHandler、ErrorHandler、ContextHandler和EntityResolver接口，通常做法是，继承该基类，重写需要的方法，如startDocument()\n【创建SAX解析器】SAXParserFactory saxf = SAXParserFactory.newInstance();SAXParser sax = saxf.newSAXParser();\n注：关于遍历\n\n①深度优先遍历(Depthi-First Traserval)\n②广度优先遍历(Width-First Traserval)\n\n(3)JDOM(Java-based Document Object Model)\nJava特定的文档对象模型。自身不包含解析器，使用SAX\n\n【优点】\n①使用具体类而不是接口，简化了DOM的API。\n②大量使用了Java集合类，方便了Java开发人员。\n\n【缺点】\n①没有较好的灵活性。\n②性能较差。\n\n(4)DOM4J(Document Object Model for Java)\n简单易用，采用Java集合框架，并完全支持DOM、SAX和JAXP\n\n【优点】\n①大量使用了Java集合类，方便Java开发人员，同时提供一些提高性能的替代方法。\n②支持XPath。\n③有很好的性能。\n\n【缺点】\n①大量使用了接口，API较为复杂。\n\n(5)StAX(Streaming API for XML)\n流模型中的拉模型分析方式。提供基于指针和基于迭代器两种方式的支持,JDK1.6新特性\n\n【和推式解析相比的优点】\n①在拉式解析中，事件是由解析应用产生的，因此拉式解析中向客户端提供的是解析规则，而不是解析器。\n②同推式解析相比，拉式解析的代码更简单，而且不用那么多库。\n③拉式解析客户端能够一次读取多个XML文件。\n④拉式解析允许你过滤XML文件和跳过解析事件。\n\n【简介】StAX API的实现是使用了Java Web服务开发（JWSDP）1.6，并结合了Sun Java流式XML分析器(SJSXP)-它位于javax.xml.stream包中。XMLStreamReader接口用于分析一个XML文档，而XMLStreamWriter接口用于生成一个XML文档。XMLEventReader负责使用一个对象事件迭代子分析XML事件-这与XMLStreamReader所使用的光标机制形成对照。\n总结DOM4J\n","tags":["Java","XML"]},{"title":"Jenkins和SonarQube集成","url":"/2019/10/25/Jenkins%E5%92%8CSonarQube%E9%9B%86%E6%88%90/","content":"Jenkins和SonarQube集成安装插件SonarQube Scanner for Jenkins\n系统管理 - 插件管理\n可选插件（没有内容点立即获取）\n过滤 - 搜索SonarQube\n安装SonarQube Scanner for Jenkins\n安装成功 - 重启\n\n配置插件\n系统管理 - 系统设置 - SonarQube servers\n\n给项目配置启动SonarQube\n创建一个maven项目\n\n配置maven项目\n\n配置SonarQube构建环境 - Pre Steps/Post Steps - Add pre-build step - Execute SonarQube Scanner\n//多模块配置sonar.projectKey=project_keysonar.projectName=project_namesonar.projectVersion=1.0sonar.sourceEncoding=UTF-8sonar.modules=root,coreroot.sonar.projectBaseDir=./root.sonar.modules=com.thirdserviceroot.sonar.sources=src/main/javaroot.sonar.java.binaries=target/classescore.sonar.projectBaseDir=third/core/core.sonar.modules=com.bridgecore.sonar.sources=src/main/javacore.sonar.java.binaries=target/classes\n\n配置参数相关问题(注意)\n\n\nmysql参数设置问题17:08:15.777 DEBUG: Upload report17:08:22.648 DEBUG: POST 500 http:&#x2F;&#x2F;192.168.99.108:9000&#x2F;api&#x2F;ce&#x2F;submit?projectKey&#x3D;bi_build_sonar&amp;projectName&#x3D;bi_build_sonar | time&#x3D;6864ms17:08:22.662 INFO: ------------------------------------------------------------------------17:08:22.663 INFO: EXECUTION FAILURE17:08:22.663 INFO: ------------------------------------------------------------------------17:08:22.663 INFO: Total time: 5:01.152s17:08:22.951 INFO: Final Memory: 47M&#x2F;1469M17:08:22.951 INFO: ------------------------------------------------------------------------17:08:22.951 ERROR: Error during SonarQube Scanner execution17:08:22.951 ERROR: Failed to upload report - An error has occurred. Please contact your administratorWARN: Unable to locate &#39;report-task.txt&#39; in the workspace. Did the SonarScanner succeeded?ERROR: SonarQube scanner exited with non-zero code: 1\nSHOW VARIABLES LIKE &#39;max_allowed_packet&#39;;修改&#x2F;etc&#x2F;my.cnf文件：max_allowed_packet &#x3D; 100M\n\n\n","tags":["部署文档","Jenkins","SonarQube"]},{"title":"Jenkins安装部署","url":"/2019/10/22/Jenkins%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/","content":"Jenkins搭建指南(CentOS7)\n下载链接\nwget -i https://pkg.jenkins.io/redhat-stable/jenkins-2.164.3-1.1.noarch.rpm\n安装手册\n\n软件要求\nJava\nMaven# 安装到 /opt/ 路径下tar xzvf apache-maven-3.6.1-bin.tar.gz -C /opt/\n# 修改系统环境变量vim /etc/profile\n在最后一行添加下面的内容# customer settingsexport JAVA_HOME=/usr/java/jdk1.8.0_211-amd64export MVN_HOME=/opt/apache-maven-3.6.1export PATH=$JAVA_HOME/bin:$MVN_HOME/bin:$PATH\nSubversionyum install subversion\n\n\n\n安装启动#安装rpm -i jenkins-2.164.3-1.1.noarch.rpm#启动service jenkins start\n\n配置项目系统管理 &gt; 插件管理 &gt; 可选插件SonarQube ScannerMaven Integration\n集成SonarQube全局工具配置JDKSonarQube ScannerMaven\n","tags":["部署文档","Jenkins"]},{"title":"MySQL复制-基于全局事务标识符","url":"/2020/04/17/MySQL-Replication-GTID/","content":"\n官方参考文档\n\n全局事务标识符全局事务标识符（GTID）是创建的唯一标识符，并且与在源服务器（主服务器）上提交的每个事务相关联。该标识符不仅对于它起源的服务器是唯一的，而且在给定复制拓扑中的所有服务器上也是唯一的。GTID表示为一对坐标，并用冒号（:）分隔，如下所示：\nGTID &#x3D; source_id:transaction_id\nsource_id：主服务器上的uuid\ntransaction_id：提交的事务id\n例如，最初要在服务器上使用UUID提交的第二十三笔交易 3E11FA47-71CA-11E1-9E33-C80AA9429562具有以下GTID：3E11FA47-71CA-11E1-9E33-C80AA9429562:23\nGTID集：2174B383-5441-11E8-B90A-C80AA9429562:1-3, 24DA167-0C0C-11E8-8442-00059A3C7B00:1-19\n事务的GTID通过mysqlbinlog命令可以查询出来\n设置复制1. 如果复制已经在运行，则通过将它们设置为只读来同步两个服务器。SET @@GLOBAL.read_only &#x3D; ON;\n\n通过上面的命令，使主从都变成只读状态，然后开始同步。\n2. 停止两个服务器。3. 重新启动两个启用了GTID并配置了正确选项的服务器。\n主服务器配置\n\n###########必须############server-id=142log-bin=mysql-bingtid_mode=ONenforce-gtid-consistency=ON###########可选############\n\n\n从服务器配置\n\n###########必须############server-id=168gtid_mode=ONenforce-gtid-consistency=ON###########可选############\n\n4. 指示从属服务器将主服务器用作复制数据源并使用自动定位在从服务器端执行\nCHANGE MASTER TO  MASTER_HOST &#x3D; host,  MASTER_PORT &#x3D; port,  MASTER_USER &#x3D; user,  MASTER_PASSWORD &#x3D; password,  MASTER_AUTO_POSITION &#x3D; 1;\n\n5. 进行新的备份包含没有GTID的事务的二进制日志不能在启用了GTID的服务器上使用，因此在此之前进行的备份不能与新配置一起使用。\n在主服务器端执行\nFLUSH LOGS\n\n6. 启动从属服务器，然后在两台服务器上禁用只读模式，以便它们可以接受更新。START SLAVE;\n\nSET @@GLOBAL.read_only &#x3D; OFF;\n","tags":["MySQL","数据库"]},{"title":"Maven私服搭建","url":"/2020/03/10/Maven%E7%A7%81%E6%9C%8D%E6%90%AD%E5%BB%BA/","content":"Nexus\nhttps://help.sonatype.com/repomanager3/installation\n\n","tags":["部署文档","Maven"]},{"title":"MySQL复制-基于二进制日志文件","url":"/2020/04/17/MySQL-Replication-Binary/","content":"\n 参考文档\n\n修改配置文件\n 配置介绍\n\n\nMaster数据库\n\n编辑配置文件vim /etc/my.cnf，文件内容如下\n[mysqld]# 必须配置的服务id，一般配置主机的ipserver-id=168# 必须配置，开启二进制log-bin=mysql-bin# 下面都是可选配置# 要同步的数据库binlog-do-db=demo# 同步表# replicate-do-table=user\n\n\nSlave数据库\n\n修改配置文件如下\n[mysqld]# 必须配置server-id=169# 下面都是可选配置sync_binlog=0innodb_flush_log_at_trx_commit=2slave-skip-errors=all# 配置忽略的数据库# binlog-ignore-db=mysql# 配置同步的数据库# replicate-do-db=demo\n\n\n重启修改配置后的数据库\n\nsystemctl restart mysqld\n\n在Master创建用户用于同步CREATE USER &#39;slave&#39;@&#39;%&#39; IDENTIFIED BY &#39;123456&#39;;GRANT REPLICATION SLAVE ON *.* TO &#39;slave&#39;@&#39;%&#39;;flush privileges;\n\n查看Master二进制文件信息# 锁住全部表FLUSH TABLES WITH READ LOCK;# 同步完解锁unlock tables;# File和Position的参数是我们配置Slave的时候需要的SHOW MASTER STATUS;+------------------+----------+--------------+------------------+-------------------+| File             | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set |+------------------+----------+--------------+------------------+-------------------+| mysql-bin.000001 |     1077 |              |                  |                   |+------------------+----------+--------------+------------------+-------------------+\n\nSlave配置连接\n在Slave配置连接Master信息 命令详解\n\n登录MySQL命令控制台\n# MASTER_HOST 主机地址# MASTER_USER Master创建好的用户# MASTER_PASSWORD 密码# MASTER_PORT 端口，可选# MASTER_LOG_FILE 二进制文件名# MASTER_LOG_POS 开始同步位置CHANGE MASTER TO    MASTER_HOST&#x3D;&#39;192.168.99.168&#39;,    MASTER_USER&#x3D;&#39;slave&#39;,    MASTER_PASSWORD&#x3D;&#39;123456&#39;,    MASTER_PORT&#x3D;&#39;3306&#39;,    MASTER_LOG_FILE&#x3D;&#39;mysql-bin.000002&#39;,    MASTER_LOG_POS&#x3D;2216;\n\n\n启动\n\n# 启动start slave;\n\n\n查看Slave\n\n输入show slave status\\G，会显示类似如下\nmysql&gt; show slave status\\G;*************************** 1. row ***************************               Slave_IO_State: Waiting for master to send event                  Master_Host: 192.168.99.168                  Master_User: slave                  Master_Port: 3306                Connect_Retry: 60              Master_Log_File: mysql-bin.000003          Read_Master_Log_Pos: 700               Relay_Log_File: LENOVO-PC-relay-bin.000002                Relay_Log_Pos: 867        Relay_Master_Log_File: mysql-bin.000003             Slave_IO_Running: Yes            Slave_SQL_Running: Yes              Replicate_Do_DB: demo          Replicate_Ignore_DB: mysql           Replicate_Do_Table:       Replicate_Ignore_Table:      Replicate_Wild_Do_Table:  Replicate_Wild_Ignore_Table:                   Last_Errno: 0                   Last_Error:                 Skip_Counter: 0          Exec_Master_Log_Pos: 700              Relay_Log_Space: 1079              Until_Condition: None               Until_Log_File:                Until_Log_Pos: 0           Master_SSL_Allowed: No           Master_SSL_CA_File:           Master_SSL_CA_Path:              Master_SSL_Cert:            Master_SSL_Cipher:               Master_SSL_Key:        Seconds_Behind_Master: 0Master_SSL_Verify_Server_Cert: No                Last_IO_Errno: 0                Last_IO_Error:               Last_SQL_Errno: 0               Last_SQL_Error:  Replicate_Ignore_Server_Ids:             Master_Server_Id: 168                  Master_UUID: bd1531fb-f568-11e9-bc4b-46afd4d32e02             Master_Info_File: mysql.slave_master_info                    SQL_Delay: 0          SQL_Remaining_Delay: NULL      Slave_SQL_Running_State: Slave has read all relay log; waiting for more updates           Master_Retry_Count: 86400                  Master_Bind:      Last_IO_Error_Timestamp:     Last_SQL_Error_Timestamp:               Master_SSL_Crl:           Master_SSL_Crlpath:           Retrieved_Gtid_Set:            Executed_Gtid_Set:                Auto_Position: 0         Replicate_Rewrite_DB:                 Channel_Name:           Master_TLS_Version:       Master_public_key_path:        Get_master_public_key: 0            Network_Namespace:1 row in set (0.00 sec)ERROR:No query specified\n\n注意显示状态是这样表示成功连到Master：\nSlave_IO_Running: YesSlave_SQL_Running: Yes\n不是这个状态会有错误显示\n# 每次重新配置Slave需要停止stop slave\n\n注意事项\n配置前务必将两个数据库手工同步\n\n","tags":["MySQL","数据库"]},{"title":"MySQL性能优化","url":"/2019/11/12/MySQL%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/","content":"慢查询配置\nMySQL的慢查询日志是MySQL提供的一种日志记录，它是用来记录在MySQL中响应时间超过阀值的语句。系统默认情况下，MySQL并不启动慢查询日志，需要我们手动来设置这个参数，当然，如果不是调优需要的话，一般不建议启动该参数，因为开启慢查询日志会或多或少带来一定的性能影响。\n\n默认情况下slow_query_log的值为OFF，表示慢查询日志是禁用的，可以通过设置slow_query_log的值来开启，如下所示：\nmysql&gt; show variables like &#39;%slow_query_log%&#39;+---------------------+-------------------------------+| Variable_name       | Value                         |+---------------------+-------------------------------+| slow_query_log      | OFF                           || slow_query_log_file | &#x2F;var&#x2F;lib&#x2F;mysql&#x2F;mysql-slow.log |+---------------------+-------------------------------+2 rows in set (0.00 sec)\n\n开启慢查询日志：\nmysql&gt; set global slow_query_log&#x3D;1;Query OK, 0 rows affected (0.00 sec)\n\n然后看状态：\nmysql&gt; show variables like &#39;%slow_query_log%&#39;;+---------------------+-------------------------------+| Variable_name       | Value                         |+---------------------+-------------------------------+| slow_query_log      | ON                            || slow_query_log_file | &#x2F;var&#x2F;lib&#x2F;mysql&#x2F;mysql-slow.log |+---------------------+-------------------------------+2 rows in set (0.00 sec)\n\n使用set global slow_query_log=1开启了慢查询日志只对当前数据库生效，如果MySQL重启后则会失效。如果要永久生效，就必须修改配置文件my.cnf（其它系统变量也是如此）。\n例如如下所示：\n[root@mysql ~]# vim &#x2F;etc&#x2F;my.cnfslow_query_log&#x3D;1slow_query_log_file&#x3D;&#x2F;var&#x2F;lib&#x2F;mysql&#x2F;mysql-slow.log\n\n参数说明：\n\nslow_query_log 慢查询开启状态；\nslow_query_log_file 慢查询日志存放的位置；\nlong_query_time查询超过多少秒才记录。\n\n日志分析工具\n MySQL 自带了一个查看慢日志的工具 mysqldumpslow，执行mysqldumpslow –help 可以查看其相关参数和说明：\n\n[root@mysql ~]# mysqldumpslow --helpUsage: mysqldumpslow [ OPTS... ] [ LOGS... ]Parse and summarize the MySQL slow query log. Options are  --verbose    verbose  --debug      debug  --help       write this text to standard output  -v           verbose  -d           debug  -s ORDER     what to sort by (al, at, ar, c, l, r, t), &#39;at&#39; is default                al: average lock time                ar: average rows sent                at: average query time                 c: count                 l: lock time                 r: rows sent                 t: query time  -r           reverse the sort order (largest last instead of first)  -t NUM       just show the top n queries  -a           don&#39;t abstract all numbers to N and strings to &#39;S&#39;  -n NUM       abstract numbers with at least n digits within names  -g PATTERN   grep: only consider stmts that include this string  -h HOSTNAME  hostname of db server for *-slow.log filename (can be wildcard),               default is &#39;*&#39;, i.e. match all  -i NAME      name of server instance (if using mysql.server startup script)  -l           don&#39;t subtract lock time from total time\n\n参数解释：\n\n-s:是表示按照何种方式排序；\nc: 访问计数；\nl: 锁定时间；\nr: 返回记录；\nt: 查询时间；\nal:平均锁定时间；\nar:平均返回记录数；\nat:平均查询时间；\n-t:是top n的意思，即为返回前面多少条的数据；\n-g:后边可以写一个正则匹配模式，大小写不敏感的。\n\n例如，得到返回记录集最多的10个SQL：\nmysqldumpslow -s r -t 10 /mysql/mysql_slow.log\n\n得到访问次数最多的10个SQL：\nmysqldumpslow -s c -t 10 /mysql/mysql_slow.log\n","tags":["MySQL","数据库"]},{"title":"MacBook实用技能","url":"/2019/12/24/MacBook%E5%AE%9E%E7%94%A8%E6%8A%80%E8%83%BD/","content":"系统快捷键\n\n\n键\n说明\n\n\n\n⌘⌥⎋\n强制退出\n\n\n重装系统https://support.apple.com/zh-cn/HT204904\n关闭自动更新AppStore → Preference → Automatically check for updates\n关于shell\n在 Mac 上将 zsh 用作默认 Shell\n\n配置环境变量\n# 旧版本Mac系统的环境变量，加载顺序为：/etc/profile\t\t\t\t\t\t\t\t/etc/paths~/.bash_profile~/.bash_login~/.profile~/.bashrc\n\nsudo vi /etc/zprofile# 中间用冒号隔开export PATH=$PATH:&lt;PATH 1&gt;:&lt;PATH 2&gt;:&lt;PATH 3&gt;:------:&lt;PATH N&gt;# environment variableexport JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_231.jdk/Contents/Homeexport MVN_HOME=/opt/apache-maven-3.6.2export PATH=$JAVA_HOME/bin:$MVN_HOME/bin:$PATHexport CLASSPATH=.:$JAVA_HOME/lib\n\n修改主机名称\n\n在某些情况下，主机名称会改变成bogon，用下面这个命令修改\n\nsudo scutil --set HostName XXXX\n\n命令行下载\ncurl [options...] &lt;url&gt;-O, --remote-name   Write output to a file named as the remote file    --remote-name-all Use the remote file name for all URLscurl -O &lt;URL&gt;\n\n权限授予\n\n某些情况下，如果发现某些应用无法启动，可能是因为权限的问题\n\n# 查看当前路径下文件的全部权限ls -l# 授权chmod [ugoa...][[+-=][rwxX]...][,...]其中：u 表示该文件的拥有者，g 表示与该文件的拥有者属于同一个群体(group)者，o 表示其他以外的人，a 表示这三者皆是。+ 表示增加权限、- 表示取消权限、= 表示唯一设定权限。r 表示可读取，w 表示可写入，x 表示可执行，X 表示只有当该文件是个子目录或者该文件已经被设定过为可执行。r=4，w=2，x=1其他参数说明：-c : 若该文件权限确实已经更改，才显示其更改动作-f : 若该文件权限无法被更改也不要显示错误讯息-v : 显示权限变更的详细资料-R : 对目前目录下的所有文件与子目录进行相同的权限变更(即以递回的方式逐个变更)--help : 显示辅助说明--version : 显示版本\n\n\n\nGit命令\n一直没有遇到自己喜欢的图形化\n\n# 查看文件跟踪状态git status# 把文件加入到跟踪索引git add &lt;file&gt;# 移除文件跟踪，并且保留文件git rm --cached &lt;file&gt;\n\nIDEA常用快捷键(macOS)\n\n\n名称\n键\n说明\n\n\n\nImplementation(s)\n⌥⌘B\n查看实现类\n\n\nEvaluate Expression…\n⌥F8\nDEBUG模式下调试参数\n\n\nRun to Cursor\n⌥F9\n运行到这一行\n\n\nGenerate…\n⌘N\n代码生成\n\n\nFile…\n⇧⌘O\n文件打开\n\n\nFind Usages\n⌥F7\n查找类的使用\n\n\nShow Context Actions\n⌥↩\n导入依赖\n\n\nFind in Path…\n⇧⌘F\n在路径中查询\n\n\nReformat Code\n⌥⌘L\n格式化代码\n\n\nDelete Line\n⌘⌦\n删除一行\n\n\nLast Edit Location\n⇧⌘⌦\n退到上一次编辑的地方\n\n\n按键符号⌘ == Command⇧ == Shift⇪ == Caps Lock⌥ == Option⌃ == Control↩ == Return/Enter⌫ == Delete⌦ == 向前删除键（Fn+Delete）↑ == 上箭头↓ == 下箭头← == 左箭头→ == 右箭头⇞ == Page Up（Fn+↑）⇟ == Page Down（Fn+↓）Home == Fn + ←End == Fn + →⇥ == 右制表符（Tab键）⇤ == 左制表符（Shift+Tab）⎋ == Escape (Esc)⏏ == 电源开关键\n","tags":["Mac"]},{"title":"Nginx-安装部署","url":"/2019/11/12/Nginx%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/","content":"下载\n官方下载\n文档\n\n安装解压到安装路径即可\n启动nginx -s [ stop | quit | reopen | reload ]\n","tags":["部署文档","Nginx"]},{"title":"Netty源码分析","url":"/2020/07/08/Netty%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/","content":"一、介绍官网：https://netty.io/\nNetty是一个NIO客户端服务端框架。\n二、要素ChannelHandler处理和拦截I/O操作，并且将处理后的数据传至下一个管道\n三、源码分析Netty版本：4.1.44.Final\nChannelHandler调用过程注册ServerBootstrap.childHandler(ChannelHandler childHandler)\nprivate void register0(ChannelPromise promise) &#123;    try &#123;        // check if the channel is still open as it could be closed in the mean time when the register        // call was outside of the eventLoop        if (!promise.setUncancellable() || !ensureOpen(promise)) &#123;            return;        &#125;        boolean firstRegistration = neverRegistered;        doRegister();        neverRegistered = false;        registered = true;        // Ensure we call handlerAdded(...) before we actually notify the promise. This is needed as the        // user may already fire events through the pipeline in the ChannelFutureListener.        pipeline.invokeHandlerAddedIfNeeded();        safeSetSuccess(promise);        pipeline.fireChannelRegistered();        // Only fire a channelActive if the channel has never been registered. This prevents firing        // multiple channel actives if the channel is deregistered and re-registered.        if (isActive()) &#123;            if (firstRegistration) &#123;                pipeline.fireChannelActive();            &#125; else if (config().isAutoRead()) &#123;                // This channel was registered before and autoRead() is set. This means we need to begin read                // again so that we process inbound data.                //                // See https://github.com/netty/netty/issues/4805                beginRead();            &#125;        &#125;    &#125; catch (Throwable t) &#123;        // Close the channel directly to avoid FD leak.        closeForcibly();        closeFuture.setClosed();        safeSetFailure(promise, t);    &#125;&#125;\n\n四、相关文章认真的 Netty 源码解析（一）\n认真的 Netty 源码解析（二）\nhttps://www.jianshu.com/nb/22889712\nhttps://www.jianshu.com/p/46861a05ce1e\n","tags":["Java","Netty"]},{"title":"Oracle历史记录","url":"/2020/06/03/Oracle%E5%8E%86%E5%8F%B2%E8%AE%B0%E5%BD%95/","content":"&#x2F;*分为四步 *&#x2F;&#x2F;*第1步：创建临时表空间  *&#x2F;create temporary tablespace yuhang_temptempfile &#39;D:\\oracledata\\yuhang_temp.dbf&#39;size 50mautoextend onnext 50m maxsize 20480mextent management local;&#x2F;*第2步：创建数据表空间  *&#x2F;create tablespace yuhang_dataloggingdatafile &#39;D:\\oracledata\\yuhang_data.dbf&#39;size 50mautoextend onnext 50m maxsize 20480mextent management local;&#x2F;*第3步：创建用户并指定表空间  *&#x2F;create user yuhang identified by yuhangdefault tablespace yuhang_datatemporary tablespace yuhang_temp;&#x2F;*第4步：给用户授予权限  *&#x2F;grant connect,resource,dba to yuhang;\n","tags":["数据库","Oracle","SQL"]},{"title":"RPC框架介绍","url":"/2020/02/28/RPC%E6%A1%86%E6%9E%B6%E4%BB%8B%E7%BB%8D/","content":"RMIC:\\Program Files\\Java\\jdk1.8.0_211\\jre\\lib\\rt.jar!\\java\\rmi\nHessian","tags":["RPC"]},{"title":"Quartz框架介绍","url":"/2020/06/03/Quartz%E6%A1%86%E6%9E%B6%E4%BB%8B%E7%BB%8D/","content":"\nhttps://www.w3cschool.cn/quartz_doc/\n\n我们需要明白 Quartz 的几个核心概念，这样理解起 Quartz 的原理就会变得简单了。\n\nJob\n表示一个工作，要执行的具体内容。此接口中只有一个方法，如下：\n\n\nvoid execute(JobExecutionContext context)\n\nJobDetail\n表示一个具体的可执行的调度程序，Job 是这个可执行程调度程序所要执行的内容，另外 JobDetail 还包含了这个任务调度的方案和策略。\n\nTrigger\n代表一个调度参数的配置，什么时候去调。\n\nScheduler\n代表一个调度容器，一个调度容器中可以注册多个 JobDetail 和 Trigger。当 Trigger 与 JobDetail 组合，就可以被 Scheduler 容器调度了。\n\n\n示例代码：\nScheduler scheduler = StdSchedulerFactory.getDefaultScheduler();// and start it offscheduler.start();// define the job and tie it to our HelloJob classJobDetail job = newJob(HelloJob.class)    .withIdentity(\"job1\", \"group1\")    .usingJobData(\"COUNT\", 1)    .build();// Trigger the job to run now, and then repeat every 40 secondsTrigger trigger = newTrigger()    .withIdentity(\"trigger1\", \"group1\")    .startNow()    .withSchedule(simpleSchedule()                  .withIntervalInSeconds(1)                  .repeatForever())    .build();// Tell apacheusage.quartz to schedule the job using our triggerscheduler.scheduleJob(job, trigger);// scheduler.shutdown();\n","tags":["Java"]},{"title":"Maven打包介绍","url":"/2019/10/22/Maven%E6%89%93%E5%8C%85%E4%BB%8B%E7%BB%8D/","content":"依赖JAR方式一&lt;build&gt;    &lt;!--maven-assembly-plugin打包方式运行assembly:single--&gt;    &lt;plugins&gt;        &lt;plugin&gt;            &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt;            &lt;version&gt;3.1.1&lt;/version&gt;            &lt;configuration&gt;                &lt;archive&gt;                    &lt;manifest&gt;                        &lt;mainClass&gt;com.yss.jdbc.util.Sample&lt;/mainClass&gt;                    &lt;/manifest&gt;                &lt;/archive&gt;                &lt;descriptorRefs&gt;                    &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt;                &lt;/descriptorRefs&gt;            &lt;/configuration&gt;            &lt;executions&gt;                &lt;execution&gt;                    &lt;id&gt;make-assembly&lt;/id&gt;  &lt;!--继承合并--&gt;                    &lt;phase&gt;package&lt;/phase&gt;  &lt;!--绑定到打包阶段--&gt;                    &lt;goals&gt;                        &lt;goal&gt;single&lt;/goal&gt;                    &lt;/goals&gt;                &lt;/execution&gt;            &lt;/executions&gt;        &lt;/plugin&gt;    &lt;/plugins&gt;&lt;/build&gt;\n方式二&lt;plugin&gt;    &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;    &lt;artifactId&gt;maven-jar-plugin&lt;/artifactId&gt;    &lt;version&gt;2.4&lt;/version&gt;    &lt;configuration&gt;        &lt;archive&gt;            &lt;addMavenDescriptor&gt;false&lt;/addMavenDescriptor&gt;            &lt;manifest&gt;&lt;!--MANIFEST.MF文件设置--&gt;                &lt;mainClass&gt;com.XX.Main&lt;/mainClass&gt;                &lt;addClasspath&gt;true&lt;/addClasspath&gt;                &lt;classpathPrefix&gt;lib/&lt;/classpathPrefix&gt;            &lt;/manifest&gt;        &lt;/archive&gt;        &lt;outputDirectory&gt;target/deploy/svntool&lt;/outputDirectory&gt;    &lt;/configuration&gt;&lt;/plugin&gt;&lt;plugin&gt;    &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;    &lt;artifactId&gt;maven-dependency-plugin&lt;/artifactId&gt;    &lt;version&gt;2.4&lt;/version&gt;    &lt;executions&gt;        &lt;execution&gt;            &lt;id&gt;copy-dependencies&lt;/id&gt;            &lt;phase&gt;package&lt;/phase&gt;            &lt;goals&gt;                &lt;goal&gt;copy-dependencies&lt;/goal&gt;            &lt;/goals&gt;            &lt;configuration&gt;                &lt;outputDirectory&gt;target/deploy/svntool/lib&lt;/outputDirectory&gt;            &lt;/configuration&gt;        &lt;/execution&gt;    &lt;/executions&gt;&lt;/plugin&gt;&lt;/plugins&gt;\n\n可执行JAR方式一&lt;build&gt;    &lt;!--maven-assembly-plugin打包方式运行assembly:single--&gt;    &lt;plugins&gt;        &lt;plugin&gt;            &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt;            &lt;version&gt;3.1.1&lt;/version&gt;            &lt;configuration&gt;                &lt;archive&gt;                    &lt;manifest&gt;                        &lt;mainClass&gt;com.yss.jdbc.util.Sample&lt;/mainClass&gt;                    &lt;/manifest&gt;                &lt;/archive&gt;                &lt;descriptorRefs&gt;                    &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt;                &lt;/descriptorRefs&gt;            &lt;/configuration&gt;            &lt;executions&gt;                &lt;execution&gt;                    &lt;id&gt;make-assembly&lt;/id&gt;  &lt;!--继承合并--&gt;                    &lt;phase&gt;package&lt;/phase&gt;  &lt;!--绑定到打包阶段--&gt;                    &lt;goals&gt;                        &lt;goal&gt;single&lt;/goal&gt;                    &lt;/goals&gt;                &lt;/execution&gt;            &lt;/executions&gt;        &lt;/plugin&gt;    &lt;/plugins&gt;&lt;/build&gt;\n方式二&lt;plugin&gt;    &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;    &lt;artifactId&gt;maven-jar-plugin&lt;/artifactId&gt;    &lt;version&gt;2.4&lt;/version&gt;    &lt;configuration&gt;        &lt;archive&gt;            &lt;addMavenDescriptor&gt;false&lt;/addMavenDescriptor&gt;            &lt;manifest&gt;&lt;!--MANIFEST.MF文件设置--&gt;                &lt;mainClass&gt;com.XX.Main&lt;/mainClass&gt;                &lt;addClasspath&gt;true&lt;/addClasspath&gt;                &lt;classpathPrefix&gt;lib/&lt;/classpathPrefix&gt;            &lt;/manifest&gt;        &lt;/archive&gt;        &lt;outputDirectory&gt;target/deploy/svntool&lt;/outputDirectory&gt;    &lt;/configuration&gt;&lt;/plugin&gt;&lt;plugin&gt;    &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;    &lt;artifactId&gt;maven-dependency-plugin&lt;/artifactId&gt;    &lt;version&gt;2.4&lt;/version&gt;    &lt;executions&gt;        &lt;execution&gt;            &lt;id&gt;copy-dependencies&lt;/id&gt;            &lt;phase&gt;package&lt;/phase&gt;            &lt;goals&gt;                &lt;goal&gt;copy-dependencies&lt;/goal&gt;            &lt;/goals&gt;            &lt;configuration&gt;                &lt;outputDirectory&gt;target/deploy/svntool/lib&lt;/outputDirectory&gt;            &lt;/configuration&gt;        &lt;/execution&gt;    &lt;/executions&gt;&lt;/plugin&gt;&lt;/plugins&gt;\n生成JAR文件后增加时间戳自带属性实现&lt;properties&gt;    &lt;maven.build.timestamp.format&gt;yyyyMMddHHmmss&lt;/maven.build.timestamp.format&gt;&lt;/properties&gt;\n\n&lt;build&gt;    &lt;finalName&gt;        $&#123;project.artifactId&#125;-$&#123;project.version&#125;.$&#123;maven.build.timestamp&#125;    &lt;/finalName&gt;&lt;/build&gt;\n\nMaven自带时间戳使用${maven.build.timestamp}，但是时区是UTC。\n如果要使用GMT+8，就需要插件提供支持，以下两个插件可以实现。\nbuildnubmer-maven-plugin&lt;build&gt;    &lt;finalName&gt;        $&#123;project.artifactId&#125;-$&#123;project.version&#125;_$&#123;timestamp&#125;    &lt;/finalName&gt;    &lt;plugin&gt;        &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt;        &lt;artifactId&gt;buildnumber-maven-plugin&lt;/artifactId&gt;        &lt;version&gt;1.4&lt;/version&gt;        &lt;configuration&gt;            &lt;timestampFormat&gt;yyyyMMdd&lt;/timestampFormat&gt;        &lt;/configuration&gt;        &lt;executions&gt;            &lt;execution&gt;                &lt;goals&gt;                    &lt;goal&gt;create-timestamp&lt;/goal&gt;                &lt;/goals&gt;            &lt;/execution&gt;        &lt;/executions&gt;        &lt;inherited&gt;false&lt;/inherited&gt;    &lt;/plugin&gt;&lt;/build&gt;\n\nbuild-helper-maven-plugin&lt;build&gt;    &lt;finalName&gt;ProjectName-$&#123;current.time&#125;&lt;/finalName&gt;    &lt;plugins&gt;        &lt;plugin&gt;            &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt;            &lt;artifactId&gt;build-helper-maven-plugin&lt;/artifactId&gt;            &lt;version&gt;1.9.1&lt;/version&gt;            &lt;executions&gt;                &lt;execution&gt;                    &lt;id&gt;timestamp-property&lt;/id&gt;                    &lt;goals&gt;                        &lt;goal&gt;timestamp-property&lt;/goal&gt;                    &lt;/goals&gt;                &lt;/execution&gt;            &lt;/executions&gt;            &lt;configuration&gt;                &lt;name&gt;current.time&lt;/name&gt;                &lt;pattern&gt;yyyyMMdd-HHmmss&lt;/pattern&gt;                &lt;timeZone&gt;GMT+8&lt;/timeZone&gt;            &lt;/configuration&gt;        &lt;/plugin&gt;    &lt;/plugins&gt;&lt;/build&gt;\n","tags":["Maven"]},{"title":"PING命令使用","url":"/2019/11/04/PING%E5%91%BD%E4%BB%A4%E4%BD%BF%E7%94%A8/","content":"一、ping基本使用详解在网络中ping是一个十分强大的TCP/IP工具。它的作用主要为：\n\n用来检测网络的连通情况和分析网络速度\n\n根据域名得到服务器IP\n\n根据ping返回的TTL值来判断对方所使用的操作系统及数据包经过路由器数量。\n\n\n我们通常会用它来直接ping ip地址，来测试网络的连通情况。\n\n类如这种，直接ping ip地址或网关，ping通会显示出以上数据，有朋友可能会问，bytes=32；time&lt;1ms；TTL=128 这些是什么意思。\nbytes值：数据包大小，也就是字节。\ntime值：响应时间，这个时间越小，说明你连接这个地址速度越快。\nTTL值：Time To Live,表示DNS记录在DNS服务器上存在的时间，它是IP协议包的一个值，告诉路由器该数据包何时需要被丢弃。可以通过Ping返回的TTL值大小，粗略地判断目标系统类型是Windows系列还是UNIX/Linux系列。\n默认情况下，Linux系统的TTL值为64或255，WindowsNT/2000/XP系统的TTL值为128，Windows98系统的TTL值为32，UNIX主机的TTL值为255。\n因此一般TTL值：\n100~130ms之间，Windows系统 ；\n240~255ms之间，UNIX/Linux系统。\n当然，我们今天主要了解并不是这些，而是ping的其它参考。\nping命令除了直接ping网络的ip地址，验证网络畅通和速度之外，它还有这些用法。\n\n二、ping -t的使用不间断地Ping指定计算机，直到管理员中断。\n\n这就说明电脑连接路由器是通的，网络效果很好。下面按按住键盘的Ctrl+c终止它继续ping下去，就会停止了，会总结出运行的数据包有多少，通断的有多少了。\n三、ping -a的使用ping-a解析计算机名与NetBios名。就是可以通过ping它的ip地址，可以解析出主机名。\n\n四、ping -n的使用在默认情况下，一般都只发送四个数据包，通过这个命令可以自己定义发送的个数，对衡量网络速度很有帮助，比如我想测试发送10个数据包的返回的平均时间为多少，最快时间为多少，最慢时间为多少就可以通过以下获知：\n\n从以上我就可以知道在给47.93.187.142发送10个数据包的过程当中，返回了10个，没有丢失，这10个数据包当中返回速度最快为32ms，最慢为55ms，平均速度为37ms。说明我的网络良好。\n如果对于一些不好的网络，比如监控系统中非常卡顿，这样测试，返回的结果可能会显示出丢失出一部分，如果丢失的比较多的话，那么就说明网络不好，可以很直观的判断出网络的情况。\n五、ping -l size的使用-l size：发送size指定大小的到目标主机的数据包。\n在默认的情况下Windows的ping发送的数据包大小为32byte，最大能发送65500byte。当一次发送的数据包大于或等于65500byte时，将可能导致接收方计算机宕机。所以微软限制了这一数值；这个参数配合其它参数以后危害非常强大，比如攻击者可以结合-t参数实施DOS攻击。（所以它具有危险性，不要轻易向别人计算机使用）。\n例如：ping -l 65500 -t  211.84.7.46\n会连续对IP地址执行ping命令，直到被用户以Ctrl+C中断.\n\n这样它就会不停的向211.84.7.46计算机发送大小为65500byte的数据包，如果你只有一台计算机也许没有什么效果，但如果有很多计算机那么就可以使对方完全瘫痪，网络严重堵塞，由此可见威力非同小可。\n六、ping -r count的使用在“记录路由”字段中记录传出和返回数据包的路由，探测经过的\n路由个数，但最多只能跟踪到9个路由。\nping -n 1 -r 9 202.102.224.25 （发送一个数据包，最多记录9个路由）\n\n将经过 9个路由都显示出来了，可以看上图。\nping命令用的较多的就这6类的，大家有可能在项目中会用到的。\n七、批量ping网段对于一个网段ip地址众多，如果单个检测实在麻烦，那么我们可以直接批量ping网段检测，那个ip地址出了问题，一目了然。\n先看代码，直接在命令行窗口输入：\nfor /L %D in (1,1,255) do ping 10.168.1.%D\nIP地址段修改成你要检查的IP地址段。\n\n当输入批量命令后，那么它就自动把网段内所有的ip地址都ping完为止。\n那么这段for /L %D in(1,1,255) do ping 10.168.1.%D 代码是什么意思呢？\n代码中的这个(1,1,255)就是网段起与始，就是检测网段192.168.1.1到192.168.1.255之间的所有的ip地址，每次逐增1，直接到1到255这255个ip检测完为止。\n"},{"title":"SonarQube安装部署","url":"/2019/10/23/SonarQube%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/","content":"\n详细查看官方文档\n\n安装前准备环境Centos7(Linux version 3.10.0-957.el7.x86_64):vm.max_map_count 大于或等于262144fs.file-max 大于或等于65536运行SonarQube的用户可以打开至少65536个文件描述符运行SonarQube的用户可以打开至少2048个线程\nOracle JRE 8下载链接    - 选择jdk-8u211-linux-x64.rpm下载\n# 查看安装的路径rpm -qpl jdk-8u211-linux-x64.rpm## 安装rpm -i jdk-8u211-linux-x64.rpm\nMySQL 5.7下载链接 参考文档\n下载mysql80-community-release-el7-3.noarch.rpm\n# 安装rpm源sudo rpm -Uvh mysql80-community-release-el7-3.noarch.rpm# 编辑，找到Enable to use MySQL 5.7，改为enabled=1，其他版本设置成enabled=0vim /etc/yum.repos.d/mysql-community.repo# 检查只有MySQL 5.7启动yum repolist enabled | grep mysql# 安装MySQLsudo yum install mysql-community-server# 启动MySQL服务器sudo service mysqld start# MySQL服务器的状态sudo service mysqld status# 查看超级用户的密码sudo grep 'temporary password' /var/log/mysqld.log# 登录mysqlmysql -uroot -p# 修改密码ALTER USER 'root'@'localhost' IDENTIFIED BY 'MyNewPass4!';# 修改密码校验set global validate_password_policy=0;set global validate_password_length=1;# 默认mysql的root用户不支持远程访问，开启访问权限GRANT ALL ON *.* TO root@'%' IDENTIFIED BY '123456' WITH GRANT OPTION;update user set host='%' where user='root';flush privileges;# 创建数据库sonarqubeCREATE DATABASE `sonarqube` CHARACTER SET 'utf8';新增用户sonarqube并授予sonarqube数据库全部权限rant all privileges on sonarqube.* to sonarqube@'%' identified by \"password\";开启3306端口irewall-cmd --add-port=3306/tcp(a)数据库目录var/lib/mysql/(b)配置文件usr/share /mysql（mysql.server命令及配置文件）etc/my.cnf(c)相关命令/usr/bin（mysqladmin mysqldump等命令）# (d)启动脚本/etc/rc.d/init.d/（启动脚本文件mysql的目录）\n安装SonarQube1. 下载 SonarQube当前版本： 7.7 下载链接\n2. 解压得到当前路径: /opt/sonarqube-7.7\n3. 修改配置文件vim /opt/sonarqube-7.7/conf/sonar.properties\n# 对应的数据库信息sonar.jdbc.username=sonar.jdbc.password=sonar.jdbc.url=\n4. 新增用户SonarQube不能以root启动\n# 添加用户adduser sonarqube# 设置密码passwd sonarqube# 授权chown -R sonarqube:sonarqube /opt/sonarqube-7.7\n5.默认SonarQube启动在9000端口firewall-cmd --add-port=9000/tcpsystemctl stop firewalldsystemctl disable firewalld\n6.以服务启动SonarQube\n创建文件\n\nvim /etc/systemd/system/sonarqube.service\n\n\n添加下面内容：\n\n[Unit]Description=SonarQube serviceAfter=syslog.target network.target[Service]Type=simpleUser=sonarqubeGroup=sonarqubePermissionsStartOnly=trueExecStart=/bin/nohup /bin/java -Xms32m -Xmx32m -Djava.net.preferIPv4Stack=true -jar /opt/sonarqube-7.7/lib/sonar-application-7.7.jarStandardOutput=syslogLimitNOFILE=65536LimitNPROC=8192TimeoutStartSec=5Restart=always[Install]WantedBy=multi-user.target\n\n\n安装启动\n\nsu sonarqube# 注册服务sudo systemctl enable sonarqube.service# 启动服务sudo systemctl start sonarqube.service# 重启sudo systemctl restart sonarqube.service\n\n查看日志\n\nvim /opt/sonarqube-7.7/logs/web.log\n\n常见问题\n使用sudo命令时，出现问题：\n\n\n​         编辑vim /etc/sudoers\nroot    ALL=(ALL:ALL) ALLsonarqube    ALL=(ALL:ALL) ALL\n\n\n扫描成功但是上传失败\n\n可能是因为mysql数据库上传数据限制了包的大小，查看web.log日志\nCaused by: com.mysql.jdbc.PacketTooBigException: Packet for query is too large (6980220 &gt; 4194304). You can change this value on the server by setting the max_allowed_packet&#39; variable.\n\n# For advice on how to change settings please see# http://dev.mysql.com/doc/refman/5.7/en/server-configuration-defaults.html[mysqld]max_allowed_packet=100M\n","tags":["部署文档","SonarQube"]},{"title":"R语言开发环境部署","url":"/2019/10/23/R%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2/","content":"R语言介绍：R是用于统计分析、绘图的语言和操作环境。R是属于GNU系统的一个自由、免费、源代码开放的软件，它是一个用于统计计算和统计制图的优秀工具。\nR语言开发环境\n官方网站\n下载地址\n\nR语言开发工具 RStudio\n官方网站\n下载地址\n\n相关学习网站：https://www.w3cschool.cn/r/r_environment_setup.html\n未完待续。。。\n","tags":["部署文档","R"]},{"title":"SQL历史记录","url":"/2019/10/22/SQL%E5%8E%86%E5%8F%B2%E8%AE%B0%E5%BD%95/","content":"SQL历史记录select logon_time,       last_call_et \"time inactive\",       nvl(s.username, 'ORACLE PROCESS') username,       s.machine,       s.program,       s.sid session_id,       s.status,       sql_text,       ss.value \"CPU used\",       trunc(buffer_gets / (executions + 1)) \"BUFF-EXEC\",       trunc(buffer_gets / (rows_processed + 1)) \"BUFF-ROWS\",       first_load_time,       executions,       parse_calls,       disk_reads,       buffer_gets,       rows_processed  from v$session s, v$sesstat ss, v$statname sn, v$sqlarea sawhere s.sid = ss.sid   and ss.statistic# = sn.statistic#   and sn.name = 'CPU used by this session'   and s.sql_address = sa.address   and s.sql_hash_value = sa.hash_value   and last_call_et &gt; 5000  --超过5秒不释放的sqlorder by machine, status, program, last_call_et asc;\n\nselect ss.value \"CPU used\",       sa.SQL_FULLTEXT,       sql_text,       s.status,       last_call_et \"time inactive\",       nvl(s.username, 'ORACLE PROCESS') username,       s.sid,     s.serial#,       logon_time,       s.machine,       s.program,       s.sid session_id,       trunc(buffer_gets / (executions + 1)) \"BUFF-EXEC\",       trunc(buffer_gets / (rows_processed + 1)) \"BUFF-ROWS\",       first_load_time,       executions,       parse_calls,       disk_reads,       buffer_gets,       rows_processed  from v$session s, v$sesstat ss, v$statname sn, v$sqlarea sa where s.sid = ss.sid   and ss.statistic# = sn.statistic#   and sn.name = 'CPU used by this session'   and s.sql_address = sa.address   and s.sql_hash_value = sa.hash_value--   and status='ACTIVE'--   and username=''--   and last_call_et &gt; 1000 order by ss.value desc\nbigint类型转换为datetime类型假设 1164691264437 是 Java 里的“日期时间”：即：自1970-01-01 00:00:00以来的毫秒数\nmysql&gt; select from_unixtime(1164691264437&#x2F;1000);+-----------------------------------+| from_unixtime(1164691264437&#x2F;1000) |+-----------------------------------+| 2006-11-28 13:21:04               |+-----------------------------------+\ndatetime类型转换为bigint类型假设 “2011-05-31 23:59:59” 是 Java 里的“日期时间”：即：自1970-01-01 00:00:00以来的毫秒数\nmysql&gt; select UNIX_TIMESTAMP(&#39;2011-05-31 23:59:59&#39;);+-----------------------------------+| from_unixtime(1306857599&#x2F;1000) |+-----------------------------------+\n\nMYSQL查询锁# 查询是否锁表show OPEN TABLES where In_use &gt; 0;# 查看所有进程# MySQL:show processlist;# mariabd:show full processlist;# 杀掉指定mysql连接的进程号kill $pid# 查看正在锁的事务SELECT * FROM INFORMATION_SCHEMA.INNODB_LOCKS;# 查看等待锁的事务SELECT * FROM INFORMATION_SCHEMA.INNODB_LOCK_WAITS;# 查看innodb引擎的运行时信息show engine innodb status\\G;# 查看造成死锁的sql语句，分析索引情况，然后优化sql语句；# 查看服务器状态show status like &#39;%lock%&#39;;# 查看超时时间：show variables like &#39;%timeout%&#39;;\n","tags":["SQL"]},{"title":"SonarQube插件开发","url":"/2020/05/11/SonarQube%E6%8F%92%E4%BB%B6%E5%BC%80%E5%8F%91/","content":"\n官方文档\n最近涉及到开发sonarqube的一些插件工作，记录一下工作需要的内容。\n\nSonarQube开发分为三部分：web服务开发、计算引擎开发、扫描开发。针对这三种开发方式，其官方为我们提供了三种远程调试方式。\nDebugging web server extensions\n修改配置文件：conf/sonar.properties\nsonar.web.javaAdditionalOpts=-agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=8000\n\nDebugging compute engine extensions\n修改配置文件：conf/sonar.properties\nsonar.ce.javaAdditionalOpts=-agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=8000\n\nDebugging scanner extensions\n控制台命令窗口设置环境变量\nexport SONAR_SCANNER_OPTS=\"-agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=8000\"\nset SONAR_SCANNER_OPTS=\"-agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=8000\"\n然后在需要检查的代码根路径执行扫描\nmvnDebug sonar:sonar\n或者\nsonar-scanner\n","tags":["SonarQube"]},{"title":"javaagent开发指南","url":"/2020/04/15/javaagent%E5%BC%80%E5%8F%91%E6%8C%87%E5%8D%97/","content":"场景：你需要在Java进程中获取所有已知加载类的字节码。或者你想要调试运行时发生的某种类型的instrumentation。\n看这篇文章前你需要了解：Java字节码。\n下面介绍如何获取加载到JVM中的所有类的字节码。\njavaagent\n通常，我们可以把javaagent当成一个JVM插件。一种专门的jar文件，它可以利用JVM提供的InstrumentationAPI。Java1.5提供了InstrumentationAPI。\n\n\n成功创建一个javaagent需要三个环节：\n代理类代理类必须有premain方法，当Java虚拟机启动时，在执行main函数之前，JVM会先运行-javaagent所指定jar包内Premain-Class这个类的premain方法，其中，该方法可以签名如下：\n\npublicstaticvoidpremain(StringagentArgs,Instrumentationinst)\npublicstaticvoidpremain(StringagentArgs)\n\nJVM会优先加载1签名的方法，加载成功忽略2，如果1没有，加载2方法。这个逻辑在sun.instrument.InstrumentationImpl类中。\n一些元信息（告诉JVM为我们的代理类提供哪些功能）定义一个MANIFEST.MF文件，必须包含Premain-Class选项，且指定我们的代理类，通常也会加入Can-Redefine-Classes和Can-Retransform-Classes选项。\n将代理类和MANIFEST.MF文件打成jar包。\n一种使JVM加载jar文件和代理的方式使用参数-javaagent:agent.jar=[agentArgs]启动要代理类中的premain方法。例如：\njava-javaagent:sample-agent.jar=hello-jarsample-release.jar\n\n\nagentmain\n定义一个MANIFEST.MF文件，文件中必须包含Agent-Class\n创建一个Agent-Class指定的类，该类必须包含agentmain方法（参数和premian相同）\n将MANIFEST.MF和Agent类打成jar包\n将jar包载入目标虚拟机。目标虚拟机将会自动执行agentmain方法执行方法逻辑，同时，ClassFileTransformer也会长期有效，在每一个类加载器加载Class的时候都会拦截\n\n\n相关文章：https://www.cnblogs.com/stateis0/p/9062199.htmlhttps://www.cnblogs.com/stateis0/p/9062201.htmlhttps://www.jrebel.com/blog/java-bytecode-tutorialhttps://blogs.oracle.com/ouchina/javaagent\n","tags":["Java"]},{"title":"char、varchar、varchar2的区别","url":"/2020/04/08/char%E3%80%81varchar%E3%80%81varchar2%E7%9A%84%E5%8C%BA%E5%88%AB/","content":"1、长度上的区别\nCHAR的长度是固定的，VARCHAR2的长度是可以变化的。\n例如，存储字符zhidao串“abc”，对于CHAR (20)，表示存储的字符占20个字节，而同样的VARCHAR2 (20)就只占3个字节的长度，20只是最大值，而且当存储的字符小于20时，按实际的长度来存储。\n2、意义上的区别\nVARCHAR是VARCHAR2的同义词，工业标准的VARCHAR类型可以用来存储空字符串，但是Oracle自己开发了一个数据类型VARCHAR2，这个类型不是一个标准的VARCHAR，它在数据库中varchar列可以存储空字符串的特性改为存储NULL值。\n\n3、空间大小上的区别\nVARCHAR2比CHAR要节省空间，VARCHAR2在效率上也比CHAR差一些，所以如果想获得效率，就必须牺牲一定的空间，这就是在数据库设计上常说的‘以空间换效率’。\nVARCHAR2虽然比CHAR节省空间，但如果一个VARCHAR2列经常被修改，且每次被修改数据的长度不同会引起‘行迁移’现象。\n","tags":["数据库","Oracle"]},{"title":"Wireshark操作指南","url":"/2020/07/22/Wireshark%E6%93%8D%E4%BD%9C%E6%8C%87%E5%8D%97/","content":"简介就不介绍了，百度去吧。附上下载地址\nhttps://www.wireshark.org/download.html\n操作记录查看数据包是哪个进程发送的\n背景：今天使用wireshark分析数据包，看到一个奇怪的包。我什么时候给小米请求东西了。\n\n\n\n选择这个包，我们看到下面一系列参数，Src Port：64240说明我们这个端口被占用\n\n\n\n打开命令提示符，查询这个端口被PID为8536的进程占用\n\nC:\\Users\\lenovo&gt;netstat -ano|findstr 64240  TCP    192.168.96.142:64240   118.26.252.226:5222    ESTABLISHED     8536\n\n\n开始想到打开任务管理器，结果发现没有这个PID\n\n\n\n可以使用命令提示符\n\nC:\\Users\\lenovo&gt;tasklist|findstr 8536wps.exe                       8536 Console                    1     12,000 K\n\n\n系统工具里面有个资源监视器，找不到可以直接搜索，打开，选择网路，然后PID排个序就找到8536了，原来是WPS，太坑了，立马干掉他\n\n\n\n相关命令\n\ntaskkill /f /t /im Tencentdl.exe\n\nFoxmail收取邮件发生了什么\n","tags":["Wireshark"]},{"title":"二叉树遍历","url":"/2020/04/01/%E4%BA%8C%E5%8F%89%E6%A0%91%E9%81%8D%E5%8E%86/","content":"\n二叉树遍历有三种遍历方式，前、中、后。这些不多介绍了。\n如果使用代码实现有可以有多种不同的方式。本文着重讲解这几种方式，并且探究不同方式的时间和空间复杂度。\n\n定义二叉树public class TreeNode &#123;    int val;    TreeNode left;    TreeNode right;    TreeNode(int x) &#123;        val = x;    &#125;&#125;\n\n前序遍历递归法public static void preorderTraversal(TreeNode root) &#123;    if (root != null) &#123;        System.out.println(root.val);        preorderTraversal(root.left);        preorderTraversal(root.right);    &#125;&#125;\n\n迭代法public List&lt;Integer&gt; preorderTraversal(TreeNode root) &#123;    LinkedList&lt;TreeNode&gt; stack = new LinkedList&lt;&gt;();    LinkedList&lt;Integer&gt; output = new LinkedList&lt;&gt;();    if (root == null) &#123;        return output;    &#125;    stack.add(root);    while (!stack.isEmpty()) &#123;        TreeNode node = stack.pollLast();        output.add(node.val);        if (node.right != null) &#123;            stack.add(node.right);        &#125;        if (node.left != null) &#123;            stack.add(node.left);        &#125;    &#125;    return output;&#125;\n\n莫里斯法public List&lt;Integer&gt; preorderTraversal(TreeNode root) &#123;    LinkedList&lt;Integer&gt; output = new LinkedList&lt;&gt;();    TreeNode node = root;    while (node != null) &#123;        if (node.left == null) &#123;            output.add(node.val);            node = node.right;        &#125;        else &#123;            TreeNode predecessor = node.left;            while ((predecessor.right != null) &amp;&amp; (predecessor.right != node)) &#123;                predecessor = predecessor.right;            &#125;            if (predecessor.right == null) &#123;                output.add(node.val);                predecessor.right = node;                node = node.left;            &#125;            else&#123;                predecessor.right = null;                node = node.right;            &#125;        &#125;    &#125;    return output;&#125;\n\n中序遍历递归法public static void inorderTraversal(TreeNode root) &#123;    if (root != null) &#123;        inorderTraversal(root.left);        System.out.println(root.val);        inorderTraversal(root.right);    &#125;&#125;\n\n迭代法public List&lt;Integer&gt; inorderTraversal(TreeNode root) &#123;    List&lt;Integer&gt; res = new ArrayList&lt;&gt;();    Stack&lt;TreeNode&gt; stack = new Stack&lt;&gt;();    TreeNode curr = root;    while (curr != null || !stack.isEmpty()) &#123;        while (curr != null) &#123;            stack.push(curr);            curr = curr.left;        &#125;        curr = stack.pop();        res.add(curr.val);        curr = curr.right;    &#125;    return res;&#125;\n\n莫里斯法public List&lt;Integer&gt; inorderTraversal(TreeNode root) &#123;    List&lt;Integer&gt; ldr = new ArrayList&lt;Integer&gt;();    TreeNode cur = root;    TreeNode pre = null;    while(cur!=null)&#123;        if(cur.left==null)&#123;//左子树为空，输出当前节点，将其右孩子作为当前节点            ldr.add(cur.val);            cur = cur.right;        &#125;        else&#123;            pre = cur.left;//左子树            while(pre.right!=null&amp;&amp;pre.right!=cur)&#123;//找到前驱节点，即左子树中的最右节点                pre = pre.right;            &#125;            //如果前驱节点的右孩子为空，将它的右孩子设置为当前节点。当前节点更新为当前节点的左孩子。            if(pre.right==null)&#123;                pre.right = cur;                cur = cur.left;            &#125;            //如果前驱节点的右孩子为当前节点，将它的右孩子重新设为空（恢复树的形状）。输出当前节点。当前节点更新为当前节点的右孩子。            if(pre.right==cur)&#123;                pre.right = null;                ldr.add(cur.val);                cur = cur.right;            &#125;        &#125;    &#125;    return ldr;&#125;\n\n后序遍历递归法public static void postorderTraversal(TreeNode root) &#123;    if (root != null) &#123;        postorderTraversal(root.left);        postorderTraversal(root.right);        System.out.println(root.val);    &#125;&#125;\n\n迭代法public List&lt;Integer&gt; postorderTraversal(TreeNode root) &#123;    LinkedList&lt;TreeNode&gt; stack = new LinkedList&lt;&gt;();    LinkedList&lt;Integer&gt; output = new LinkedList&lt;&gt;();    if (root == null) &#123;        return output;    &#125;    stack.add(root);    while (!stack.isEmpty()) &#123;        TreeNode node = stack.pollLast();        output.addFirst(node.val);        if (node.left != null) &#123;            stack.add(node.left);        &#125;        if (node.right != null) &#123;            stack.add(node.right);        &#125;    &#125;    return output;&#125;\n\n莫里斯法\n\n总结\n\n\n\n时间复杂度\n空间复杂度\n\n\n\n递归\n\n\n\n\n迭代\n\n\n\n\n莫里斯\n\n\n\n\n","tags":["算法相关"]},{"title":"元注解介绍","url":"/2020/01/09/%E5%85%83%E6%B3%A8%E8%A7%A3%E4%BB%8B%E7%BB%8D/","content":"元注解：元注解的作用就是负责注解其他注解。Java5.0定义了4个标准的meta-annotation类型，它们被用来提供对其它 annotation类型作说明。Java5.0定义的元注解：\n\n@Target,\n@Retention,\n@Documented,\n@Inherited　\n\n这些类型和它们所支持的类在java.lang.annotation包中可以找到。下面我们看一下每个元注解的作用和相应分参数的使用说明。\n@Target说明了Annotation所修饰的对象范围：Annotation可被用于 packages、types（类、接口、枚举、Annotation类型）、类型成员（方法、构造方法、成员变量、枚举值）、方法参数和本地变量（如循环变量、catch参数）。在Annotation类型的声明中使用了target可更加明晰其修饰的目标。\n\n作用：用于描述注解的使用范围（即：被描述的注解可以用在什么地方）\n取值(ElementType)有：\nCONSTRUCTOR:用于描述构造器\nFIELD:用于描述域\nLOCAL_VARIABLE:用于描述局部变量\nMETHOD:用于描述方法\nPACKAGE:用于描述包\nPARAMETER:用于描述参数\nTYPE:用于描述类、接口(包括注解类型) 或enum声明\n\n\n\n@Retention定义了该Annotation被保留的时间长短：某些Annotation仅出现在源代码中，而被编译器丢弃；而另一些却被编译在class文件中；编译在class文件中的Annotation可能会被虚拟机忽略，而另一些在class被装载时将被读取（请注意并不影响class的执行，因为Annotation与class在使用上是被分离的）。使用这个meta-Annotation可以对 Annotation的“生命周期”限制。\n\n作用：表示需要在什么级别保存该注释信息，用于描述注解的生命周期（即：被描述的注解在什么范围内有效）\n取值（RetentionPoicy）有：\nSOURCE:在源文件中有效（即源文件保留）\nCLASS:在class文件中有效（即class保留）\nRUNTIME:在运行时有效（即运行时保留）\n\n\n\n@Documented用于描述其它类型的annotation应该被作为被标注的程序成员的公共API，因此可以被例如javadoc此类的工具文档化。Documented是一个标记注解，没有成员。\n@Inherited元注解是一个标记注解，@Inherited阐述了某个被标注的类型是被继承的。如果一个使用了@Inherited修饰的annotation类型被用于一个class，则这个annotation将被用于该class的子类。\n\n注意：@Inherited annotation类型是被标注过的class的子类所继承。类并不从它所实现的接口继承annotation，方法并不从它所重载的方法继承annotation。\n\n当@Inherited annotation类型标注的annotation的Retention是RetentionPolicy.RUNTIME，则反射API增强了这种继承性。如果我们使用java.lang.reflect去查询一个@Inherited annotation类型的annotation时，反射代码检查将展开工作：检查class和其父类，直到发现指定的annotation类型被发现，或者到达类继承结构的顶层。\n\n\n@自定义注解使用@interface自定义注解时，自动继承了java.lang.annotation.Annotation接口，由编译程序自动完成其他细节。在定义注解时，不能继承其他的注解或接口。@interface用来声明一个注解，其中的每一个方法实际上是声明了一个配置参数。方法的名称就是参数的名称，返回值类型就是参数的类型（返回值类型只能是基本类型、Class、String、enum）。可以通过default来声明参数的默认值。\n定义注解格式：\npublic @interface 注解名 {定义体}\n注解参数的可支持数据类型：\n\n所有基本数据类型（int,float,boolean,byte,double,char,long,short)\nString类型\nClass类型\nenum类型\nAnnotation类型\n以上所有类型的数组\n\nAnnotation类型里面的参数该怎么设定:\n\n只能用public或默认(default)这两个访问权修饰.例如,String value();这里把方法设为default默认类型 　\n参数成员只能用基本类型byte,short,char,int,long,float,double,boolean八种基本数据类型和 String,Enum,Class,annotations等数据类型,以及这一些类型的数组.例如,String value();这里的参数成员就为String\n如果只有一个参数成员,最好把参数名称设为”value”,后加小括号.例:下面的例子FruitName注解就只有一个参数成员\n\n","tags":["Java"]},{"title":"哀悼日网站全站变灰代码","url":"/2020/04/07/%E5%93%80%E6%82%BC%E6%97%A5%E7%BD%91%E7%AB%99%E5%85%A8%E7%AB%99%E5%8F%98%E7%81%B0%E4%BB%A3%E7%A0%81/","content":"第一种：修改CSS文件html &#123;    filter: progid:DXImageTransform.Microsoft.BasicImage(grayscale=1);    -webkit-filter: grayscale(100%);&#125;\n\n第二种：在网页的&lt;head&gt;标签内加入以下代码&lt;style type=\"text/css\"&gt;    html &#123;        filter: progid:DXImageTransform.Microsoft.BasicImage(grayscale=1);        -webkit-filter: grayscale(100%);&#125;&lt;/style&gt;\n\n第三种：修改&lt;html&gt;标签加入内联样式&lt;html style=\"filter: progid:DXImageTransform.Microsoft.BasicImage(grayscale=1);             -webkit-filter: grayscale(100%);\"&gt;\n\n第四种：bodybody *&#123;    -webkit-filter: grayscale(100%); /* webkit */    -moz-filter: grayscale(100%); /*firefox*/    -ms-filter: grayscale(100%); /*ie9*/    -o-filter: grayscale(100%); /*opera*/    filter: grayscale(100%);    filter:progid:DXImageTransform.Microsoft.BasicImage(grayscale=1);    filter:gray; /*ie9- */&#125;\n\n第五种：Nginx sub_filterlocation / &#123;    root   /opt/app/code/;    random_index on;    index  index.html index.htm;    sub_filter '&lt;h1&gt;Admin' '&lt;h1&gt;ggggg';  //第一个参数是要被替换的，第二个参数是替换后的    sub_filter_once off;   //替换所有的，默认是on，替换第一个&#125;\n","tags":["前端开发"]},{"title":"关于Java中的变量","url":"/2019/11/28/%E5%85%B3%E4%BA%8EJava%E4%B8%AD%E7%9A%84%E5%8F%98%E9%87%8F/","content":"成员变量\n存在于堆内存中，和类一起创建\n\n\n实例变量（不以static修饰）\n\n实例变量则从该类的实例被创建起开始存在，直到系统完全销毁这个实例，实例变量的作用域与对应实例的生存范围相同\n\n类变量（以static修饰）\n\n类变量从该类的准备阶段起开始存在，直到系统完全销毁这个类，类变量的作用域与这个类的生存范围相同\n局部变量\n存在于栈内存中，当方法执行完成，回收内存\n\n\n形参（方法签名中定义的变量）\n\n在定义方法签名时定义的变量，形参的作用域在整个方法中都有效\n\n方法局部变量（在方法内定义）\n\n在方法体内定义的局部变量，它的作用域是从定义该变量的地方生效，到该方法结束时失效\n\n代码块局部变量（在代码块内定义）\n\n这个局部变量的作用域从定义该变量的地方生效，到该代码结束时失效。\n","tags":["Java"]},{"title":"常见网络协议报文头格式","url":"/2020/07/22/%E5%B8%B8%E8%A7%81%E7%BD%91%E7%BB%9C%E5%8D%8F%E8%AE%AE%E6%8A%A5%E6%96%87%E5%A4%B4%E6%A0%BC%E5%BC%8F/","content":"序言常见的一些协议类型\nUDPUDP报头结构\n\n源端口（16bits）：用来标识源端应用进程\n目的端口（16bits）：用来标识目的端应用进程\n长度字段（16bits）：标明UDP头部和UDP数据的总长度，字节计\n校验和（16bits）：用来对UDP头部和UDP数据进行校验，需要添加UDP伪头部参与计算\n\nTCPTCP报头结构\n\n源端口(Source Port，16bits)：源端口字段包含初始化通信的端口号。源端口和IP地址的作用是标识报文的返回地址。\n目的端口(Destination Port，16bits)：目的端口字段定义传输的目的地。这个端口指明接收方计算机上的应用程序接口。\n序列号（Sequence Number，32bits）：该字段用来标识TCP源端设备向目的端设备发送的字节流，它表示在这个报文段中的第几个数据字节。\n确认号（Acknowledge Number，32bits）：TCP使用32位的确认号字段标识期望收到的下一个段的第一个字节，并声明此前的所有数据已经正确无误地收到。因此，确认号应该是上次已成功收到的数据字节序列号加1。收到确认号的源计算机会知道特定的段已经被收到。确认号的字段只在ACK标志被设置时才有效。\n数据偏移（Data Offset，4bits）：该字段字段表示TCP头部大小，以4字节为单位，最长60字节。\n保留字段（Reserved，6bits）：为将来定义新的用途保留，均置0。\n控制位(Control Bits，6bits)：共6位，每一位标志可以打开一个控制功能。\nURG(Urgent Pointer Field Significant，1bit)：紧急指针字段标志，与紧急指针字段配合使用。表示TCP包的紧急指针字段有效，用来保证TCP连接不被中断，并且督促中间齐备尽快处理这些数据。\nACK（Acknowledgement field significant，1bit）：确认字段标志。取1时表示应答字段有效，也即TCP应答号将包含在TCP段中，为0无效。\nPSH(Push Function，1bit)：推功能。Push操作指在数据包到达接收端以后，立即送给应用层/应用程序，而不是在缓冲区中排队，等填满之后再向上交付。\nRST（Reset the connection，1bit）：重置连接。这个标志表示表示连接复位请求，用来复位那些产生错误的连接，也被用来拒绝错误和非法的数据包。当RST=1时，表示呈现严重错误，必须断开连接，然后再重建传输连接。\nSYN（Synchronize sequence numbers，1bit）：同步序列号。表示同步序号，用来建立连接。\nFIN（No more data from sender，1bit）：表示发送端已经发送到数据末尾，数据传送完成，发送FIN标志位的TCP段，连接将被断开。\n\n\n窗口（Window，16bits）：默示报文段发送方的接管窗口，单位为字节。此窗口告诉对方，“在未收到我的确认时，你可以发送的数据的字节数至多是此窗口的大小“。\n校验和（Checksum，16bits）：TCP头包括16位的校验和字段用于错误检查。源主机基于部分IP头信息，TCP头和数据内容计算一个校验和，目的主机也要进行相同的计算，如果收到的内容没有错误，两个计算应该完全一样，从而证明数据的有效性。\n紧急指针（Urgent Pointer，16bits）：紧急指针字段是一个可选的16位指针，指向段内的最后一个字节位置，这个字段只在URG标志被设置时才有效。\n选项（Option，长度不定）：至少1字节的可变长字段，标识哪个选项（有多种选项类型，比如”窗口扩大因子”、”时间戳”等选项）有效。如果没有选项，这个字节等于0，说明选项的结束。这个字节等于1表示无需再有操作；等于2表示下四个字节包括源机器的最大长度（Maximum Segment Size，MSS）等。\n填充（Padding，长度不定）：这个字段中加入额外的零，以保证TCP头是32比特的整数倍。\n\n","tags":["网络"]},{"title":"树状结构数据库设计","url":"/2020/05/25/%E6%A0%91%E7%8A%B6%E7%BB%93%E6%9E%84%E6%95%B0%E6%8D%AE%E5%BA%93%E8%AE%BE%E8%AE%A1/","content":"\n了解Modified Preorder Tree\n\nid，本节点的primary keyparent_id，其值为父节点的primary keykey，忘了学名叫啥了，你可以称为线索level，表示当前节点到根节点的距离其中，key字段的值为：从跟节点到父节点的primary key，中间用任意非数字符号分割。\n例如以下树状结构\n├── a│   ├── d│   │   ├── p│   │   ├── q│   │   └── r│   ├── e│   └── f├── b│   ├── x│   ├── y│   └── z├── c\n对应的数据库表值为：\n| id | value | parent_id | key    | level || 1  | a     | 0         | &quot;-&quot;    | 1     || 2  | b     | 0         | &quot;-&quot;    | 1     || 3  | c     | 0         | &quot;-&quot;    | 1     || 4  | d     | 1         | &quot;1-&quot;   | 2     || 5  | e     | 1         | &quot;1-&quot;   | 2     || 6  | f     | 1         | &quot;1-&quot;   | 2     || 7  | x     | 2         | &quot;2-&quot;   | 2     || 8  | y     | 2         | &quot;2-&quot;   | 2     || 9  | z     | 2         | &quot;2-&quot;   | 2     || 10 | p     | 4         | &quot;1-4-&quot; | 3     || 11 | q     | 4         | &quot;1-4-&quot; | 3     || 12 | r     | 4         | &quot;1-4-&quot; | 3     |\n于是，在给定一个节点d的时候，查找d的所有子孙节点：select * from table_name where key like &quot;${d.key}-${d.id}-%&quot;查找某个节点的所有子节点：select * from table_name where key like &quot;${d.key}-${d.id}-%&quot; and level=${d.level}+1这个设计，结构非常简单。key和level是辅助字段，维护这两个字段成本很低，即使全部重建要比MPT简单多了。\n","tags":["数据库"]},{"title":"关于JVM对反射调用的优化","url":"/2019/11/12/%E5%85%B3%E4%BA%8EJVM%E5%AF%B9%E5%8F%8D%E5%B0%84%E8%B0%83%E7%94%A8%E7%9A%84%E4%BC%98%E5%8C%96/","content":"关于JVM对反射调用的优化Java中对反射的优化使用反射调用某个类的方法，jvm内部有两种方式\n1. JNI使用native方法进行反射操作。\n2. pure-Java生成bytecode进行反射操作，即生成类sun.reflect.GeneratedMethodAccessor，它是一个被反射调用方法的包装类，代理不同的方法，类后缀序号会递增。这种方式第一次调用速度较慢，较之第一种会慢3-4倍，但是多次调用后速度会提升20倍\n对于使用JNI的方式，因为每次都要调用native方法再返回，速度会比较慢。所以，当一个方法被反射调用的次数超过一定次数（默认15次）时，JVM内部会进行优化，使用第2种方法，来加快运行速度。\nJVM有两个参数来控制这种优化\n-Dsun.reflect.inflationThreshold=&lt;value&gt;\nvalue默认为15，即反射调用某个方法15次后，会由JNI的方式变为pure-java的方式\n-Dsun.reflect.noInflation=true\n默认为false。当设置为true时，表示在第一次反射调用时，就转为pure-java的方式\n下面是一个验证反射优化的样例：\npublic class TestMethodInvoke &#123;    public static void main(String[] args) throws Exception &#123;        Class&lt;?&gt; clz = Class.forName(\"A\");        Object o = clz.newInstance();        Method m = clz.getMethod(\"foo\", String.class);        for (int i = 0; i &lt; 100; i++) &#123;            m.invoke(o, Integer.toString(i));        &#125;    &#125;&#125;\n\npublic class A &#123;    public void foo(String name) &#123;        System.out.println(\"Hello, \" + name);    &#125;&#125;\n\n配置如下JVM参数，使得在第一次反射调用时，就转为pure-java的方式\n-Dsun.reflect.noInflation=true\n如何关闭JVM对反射调用的优化？想关闭JVM对反射优化怎么办?\nJVM中只提供了两个参数，因此，没有办法完全关闭反射优化。\n一种能想到的接近于关闭反射优化的方法就是将inflationThreshold设为的一个特别大的数。\ninflationThreshold是java中的int型值，可以考虑把其设置为Integer.MAX_VALUE ((2^31)-1)。\n$ java-Dsun.reflect.inflationThreshold&#x3D;2147483647MyApp\n","tags":["Java"]},{"title":"如何保护自己的Java代码","url":"/2020/04/15/%E5%A6%82%E4%BD%95%E4%BF%9D%E6%8A%A4%E8%87%AA%E5%B7%B1%E7%9A%84Java%E4%BB%A3%E7%A0%81/","content":"\n代码混淆是为了防止反编译。如果没有对代码混淆，那么其他人很容易就可以得到你的项目中的所有代码。而混淆之后，其他人就没那么容易获得了。\n保护软件有着双重意义： 一是保护软件的知识产权 (intellectual property)， 防止被人盗用； 二是保护软件中可能隐含的诸如技术漏洞等私密信息， 防止被人利用。 就保护思路而言， 目前主要有两条： 一条是加密 (encryption)， 另一条是代码混淆 (obfuscation)。 两者的主要区别是前者需解密 (decryption)， 后者则不需要——因为后者只是将代码换成普通人难以读懂、 在计算机上却仍能运行， 且功能相同的形式， 很多网站采用的 JavaScript 代码混淆就是很好的例子。\n\nProGuard\n官方网站\nProGuard是最流行的Java字节码优化器。它使Java和Android应用程序的体积缩小了90％，速度提高了20％。ProGuard还通过混淆类，字段和方法的名称来提供最小的保护，以防止逆向工程。\nProGuard可以免费使用来处理您的应用程序，无论是否商业。ProGuard代码本身受版权保护，并根据GNU通用公共许可证（GPL）版本2进行分配。该用户手册也受版权保护，并且只能以其原始形式与未经修改的代码一起重新分发。\n\n看一下 https://www.guardsquare.com/en/products/proguard/manual/gui\n","tags":["代码混淆"]},{"title":"设计模式之Reactor模式","url":"/2020/07/14/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8BReactor%E6%A8%A1%E5%BC%8F/","content":"一、概念Reactor设计模式，是一种基于事件驱动的设计模式。它有一个或多个并发输入源，有一个Service Handler，和多个Request Handler，Service Handler会同步的将输入的请求（Event）多路复用的分发给相应的Request Handler。\n\n二、Reactor模式结构Reactor模式由5个角色组成。\n\nHandle：即操作系统中的句柄，是操作系统对资源的一种抽象，可以是打开的文件、一个连接(Socket)、Timer等。在网络编程中，一般指Socket Handle，文件描述符（fd）。将这个Handle注册到Synchronous Event Demultiplexer中，就可以它发生的事件，如READ、WRITE、CLOSE等事件。\nSynchronous Event Demultiplexer：同步事件多路分用器，本质上是系统调用。比如linux中的select、poll、epoll等。它会一直阻塞直在handle上，直到有事件发生时才会返回。\nInitiation Dispatcher：初始分发器，它提供了注册、删除与转发event handler的方法。当Synchronous Event Demultiplexer检测到handle上有事件发生时，便会通知initiation dispatcher调用特定的event handler的回调（handle_event()）方法。\nEvent Handler：事件处理器，定义事件处理的回调方法：handle_event()，以供InitiationDispatcher回调使用。\nConcrete Event Handler：具体的事件处理器，继承自Event Handler，在回调方法中会实现具体的业务逻辑。\n三、Reactor模式处理流程\n注册Concrete Event Handler到Initiation Dispatcher中，当Initiation Dispatcher在某种类型的事件发生时向其通知，事件与handle关联。\n\nInitiation Dispatcher调用每个Event Handler的get_handle接口获取其绑定的Handle。\n\nInitiation Dispatcher调用handle_events开始事件处理循环。在这里，Initiation Dispatcher会将步骤2获取的所有Handle都收集起来，使用Synchronous Event Demultiplexer来等待这些Handle的事件发生。\n\n当某个（或某几个）Handle的事件发生时，Synchronous Event Demultiplexer通知Initiation Dispatcher，select()根据发生事件的Handle找出对应的回调Handler。\n\nInitiation Dispatcher调用特定的Concrete Event Handler的回调方法（handel_event()）来响应其关联的handle上发生的事件。\n\n\n时序图：\n\n四、相关文章http://gee.cs.oswego.edu/dl/cpjslides/nio.pdf\nhttps://www.cnblogs.com/dafanjoy/p/11217708.html\nhttps://zhuanlan.zhihu.com/p/53191925\nhttps://cloud.tencent.com/developer/article/1513447\nhttps://blog.csdn.net/u010168160/article/details/53019039\n","tags":["设计模式"]},{"title":"设计模式之Proactor模式","url":"/2020/07/14/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8BProactor%E6%A8%A1%E5%BC%8F/","content":"Proactor模式结构Proactor主动器模式包含如下角色：\nHandle 句柄；用来标识socket连接或是打开文件；\nAsynchronous Operation Processor：异步操作处理器；负责执行异步操作，一般由操作系统内核实现；\nAsynchronous Operation：异步操作\nCompletion Event Queue：完成事件队列；异步操作完成的结果放到队列中等待后续使用\nProactor：主动器；为应用程序进程提供事件循环；从完成事件队列中取出异步操作的结果，分发调用相应的后续处理逻辑；\nCompletion Handler：完成事件接口；一般是由回调函数组成的接口；\nConcrete Completion Handler：完成事件处理逻辑；实现接口定义特定的应用处理逻辑；\nProactor模式时序图\n\n应用程序启动，调用异步操作处理器提供的异步操作接口函数，调用之后应用程序和异步操作处理就独立运行；应用程序可以调用新的异步操作，而其它操作可以并发进行。\n\n应用程序启动Proactor主动器，进行无限的事件循环，等待完成事件到来。\n\n异步操作处理器执行异步操作，完成后将结果放入到完成事件队列。\n\n主动器从完成事件队列中取出结果，分发到相应的完成事件回调函数处理逻辑中。\n\n\n","tags":["设计模式"]},{"title":"深入理解Reactor和Proactor","url":"/2020/07/14/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3Reactor%E5%92%8CProactor/","content":"1、标准定义两种I/O多路复用模式：Reactor和Proactor\n一般地,I/O多路复用机制都依赖于一个事件多路分离器(Event Demultiplexer)。分离器对象可将来自事件源的I/O事件分离出来，并分发到对应的read/write事件处理器(Event Handler)。开发人员预先注册需要处理的事件及其事件处理器（或回调函数）；事件分离器负责将请求事件传递给事件处理器。\n两个与事件分离器有关的模式是Reactor和Proactor。Reactor模式采用同步IO，而Proactor采用异步IO。\n在Reactor中，事件分离器负责等待文件描述符或socket为读写操作准备就绪，然后将就绪事件传递给对应的处理器，最后由处理器负责完成实际的读写工作。\n而在Proactor模式中，处理器–或者兼任处理器的事件分离器，只负责发起异步读写操作。IO操作本身由操作系统来完成。传递给操作系统的参数需要包括用户定义的数据缓冲区地址和数据大小，操作系统才能从中得到写出操作所需数据，或写入从socket读到的数据。事件分离器捕获IO操作完成事件，然后将事件传递给对应处理器。比如，在windows上，处理器发起一个异步IO操作，再由事件分离器等待IOCompletion事件。典型的异步模式实现，都建立在操作系统支持异步API的基础之上，我们将这种实现称为“系统级”异步或“真”异步，因为应用程序完全依赖操作系统执行真正的IO工作。\n举个例子，将有助于理解Reactor与Proactor二者的差异，以读操作为例（类操作类似）。\n在Reactor中实现读：\n\n注册读就绪事件和相应的事件处理器\n事件分离器等待事件\n事件到来，激活分离器，分离器调用事件对应的处理器。\n事件处理器完成实际的读操作，处理读到的数据，注册新的事件，然后返还控制权。\n\n在Proactor中实现读：\n\n处理器发起异步读操作（注意：操作系统必须支持异步IO）。在这种情况下，处理器无视IO就绪事件，它关注的是完成事件。\n事件分离器等待操作完成事件\n在分离器等待过程中，操作系统利用并行的内核线程执行实际的读操作，并将结果数据存入用户自定义缓冲区，最后通知事件分离器读操作完成。\n事件分离器呼唤处理器。\n事件处理器处理用户自定义缓冲区中的数据，然后启动一个新的异步操作，并将控制权返回事件分离器。\n\n可以看出，两个模式的相同点，都是对某个IO事件的事件通知(即告诉某个模块，这个IO操作可以进行或已经完成)。在结构上，两者也有相同点：demultiplexor负责提交IO操作(异步)、查询设备是否可操作(同步)，然后当条件满足时，就回调handler；不同点在于，异步情况下(Proactor)，当回调handler时，表示IO操作已经完成；同步情况下(Reactor)，回调handler时，表示IO设备可以进行某个操作(can read or can write)。\n2、通俗理解使用Proactor框架和Reactor框架都可以极大的简化网络应用的开发，但它们的重点却不同。\nReactor框架中用户定义的操作是在实际操作之前调用的。比如你定义了操作是要向一个SOCKET写数据，那么当该SOCKET可以接收数据的时候，你的操作就会被调用；而Proactor框架中用户定义的操作是在实际操作之后调用的。比如你定义了一个操作要显示从SOCKET中读入的数据，那么当读操作完成以后，你的操作才会被调用。\nProactor和Reactor都是并发编程中的设计模式。在我看来，他们都是用于派发/分离IO操作事件的。这里所谓的IO事件也就是诸如read/write的IO操作。”派发/分离”就是将单独的IO事件通知到上层模块。两个模式不同的地方在于，Proactor用于异步IO，而Reactor用于同步IO。\n","tags":["设计模式"]},{"title":"计算机中的回车符和换行符","url":"/2020/08/14/%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%B8%AD%E7%9A%84%E5%9B%9E%E8%BD%A6%E7%AC%A6%E5%92%8C%E6%8D%A2%E8%A1%8C%E7%AC%A6/","content":"概述\n\\r是回车符(carriage return，缩写CR)，使光标移至行首，十进制ASCII代码是13, 十六进制代码为0x0D\n\n\\n是换行符(line feed，缩写LF)，使光标下移一格，ASCII代码是10, 十六进制为0x0A\n\n\n不同系统中换行的表现形式Unix系统每行结尾只有“&lt;换行&gt;”，即\\n；十六进制：0A\nWindows系统里面每行结尾是“&lt;回车&gt;&lt;换行&gt;”，即\\r\\n；十六进制：0D 0A\nMac系统里每行结尾是“&lt;回车&gt;”,即\\r。十六进制：0D\n"},{"title":"缓存算法","url":"/2019/12/25/%E7%BC%93%E5%AD%98%E7%AE%97%E6%B3%95/","content":"FIFO算法FIFO 算法是一种比较容易实现的算法。它的思想是先进先出（FIFO，队列），这是最简单、最公平的一种思想，即如果一个数据是最先进入的，那么可以认为在将来它被访问的可能性很小。空间满的时候，最先进入的数据会被最早置换（淘汰）掉。\nFIFO 算法的描述：设计一种缓存结构，该结构在构造时确定大小，假设大小为 K，并有两个功能：\n\nset(key,value)：将记录(key,value)插入该结构。当缓存满时，将最先进入缓存的数据置换掉。\nget(key)：返回key对应的value值。\n\n实现：维护一个FIFO队列，按照时间顺序将各数据（已分配页面）链接起来组成队列，并将置换指针指向队列的队首。再进行置换时，只需把置换指针所指的数据（页面）顺次换出，并把新加入的数据插到队尾即可。\n缺点：判断一个页面置换算法优劣的指标就是缺页率，而FIFO算法的一个显著的缺点是，在某些特定的时刻，缺页率反而会随着分配页面的增加而增加，这称为Belady现象。产生Belady现象现象的原因是，FIFO置换算法与进程访问内存的动态特征是不相容的，被置换的内存页面往往是被频繁访问的，或者没有给进程分配足够的页面，因此FIFO算法会使一些页面频繁地被替换和重新申请内存，从而导致缺页率增加。因此，现在不再使用FIFO算法。\nLRU算法LRU（The Least Recently Used，最近最久未使用算法）是一种常见的缓存算法，在很多分布式缓存系统（如Redis, Memcached）中都有广泛使用。\nLRU算法的思想是：如果一个数据在最近一段时间没有被访问到，那么可以认为在将来它被访问的可能性也很小。因此，当空间满时，最久没有访问的数据最先被置换（淘汰）。\nLRU算法的描述： 设计一种缓存结构，该结构在构造时确定大小，假设大小为 K，并有两个功能：\n\nset(key,value)：将记录(key,value)插入该结构。当缓存满时，将最久未使用的数据置换掉。\nget(key)：返回key对应的value值。\n\n实现：最朴素的思想就是用数组+时间戳的方式，不过这样做效率较低。因此，我们可以用双向链表（LinkedList）+哈希表（HashMap）实现（链表用来表示位置，哈希表用来存储和查找），在Java里有对应的数据结构LinkedHashMap。\nLinkedHashMap利用Java的LinkedHashMap用非常简单的代码来实现基于LRU算法的Cache功能\nCopyimport java.util.LinkedHashMap;import java.util.Map;&#x2F;** * 简单用LinkedHashMap来实现的LRU算法的缓存 *&#x2F;public class LRUCache&lt;K, V&gt; extends LinkedHashMap&lt;K, V&gt; &#123;    private int cacheSize;    public LRUCache(int cacheSize) &#123;        super(16, (float) 0.75, true);        this.cacheSize &#x3D; cacheSize;    &#125;    protected boolean removeEldestEntry(Map.Entry&lt;K, V&gt; eldest) &#123;        return size() &gt; cacheSize;    &#125;&#125;\n\n测试：\nCopyimport org.slf4j.Logger;import org.slf4j.LoggerFactory;public class LRUCacheTest &#123;    private static final Logger log &#x3D; LoggerFactory.getLogger(LRUCacheTest.class);    private static LRUCache&lt;String, Integer&gt; cache &#x3D; new LRUCache&lt;&gt;(10);    public static void main(String[] args) &#123;        for (int i &#x3D; 0; i &lt; 10; i++) &#123;            cache.put(&quot;k&quot; + i, i);        &#125;        log.info(&quot;all cache :&#39;&#123;&#125;&#39;&quot;,cache);        cache.get(&quot;k3&quot;);        log.info(&quot;get k3 :&#39;&#123;&#125;&#39;&quot;, cache);        cache.get(&quot;k4&quot;);        log.info(&quot;get k4 :&#39;&#123;&#125;&#39;&quot;, cache);        cache.get(&quot;k4&quot;);        log.info(&quot;get k4 :&#39;&#123;&#125;&#39;&quot;, cache);        cache.put(&quot;k&quot; + 10, 10);        log.info(&quot;After running the LRU algorithm cache :&#39;&#123;&#125;&#39;&quot;, cache);    &#125;&#125;\n\nOutput:\nCopyall cache :&#39;&#123;k0&#x3D;0, k1&#x3D;1, k2&#x3D;2, k3&#x3D;3, k4&#x3D;4, k5&#x3D;5, k6&#x3D;6, k7&#x3D;7, k8&#x3D;8, k9&#x3D;9&#125;&#39;get k3 :&#39;&#123;k0&#x3D;0, k1&#x3D;1, k2&#x3D;2, k4&#x3D;4, k5&#x3D;5, k6&#x3D;6, k7&#x3D;7, k8&#x3D;8, k9&#x3D;9, k3&#x3D;3&#125;&#39;get k4 :&#39;&#123;k0&#x3D;0, k1&#x3D;1, k2&#x3D;2, k5&#x3D;5, k6&#x3D;6, k7&#x3D;7, k8&#x3D;8, k9&#x3D;9, k3&#x3D;3, k4&#x3D;4&#125;&#39;get k4 :&#39;&#123;k0&#x3D;0, k1&#x3D;1, k2&#x3D;2, k5&#x3D;5, k6&#x3D;6, k7&#x3D;7, k8&#x3D;8, k9&#x3D;9, k3&#x3D;3, k4&#x3D;4&#125;&#39;After running the LRU algorithm cache :&#39;&#123;k1&#x3D;1, k2&#x3D;2, k5&#x3D;5, k6&#x3D;6, k7&#x3D;7, k8&#x3D;8, k9&#x3D;9, k3&#x3D;3, k4&#x3D;4, k10&#x3D;10&#125;&#39;\n\nLFU算法LFU（Least Frequently Used ，最近最少使用算法）也是一种常见的缓存算法。\n顾名思义，LFU算法的思想是：如果一个数据在最近一段时间很少被访问到，那么可以认为在将来它被访问的可能性也很小。因此，当空间满时，最小频率访问的数据最先被淘汰。\nLFU 算法的描述：\n设计一种缓存结构，该结构在构造时确定大小，假设大小为 K，并有两个功能：\n\nset(key,value)：将记录(key,value)插入该结构。当缓存满时，将访问频率最低的数据置换掉。\nget(key)：返回key对应的value值。\n\n算法实现策略：考虑到 LFU 会淘汰访问频率最小的数据，我们需要一种合适的方法按大小顺序维护数据访问的频率。LFU  算法本质上可以看做是一个 top K 问题(K =  1)，即选出频率最小的元素，因此我们很容易想到可以用二项堆来选择频率最小的元素，这样的实现比较高效。最终实现策略为小顶堆+哈希表。\n","tags":["算法相关"]},{"title":"02 | 如何抓住重点，系统高效地学习数据结构与算法？","url":"/2020/08/07/02%E5%A6%82%E4%BD%95%E6%8A%93%E4%BD%8F%E9%87%8D%E7%82%B9%EF%BC%8C%E7%B3%BB%E7%BB%9F%E9%AB%98%E6%95%88%E5%9C%B0%E5%AD%A6%E4%B9%A0%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/","content":"你是否曾跟我一样，因为看不懂数据结构和算法，而一度怀疑是自己太笨？实际上，很多人在第一次接触这门课时，都会有这种感觉，觉得数据结构和算法很抽象，晦涩难懂，宛如天书。正是这个原因，让很多初学者对这门课望而却步。\n我个人觉得，其实真正的原因是你没有找到好的学习方法，没有抓住学习的重点。实际上，数据结构和算法的东西并不多，常用的、基础的知识点更是屈指可数。只要掌握了正确的学习方法，学起来并没有看上去那么难，更不需要什么高智商、厚底子。\n还记得大学里每次考前老师都要划重点吗？今天，我就给你划划我们这门课的重点，再告诉你一些我总结的学习小窍门。相信有了这些之后，你学起来就会有的放矢、事半功倍了。\n什么是数据结构？什么是算法？大部分数据结构和算法教材，在开篇都会给这两个概念下一个明确的定义。但是，这些定义都很抽象，对理解这两个概念并没有实质性的帮助，反倒会让你陷入死抠定义的误区。毕竟，我们现在学习，并不是为了考试，所以，概念背得再牢，不会用也就没什么用。\n虽然我们说没必要深挖严格的定义，但是这并不等于不需要理解概念。 下面我就从广义和狭义两个层面，来帮你理解数据结构与算法这两个概念。\n从广义上讲，数据结构就是指一组数据的存储结构。算法就是操作数据的一组方法。\n图书馆储藏书籍你肯定见过吧？为了方便查找，图书管理员一般会将书籍分门别类进行“存储”。按照一定规律编号，就是书籍这种“数据”的存储结构。\n那我们如何来查找一本书呢？有很多种办法，你当然可以一本一本地找，也可以先根据书籍类别的编号，是人文，还是科学、计算机，来定位书架，然后再依次查找。笼统地说，这些查找方法都是算法。\n从狭义上讲，也就是我们专栏要讲的，是指某些著名的数据结构和算法，比如队列、栈、堆、二分查找、动态规划等。这些都是前人智慧的结晶，我们可以直接拿来用。我们要讲的这些经典数据结构和算法，都是前人从很多实际操作场景中抽象出来的，经过非常多的求证和检验，可以高效地帮助我们解决很多实际的开发问题。\n那数据结构和算法有什么关系呢？为什么大部分书都把这两个东西放到一块儿来讲呢？\n这是因为，数据结构和算法是相辅相成的。数据结构是为算法服务的，算法要作用在特定的数据结构之上。 因此，我们无法孤立数据结构来讲算法，也无法孤立算法来讲数据结构。\n比如，因为数组具有随机访问的特点，常用的二分查找算法需要用数组来存储数据。但如果我们选择链表这种数据结构，二分查找算法就无法工作了，因为链表并不支持随机访问。\n数据结构是静态的，它只是组织数据的一种方式。如果不在它的基础上操作、构建算法，孤立存在的数据结构就是没用的。\n现在你对数据结构与算法是不是有了比较清晰的理解了呢？有了这些储备，下面我们来看看，究竟该怎么学数据结构与算法。\n学习这个专栏需要什么基础？看到数据结构和算法里的“算法”两个字，很多人就会联想到“数学”，觉得算法会涉及到很多深奥的数学知识。那我数学基础不是很好，学起来会不会很吃力啊？\n数据结构和算法课程确实会涉及一些数学方面的推理、证明，尤其是在分析某个算法的时间、空间复杂度的时候，但是这个你完全不需要担心。\n这个专栏不会像《算法导论》那样，里面有非常复杂的数学证明和推理。我会由浅入深，从概念到应用，一点一点给你解释清楚。你只要有高中数学水平，就完全可以学习。\n当然，我希望你最好有些编程基础，如果有项目经验就更好了。这样我给你讲数据结构和算法如何提高效率、如何节省存储空间，你就会有很直观的感受。因为，对于每个概念和实现过程，我都会从实际场景出发，不仅教你“是什么”，还会教你“为什么”，并且告诉你遇到同类型问题应该“怎么做”。\n学习的重点在什么地方？提到数据结构和算法，很多人就很头疼，因为这里面的内容实在是太多了。这里，我就帮你梳理一下，应该先学什么，后学什么。你可以对照看看，你属于哪个阶段，然后有针对性地进行学习。\n想要学习数据结构与算法，首先要掌握一个数据结构与算法中最重要的概念——复杂度分析。\n这个概念究竟有多重要呢？可以这么说，它几乎占了数据结构和算法这门课的半壁江山，是数据结构和算法学习的精髓。\n数据结构和算法解决的是如何更省、更快地存储和处理数据的问题，因此，我们就需要一个考量效率和资源消耗的方法，这就是复杂度分析方法。所以，如果你只掌握了数据结构和算法的特点、用法，但是没有学会复杂度分析，那就相当于只知道操作口诀，而没掌握心法。只有把心法了然于胸，才能做到无招胜有招！\n所以，复杂度分析这个内容，我会用很大篇幅给你讲透。你也一定要花大力气来啃，必须要拿下，并且要搞得非常熟练。否则，后面的数据结构和算法也很难学好。\n搞定复杂度分析，下面就要进入数据结构与算法的正文内容了。\n为了让你对数据结构和算法能有个全面的认识，我画了一张图，里面几乎涵盖了所有数据结构和算法书籍中都会讲到的知识点。\n（图谱内容较多，建议长按保存后浏览）\n但是，作为初学者，或者一个非算法工程师来说，你并不需要掌握图里面的所有知识点。很多高级的数据结构与算法，比如二分图、最大流等，这些在我们平常的开发中很少会用到。所以，你暂时可以不用看。我还是那句话，咱们学习要学会找重点。如果不分重点地学习，眉毛胡子一把抓，学起来肯定会比较吃力。\n所以，结合我自己的学习心得，还有这些年的面试、开发经验，我总结了20个最常用的、最基础数据结构与算法，不管是应付面试还是工作需要，只要集中精力逐一攻克这20个知识点就足够了。\n这里面有10个数据结构：数组、链表、栈、队列、散列表、二叉树、堆、跳表、图、Trie树；10个算法：递归、排序、二分查找、搜索、哈希算法、贪心算法、分治算法、回溯算法、动态规划、字符串匹配算法。\n掌握了这些基础的数据结构和算法，再学更加复杂的数据结构和算法，就会非常容易、非常快。\n在学习数据结构和算法的过程中，你也要注意，不要只是死记硬背，不要为了学习而学习，而是要学习它的“来历”“自身的特点”“适合解决的问题”以及“实际的应用场景”。对于每一种数据结构或算法，我都会从这几个方面进行详细讲解。只要你掌握了我每节课里讲的内容，就能在开发中灵活应用。\n学习数据结构和算法的过程，是非常好的思维训练的过程，所以，千万不要被动地记忆，要多辩证地思考，多问为什么。如果你一直这么坚持做，你会发现，等你学完之后，写代码的时候就会不由自主地考虑到很多性能方面的事情，时间复杂度、空间复杂度非常高的垃圾代码出现的次数就会越来越少。你的编程内功就真正得到了修炼。\n一些可以让你事半功倍的学习技巧前面我给你划了学习的重点，也讲了学习这门课需要具备的基础。作为一个过来人，现在我就给你分享一下，专栏学习的一些技巧。掌握了这些技巧，可以让你化被动为主动，学起来更加轻松，更加有动力！\n1.边学边练，适度刷题“边学边练”这一招非常有用。建议你每周花1～2个小时的时间，集中把这周的三节内容涉及的数据结构和算法，全都自己写出来，用代码实现一遍。这样一定会比单纯地看或者听的效果要好很多！\n有面试需求的同学，可能会问了，那我还要不要去刷题呢？\n我个人的观点是可以“适度”刷题，但一定不要浪费太多时间在刷题上。我们学习的目的还是掌握，然后应用。除非你要面试Google、Facebook这样的公司，它们的算法题目非常非常难，必须大量刷题，才能在短期内提升应试正确率。如果是应对国内公司的技术面试，即便是BAT这样的公司，你只要彻底掌握这个专栏的内容，就足以应对。\n2.多问、多思考、多互动学习最好的方法是，找到几个人一起学习，一块儿讨论切磋，有问题及时寻求老师答疑。 但是，离开大学之后，既没有同学也没有老师，这个条件就比较难具备了。\n不过，这也就是咱们专栏学习的优势。专栏里有很多跟你一样的学习者。你可以多在留言区写下自己的疑问、思考和总结，也可以经常看看别人的留言，和他们进行互动。\n除此之外，如果你有疑问，你可以随时在留言区给我留言，我只要有空就会及时回复你。你不要担心问的问题太小白。因为我初学的时候，也常常会被一些小白问题困扰。不懂一点都不丢人，只要你勇敢提出来，我们一起解决了就可以了。\n我也会力争每节课都最大限度地给你讲透，帮你扫除知识盲点，而你要做的就是，避免一知半解，要想尽一切办法去搞懂我讲的所有内容。\n3.打怪升级学习法学习的过程中，我们碰到最大的问题就是，坚持不下来。 是的，很多基础课程学起来都非常枯燥。为此，我自己总结了一套“打怪升级学习法”。\n游戏你肯定玩过吧？为什么很多看起来非常简单又没有乐趣的游戏，你会玩得不亦乐乎呢？这是因为，当你努力打到一定级别之后，每天看着自己的经验值、战斗力在慢慢提高，那种每天都在一点一点成长的成就感就不由自主地产生了。\n所以，我们在枯燥的学习过程中，也可以给自己设立一个切实可行的目标，就像打怪升级一样。\n比如，针对这个专栏，你就可以设立这样一个目标：每节课后的思考题都认真思考，并且回复到留言区。当你看到很多人给你点赞之后，你就会为了每次都能发一个漂亮的留言，而更加认真地学习。\n当然，还有很多其他的目标，比如，每节课后都写一篇学习笔记或者学习心得；或者你还可以每节课都找一下我讲得不对、不合理的地方……诸如此类，你可以总结一个适合你的“打怪升级攻略”。\n如果你能这样学习一段时间，不仅能收获到知识，你还会有意想不到的成就感。因为，这其实帮你改掉了一点学习的坏习惯。这个习惯一旦改掉了，你的人生也会变得不一样。\n4.知识需要沉淀，不要想试图一下子掌握所有在学习的过程中，一定会碰到“拦路虎”。如果哪个知识点没有怎么学懂，不要着急，这是正常的。因为，想听一遍、看一遍就把所有知识掌握，这肯定是不可能的。学习知识的过程是反复迭代、不断沉淀的过程。\n如果碰到“拦路虎”，你可以尽情地在留言区问我，也可以先沉淀一下，过几天再重新学一遍。所谓，书读百遍其义自见，我觉得是很有道理的！\n我讲的这些学习方法，不仅仅针对咱们这一个课程的学习，其实完全适用任何知识的学习过程。你可以通过这个专栏的学习，实践一下这些方法。如果效果不错，再推广到之后的学习过程中。\n内容小结今天，我带你划了划数据结构和算法的学习重点，复杂度分析，以及10个数据结构和10个算法。\n这些内容是我根据平时的学习和工作、面试经验积累，精心筛选出来的。只要掌握这些内容，应付日常的面试、工作，基本不会有问题。\n除此之外，我还给你分享了我总结的一些学习技巧，比如边学边练、多问、多思考，还有两个比较通用的学习方法，打怪升级法和沉淀法。掌握了这些学习技巧，可以让你学习过程中事半功倍。所以，你一定要好好实践哦！\n课后思考今天的内容是一个准备课，从下节开始，我们就要正式开始学习精心筛选出的这20个数据结构和算法了。所以，今天给你布置一个任务，对照我上面讲的“打怪升级学习法”，请思考一下你自己学习这个专栏的方法，让我们一起在留言区立下Flag，相互鼓励！\n另外，你在之前学习数据结构和算法的过程中，遇到过什么样的困难或者疑惑吗？\n欢迎留言和我分享，我会第一时间给你反馈。\n","categories":["数据结构与算法","入门篇"]},{"title":"01 | 为什么要学习数据结构和算法？","url":"/2020/08/07/01%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E5%AD%A6%E4%B9%A0%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E5%92%8C%E7%AE%97%E6%B3%95/","content":"你是不是觉得数据结构和算法，跟操作系统、计算机网络一样，是脱离实际工作的知识？可能除了面试，这辈子也用不着？\n尽管计算机相关专业的同学在大学都学过这门课程，甚至很多培训机构也会培训这方面的知识，但是据我了解，很多程序员对数据结构和算法依旧一窍不通。还有一些人也只听说过数组、链表、快排这些最最基本的数据结构和算法，稍微复杂一点的就完全没概念。\n当然，也有很多人说，自己实际工作中根本用不到数据结构和算法。所以，就算不懂这块知识，只要Java API、开发框架用得熟练，照样可以把代码写得“飞”起来。事实真的是这样吗？\n今天我们就来详细聊一聊，为什么要学习数据结构和算法。\n想要通关大厂面试，千万别让数据结构和算法拖了后腿很多大公司，比如BAT、Google、Facebook，面试的时候都喜欢考算法、让人现场写代码。有些人虽然技术不错，但每次去面试都会“跪”在算法上，很是可惜。那你有没有想过，为什么这些大公司都喜欢考算法呢？\n校招的时候，参加面试的学生通常没有实际项目经验，公司只能考察他们的基础知识是否牢固。社招就更不用说了，越是厉害的公司，越是注重考察数据结构与算法这类基础知识。相比短期能力，他们更看中你的长期潜力。\n你可能要说了，我不懂数据结构与算法，照样找到了好工作啊。那我是不是就不用学数据结构和算法呢？当然不是，你别忘了，我们学任何知识都是为了“用”的，是为了解决实际工作问题的，学习数据结构和算法自然也不例外。\n业务开发工程师，你真的愿意做一辈子CRUD boy吗？如果你是一名业务开发工程师，你可能要说，我整天就是做数据库CRUD（增删改查），哪里用得到数据结构和算法啊？\n是的，对于大部分业务开发来说，我们平时可能更多的是利用已经封装好的现成的接口、类库来堆砌、翻译业务逻辑，很少需要自己实现数据结构和算法。但是，不需要自己实现，并不代表什么都不需要了解。\n如果不知道这些类库背后的原理，不懂得时间、空间复杂度分析，你如何能用好、用对它们？存储某个业务数据的时候，你如何知道应该用ArrayList，还是Linked List呢？调用了某个函数之后，你又该如何评估代码的性能和资源的消耗呢？\n作为业务开发，我们会用到各种框架、中间件和底层系统，比如Spring、RPC框架、消息中间件、Redis等等。在这些基础框架中，一般都揉和了很多基础数据结构和算法的设计思想。\n比如，我们常用的Key-Value数据库Redis中，里面的有序集合是用什么数据结构来实现的呢？为什么要用跳表来实现呢？为什么不用二叉树呢？\n如果你能弄明白这些底层原理，你就能更好地使用它们。即便出现问题，也很容易就能定位。因此，掌握数据结构和算法，不管对于阅读框架源码，还是理解其背后的设计思想，都是非常有用的。\n在平时的工作中，数据结构和算法的应用到处可见。我来举一个你非常熟悉的例子：如何实时地统计业务接口的99%响应时间？\n你可能最先想到，每次查询时，从小到大排序所有的响应时间，如果总共有1200个数据，那第1188个数据就是99%的响应时间。很显然，每次用这个方法查询的话都要排序，效率是非常低的。但是，如果你知道“堆”这个数据结构，用两个堆可以非常高效地解决这个问题。\n基础架构研发工程师，写出达到开源水平的框架才是你的目标！现在互联网上的技术文章、架构分享、开源项目满天飞，照猫画虎做一套基础框架并不难。我就拿RPC框架举例。\n不同的公司、不同的人做出的RPC框架，架构设计思路都差不多，最后实现的功能也都差不多。但是有的人做出来的框架，Bug很多、性能一般、扩展性也不好，只能在自己公司仅有的几个项目里面用一下。而有的人做的框架可以开源到GitHub上给很多人用，甚至被Apache收录。为什么会有这么大的差距呢？\n我觉得，高手之间的竞争其实就在细节。这些细节包括：你用的算法是不是够优化，数据存取的效率是不是够高，内存是不是够节省等等。这些累积起来，决定了一个框架是不是优秀。所以，如果你还不懂数据结构和算法，没听说过大O复杂度分析，不知道怎么分析代码的时间复杂度和空间复杂度，那肯定说不过去了，赶紧来补一补吧！\n对编程还有追求？不想被行业淘汰？那就不要只会写凑合能用的代码！何为编程能力强？是代码的可读性好、健壮？还是扩展性好？我觉得没法列，也列不全。但是，在我看来，性能好坏起码是其中一个非常重要的评判标准。但是，如果你连代码的时间复杂度、空间复杂度都不知道怎么分析，怎么写出高性能的代码呢？\n你可能会说，我在小公司工作，用户量很少，需要处理的数据量也很少，开发中不需要考虑那么多性能的问题，完成功能就可以，用什么数据结构和算法，差别根本不大。但是你真的想“十年如一日”地做一样的工作吗？\n经常有人说，程序员35岁之后很容易陷入瓶颈，被行业淘汰，我觉得原因其实就在此。有的人写代码的时候，从来都不考虑非功能性的需求，只是完成功能，凑合能用就好；做事情的时候，也从来没有长远规划，只把眼前事情做好就满足了。\n我曾经面试过很多大龄候选人，简历能写十几页，经历的项目有几十个，但是细看下来，每个项目都是重复地堆砌业务逻辑而已，完全没有难度递进，看不出有能力提升。久而久之，十年的积累可能跟一年的积累没有任何区别。这样的人，怎么不会被行业淘汰呢？\n如果你在一家成熟的公司，或者BAT这样的大公司，面对的是千万级甚至亿级的用户，开发的是TB、PB级别数据的处理系统。性能几乎是开发过程中时刻都要考虑的问题。一个简单的ArrayList、Linked  List的选择问题，就可能会产生成千上万倍的性能差别。这个时候，数据结构和算法的意义就完全凸显出来了。\n其实，我觉得，数据结构和算法这个东西，如果你不去学，可能真的这辈子都用不到，也感受不到它的好。但是一旦掌握，你就会常常被它的强大威力所折服。之前你可能需要费很大劲儿来优化的代码，需要花很多心思来设计的架构，用了数据结构和算法之后，很容易就可以解决了。\n内容小结我们学习数据结构和算法，并不是为了死记硬背几个知识点。我们的目的是建立时间复杂度、空间复杂度意识，写出高质量的代码，能够设计基础架构，提升编程技能，训练逻辑思维，积攒人生经验，以此获得工作回报，实现你的价值，完善你的人生。\n所以，不管你是业务开发工程师，还是基础架构工程师；不管你是初入职场的初级工程师，还是工作多年的资深架构师，又或者是想转人工智能、区块链这些热门领域的程序员，数据结构与算法作为计算机的基础知识、核心知识，都是必须要掌握的。\n掌握了数据结构与算法，你看待问题的深度，解决问题的角度就会完全不一样。因为这样的你，就像是站在巨人的肩膀上，拿着生存利器行走世界。数据结构与算法，会为你的编程之路，甚至人生之路打开一扇通往新世界的大门。\n课后思考你为什么要学习数据结构和算法呢？在过去的软件开发中，数据结构和算法在哪些地方帮到了你？\n欢迎留言和我分享，我会第一时间给你反馈。如果你的朋友也在学习算法这个问题上犹豫不决，欢迎你把这篇文章分享给他！\n","categories":["数据结构与算法","入门篇"]},{"title":"07 | 链表（下）：如何轻松写出正确的链表代码？","url":"/2020/08/07/07%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%9A%E9%93%BE%E8%A1%A8%EF%BC%88%E4%B8%8B%EF%BC%89/","content":"上一节我讲了链表相关的基础知识。学完之后，我看到有人留言说，基础知识我都掌握了，但是写链表代码还是很费劲。哈哈，的确是这样的！\n想要写好链表代码并不是容易的事儿，尤其是那些复杂的链表操作，比如链表反转、有序链表合并等，写的时候非常容易出错。从我上百场面试的经验来看，能把“链表反转”这几行代码写对的人不足10%。\n为什么链表代码这么难写？究竟怎样才能比较轻松地写出正确的链表代码呢？\n只要愿意投入时间，我觉得大多数人都是可以学会的。比如说，如果你真的能花上一个周末或者一整天的时间，就去写链表反转这一个代码，多写几遍，一直练到能毫不费力地写出Bug free的代码。这个坎还会很难跨吗？\n当然，自己有决心并且付出精力是成功的先决条件，除此之外，我们还需要一些方法和技巧。我根据自己的学习经历和工作经验，总结了几个写链表代码技巧。如果你能熟练掌握这几个技巧，加上你的主动和坚持，轻松拿下链表代码完全没有问题。\n技巧一：理解指针或引用的含义事实上，看懂链表的结构并不是很难，但是一旦把它和指针混在一起，就很容易让人摸不着头脑。所以，要想写对链表代码，首先就要理解好指针。\n我们知道，有些语言有“指针”的概念，比如C语言；有些语言没有指针，取而代之的是“引用”，比如Java、Python。不管是“指针”还是“引用”，实际上，它们的意思都是一样的，都是存储所指对象的内存地址。\n接下来，我会拿C语言中的“指针”来讲解，如果你用的是Java或者其他没有指针的语言也没关系，你把它理解成“引用”就可以了。\n实际上，对于指针的理解，你只需要记住下面这句话就可以了：\n将某个变量赋值给指针，实际上就是将这个变量的地址赋值给指针，或者反过来说，指针中存储了这个变量的内存地址，指向了这个变量，通过指针就能找到这个变量。\n这句话听起来还挺拗口的，你可以先记住。我们回到链表代码的编写过程中，我来慢慢给你解释。\n在编写链表代码的时候，我们经常会有这样的代码：p-&gt;next=q。这行代码是说，p结点中的next指针存储了q结点的内存地址。\n还有一个更复杂的，也是我们写链表代码经常会用到的：p-&gt;next=p-&gt;next-&gt;next。这行代码表示，p结点的next指针存储了p结点的下下一个结点的内存地址。\n掌握了指针或引用的概念，你应该可以很轻松地看懂链表代码。恭喜你，已经离写出链表代码近了一步！\n技巧二：警惕指针丢失和内存泄漏不知道你有没有这样的感觉，写链表代码的时候，指针指来指去，一会儿就不知道指到哪里了。所以，我们在写的时候，一定注意不要弄丢了指针。\n指针往往都是怎么弄丢的呢？我拿单链表的插入操作为例来给你分析一下。\n\n如图所示，我们希望在结点a和相邻的结点b之间插入结点x，假设当前指针p指向结点a。如果我们将代码实现变成下面这个样子，就会发生指针丢失和内存泄露。\np-&gt;next &#x3D; x;  &#x2F;&#x2F; 将p的next指针指向x结点；x-&gt;next &#x3D; p-&gt;next;  &#x2F;&#x2F; 将x的结点的next指针指向b结点；\n初学者经常会在这儿犯错。p-&gt;next指针在完成第一步操作之后，已经不再指向结点b了，而是指向结点x。第2行代码相当于将x赋值给x-&gt;next，自己指向自己。因此，整个链表也就断成了两半，从结点b往后的所有结点都无法访问到了。\n对于有些语言来说，比如C语言，内存管理是由程序员负责的，如果没有手动释放结点对应的内存空间，就会产生内存泄露。所以，我们插入结点时，一定要注意操作的顺序，要先将结点x的next指针指向结点b，再把结点a的next指针指向结点x，这样才不会丢失指针，导致内存泄漏。所以，对于刚刚的插入代码，我们只需要把第1行和第2行代码的顺序颠倒一下就可以了。\n同理，删除链表结点时，也一定要记得手动释放内存空间，否则，也会出现内存泄漏的问题。当然，对于像Java这种虚拟机自动管理内存的编程语言来说，就不需要考虑这么多了。\n技巧三：利用哨兵简化实现难度首先，我们先来回顾一下单链表的插入和删除操作。如果我们在结点p后面插入一个新的结点，只需要下面两行代码就可以搞定。\nnew_node-&gt;next = p-&gt;next;p-&gt;next = new_node;\n但是，当我们要向一个空链表中插入第一个结点，刚刚的逻辑就不能用了。我们需要进行下面这样的特殊处理，其中head表示链表的头结点。所以，从这段代码，我们可以发现，对于单链表的插入操作，第一个结点和其他结点的插入逻辑是不一样的。\nif (head == null) &#123;    head = new_node;&#125;\n我们再来看单链表结点删除操作。如果要删除结点p的后继结点，我们只需要一行代码就可以搞定。\np-&gt;next = p-&gt;next-&gt;next;\n但是，如果我们要删除链表中的最后一个结点，前面的删除代码就不work了。跟插入类似，我们也需要对于这种情况特殊处理。写成代码是这样子的：\nif (head-&gt;next == null) &#123;    head = null;&#125;\n从前面的一步一步分析，我们可以看出，针对链表的插入、删除操作，需要对插入第一个结点和删除最后一个结点的情况进行特殊处理。这样代码实现起来就会很繁琐，不简洁，而且也容易因为考虑不全而出错。如何来解决这个问题呢？\n技巧三中提到的哨兵就要登场了。哨兵，解决的是国家之间的边界问题。同理，这里说的哨兵也是解决“边界问题”的，不直接参与业务逻辑。\n还记得如何表示一个空链表吗？head=null表示链表中没有结点了。其中head表示头结点指针，指向链表中的第一个结点。\n如果我们引入哨兵结点，在任何时候，不管链表是不是空，head指针都会一直指向这个哨兵结点。我们也把这种有哨兵结点的链表叫带头链表。相反，没有哨兵结点的链表就叫作不带头链表。\n我画了一个带头链表，你可以发现，哨兵结点是不存储数据的。因为哨兵结点一直存在，所以插入第一个结点和插入其他结点，删除最后一个结点和删除其他结点，都可以统一为相同的代码实现逻辑了。\n\n实际上，这种利用哨兵简化编程难度的技巧，在很多代码实现中都有用到，比如插入排序、归并排序、动态规划等。这些内容我们后面才会讲，现在为了让你感受更深，我再举一个非常简单的例子。代码我是用C语言实现的，不涉及语言方面的高级语法，很容易看懂，你可以类比到你熟悉的语言。\n代码一：\n// 在数组a中，查找key，返回key所在的位置// 其中，n表示数组a的长度int find(char* a, int n, char key) &#123;    // 边界条件处理，如果a为空，或者n&lt;=0，说明数组中没有数据，就不用while循环比较了    if(a == null || n &lt;= 0) &#123;        return -1;    &#125;    int i = 0;    // 这里有两个比较操作：i&lt;n和a[i]==key.    while (i &lt; n) &#123;        if (a[i] == key) &#123;            return i;        &#125;        ++i;    &#125;    return -1;&#125;\n代码二：\n// 在数组a中，查找key，返回key所在的位置// 其中，n表示数组a的长度// 我举2个例子，你可以拿例子走一下代码// a = &#123;4, 2, 3, 5, 9, 6&#125;  n=6 key = 7// a = &#123;4, 2, 3, 5, 9, 6&#125;  n=6 key = 6int find(char* a, int n, char key) &#123;    if(a == null || n &lt;= 0) &#123;        return -1;    &#125;    // 这里因为要将a[n-1]的值替换成key，所以要特殊处理这个值    if (a[n-1] == key) &#123;        return n-1;    &#125;    // 把a[n-1]的值临时保存在变量tmp中，以便之后恢复。tmp=6。    // 之所以这样做的目的是：希望find()代码不要改变a数组中的内容    char tmp = a[n-1];    // 把key的值放到a[n-1]中，此时a = &#123;4, 2, 3, 5, 9, 7&#125;    a[n-1] = key;    int i = 0;    // while 循环比起代码一，少了i&lt;n这个比较操作    while (a[i] != key) &#123;        ++i;    &#125;    // 恢复a[n-1]原来的值,此时a= &#123;4, 2, 3, 5, 9, 6&#125;    a[n-1] = tmp;    if (i == n-1) &#123;        // 如果i == n-1说明，在0...n-2之间都没有key，所以返回-1        return -1;    &#125; else &#123;        // 否则，返回i，就是等于key值的元素的下标        return i;    &#125;&#125;\n对比两段代码，在字符串a很长的时候，比如几万、几十万，你觉得哪段代码运行得更快点呢？答案是代码二，因为两段代码中执行次数最多就是while循环那一部分。第二段代码中，我们通过一个哨兵a[n-1] = key，成功省掉了一个比较语句i&lt;n，不要小看这一条语句，当累积执行万次、几十万次时，累积的时间就很明显了。\n当然，这只是为了举例说明哨兵的作用，你写代码的时候千万不要写第二段那样的代码，因为可读性太差了。大部分情况下，我们并不需要如此追求极致的性能。\n技巧四：重点留意边界条件处理软件开发中，代码在一些边界或者异常情况下，最容易产生Bug。链表代码也不例外。要实现没有Bug的链表代码，一定要在编写的过程中以及编写完成之后，检查边界条件是否考虑全面，以及代码在边界条件下是否能正确运行。\n我经常用来检查链表代码是否正确的边界条件有这样几个：\n\n如果链表为空时，代码是否能正常工作？\n\n如果链表只包含一个结点时，代码是否能正常工作？\n\n如果链表只包含两个结点时，代码是否能正常工作？\n\n代码逻辑在处理头结点和尾结点的时候，是否能正常工作？\n\n\n当你写完链表代码之后，除了看下你写的代码在正常的情况下能否工作，还要看下在上面我列举的几个边界条件下，代码仍然能否正确工作。如果这些边界条件下都没有问题，那基本上可以认为没有问题了。\n当然，边界条件不止我列举的那些。针对不同的场景，可能还有特定的边界条件，这个需要你自己去思考，不过套路都是一样的。\n实际上，不光光是写链表代码，你在写任何代码时，也千万不要只是实现业务正常情况下的功能就好了，一定要多想想，你的代码在运行的时候，可能会遇到哪些边界情况或者异常情况。遇到了应该如何应对，这样写出来的代码才够健壮！\n技巧五：举例画图，辅助思考对于稍微复杂的链表操作，比如前面我们提到的单链表反转，指针一会儿指这，一会儿指那，一会儿就被绕晕了。总感觉脑容量不够，想不清楚。所以这个时候就要使用大招了，举例法和画图法。\n你可以找一个具体的例子，把它画在纸上，释放一些脑容量，留更多的给逻辑思考，这样就会感觉到思路清晰很多。比如往单链表中插入一个数据这样一个操作，我一般都是把各种情况都举一个例子，画出插入前和插入后的链表变化，如图所示：\n\n看图写代码，是不是就简单多啦？而且，当我们写完代码之后，也可以举几个例子，画在纸上，照着代码走一遍，很容易就能发现代码中的Bug。\n技巧六：多写多练，没有捷径如果你已经理解并掌握了我前面所讲的方法，但是手写链表代码还是会出现各种各样的错误，也不要着急。因为我最开始学的时候，这种状况也持续了一段时间。\n现在我写这些代码，简直就和“玩儿”一样，其实也没有什么技巧，就是把常见的链表操作都自己多写几遍，出问题就一点一点调试，熟能生巧！\n所以，我精选了5个常见的链表操作。你只要把这几个操作都能写熟练，不熟就多写几遍，我保证你之后再也不会害怕写链表代码。\n\n单链表反转\n\n链表中环的检测\n\n两个有序的链表合并\n\n删除链表倒数第n个结点\n\n求链表的中间结点\n\n\n内容小结这节我主要和你讲了写出正确链表代码的六个技巧。分别是理解指针或引用的含义、警惕指针丢失和内存泄漏、利用哨兵简化实现难度、重点留意边界条件处理，以及举例画图、辅助思考，还有多写多练。\n我觉得，写链表代码是最考验逻辑思维能力的。因为，链表代码到处都是指针的操作、边界条件的处理，稍有不慎就容易产生Bug。链表代码写得好坏，可以看出一个人写代码是否够细心，考虑问题是否全面，思维是否缜密。所以，这也是很多面试官喜欢让人手写链表代码的原因。所以，这一节讲到的东西，你一定要自己写代码实现一下，才有效果。\n课后思考今天我们讲到用哨兵来简化编码实现，你是否还能够想到其他场景，利用哨兵可以大大地简化编码难度？\n欢迎留言和我分享，我会第一时间给你反馈。\n我已将本节内容相关的详细代码更新到GitHub，戳此即可查看。\n","categories":["数据结构与算法","基础篇"]},{"title":"08 | 栈：如何实现浏览器的前进和后退功能？","url":"/2020/08/07/08%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%9A%E6%A0%88/","content":"浏览器的前进、后退功能，我想你肯定很熟悉吧？\n当你依次访问完一串页面a-b-c之后，点击浏览器的后退按钮，就可以查看之前浏览过的页面b和a。当你后退到页面a，点击前进按钮，就可以重新查看页面b和c。但是，如果你后退到页面b后，点击了新的页面d，那就无法再通过前进、后退功能查看页面c了。\n假设你是Chrome浏览器的开发工程师，你会如何实现这个功能呢？\n这就要用到我们今天要讲的“栈”这种数据结构。带着这个问题，我们来学习今天的内容。\n如何理解“栈”？关于“栈”，我有一个非常贴切的例子，就是一摞叠在一起的盘子。我们平时放盘子的时候，都是从下往上一个一个放；取的时候，我们也是从上往下一个一个地依次取，不能从中间任意抽出。后进者先出，先进者后出，这就是典型的“栈”结构。\n\n从栈的操作特性上来看，栈是一种“操作受限”的线性表，只允许在一端插入和删除数据。\n我第一次接触这种数据结构的时候，就对它存在的意义产生了很大的疑惑。因为我觉得，相比数组和链表，栈带给我的只有限制，并没有任何优势。那我直接使用数组或者链表不就好了吗？为什么还要用这个“操作受限”的“栈”呢？\n事实上，从功能上来说，数组或链表确实可以替代栈，但你要知道，特定的数据结构是对特定场景的抽象，而且，数组或链表暴露了太多的操作接口，操作上的确灵活自由，但使用时就比较不可控，自然也就更容易出错。\n当某个数据集合只涉及在一端插入和删除数据，并且满足后进先出、先进后出的特性，我们就应该首选“栈”这种数据结构。\n如何实现一个“栈”？从刚才栈的定义里，我们可以看出，栈主要包含两个操作，入栈和出栈，也就是在栈顶插入一个数据和从栈顶删除一个数据。理解了栈的定义之后，我们来看一看如何用代码实现一个栈。\n实际上，栈既可以用数组来实现，也可以用链表来实现。用数组实现的栈，我们叫作顺序栈，用链表实现的栈，我们叫作链式栈。\n我这里实现一个基于数组的顺序栈。基于链表实现的链式栈的代码，你可以自己试着写一下。我会将我写好的代码放到Github上，你可以去看一下自己写的是否正确。\n我这段代码是用Java来实现的，但是不涉及任何高级语法，并且我还用中文做了详细的注释，所以你应该是可以看懂的。\n// 基于数组实现的顺序栈public class ArrayStack &#123;    private String[] items;  // 数组    private int count;       // 栈中元素个数    private int n;           //栈的大小    // 初始化数组，申请一个大小为n的数组空间    public ArrayStack(int n) &#123;        this.items = new String[n];        this.n = n;        this.count = 0;    &#125;    // 入栈操作    public boolean push(String item) &#123;        // 数组空间不够了，直接返回false，入栈失败。        if (count == n) return false;        // 将item放到下标为count的位置，并且count加一        items[count] = item;        ++count;        return true;    &#125;    // 出栈操作    public String pop() &#123;        // 栈为空，则直接返回null        if (count == 0) return null;        // 返回下标为count-1的数组元素，并且栈中元素个数count减一        String tmp = items[count-1];        --count;        return tmp;    &#125;&#125;\n了解了定义和基本操作，那它的操作的时间、空间复杂度是多少呢？\n不管是顺序栈还是链式栈，我们存储数据只需要一个大小为n的数组就够了。在入栈和出栈过程中，只需要一两个临时变量存储空间，所以空间复杂度是O(1)。\n注意，这里存储数据需要一个大小为n的数组，并不是说空间复杂度就是O(n)。因为，这n个空间是必须的，无法省掉。所以我们说空间复杂度的时候，是指除了原本的数据存储空间外，算法运行还需要额外的存储空间。\n空间复杂度分析是不是很简单？时间复杂度也不难。不管是顺序栈还是链式栈，入栈、出栈只涉及栈顶个别数据的操作，所以时间复杂度都是O(1)。\n支持动态扩容的顺序栈刚才那个基于数组实现的栈，是一个固定大小的栈，也就是说，在初始化栈时需要事先指定栈的大小。当栈满之后，就无法再往栈里添加数据了。尽管链式栈的大小不受限，但要存储next指针，内存消耗相对较多。那我们如何基于数组实现一个可以支持动态扩容的栈呢？\n你还记得，我们在数组那一节，是如何来实现一个支持动态扩容的数组的吗？当数组空间不够时，我们就重新申请一块更大的内存，将原来数组中数据统统拷贝过去。这样就实现了一个支持动态扩容的数组。\n所以，如果要实现一个支持动态扩容的栈，我们只需要底层依赖一个支持动态扩容的数组就可以了。当栈满了之后，我们就申请一个更大的数组，将原来的数据搬移到新数组中。我画了一张图，你可以对照着理解一下。\n\n实际上，支持动态扩容的顺序栈，我们平时开发中并不常用到。我讲这一块的目的，主要还是希望带你练习一下前面讲的复杂度分析方法。所以这一小节的重点是复杂度分析。\n你不用死记硬背入栈、出栈的时间复杂度，你需要掌握的是分析方法。能够自己分析才算是真正掌握了。现在我就带你分析一下支持动态扩容的顺序栈的入栈、出栈操作的时间复杂度。\n对于出栈操作来说，我们不会涉及内存的重新申请和数据的搬移，所以出栈的时间复杂度仍然是O(1)。但是，对于入栈操作来说，情况就不一样了。当栈中有空闲空间时，入栈操作的时间复杂度为O(1)。但当空间不够时，就需要重新申请内存和数据搬移，所以时间复杂度就变成了O(n)。\n也就是说，对于入栈操作来说，最好情况时间复杂度是O(1)，最坏情况时间复杂度是O(n)。那平均情况下的时间复杂度又是多少呢？还记得我们在复杂度分析那一节中讲的摊还分析法吗？这个入栈操作的平均情况下的时间复杂度可以用摊还分析法来分析。我们也正好借此来实战一下摊还分析法。\n为了分析的方便，我们需要事先做一些假设和定义：\n\n栈空间不够时，我们重新申请一个是原来大小两倍的数组；\n\n为了简化分析，假设只有入栈操作没有出栈操作；\n\n定义不涉及内存搬移的入栈操作为simple-push操作，时间复杂度为O(1)。\n\n\n如果当前栈大小为K，并且已满，当再有新的数据要入栈时，就需要重新申请2倍大小的内存，并且做K个数据的搬移操作，然后再入栈。但是，接下来的K-1次入栈操作，我们都不需要再重新申请内存和搬移数据，所以这K-1次入栈操作都只需要一个simple-push操作就可以完成。为了让你更加直观地理解这个过程，我画了一张图。\n\n你应该可以看出来，这K次入栈操作，总共涉及了K个数据的搬移，以及K次simple-push操作。将K个数据搬移均摊到K次入栈操作，那每个入栈操作只需要一个数据搬移和一个simple-push操作。以此类推，入栈操作的均摊时间复杂度就为O(1)。\n通过这个例子的实战分析，也印证了前面讲到的，均摊时间复杂度一般都等于最好情况时间复杂度。因为在大部分情况下，入栈操作的时间复杂度O都是O(1)，只有在个别时刻才会退化为O(n)，所以把耗时多的入栈操作的时间均摊到其他入栈操作上，平均情况下的耗时就接近O(1)。\n栈在函数调用中的应用前面我讲的都比较偏理论，我们现在来看下，栈在软件工程中的实际应用。栈作为一个比较基础的数据结构，应用场景还是蛮多的。其中，比较经典的一个应用场景就是函数调用栈。\n我们知道，操作系统给每个线程分配了一块独立的内存空间，这块内存被组织成“栈”这种结构,用来存储函数调用时的临时变量。每进入一个函数，就会将临时变量作为一个栈帧入栈，当被调用函数执行完成，返回之后，将这个函数对应的栈帧出栈。为了让你更好地理解，我们一块来看下这段代码的执行过程。\nint main() &#123;    int a = 1;     int ret = 0;    int res = 0;    ret = add(3, 5);    res = a + ret;    printf(\"%d\", res);    reuturn 0;&#125;int add(int x, int y) &#123;    int sum = 0;    sum = x + y;    return sum;&#125;\n从代码中我们可以看出，main()函数调用了add()函数，获取计算结果，并且与临时变量a相加，最后打印res的值。为了让你清晰地看到这个过程对应的函数栈里出栈、入栈的操作，我画了一张图。图中显示的是，在执行到add()函数时，函数调用栈的情况。\n\n栈在表达式求值中的应用我们再来看栈的另一个常见的应用场景，编译器如何利用栈来实现表达式求值。\n为了方便解释，我将算术表达式简化为只包含加减乘除四则运算，比如：34+13*9+44-12/3。对于这个四则运算，我们人脑可以很快求解出答案，但是对于计算机来说，理解这个表达式本身就是个挺难的事儿。如果换作你，让你来实现这样一个表达式求值的功能，你会怎么做呢？\n实际上，编译器就是通过两个栈来实现的。其中一个保存操作数的栈，另一个是保存运算符的栈。我们从左向右遍历表达式，当遇到数字，我们就直接压入操作数栈；当遇到运算符，就与运算符栈的栈顶元素进行比较。\n如果比运算符栈顶元素的优先级高，就将当前运算符压入栈；如果比运算符栈顶元素的优先级低或者相同，从运算符栈中取栈顶运算符，从操作数栈的栈顶取2个操作数，然后进行计算，再把计算完的结果压入操作数栈，继续比较。\n我将3+5*8-6这个表达式的计算过程画成了一张图，你可以结合图来理解我刚讲的计算过程。\n\n这样用两个栈来解决的思路是不是非常巧妙？你有没有想到呢？\n栈在括号匹配中的应用除了用栈来实现表达式求值，我们还可以借助栈来检查表达式中的括号是否匹配。\n我们同样简化一下背景。我们假设表达式中只包含三种括号，圆括号()、方括号[]和花括号{}，并且它们可以任意嵌套。比如，{[] ()[{}]}或[{()}([])]等都为合法格式，而{[}()]或[({)]为不合法的格式。那我现在给你一个包含三种括号的表达式字符串，如何检查它是否合法呢？\n这里也可以用栈来解决。我们用栈来保存未匹配的左括号，从左到右依次扫描字符串。当扫描到左括号时，则将其压入栈中；当扫描到右括号时，从栈顶取出一个左括号。如果能够匹配，比如“(”跟“)”匹配，“[”跟“]”匹配，“{”跟“}”匹配，则继续扫描剩下的字符串。如果扫描的过程中，遇到不能配对的右括号，或者栈中没有数据，则说明为非法格式。\n当所有的括号都扫描完成之后，如果栈为空，则说明字符串为合法格式；否则，说明有未匹配的左括号，为非法格式。\n解答开篇好了，我想现在你已经完全理解了栈的概念。我们再回来看看开篇的思考题，如何实现浏览器的前进、后退功能？其实，用两个栈就可以非常完美地解决这个问题。\n我们使用两个栈，X和Y，我们把首次浏览的页面依次压入栈X，当点击后退按钮时，再依次从栈X中出栈，并将出栈的数据依次放入栈Y。当我们点击前进按钮时，我们依次从栈Y中取出数据，放入栈X中。当栈X中没有数据时，那就说明没有页面可以继续后退浏览了。当栈Y中没有数据，那就说明没有页面可以点击前进按钮浏览了。\n比如你顺序查看了a，b，c三个页面，我们就依次把a，b，c压入栈，这个时候，两个栈的数据就是这个样子：\n\n当你通过浏览器的后退按钮，从页面c后退到页面a之后，我们就依次把c和b从栈X中弹出，并且依次放入到栈Y。这个时候，两个栈的数据就是这个样子：\n\n这个时候你又想看页面b，于是你又点击前进按钮回到b页面，我们就把b再从栈Y中出栈，放入栈X中。此时两个栈的数据是这个样子：\n\n这个时候，你通过页面b又跳转到新的页面d了，页面c就无法再通过前进、后退按钮重复查看了，所以需要清空栈Y。此时两个栈的数据这个样子：\n\n内容小结我们来回顾一下今天讲的内容。栈是一种操作受限的数据结构，只支持入栈和出栈操作。后进先出是它最大的特点。栈既可以通过数组实现，也可以通过链表来实现。不管基于数组还是链表，入栈、出栈的时间复杂度都为O(1)。除此之外，我们还讲了一种支持动态扩容的顺序栈，你需要重点掌握它的均摊时间复杂度分析方法。\n课后思考\n我们在讲栈的应用时，讲到用函数调用栈来保存临时变量，为什么函数调用要用“栈”来保存临时变量呢？用其他数据结构不行吗？\n\n我们都知道，JVM内存管理中有个“堆栈”的概念。栈内存用来存储局部变量和方法调用，堆内存用来存储Java中的对象。那JVM里面的“栈”跟我们这里说的“栈”是不是一回事呢？如果不是，那它为什么又叫作“栈”呢？\n\n\n欢迎留言和我分享，我会第一时间给你反馈。\n我已将本节内容相关的详细代码更新到GitHub，戳此即可查看。\n","categories":["数据结构与算法","基础篇"]},{"title":"09 | 队列：队列在线程池等有限资源池中的应用","url":"/2020/08/07/09%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%9A%E9%98%9F%E5%88%97/","content":"我们知道，CPU资源是有限的，任务的处理速度与线程个数并不是线性正相关。相反，过多的线程反而会导致CPU频繁切换，处理性能下降。所以，线程池的大小一般都是综合考虑要处理任务的特点和硬件环境，来事先设置的。\n当我们向固定大小的线程池中请求一个线程时，如果线程池中没有空闲资源了，这个时候线程池如何处理这个请求？是拒绝请求还是排队请求？各种处理策略又是怎么实现的呢？\n实际上，这些问题并不复杂，其底层的数据结构就是我们今天要学的内容，队列（queue）。\n如何理解“队列”？队列这个概念非常好理解。你可以把它想象成排队买票，先来的先买，后来的人只能站末尾，不允许插队。先进者先出，这就是典型的“队列”。\n我们知道，栈只支持两个基本操作：入栈push()和出栈pop()。队列跟栈非常相似，支持的操作也很有限，最基本的操作也是两个：入队enqueue()，放一个数据到队列尾部；出队dequeue()，从队列头部取一个元素。\n\n所以，队列跟栈一样，也是一种操作受限的线性表数据结构。\n队列的概念很好理解，基本操作也很容易掌握。作为一种非常基础的数据结构，队列的应用也非常广泛，特别是一些具有某些额外特性的队列，比如循环队列、阻塞队列、并发队列。它们在很多偏底层系统、框架、中间件的开发中，起着关键性的作用。比如高性能队列Disruptor、Linux环形缓存，都用到了循环并发队列；Java concurrent并发包利用ArrayBlockingQueue来实现公平锁等。\n顺序队列和链式队列我们知道了，队列跟栈一样，也是一种抽象的数据结构。它具有先进先出的特性，支持在队尾插入元素，在队头删除元素，那究竟该如何实现一个队列呢？\n跟栈一样，队列可以用数组来实现，也可以用链表来实现。用数组实现的栈叫作顺序栈，用链表实现的栈叫作链式栈。同样，用数组实现的队列叫作顺序队列，用链表实现的队列叫作链式队列。\n我们先来看下基于数组的实现方法。我用Java语言实现了一下，不过并不包含Java语言的高级语法，而且我做了比较详细的注释，你应该可以看懂。\n// 用数组实现的队列public class ArrayQueue &#123;    // 数组：items，数组大小：n    private String[] items;    private int n = 0;    // head表示队头下标，tail表示队尾下标    private int head = 0;    private int tail = 0;    // 申请一个大小为capacity的数组    public ArrayQueue(int capacity) &#123;        items = new String[capacity];        n = capacity;    &#125;    // 入队    public boolean enqueue(String item) &#123;        // 如果tail == n 表示队列已经满了        if (tail == n) return false;        items[tail] = item;        ++tail;        return true;    &#125;    // 出队    public String dequeue() &#123;        // 如果head == tail 表示队列为空        if (head == tail) return null;        // 为了让其他语言的同学看的更加明确，把--操作放到单独一行来写了        String ret = items[head];        ++head;        return ret;    &#125;&#125;\n比起栈的数组实现，队列的数组实现稍微有点儿复杂，但是没关系。我稍微解释一下实现思路，你很容易就能明白了。\n对于栈来说，我们只需要一个栈顶指针就可以了。但是队列需要两个指针：一个是head指针，指向队头；一个是tail指针，指向队尾。\n你可以结合下面这幅图来理解。当a、b、c、d依次入队之后，队列中的head指针指向下标为0的位置，tail指针指向下标为4的位置。\n\n当我们调用两次出队操作之后，队列中head指针指向下标为2的位置，tail指针仍然指向下标为4的位置。\n\n你肯定已经发现了，随着不停地进行入队、出队操作，head和tail都会持续往后移动。当tail移动到最右边，即使数组中还有空闲空间，也无法继续往队列中添加数据了。这个问题该如何解决呢？\n你是否还记得，在数组那一节，我们也遇到过类似的问题，就是数组的删除操作会导致数组中的数据不连续。你还记得我们当时是怎么解决的吗？对，用数据搬移！但是，每次进行出队操作都相当于删除数组下标为0的数据，要搬移整个队列中的数据，这样出队操作的时间复杂度就会从原来的O(1)变为O(n)。能不能优化一下呢？\n实际上，我们在出队时可以不用搬移数据。如果没有空闲空间了，我们只需要在入队时，再集中触发一次数据的搬移操作。借助这个思想，出队函数dequeue()保持不变，我们稍加改造一下入队函数enqueue()的实现，就可以轻松解决刚才的问题了。下面是具体的代码：\n// 入队操作，将item放入队尾public boolean enqueue(String item) &#123;    // tail == n表示队列末尾没有空间了    if (tail == n) &#123;        // tail ==n &amp;&amp; head==0，表示整个队列都占满了        if (head == 0) return false;        // 数据搬移        for (int i = head; i &lt; tail; ++i) &#123;            items[i-head] = items[i];        &#125;        // 搬移完之后重新更新head和tail        tail -= head;        head = 0;    &#125;    items[tail] = item;    ++tail;    return true;&#125;\n从代码中我们看到，当队列的tail指针移动到数组的最右边后，如果有新的数据入队，我们可以将head到tail之间的数据，整体搬移到数组中0到tail-head的位置。\n\n这种实现思路中，出队操作的时间复杂度仍然是O(1)，但入队操作的时间复杂度还是O(1)吗？你可以用我们第3节、第4节讲的算法复杂度分析方法，自己试着分析一下。\n接下来，我们再来看下基于链表的队列实现方法。\n基于链表的实现，我们同样需要两个指针：head指针和tail指针。它们分别指向链表的第一个结点和最后一个结点。如图所示，入队时，tail-&gt;next= new_node, tail = tail-&gt;next；出队时，head = head-&gt;next。我将具体的代码放到GitHub上，你可以自己试着实现一下，然后再去GitHub上跟我实现的代码对比下，看写得对不对。\n\n循环队列我们刚才用数组来实现队列的时候，在tail==n时，会有数据搬移操作，这样入队操作性能就会受到影响。那有没有办法能够避免数据搬移呢？我们来看看循环队列的解决思路。\n循环队列，顾名思义，它长得像一个环。原本数组是有头有尾的，是一条直线。现在我们把首尾相连，扳成了一个环。我画了一张图，你可以直观地感受一下。\n\n我们可以看到，图中这个队列的大小为8，当前head=4，tail=7。当有一个新的元素a入队时，我们放入下标为7的位置。但这个时候，我们并不把tail更新为8，而是将其在环中后移一位，到下标为0的位置。当再有一个元素b入队时，我们将b放入下标为0的位置，然后tail加1更新为1。所以，在a，b依次入队之后，循环队列中的元素就变成了下面的样子：\n\n通过这样的方法，我们成功避免了数据搬移操作。看起来不难理解，但是循环队列的代码实现难度要比前面讲的非循环队列难多了。要想写出没有bug的循环队列的实现代码，我个人觉得，最关键的是，确定好队空和队满的判定条件。\n在用数组实现的非循环队列中，队满的判断条件是tail == n，队空的判断条件是head == tail。那针对循环队列，如何判断队空和队满呢？\n队列为空的判断条件仍然是head == tail。但队列满的判断条件就稍微有点复杂了。我画了一张队列满的图，你可以看一下，试着总结一下规律。\n\n就像我图中画的队满的情况，tail=3，head=4，n=8，所以总结一下规律就是：(3+1)%8=4。多画几张队满的图，你就会发现，当队满时，(tail+1)%n=head。\n你有没有发现，当队列满时，图中的tail指向的位置实际上是没有存储数据的。所以，循环队列会浪费一个数组的存储空间。\nTalk is cheap，如果还是没怎么理解，那就show you code吧。\npublic class CircularQueue &#123;    // 数组：items，数组大小：n    private String[] items;    private int n = 0;    // head表示队头下标，tail表示队尾下标    private int head = 0;    private int tail = 0;    // 申请一个大小为capacity的数组    public CircularQueue(int capacity) &#123;        items = new String[capacity];        n = capacity;    &#125;    // 入队    public boolean enqueue(String item) &#123;        // 队列满了        if ((tail + 1) % n == head) return false;        items[tail] = item;        tail = (tail + 1) % n;        return true;    &#125;    // 出队    public String dequeue() &#123;        // 如果head == tail 表示队列为空        if (head == tail) return null;        String ret = items[head];        head = (head + 1) % n;        return ret;    &#125;&#125;\n阻塞队列和并发队列前面讲的内容理论比较多，看起来很难跟实际的项目开发扯上关系。确实，队列这种数据结构很基础，平时的业务开发不大可能从零实现一个队列，甚至都不会直接用到。而一些具有特殊特性的队列应用却比较广泛，比如阻塞队列和并发队列。\n阻塞队列其实就是在队列基础上增加了阻塞操作。简单来说，就是在队列为空的时候，从队头取数据会被阻塞。因为此时还没有数据可取，直到队列中有了数据才能返回；如果队列已经满了，那么插入数据的操作就会被阻塞，直到队列中有空闲位置后再插入数据，然后再返回。\n\n你应该已经发现了，上述的定义就是一个“生产者-消费者模型”！是的，我们可以使用阻塞队列，轻松实现一个“生产者-消费者模型”！\n这种基于阻塞队列实现的“生产者-消费者模型”，可以有效地协调生产和消费的速度。当“生产者”生产数据的速度过快，“消费者”来不及消费时，存储数据的队列很快就会满了。这个时候，生产者就阻塞等待，直到“消费者”消费了数据，“生产者”才会被唤醒继续“生产”。\n而且不仅如此，基于阻塞队列，我们还可以通过协调“生产者”和“消费者”的个数，来提高数据的处理效率。比如前面的例子，我们可以多配置几个“消费者”，来应对一个“生产者”。\n\n前面我们讲了阻塞队列，在多线程情况下，会有多个线程同时操作队列，这个时候就会存在线程安全问题，那如何实现一个线程安全的队列呢？\n线程安全的队列我们叫作并发队列。最简单直接的实现方式是直接在enqueue()、dequeue()方法上加锁，但是锁粒度大并发度会比较低，同一时刻仅允许一个存或者取操作。实际上，基于数组的循环队列，利用CAS原子操作，可以实现非常高效的并发队列。这也是循环队列比链式队列应用更加广泛的原因。在实战篇讲Disruptor的时候，我会再详细讲并发队列的应用。\n解答开篇队列的知识就讲完了，我们现在回过来看下开篇的问题。线程池没有空闲线程时，新的任务请求线程资源时，线程池该如何处理？各种处理策略又是如何实现的呢？\n我们一般有两种处理策略。第一种是非阻塞的处理方式，直接拒绝任务请求；另一种是阻塞的处理方式，将请求排队，等到有空闲线程时，取出排队的请求继续处理。那如何存储排队的请求呢？\n我们希望公平地处理每个排队的请求，先进者先服务，所以队列这种数据结构很适合来存储排队请求。我们前面说过，队列有基于链表和基于数组这两种实现方式。这两种实现方式对于排队请求又有什么区别呢？\n基于链表的实现方式，可以实现一个支持无限排队的无界队列（unbounded queue），但是可能会导致过多的请求排队等待，请求处理的响应时间过长。所以，针对响应时间比较敏感的系统，基于链表实现的无限排队的线程池是不合适的。\n而基于数组实现的有界队列（bounded queue），队列的大小有限，所以线程池中排队的请求超过队列大小时，接下来的请求就会被拒绝，这种方式对响应时间敏感的系统来说，就相对更加合理。不过，设置一个合理的队列大小，也是非常有讲究的。队列太大导致等待的请求太多，队列太小会导致无法充分利用系统资源、发挥最大性能。\n除了前面讲到队列应用在线程池请求排队的场景之外，队列可以应用在任何有限资源池中，用于排队请求，比如数据库连接池等。实际上，对于大部分资源有限的场景，当没有空闲资源时，基本上都可以通过“队列”这种数据结构来实现请求排队。\n内容小结今天我们讲了一种跟栈很相似的数据结构，队列。关于队列，你能掌握下面的内容，这节就没问题了。\n队列最大的特点就是先进先出，主要的两个操作是入队和出队。跟栈一样，它既可以用数组来实现，也可以用链表来实现。用数组实现的叫顺序队列，用链表实现的叫链式队列。特别是长得像一个环的循环队列。在数组实现队列的时候，会有数据搬移操作，要想解决数据搬移的问题，我们就需要像环一样的循环队列。\n循环队列是我们这节的重点。要想写出没有bug的循环队列实现代码，关键要确定好队空和队满的判定条件，具体的代码你要能写出来。\n除此之外，我们还讲了几种高级的队列结构，阻塞队列、并发队列，底层都还是队列这种数据结构，只不过在之上附加了很多其他功能。阻塞队列就是入队、出队操作可以阻塞，并发队列就是队列的操作多线程安全。\n课后思考\n除了线程池这种池结构会用到队列排队请求，你还知道有哪些类似的池结构或者场景中会用到队列的排队请求呢？\n\n今天讲到并发队列，关于如何实现无锁并发队列，网上有非常多的讨论。对这个问题，你怎么看呢？\n\n\n欢迎留言和我分享，我会第一时间给你反馈。\n我已将本节内容相关的详细代码更新到GitHub，戳此即可查看。\n","categories":["数据结构与算法","基础篇"]},{"title":"05 | 数组：为什么很多编程语言中数组都从0开始编号？","url":"/2020/08/07/05%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%9A%E6%95%B0%E7%BB%84/","content":"提到数组，我想你肯定不陌生，甚至还会自信地说，它很简单啊。\n是的，在每一种编程语言中，基本都会有数组这种数据类型。不过，它不仅仅是一种编程语言中的数据类型，还是一种最基础的数据结构。尽管数组看起来非常基础、简单，但是我估计很多人都并没有理解这个基础数据结构的精髓。\n在大部分编程语言中，数组都是从0开始编号的，但你是否下意识地想过，为什么数组要从0开始编号，而不是从1开始呢？ 从1开始不是更符合人类的思维习惯吗？\n你可以带着这个问题来学习接下来的内容。\n如何实现随机访问？什么是数组？我估计你心中已经有了答案。不过，我还是想用专业的话来给你做下解释。数组（Array）是一种线性表数据结构。它用一组连续的内存空间，来存储一组具有相同类型的数据。\n这个定义里有几个关键词，理解了这几个关键词，我想你就能彻底掌握数组的概念了。下面就从我的角度分别给你“点拨”一下。\n第一是线性表（Linear List）。顾名思义，线性表就是数据排成像一条线一样的结构。每个线性表上的数据最多只有前和后两个方向。其实除了数组，链表、队列、栈等也是线性表结构。\n\n而与它相对立的概念是非线性表，比如二叉树、堆、图等。之所以叫非线性，是因为，在非线性表中，数据之间并不是简单的前后关系。\n\n第二个是连续的内存空间和相同类型的数据。正是因为这两个限制，它才有了一个堪称“杀手锏”的特性：“随机访问”。但有利就有弊，这两个限制也让数组的很多操作变得非常低效，比如要想在数组中删除、插入一个数据，为了保证连续性，就需要做大量的数据搬移工作。\n说到数据的访问，那你知道数组是如何实现根据下标随机访问数组元素的吗？\n我们拿一个长度为10的int类型的数组int[] a = new int[10]来举例。在我画的这个图中，计算机给数组a[10]，分配了一块连续内存空间1000～1039，其中，内存块的首地址为base_address = 1000。\n\n我们知道，计算机会给每个内存单元分配一个地址，计算机通过地址来访问内存中的数据。当计算机需要随机访问数组中的某个元素时，它会首先通过下面的寻址公式，计算出该元素存储的内存地址：\na[i]_address &#x3D; base_address + i * data_type_size\n其中data_type_size表示数组中每个元素的大小。我们举的这个例子里，数组中存储的是int类型数据，所以data_type_size就为4个字节。这个公式非常简单，我就不多做解释了。\n这里我要特别纠正一个“错误”。我在面试的时候，常常会问数组和链表的区别，很多人都回答说，“链表适合插入、删除，时间复杂度O(1)；数组适合查找，查找时间复杂度为O(1)”。\n实际上，这种表述是不准确的。数组是适合查找操作，但是查找的时间复杂度并不为O(1)。即便是排好序的数组，你用二分查找，时间复杂度也是O(logn)。所以，正确的表述应该是，数组支持随机访问，根据下标随机访问的时间复杂度为O(1)。\n低效的“插入”和“删除”前面概念部分我们提到，数组为了保持内存数据的连续性，会导致插入、删除这两个操作比较低效。现在我们就来详细说一下，究竟为什么会导致低效？又有哪些改进方法呢？\n我们先来看插入操作。\n假设数组的长度为n，现在，如果我们需要将一个数据插入到数组中的第k个位置。为了把第k个位置腾出来，给新来的数据，我们需要将第k～n这部分的元素都顺序地往后挪一位。那插入操作的时间复杂度是多少呢？你可以自己先试着分析一下。\n如果在数组的末尾插入元素，那就不需要移动数据了，这时的时间复杂度为O(1)。但如果在数组的开头插入元素，那所有的数据都需要依次往后移动一位，所以最坏时间复杂度是O(n)。 因为我们在每个位置插入元素的概率是一样的，所以平均情况时间复杂度为(1+2+…n)/n=O(n)。\n如果数组中的数据是有序的，我们在某个位置插入一个新的元素时，就必须按照刚才的方法搬移k之后的数据。但是，如果数组中存储的数据并没有任何规律，数组只是被当作一个存储数据的集合。在这种情况下，如果要将某个数据插入到第k个位置，为了避免大规模的数据搬移，我们还有一个简单的办法就是，直接将第k位的数据搬移到数组元素的最后，把新的元素直接放入第k个位置。\n为了更好地理解，我们举一个例子。假设数组a[10]中存储了如下5个元素：a，b，c，d，e。\n我们现在需要将元素x插入到第3个位置。我们只需要将c放入到a[5]，将a[2]赋值为x即可。最后，数组中的元素如下： a，b，x，d，e，c。\n\n利用这种处理技巧，在特定场景下，在第k个位置插入一个元素的时间复杂度就会降为O(1)。这个处理思想在快排中也会用到，我会在排序那一节具体来讲，这里就说到这儿。\n我们再来看删除操作。\n跟插入数据类似，如果我们要删除第k个位置的数据，为了内存的连续性，也需要搬移数据，不然中间就会出现空洞，内存就不连续了。\n和插入类似，如果删除数组末尾的数据，则最好情况时间复杂度为O(1)；如果删除开头的数据，则最坏情况时间复杂度为O(n)；平均情况时间复杂度也为O(n)。\n实际上，在某些特殊场景下，我们并不一定非得追求数组中数据的连续性。如果我们将多次删除操作集中在一起执行，删除的效率是不是会提高很多呢？\n我们继续来看例子。数组a[10]中存储了8个元素：a，b，c，d，e，f，g，h。现在，我们要依次删除a，b，c三个元素。\n\n为了避免d，e，f，g，h这几个数据会被搬移三次，我们可以先记录下已经删除的数据。每次的删除操作并不是真正地搬移数据，只是记录数据已经被删除。当数组没有更多空间存储数据时，我们再触发执行一次真正的删除操作，这样就大大减少了删除操作导致的数据搬移。\n如果你了解JVM，你会发现，这不就是JVM标记清除垃圾回收算法的核心思想吗？没错，数据结构和算法的魅力就在于此，很多时候我们并不是要去死记硬背某个数据结构或者算法，而是要学习它背后的思想和处理技巧，这些东西才是最有价值的。如果你细心留意，不管是在软件开发还是架构设计中，总能找到某些算法和数据结构的影子。\n警惕数组的访问越界问题了解了数组的几个基本操作后，我们来聊聊数组访问越界的问题。\n首先，我请你来分析一下这段C语言代码的运行结果：\nint main(int argc, char* argv[])&#123;    int i &#x3D; 0;    int arr[3] &#x3D; &#123;0&#125;;    for(; i&lt;&#x3D;3; i++)&#123;        arr[i] &#x3D; 0;        printf(&quot;hello world\\n&quot;);    &#125;    return 0;&#125;\n你发现问题了吗？这段代码的运行结果并非是打印三行“hello word”，而是会无限打印“hello world”，这是为什么呢？\n因为，数组大小为3，a[0]，a[1]，a[2]，而我们的代码因为书写错误，导致for循环的结束条件错写为了i&lt;=3而非i&lt;3，所以当i=3时，数组a[3]访问越界。\n我们知道，在C语言中，只要不是访问受限的内存，所有的内存空间都是可以自由访问的。根据我们前面讲的数组寻址公式，a[3]也会被定位到某块不属于数组的内存地址上，而这个地址正好是存储变量i的内存地址，那么a[3]=0就相当于i=0，所以就会导致代码无限循环。\n数组越界在C语言中是一种未决行为，并没有规定数组访问越界时编译器应该如何处理。因为，访问数组的本质就是访问一段连续内存，只要数组通过偏移计算得到的内存地址是可用的，那么程序就可能不会报任何错误。\n这种情况下，一般都会出现莫名其妙的逻辑错误，就像我们刚刚举的那个例子，debug的难度非常的大。而且，很多计算机病毒也正是利用到了代码中的数组越界可以访问非法地址的漏洞，来攻击系统，所以写代码的时候一定要警惕数组越界。\n但并非所有的语言都像C一样，把数组越界检查的工作丢给程序员来做，像Java本身就会做越界检查，比如下面这几行Java代码，就会抛出java.lang.ArrayIndexOutOfBoundsException。\nint[] a &#x3D; new int[3];a[3] &#x3D; 10;\n容器能否完全替代数组？针对数组类型，很多语言都提供了容器类，比如Java中的ArrayList、C++ STL中的vector。在项目开发中，什么时候适合用数组，什么时候适合用容器呢？\n这里我拿Java语言来举例。如果你是Java工程师，几乎天天都在用ArrayList，对它应该非常熟悉。那它与数组相比，到底有哪些优势呢？\n我个人觉得，ArrayList最大的优势就是可以将很多数组操作的细节封装起来。比如前面提到的数组插入、删除数据时需要搬移其他数据等。另外，它还有一个优势，就是支持动态扩容。\n数组本身在定义的时候需要预先指定大小，因为需要分配连续的内存空间。如果我们申请了大小为10的数组，当第11个数据需要存储到数组中时，我们就需要重新分配一块更大的空间，将原来的数据复制过去，然后再将新的数据插入。\n如果使用ArrayList，我们就完全不需要关心底层的扩容逻辑，ArrayList已经帮我们实现好了。每次存储空间不够的时候，它都会将空间自动扩容为1.5倍大小。\n不过，这里需要注意一点，因为扩容操作涉及内存申请和数据搬移，是比较耗时的。所以，如果事先能确定需要存储的数据大小，最好在创建ArrayList的时候事先指定数据大小。\n比如我们要从数据库中取出10000条数据放入ArrayList。我们看下面这几行代码，你会发现，相比之下，事先指定数据大小可以省掉很多次内存申请和数据搬移操作。\nArrayList&lt;User&gt; users &#x3D; new ArrayList(10000);for (int i &#x3D; 0; i &lt; 10000; ++i) &#123;  users.add(xxx);&#125;\n作为高级语言编程者，是不是数组就无用武之地了呢？当然不是，有些时候，用数组会更合适些，我总结了几点自己的经验。\n1.Java ArrayList无法存储基本类型，比如int、long，需要封装为Integer、Long类，而Autoboxing、Unboxing则有一定的性能消耗，所以如果特别关注性能，或者希望使用基本类型，就可以选用数组。\n2.如果数据大小事先已知，并且对数据的操作非常简单，用不到ArrayList提供的大部分方法，也可以直接使用数组。\n3.还有一个是我个人的喜好，当要表示多维数组时，用数组往往会更加直观。比如Object[][] array；而用容器的话则需要这样定义：ArrayList&lt;ArrayList &gt; array。\n我总结一下，对于业务开发，直接使用容器就足够了，省时省力。毕竟损耗一丢丢性能，完全不会影响到系统整体的性能。但如果你是做一些非常底层的开发，比如开发网络框架，性能的优化需要做到极致，这个时候数组就会优于容器，成为首选。\n解答开篇现在我们来思考开篇的问题：为什么大多数编程语言中，数组要从0开始编号，而不是从1开始呢？\n从数组存储的内存模型上来看，“下标”最确切的定义应该是“偏移（offset）”。前面也讲到，如果用a来表示数组的首地址，a[0]就是偏移为0的位置，也就是首地址，a[k]就表示偏移k个type_size的位置，所以计算a[k]的内存地址只需要用这个公式：\na[k]_address &#x3D; base_address + k * type_size\n\n但是，如果数组从1开始计数，那我们计算数组元素a[k]的内存地址就会变为：\na[k]_address &#x3D; base_address + (k-1) * type_size\n\n对比两个公式，我们不难发现，从1开始编号，每次随机访问数组元素都多了一次减法运算，对于CPU来说，就是多了一次减法指令。\n数组作为非常基础的数据结构，通过下标随机访问数组元素又是其非常基础的编程操作，效率的优化就要尽可能做到极致。所以为了减少一次减法操作，数组选择了从0开始编号，而不是从1开始。\n不过我认为，上面解释得再多其实都算不上压倒性的证明，说数组起始编号非0开始不可。所以我觉得最主要的原因可能是历史原因。\nC语言设计者用0开始计数数组下标，之后的Java、JavaScript等高级语言都效仿了C语言，或者说，为了在一定程度上减少C语言程序员学习Java的学习成本，因此继续沿用了从0开始计数的习惯。实际上，很多语言中数组也并不是从0开始计数的，比如Matlab。甚至还有一些语言支持负数下标，比如Python。\n内容小结我们今天学习了数组。它可以说是最基础、最简单的数据结构了。数组用一块连续的内存空间，来存储相同类型的一组数据，最大的特点就是支持随机访问，但插入、删除操作也因此变得比较低效，平均情况时间复杂度为O(n)。在平时的业务开发中，我们可以直接使用编程语言提供的容器类，但是，如果是特别底层的开发，直接使用数组可能会更合适。\n课后思考\n前面我基于数组的原理引出JVM的标记清除垃圾回收算法的核心理念。我不知道你是否使用Java语言，理解JVM，如果你熟悉，可以在评论区回顾下你理解的标记清除垃圾回收算法。\n\n前面我们讲到一维数组的内存寻址公式，那你可以思考一下，类比一下，二维数组的内存寻址公式是怎样的呢？\n\n\n欢迎留言和我分享，我会第一时间给你反馈。\n\n我已将本节内容相关的详细代码更新到GitHub，戳此即可查看。","categories":["数据结构与算法","基础篇"]},{"title":"04 | 复杂度分析（下）：浅析最好、最坏、平均、均摊时间复杂度","url":"/2020/08/07/04%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%9A%E5%A4%8D%E6%9D%82%E5%BA%A6%E5%88%86%E6%9E%90%EF%BC%88%E4%B8%8B%EF%BC%89/","content":"上一节，我们讲了复杂度的大O表示法和几个分析技巧，还举了一些常见复杂度分析的例子，比如O(1)、O(logn)、O(n)、O(nlogn)复杂度分析。掌握了这些内容，对于复杂度分析这个知识点，你已经可以到及格线了。但是，我想你肯定不会满足于此。\n今天我会继续给你讲四个复杂度分析方面的知识点，最好情况时间复杂度（best case time complexity）、最坏情况时间复杂度（worst case time complexity）、平均情况时间复杂度（average case time complexity）、均摊时间复杂度（amortized time complexity）。如果这几个概念你都能掌握，那对你来说，复杂度分析这部分内容就没什么大问题了。\n最好、最坏情况时间复杂度上一节我举的分析复杂度的例子都很简单，今天我们来看一个稍微复杂的。你可以用我上节教你的分析技巧，自己先试着分析一下这段代码的时间复杂度。\n// n表示数组array的长度int find(int[] array, int n, int x) &#123;    int i = 0;    int pos = -1;    for (; i &lt; n; ++i) &#123;        if (array[i] == x) pos = i;    &#125;    return pos;&#125;\n\n你应该可以看出来，这段代码要实现的功能是，在一个无序的数组（array）中，查找变量x出现的位置。如果没有找到，就返回-1。按照上节课讲的分析方法，这段代码的复杂度是O(n)，其中，n代表数组的长度。\n我们在数组中查找一个数据，并不需要每次都把整个数组都遍历一遍，因为有可能中途找到就可以提前结束循环了。但是，这段代码写得不够高效。我们可以这样优化一下这段查找代码。\n// n表示数组array的长度int find(int[] array, int n, int x) &#123;    int i = 0;    int pos = -1;    for (; i &lt; n; ++i) &#123;        if (array[i] == x) &#123;            pos = i;            break;        &#125;    &#125;    return pos;&#125;\n\n这个时候，问题就来了。我们优化完之后，这段代码的时间复杂度还是O(n)吗？很显然，咱们上一节讲的分析方法，解决不了这个问题。\n因为，要查找的变量x可能出现在数组的任意位置。如果数组中第一个元素正好是要查找的变量x，那就不需要继续遍历剩下的n-1个数据了，那时间复杂度就是O(1)。但如果数组中不存在变量x，那我们就需要把整个数组都遍历一遍，时间复杂度就成了O(n)。所以，不同的情况下，这段代码的时间复杂度是不一样的。\n为了表示代码在不同情况下的不同时间复杂度，我们需要引入三个概念：最好情况时间复杂度、最坏情况时间复杂度和平均情况时间复杂度。\n顾名思义，最好情况时间复杂度就是，在最理想的情况下，执行这段代码的时间复杂度。就像我们刚刚讲到的，在最理想的情况下，要查找的变量x正好是数组的第一个元素，这个时候对应的时间复杂度就是最好情况时间复杂度。\n同理，最坏情况时间复杂度就是，在最糟糕的情况下，执行这段代码的时间复杂度。就像刚举的那个例子，如果数组中没有要查找的变量x，我们需要把整个数组都遍历一遍才行，所以这种最糟糕情况下对应的时间复杂度就是最坏情况时间复杂度。\n平均情况时间复杂度我们都知道，最好情况时间复杂度和最坏情况时间复杂度对应的都是极端情况下的代码复杂度，发生的概率其实并不大。为了更好地表示平均情况下的复杂度，我们需要引入另一个概念：平均情况时间复杂度，后面我简称为平均时间复杂度。\n平均时间复杂度又该怎么分析呢？我还是借助刚才查找变量x的例子来给你解释。\n要查找的变量x在数组中的位置，有n+1种情况：在数组的0～n-1位置中和不在数组中。我们把每种情况下，查找需要遍历的元素个数累加起来，然后再除以n+1，就可以得到需要遍历的元素个数的平均值，即：\n\n我们知道，时间复杂度的大O标记法中，可以省略掉系数、低阶、常量，所以，咱们把刚刚这个公式简化之后，得到的平均时间复杂度就是O(n)。\n这个结论虽然是正确的，但是计算过程稍微有点儿问题。究竟是什么问题呢？我们刚讲的这n+1种情况，出现的概率并不是一样的。我带你具体分析一下。（这里要稍微用到一点儿概率论的知识，不过非常简单，你不用担心。）\n我们知道，要查找的变量x，要么在数组里，要么就不在数组里。这两种情况对应的概率统计起来很麻烦，为了方便你理解，我们假设在数组中与不在数组中的概率都为1/2。另外，要查找的数据出现在0～n-1这n个位置的概率也是一样的，为1/n。所以，根据概率乘法法则，要查找的数据出现在0～n-1中任意位置的概率就是1/(2n)。\n因此，前面的推导过程中存在的最大问题就是，没有将各种情况发生的概率考虑进去。如果我们把每种情况发生的概率也考虑进去，那平均时间复杂度的计算过程就变成了这样：\n\n这个值就是概率论中的加权平均值，也叫作期望值，所以平均时间复杂度的全称应该叫加权平均时间复杂度或者期望时间复杂度。\n引入概率之后，前面那段代码的加权平均值为(3n+1)/4。用大O表示法来表示，去掉系数和常量，这段代码的加权平均时间复杂度仍然是O(n)。\n你可能会说，平均时间复杂度分析好复杂啊，还要涉及概率论的知识。实际上，在大多数情况下，我们并不需要区分最好、最坏、平均情况时间复杂度三种情况。像我们上一节课举的那些例子那样，很多时候，我们使用一个复杂度就可以满足需求了。只有同一块代码在不同的情况下，时间复杂度有量级的差距，我们才会使用这三种复杂度表示法来区分。\n均摊时间复杂度到此为止，你应该已经掌握了算法复杂度分析的大部分内容了。下面我要给你讲一个更加高级的概念，均摊时间复杂度，以及它对应的分析方法，摊还分析（或者叫平摊分析）。\n均摊时间复杂度，听起来跟平均时间复杂度有点儿像。对于初学者来说，这两个概念确实非常容易弄混。我前面说了，大部分情况下，我们并不需要区分最好、最坏、平均三种复杂度。平均复杂度只在某些特殊情况下才会用到，而均摊时间复杂度应用的场景比它更加特殊、更加有限。\n老规矩，我还是借助一个具体的例子来帮助你理解。（当然，这个例子只是我为了方便讲解想出来的，实际上没人会这么写。）\n// array表示一个长度为n的数组// 代码中的array.length就等于nint[] array = new int[n];int count = 0;void insert(int val) &#123;    if (count == array.length) &#123;        int sum = 0;        for (int i = 0; i &lt; array.length; ++i) &#123;            sum = sum + array[i];        &#125;        array[0] = sum;        count = 1;    &#125;    array[count] = val;    ++count;&#125;\n\n我先来解释一下这段代码。这段代码实现了一个往数组中插入数据的功能。当数组满了之后，也就是代码中的count == array.length时，我们用for循环遍历数组求和，并清空数组，将求和之后的sum值放到数组的第一个位置，然后再将新的数据插入。但如果数组一开始就有空闲空间，则直接将数据插入数组。\n那这段代码的时间复杂度是多少呢？你可以先用我们刚讲到的三种时间复杂度的分析方法来分析一下。\n最理想的情况下，数组中有空闲空间，我们只需要将数据插入到数组下标为count的位置就可以了，所以最好情况时间复杂度为O(1)。最坏的情况下，数组中没有空闲空间了，我们需要先做一次数组的遍历求和，然后再将数据插入，所以最坏情况时间复杂度为O(n)。\n那平均时间复杂度是多少呢？答案是O(1)。我们还是可以通过前面讲的概率论的方法来分析。\n假设数组的长度是n，根据数据插入的位置的不同，我们可以分为n种情况，每种情况的时间复杂度是O(1)。除此之外，还有一种“额外”的情况，就是在数组没有空闲空间时插入一个数据，这个时候的时间复杂度是O(n)。而且，这n+1种情况发生的概率一样，都是1/(n+1)。所以，根据加权平均的计算方法，我们求得的平均时间复杂度就是：\n\n至此为止，前面的最好、最坏、平均时间复杂度的计算，理解起来应该都没有问题。但是这个例子里的平均复杂度分析其实并不需要这么复杂，不需要引入概率论的知识。这是为什么呢？我们先来对比一下这个insert()的例子和前面那个find()的例子，你就会发现这两者有很大差别。\n首先，find()函数在极端情况下，复杂度才为O(1)。但insert()在大部分情况下，时间复杂度都为O(1)。只有个别情况下，复杂度才比较高，为O(n)。这是insert()第一个区别于find()的地方。\n我们再来看第二个不同的地方。对于insert()函数来说，O(1)时间复杂度的插入和O(n)时间复杂度的插入，出现的频率是非常有规律的，而且有一定的前后时序关系，一般都是一个O(n)插入之后，紧跟着n-1个O(1)的插入操作，循环往复。\n所以，针对这样一种特殊场景的复杂度分析，我们并不需要像之前讲平均复杂度分析方法那样，找出所有的输入情况及相应的发生概率，然后再计算加权平均值。\n针对这种特殊的场景，我们引入了一种更加简单的分析方法：摊还分析法，通过摊还分析得到的时间复杂度我们起了一个名字，叫均摊时间复杂度。\n那究竟如何使用摊还分析法来分析算法的均摊时间复杂度呢？\n我们还是继续看在数组中插入数据的这个例子。每一次O(n)的插入操作，都会跟着n-1次O(1)的插入操作，所以把耗时多的那次操作均摊到接下来的n-1次耗时少的操作上，均摊下来，这一组连续的操作的均摊时间复杂度就是O(1)。这就是均摊分析的大致思路。你都理解了吗？\n均摊时间复杂度和摊还分析应用场景比较特殊，所以我们并不会经常用到。为了方便你理解、记忆，我这里简单总结一下它们的应用场景。如果你遇到了，知道是怎么回事儿就行了。\n对一个数据结构进行一组连续操作中，大部分情况下时间复杂度都很低，只有个别情况下时间复杂度比较高，而且这些操作之间存在前后连贯的时序关系，这个时候，我们就可以将这一组操作放在一块儿分析，看是否能将较高时间复杂度那次操作的耗时，平摊到其他那些时间复杂度比较低的操作上。而且，在能够应用均摊时间复杂度分析的场合，一般均摊时间复杂度就等于最好情况时间复杂度。\n尽管很多数据结构和算法书籍都花了很大力气来区分平均时间复杂度和均摊时间复杂度，但其实我个人认为，均摊时间复杂度就是一种特殊的平均时间复杂度，我们没必要花太多精力去区分它们。你最应该掌握的是它的分析方法，摊还分析。至于分析出来的结果是叫平均还是叫均摊，这只是个说法，并不重要。\n内容小结今天我们学习了几个复杂度分析相关的概念，分别有：最好情况时间复杂度、最坏情况时间复杂度、平均情况时间复杂度、均摊时间复杂度。之所以引入这几个复杂度概念，是因为，同一段代码，在不同输入的情况下，复杂度量级有可能是不一样的。\n在引入这几个概念之后，我们可以更加全面地表示一段代码的执行效率。而且，这几个概念理解起来都不难。最好、最坏情况下的时间复杂度分析起来比较简单，但平均、均摊两个复杂度分析相对比较复杂。如果你觉得理解得还不是很深入，不用担心，在后续具体的数据结构和算法学习中，我们可以继续慢慢实践！\n课后思考我们今天学的几个复杂度分析方法，你都掌握了吗？你可以用今天学习的知识，来分析一下下面这个add()函数的时间复杂度。\n// 全局变量，大小为10的数组array，长度len，下标i。int array[] = new int[10]; int len = 10;int i = 0;// 往数组中添加一个元素void add(int element) &#123;    if (i &gt;= len) &#123; // 数组空间不够了        // 重新申请一个2倍大小的数组空间        int new_array[] = new int[len*2];        // 把原来array数组中的数据依次copy到new_array        for (int j = 0; j &lt; len; ++j) &#123;            new_array[j] = array[j];        &#125;        // new_array复制给array，array现在大小就是2倍len了        array = new_array;        len = 2 * len;    &#125;    // 将element放到下标为i的位置，下标i加一    array[i] = element;    ++i;&#125;\n\n欢迎留言和我分享，我会第一时间给你反馈。\n","categories":["数据结构与算法","入门篇"]},{"title":"14 | 排序优化：如何实现一个通用的、高性能的排序函数？","url":"/2020/08/07/14%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%9A%E6%8E%92%E5%BA%8F%E4%BC%98%E5%8C%96/","content":"几乎所有的编程语言都会提供排序函数，比如C语言中qsort()，C++ STL中的sort()、stable_sort()，还有Java语言中的Collections.sort()。在平时的开发中，我们也都是直接使用这些现成的函数来实现业务逻辑中的排序功能。那你知道这些排序函数是如何实现的吗？底层都利用了哪种排序算法呢？\n基于这些问题，今天我们就来看排序这部分的最后一块内容：如何实现一个通用的、高性能的排序函数？\n如何选择合适的排序算法？如果要实现一个通用的、高效率的排序函数，我们应该选择哪种排序算法？我们先回顾一下前面讲过的几种排序算法。\n\n我们前面讲过，线性排序算法的时间复杂度比较低，适用场景比较特殊。所以如果要写一个通用的排序函数，不能选择线性排序算法。\n如果对小规模数据进行排序，可以选择时间复杂度是O(n2)的算法；如果对大规模数据进行排序，时间复杂度是O(nlogn)的算法更加高效。所以，为了兼顾任意规模数据的排序，一般都会首选时间复杂度是O(nlogn)的排序算法来实现排序函数。\n时间复杂度是O(nlogn)的排序算法不止一个，我们已经讲过的有归并排序、快速排序，后面讲堆的时候我们还会讲到堆排序。堆排序和快速排序都有比较多的应用，比如Java语言采用堆排序实现排序函数，C语言使用快速排序实现排序函数。\n不知道你有没有发现，使用归并排序的情况其实并不多。我们知道，快排在最坏情况下的时间复杂度是O(n2)，而归并排序可以做到平均情况、最坏情况下的时间复杂度都是O(nlogn)，从这点上看起来很诱人，那为什么它还是没能得到“宠信”呢？\n还记得我们上一节讲的归并排序的空间复杂度吗？归并排序并不是原地排序算法，空间复杂度是O(n)。所以，粗略点、夸张点讲，如果要排序100MB的数据，除了数据本身占用的内存之外，排序算法还要额外再占用100MB的内存空间，空间耗费就翻倍了。\n前面我们讲到，快速排序比较适合来实现排序函数，但是，我们也知道，快速排序在最坏情况下的时间复杂度是O(n2)，如何来解决这个“复杂度恶化”的问题呢？\n如何优化快速排序？我们先来看下，为什么最坏情况下快速排序的时间复杂度是O(n2)呢？我们前面讲过，如果数据原来就是有序的或者接近有序的，每次分区点都选择最后一个数据，那快速排序算法就会变得非常糟糕，时间复杂度就会退化为O(n2)。实际上，这种O(n2)时间复杂度出现的主要原因还是因为我们分区点选得不够合理。\n那什么样的分区点是好的分区点呢？或者说如何来选择分区点呢？\n最理想的分区点是：被分区点分开的两个分区中，数据的数量差不多。\n如果很粗暴地直接选择第一个或者最后一个数据作为分区点，不考虑数据的特点，肯定会出现之前讲的那样，在某些情况下，排序的最坏情况时间复杂度是O(n2)。为了提高排序算法的性能，我们也要尽可能地让每次分区都比较平均。\n我这里介绍两个比较常用、比较简单的分区算法，你可以直观地感受一下。\n1.三数取中法我们从区间的首、尾、中间，分别取出一个数，然后对比大小，取这3个数的中间值作为分区点。这样每间隔某个固定的长度，取数据出来比较，将中间值作为分区点的分区算法，肯定要比单纯取某一个数据更好。但是，如果要排序的数组比较大，那“三数取中”可能就不够了，可能要“五数取中”或者“十数取中”。\n2.随机法随机法就是每次从要排序的区间中，随机选择一个元素作为分区点。这种方法并不能保证每次分区点都选的比较好，但是从概率的角度来看，也不大可能会出现每次分区点都选得很差的情况，所以平均情况下，这样选的分区点是比较好的。时间复杂度退化为最糟糕的O(n2)的情况，出现的可能性不大。\n好了，我这里也只是抛砖引玉，如果想了解更多寻找分区点的方法，你可以自己课下深入去学习一下。\n我们知道，快速排序是用递归来实现的。我们在递归那一节讲过，递归要警惕堆栈溢出。为了避免快速排序里，递归过深而堆栈过小，导致堆栈溢出，我们有两种解决办法：第一种是限制递归深度。一旦递归过深，超过了我们事先设定的阈值，就停止递归。第二种是通过在堆上模拟实现一个函数调用栈，手动模拟递归压栈、出栈的过程，这样就没有了系统栈大小的限制。\n举例分析排序函数为了让你对如何实现一个排序函数有一个更直观的感受，我拿Glibc中的qsort()函数举例说明一下。虽说qsort()从名字上看，很像是基于快速排序算法实现的，实际上它并不仅仅用了快排这一种算法。\n如果你去看源码，你就会发现，qsort()会优先使用归并排序来排序输入数据，因为归并排序的空间复杂度是O(n)，所以对于小数据量的排序，比如1KB、2KB等，归并排序额外需要1KB、2KB的内存空间，这个问题不大。现在计算机的内存都挺大的，我们很多时候追求的是速度。还记得我们前面讲过的用空间换时间的技巧吗？这就是一个典型的应用。\n但如果数据量太大，就跟我们前面提到的，排序100MB的数据，这个时候我们再用归并排序就不合适了。所以，要排序的数据量比较大的时候，qsort()会改为用快速排序算法来排序。\n那qsort()是如何选择快速排序算法的分区点的呢？如果去看源码，你就会发现，qsort()选择分区点的方法就是“三数取中法”。是不是也并不复杂？\n还有我们前面提到的递归太深会导致堆栈溢出的问题，qsort()是通过自己实现一个堆上的栈，手动模拟递归来解决的。我们之前在讲递归那一节也讲过，不知道你还有没有印象？\n实际上，qsort()并不仅仅用到了归并排序和快速排序，它还用到了插入排序。在快速排序的过程中，当要排序的区间中，元素的个数小于等于4时，qsort()就退化为插入排序，不再继续用递归来做快速排序，因为我们前面也讲过，在小规模数据面前，O(n2)时间复杂度的算法并不一定比O(nlogn)的算法执行时间长。我们现在就来分析下这个说法。\n我们在讲复杂度分析的时候讲过，算法的性能可以通过时间复杂度来分析，但是，这种复杂度分析是比较偏理论的，如果我们深究的话，实际上时间复杂度并不等于代码实际的运行时间。\n时间复杂度代表的是一个增长趋势，如果画成增长曲线图，你会发现O(n2)比O(nlogn)要陡峭，也就是说增长趋势要更猛一些。但是，我们前面讲过，在大O复杂度表示法中，我们会省略低阶、系数和常数，也就是说，O(nlogn)在没有省略低阶、系数、常数之前可能是O(knlogn + c)，而且k和c有可能还是一个比较大的数。\n假设k=1000，c=200，当我们对小规模数据（比如n=100）排序时，n2的值实际上比knlogn+c还要小。\nknlogn+c &#x3D; 1000 * 100 * log100 + 200 远大于10000n^2 &#x3D; 100*100 &#x3D; 10000\n所以，对于小规模数据的排序，O(n2)的排序算法并不一定比O(nlogn)排序算法执行的时间长。对于小数据量的排序，我们选择比较简单、不需要递归的插入排序算法。\n还记得我们之前讲到的哨兵来简化代码，提高执行效率吗？在qsort()插入排序的算法实现中，也利用了这种编程技巧。虽然哨兵可能只是少做一次判断，但是毕竟排序函数是非常常用、非常基础的函数，性能的优化要做到极致。\n好了，C语言的qsort()我已经分析完了，你有没有觉得其实也不是很难？基本上都是用了我们前面讲到的知识点，有了前面的知识积累，看一些底层的类库的时候是不是也更容易了呢？\n内容小结今天我带你分析了一下如何来实现一个工业级的通用的、高效的排序函数，内容比较偏实战，而且贯穿了一些前面几节的内容，你要多看几遍。我们大部分排序函数都是采用O(nlogn)排序算法来实现，但是为了尽可能地提高性能，会做很多优化。\n我还着重讲了快速排序的一些优化策略，比如合理选择分区点、避免递归太深等等。最后，我还带你分析了一个C语言中qsort()的底层实现原理，希望你对此能有一个更加直观的感受。\n课后思考在今天的内容中，我分析了C语言的中的qsort()的底层排序算法，你能否分析一下你所熟悉的语言中的排序函数都是用什么排序算法实现的呢？都有哪些优化技巧？\n欢迎留言和我分享，我会第一时间给你反馈。\n**特别说明：**\n\n专栏已经更新一月有余，我在留言区看到很多同学说，希望给出课后思考题的标准答案。鉴于留言区里本身就有很多非常好的答案，之后我会将我认为比较好的答案置顶在留言区，供需要的同学参考。\n如果文章发布一周后，留言里依旧没有比较好的答案，我会把我的答案写出来置顶在留言区。\n最后，希望你把思考的过程看得比标准答案更重要。\n","categories":["数据结构与算法","基础篇"]},{"title":"17 | 跳表：为什么Redis一定要用跳表来实现有序集合？","url":"/2020/08/07/17%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%9A%E8%B7%B3%E8%A1%A8/","content":"上两节我们讲了二分查找算法。当时我讲到，因为二分查找底层依赖的是数组随机访问的特性，所以只能用数组来实现。如果数据存储在链表中，就真的没法用二分查找算法了吗？\n实际上，我们只需要对链表稍加改造，就可以支持类似“二分”的查找算法。我们把改造之后的数据结构叫做跳表（Skip list），也就是今天要讲的内容。\n跳表这种数据结构对你来说，可能会比较陌生，因为一般的数据结构和算法书籍里都不怎么会讲。但是它确实是一种各方面性能都比较优秀的动态数据结构，可以支持快速地插入、删除、查找操作，写起来也不复杂，甚至可以替代红黑树（Red-black tree）。\nRedis中的有序集合（Sorted Set）就是用跳表来实现的。如果你有一定基础，应该知道红黑树也可以实现快速地插入、删除和查找操作。那Redis为什么会选择用跳表来实现有序集合呢？ 为什么不用红黑树呢？学完今天的内容，你就知道答案了。\n如何理解“跳表”？对于一个单链表来讲，即便链表中存储的数据是有序的，如果我们要想在其中查找某个数据，也只能从头到尾遍历链表。这样查找效率就会很低，时间复杂度会很高，是O(n)。\n\n那怎么来提高查找效率呢？如果像图中那样，对链表建立一级“索引”，查找起来是不是就会更快一些呢？每两个结点提取一个结点到上一级，我们把抽出来的那一级叫做索引或索引层。你可以看我画的图。图中的down表示down指针，指向下一级结点。\n\n如果我们现在要查找某个结点，比如16。我们可以先在索引层遍历，当遍历到索引层中值为13的结点时，我们发现下一个结点是17，那要查找的结点16肯定就在这两个结点之间。然后我们通过索引层结点的down指针，下降到原始链表这一层，继续遍历。这个时候，我们只需要再遍历2个结点，就可以找到值等于16的这个结点了。这样，原来如果要查找16，需要遍历10个结点，现在只需要遍历7个结点。\n从这个例子里，我们看出，加来一层索引之后，查找一个结点需要遍历的结点个数减少了，也就是说查找效率提高了。那如果我们再加一级索引呢？效率会不会提升更多呢？\n跟前面建立第一级索引的方式相似，我们在第一级索引的基础之上，每两个结点就抽出一个结点到第二级索引。现在我们再来查找16，只需要遍历6个结点了，需要遍历的结点数量又减少了。\n\n我举的例子数据量不大，所以即便加了两级索引，查找效率的提升也并不明显。为了让你能真切地感受索引提升查询效率。我画了一个包含64个结点的链表，按照前面讲的这种思路，建立了五级索引。\n\n从图中我们可以看出，原来没有索引的时候，查找62需要遍历62个结点，现在只需要遍历11个结点，速度是不是提高了很多？所以，当链表的长度n比较大时，比如1000、10000的时候，在构建索引之后，查找效率的提升就会非常明显。\n前面讲的这种链表加多级索引的结构，就是跳表。我通过例子给你展示了跳表是如何减少查询次数的，现在你应该比较清晰地知道，跳表确实是可以提高查询效率的。接下来，我会定量地分析一下，用跳表查询到底有多快。\n用跳表查询到底有多快？前面我讲过，算法的执行效率可以通过时间复杂度来度量，这里依旧可以用。我们知道，在一个单链表中查询某个数据的时间复杂度是O(n)。那在一个具有多级索引的跳表中，查询某个数据的时间复杂度是多少呢？\n这个时间复杂度的分析方法比较难想到。我把问题分解一下，先来看这样一个问题，如果链表里有n个结点，会有多少级索引呢？\n按照我们刚才讲的，每两个结点会抽出一个结点作为上一级索引的结点，那第一级索引的结点个数大约就是n/2，第二级索引的结点个数大约就是n/4，第三级索引的结点个数大约就是n/8，依次类推，也就是说，第k级索引的结点个数是第k-1级索引的结点个数的1/2，那第k级索引结点的个数就是n/(2k)。\n假设索引有h级，最高级的索引有2个结点。通过上面的公式，我们可以得到n/(2h)=2，从而求得h=log2n-1。如果包含原始链表这一层，整个跳表的高度就是log2n。我们在跳表中查询某个数据的时候，如果每一层都要遍历m个结点，那在跳表中查询一个数据的时间复杂度就是O(m*logn)。\n那这个m的值是多少呢？按照前面这种索引结构，我们每一级索引都最多只需要遍历3个结点，也就是说m=3，为什么是3呢？我来解释一下。\n假设我们要查找的数据是x，在第k级索引中，我们遍历到y结点之后，发现x大于y，小于后面的结点z，所以我们通过y的down指针，从第k级索引下降到第k-1级索引。在第k-1级索引中，y和z之间只有3个结点（包含y和z），所以，我们在K-1级索引中最多只需要遍历3个结点，依次类推，每一级索引都最多只需要遍历3个结点。\n\n通过上面的分析，我们得到m=3，所以在跳表中查询任意数据的时间复杂度就是O(logn)。这个查找的时间复杂度跟二分查找是一样的。换句话说，我们其实是基于单链表实现了二分查找，是不是很神奇？不过，天下没有免费的午餐，这种查询效率的提升，前提是建立了很多级索引，也就是我们在第6节讲过的空间换时间的设计思路。\n跳表是不是很浪费内存？比起单纯的单链表，跳表需要存储多级索引，肯定要消耗更多的存储空间。那到底需要消耗多少额外的存储空间呢？我们来分析一下跳表的空间复杂度。\n跳表的空间复杂度分析并不难，我在前面说了，假设原始链表大小为n，那第一级索引大约有n/2个结点，第二级索引大约有n/4个结点，以此类推，每上升一级就减少一半，直到剩下2个结点。如果我们把每层索引的结点数写出来，就是一个等比数列。\n\n这几级索引的结点总和就是n/2+n/4+n/8…+8+4+2=n-2。所以，跳表的空间复杂度是O(n)。也就是说，如果将包含n个结点的单链表构造成跳表，我们需要额外再用接近n个结点的存储空间。那我们有没有办法降低索引占用的内存空间呢？\n我们前面都是每两个结点抽一个结点到上级索引，如果我们每三个结点或五个结点，抽一个结点到上级索引，是不是就不用那么多索引结点了呢？我画了一个每三个结点抽一个的示意图，你可以看下。\n\n从图中可以看出，第一级索引需要大约n/3个结点，第二级索引需要大约n/9个结点。每往上一级，索引结点个数都除以3。为了方便计算，我们假设最高一级的索引结点个数是1。我们把每级索引的结点个数都写下来，也是一个等比数列。\n\n通过等比数列求和公式，总的索引结点大约就是n/3+n/9+n/27+…+9+3+1=n/2。尽管空间复杂度还是O(n)，但比上面的每两个结点抽一个结点的索引构建方法，要减少了一半的索引结点存储空间。\n实际上，在软件开发中，我们不必太在意索引占用的额外空间。在讲数据结构和算法时，我们习惯性地把要处理的数据看成整数，但是在实际的软件开发中，原始链表中存储的有可能是很大的对象，而索引结点只需要存储关键值和几个指针，并不需要存储对象，所以当对象比索引结点大很多时，那索引占用的额外空间就可以忽略了。\n高效的动态插入和删除跳表长什么样子我想你应该已经很清楚了，它的查找操作我们刚才也讲过了。实际上，跳表这个动态数据结构，不仅支持查找操作，还支持动态的插入、删除操作，而且插入、删除操作的时间复杂度也是O(logn)。\n我们现在来看下， 如何在跳表中插入一个数据，以及它是如何做到O(logn)的时间复杂度的？\n我们知道，在单链表中，一旦定位好要插入的位置，插入结点的时间复杂度是很低的，就是O(1)。但是，这里为了保证原始链表中数据的有序性，我们需要先找到要插入的位置，这个查找操作就会比较耗时。\n对于纯粹的单链表，需要遍历每个结点，来找到插入的位置。但是，对于跳表来说，我们讲过查找某个结点的时间复杂度是O(logn)，所以这里查找某个数据应该插入的位置，方法也是类似的，时间复杂度也是O(logn)。我画了一张图，你可以很清晰地看到插入的过程。\n\n好了，我们再来看删除操作。\n如果这个结点在索引中也有出现，我们除了要删除原始链表中的结点，还要删除索引中的。因为单链表中的删除操作需要拿到要删除结点的前驱结点，然后通过指针操作完成删除。所以在查找要删除的结点的时候，一定要获取前驱结点。当然，如果我们用的是双向链表，就不需要考虑这个问题了。\n跳表索引动态更新当我们不停地往跳表中插入数据时，如果我们不更新索引，就有可能出现某2个索引结点之间数据非常多的情况。极端情况下，跳表还会退化成单链表。\n\n作为一种动态数据结构，我们需要某种手段来维护索引与原始链表大小之间的平衡，也就是说，如果链表中结点多了，索引结点就相应地增加一些，避免复杂度退化，以及查找、插入、删除操作性能下降。\n如果你了解红黑树、AVL树这样平衡二叉树，你就知道它们是通过左右旋的方式保持左右子树的大小平衡（如果不了解也没关系，我们后面会讲），而跳表是通过随机函数来维护前面提到的“平衡性”。\n当我们往跳表中插入数据的时候，我们可以选择同时将这个数据插入到部分索引层中。如何选择加入哪些索引层呢？\n我们通过一个随机函数，来决定将这个结点插入到哪几级索引中，比如随机函数生成了值K，那我们就将这个结点添加到第一级到第K级这K级索引中。\n\n随机函数的选择很有讲究，从概率上来讲，能够保证跳表的索引大小和数据大小平衡性，不至于性能过度退化。至于随机函数的选择，我就不展开讲解了。如果你感兴趣的话，可以看看我在GitHub上的代码或者Redis中关于有序集合的跳表实现。\n跳表的实现还是稍微有点复杂的，我将Java实现的代码放到了GitHub中，你可以根据我刚刚的讲解，对照着代码仔细思考一下。你不用死记硬背代码，跳表的实现并不是我们这节的重点。\n解答开篇今天的内容到此就讲完了。现在，我来讲解一下开篇的思考题：为什么Redis要用跳表来实现有序集合，而不是红黑树？\nRedis中的有序集合是通过跳表来实现的，严格点讲，其实还用到了散列表。不过散列表我们后面才会讲到，所以我们现在暂且忽略这部分。如果你去查看Redis的开发手册，就会发现，Redis中的有序集合支持的核心操作主要有下面这几个：\n\n插入一个数据；\n\n删除一个数据；\n\n查找一个数据；\n\n按照区间查找数据（比如查找值在[100, 356]之间的数据）；\n\n迭代输出有序序列。\n\n\n其中，插入、删除、查找以及迭代输出有序序列这几个操作，红黑树也可以完成，时间复杂度跟跳表是一样的。但是，按照区间来查找数据这个操作，红黑树的效率没有跳表高。\n对于按照区间查找数据这个操作，跳表可以做到O(logn)的时间复杂度定位区间的起点，然后在原始链表中顺序往后遍历就可以了。这样做非常高效。\n当然，Redis之所以用跳表来实现有序集合，还有其他原因，比如，跳表更容易代码实现。虽然跳表的实现也不简单，但比起红黑树来说还是好懂、好写多了，而简单就意味着可读性好，不容易出错。还有，跳表更加灵活，它可以通过改变索引构建策略，有效平衡执行效率和内存消耗。\n不过，跳表也不能完全替代红黑树。因为红黑树比跳表的出现要早一些，很多编程语言中的Map类型都是通过红黑树来实现的。我们做业务开发的时候，直接拿来用就可以了，不用费劲自己去实现一个红黑树，但是跳表并没有一个现成的实现，所以在开发中，如果你想使用跳表，必须要自己实现。\n内容小结今天我们讲了跳表这种数据结构。跳表使用空间换时间的设计思路，通过构建多级索引来提高查询的效率，实现了基于链表的“二分查找”。跳表是一种动态数据结构，支持快速地插入、删除、查找操作，时间复杂度都是O(logn)。\n跳表的空间复杂度是O(n)。不过，跳表的实现非常灵活，可以通过改变索引构建策略，有效平衡执行效率和内存消耗。虽然跳表的代码实现并不简单，但是作为一种动态数据结构，比起红黑树来说，实现要简单多了。所以很多时候，我们为了代码的简单、易读，比起红黑树，我们更倾向用跳表。\n课后思考在今天的内容中，对于跳表的时间复杂度分析，我分析了每两个结点提取一个结点作为索引的时间复杂度。如果每三个或者五个结点提取一个结点作为上级索引，对应的在跳表中查询数据的时间复杂度是多少呢？\n欢迎留言和我分享，我会第一时间给你反馈。\n我已将本节内容相关的详细代码更新到GitHub，戳此即可查看。\n\n","categories":["数据结构与算法","基础篇"]},{"title":"15 | 二分查找（上）：如何用最省内存的方式实现快速查找功能？","url":"/2020/08/07/15%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%9A%E4%BA%8C%E5%88%86%E6%9F%A5%E6%89%BE%EF%BC%88%E4%B8%8A%EF%BC%89/","content":"今天我们讲一种针对有序数据集合的查找算法：二分查找（Binary Search）算法，也叫折半查找算法。二分查找的思想非常简单，很多非计算机专业的同学很容易就能理解，但是看似越简单的东西往往越难掌握好，想要灵活应用就更加困难。\n老规矩，我们还是来看一道思考题。\n假设我们有1000万个整数数据，每个数据占8个字节，如何设计数据结构和算法，快速判断某个整数是否出现在这1000万数据中？ 我们希望这个功能不要占用太多的内存空间，最多不要超过100MB，你会怎么做呢？带着这个问题，让我们进入今天的内容吧！\n无处不在的二分思想二分查找是一种非常简单易懂的快速查找算法，生活中到处可见。比如说，我们现在来做一个猜字游戏。我随机写一个0到99之间的数字，然后你来猜我写的是什么。猜的过程中，你每猜一次，我就会告诉你猜的大了还是小了，直到猜中为止。你来想想，如何快速猜中我写的数字呢？\n假设我写的数字是23，你可以按照下面的步骤来试一试。（如果猜测范围的数字有偶数个，中间数有两个，就选择较小的那个。）\n\n7次就猜出来了，是不是很快？这个例子用的就是二分思想，按照这个思想，即便我让你猜的是0到999的数字，最多也只要10次就能猜中。不信的话，你可以试一试。\n这是一个生活中的例子，我们现在回到实际的开发场景中。假设有1000条订单数据，已经按照订单金额从小到大排序，每个订单金额都不同，并且最小单位是元。我们现在想知道是否存在金额等于19元的订单。如果存在，则返回订单数据，如果不存在则返回null。\n最简单的办法当然是从第一个订单开始，一个一个遍历这1000个订单，直到找到金额等于19元的订单为止。但这样查找会比较慢，最坏情况下，可能要遍历完这1000条记录才能找到。那用二分查找能不能更快速地解决呢？\n为了方便讲解，我们假设只有10个订单，订单金额分别是：8，11，19，23，27，33，45，55，67，98。\n还是利用二分思想，每次都与区间的中间数据比对大小，缩小查找区间的范围。为了更加直观，我画了一张查找过程的图。其中，low和high表示待查找区间的下标，mid表示待查找区间的中间元素下标。\n\n看懂这两个例子，你现在对二分的思想应该掌握得妥妥的了。我这里稍微总结升华一下，二分查找针对的是一个有序的数据集合，查找思想有点类似分治思想。每次都通过跟区间的中间元素对比，将待查找的区间缩小为之前的一半，直到找到要查找的元素，或者区间被缩小为0。\nO(logn)惊人的查找速度二分查找是一种非常高效的查找算法，高效到什么程度呢？我们来分析一下它的时间复杂度。\n我们假设数据大小是n，每次查找后数据都会缩小为原来的一半，也就是会除以2。最坏情况下，直到查找区间被缩小为空，才停止。\n\n可以看出来，这是一个等比数列。其中n/2k=1时，k的值就是总共缩小的次数。而每一次缩小操作只涉及两个数据的大小比较，所以，经过了k次区间缩小操作，时间复杂度就是O(k)。通过n/2k=1，我们可以求得k=log2n，所以时间复杂度就是O(logn)。\n二分查找是我们目前为止遇到的第一个时间复杂度为O(logn)的算法。后面章节我们还会讲堆、二叉树的操作等等，它们的时间复杂度也是O(logn)。我这里就再深入地讲讲O(logn)这种对数时间复杂度。这是一种极其高效的时间复杂度，有的时候甚至比时间复杂度是常量级O(1)的算法还要高效。为什么这么说呢？\n因为logn是一个非常“恐怖”的数量级，即便n非常非常大，对应的logn也很小。比如n等于2的32次方，这个数很大了吧？大约是42亿。也就是说，如果我们在42亿个数据中用二分查找一个数据，最多需要比较32次。\n我们前面讲过，用大O标记法表示时间复杂度的时候，会省略掉常数、系数和低阶。对于常量级时间复杂度的算法来说，O(1)有可能表示的是一个非常大的常量值，比如O(1000)、O(10000)。所以，常量级时间复杂度的算法有时候可能还没有O(logn)的算法执行效率高。\n反过来，对数对应的就是指数。有一个非常著名的“阿基米德与国王下棋的故事”，你可以自行搜索一下，感受一下指数的“恐怖”。这也是为什么我们说，指数时间复杂度的算法在大规模数据面前是无效的。\n二分查找的递归与非递归实现实际上，简单的二分查找并不难写，注意我这里的“简单”二字。下一节，我们会讲到二分查找的变体问题，那才是真正烧脑的。今天，我们来看如何来写最简单的二分查找。\n最简单的情况就是有序数组中不存在重复元素，我们在其中用二分查找值等于给定值的数据。我用Java代码实现了一个最简单的二分查找算法。\npublic int bsearch(int[] a, int n, int value) &#123;    int low = 0;    int high = n - 1;    while (low &lt;= high) &#123;        int mid = (low + high) / 2;        if (a[mid] == value) &#123;            return mid;        &#125; else if (a[mid] &lt; value) &#123;            low = mid + 1;        &#125; else &#123;            high = mid - 1;        &#125;    &#125;    return -1;&#125;\n\n这个代码我稍微解释一下，low、high、mid都是指数组下标，其中low和high表示当前查找的区间范围，初始low=0， high=n-1。mid表示[low, high]的中间位置。我们通过对比a[mid]与value的大小，来更新接下来要查找的区间范围，直到找到或者区间缩小为0，就退出。如果你有一些编程基础，看懂这些应该不成问题。现在，我就着重强调一下容易出错的3个地方。\n1.循环退出条件注意是low&lt;=high，而不是low&lt;high。\n2.mid的取值实际上，mid=(low+high)/2这种写法是有问题的。因为如果low和high比较大的话，两者之和就有可能会溢出。改进的方法是将mid的计算方式写成low+(high-low)/2。更进一步，如果要将性能优化到极致的话，我们可以将这里的除以2操作转化成位运算low+((high-low)&gt;&gt;1)。因为相比除法运算来说，计算机处理位运算要快得多。\n3.low和high的更新low=mid+1，high=mid-1。注意这里的+1和-1，如果直接写成low=mid或者high=mid，就可能会发生死循环。比如，当high=3，low=3时，如果a[3]不等于value，就会导致一直循环不退出。\n如果你留意我刚讲的这三点，我想一个简单的二分查找你已经可以实现了。实际上，二分查找除了用循环来实现，还可以用递归来实现，过程也非常简单。\n我用Java语言实现了一下这个过程，正好你可以借此机会回顾一下写递归代码的技巧。\n// 二分查找的递归实现public int bsearch(int[] a, int n, int val) &#123;    return bsearchInternally(a, 0, n - 1, val);&#125;private int bsearchInternally(int[] a, int low, int high, int value) &#123;    if (low &gt; high) return -1;    int mid =  low + ((high - low) &gt;&gt; 1);    if (a[mid] == value) &#123;        return mid;    &#125; else if (a[mid] &lt; value) &#123;        return bsearchInternally(a, mid+1, high, value);    &#125; else &#123;        return bsearchInternally(a, low, mid-1, value);    &#125;&#125;\n\n二分查找应用场景的局限性前面我们分析过，二分查找的时间复杂度是O(logn)，查找数据的效率非常高。不过，并不是什么情况下都可以用二分查找，它的应用场景是有很大局限性的。那什么情况下适合用二分查找，什么情况下不适合呢？\n首先，二分查找依赖的是顺序表结构，简单点说就是数组。\n那二分查找能否依赖其他数据结构呢？比如链表。答案是不可以的，主要原因是二分查找算法需要按照下标随机访问元素。我们在数组和链表那两节讲过，数组按照下标随机访问数据的时间复杂度是O(1)，而链表随机访问的时间复杂度是O(n)。所以，如果数据使用链表存储，二分查找的时间复杂就会变得很高。\n二分查找只能用在数据是通过顺序表来存储的数据结构上。如果你的数据是通过其他数据结构存储的，则无法应用二分查找。\n其次，二分查找针对的是有序数据。\n二分查找对这一点的要求比较苛刻，数据必须是有序的。如果数据没有序，我们需要先排序。前面章节里我们讲到，排序的时间复杂度最低是O(nlogn)。所以，如果我们针对的是一组静态的数据，没有频繁地插入、删除，我们可以进行一次排序，多次二分查找。这样排序的成本可被均摊，二分查找的边际成本就会比较低。\n但是，如果我们的数据集合有频繁的插入和删除操作，要想用二分查找，要么每次插入、删除操作之后保证数据仍然有序，要么在每次二分查找之前都先进行排序。针对这种动态数据集合，无论哪种方法，维护有序的成本都是很高的。\n所以，二分查找只能用在插入、删除操作不频繁，一次排序多次查找的场景中。针对动态变化的数据集合，二分查找将不再适用。那针对动态数据集合，如何在其中快速查找某个数据呢？别急，等到二叉树那一节我会详细讲。\n再次，数据量太小不适合二分查找。\n如果要处理的数据量很小，完全没有必要用二分查找，顺序遍历就足够了。比如我们在一个大小为10的数组中查找一个元素，不管用二分查找还是顺序遍历，查找速度都差不多。只有数据量比较大的时候，二分查找的优势才会比较明显。\n不过，这里有一个例外。如果数据之间的比较操作非常耗时，不管数据量大小，我都推荐使用二分查找。比如，数组中存储的都是长度超过300的字符串，如此长的两个字符串之间比对大小，就会非常耗时。我们需要尽可能地减少比较次数，而比较次数的减少会大大提高性能，这个时候二分查找就比顺序遍历更有优势。\n最后，数据量太大也不适合二分查找。\n二分查找的底层需要依赖数组这种数据结构，而数组为了支持随机访问的特性，要求内存空间连续，对内存的要求比较苛刻。比如，我们有1GB大小的数据，如果希望用数组来存储，那就需要1GB的连续内存空间。\n注意这里的“连续”二字，也就是说，即便有2GB的内存空间剩余，但是如果这剩余的2GB内存空间都是零散的，没有连续的1GB大小的内存空间，那照样无法申请一个1GB大小的数组。而我们的二分查找是作用在数组这种数据结构之上的，所以太大的数据用数组存储就比较吃力了，也就不能用二分查找了。\n解答开篇二分查找的理论知识你应该已经掌握了。我们来看下开篇的思考题：如何在1000万个整数中快速查找某个整数？\n这个问题并不难。我们的内存限制是100MB，每个数据大小是8字节，最简单的办法就是将数据存储在数组中，内存占用差不多是80MB，符合内存的限制。借助今天讲的内容，我们可以先对这1000万数据从小到大排序，然后再利用二分查找算法，就可以快速地查找想要的数据了。\n看起来这个问题并不难，很轻松就能解决。实际上，它暗藏了“玄机”。如果你对数据结构和算法有一定了解，知道散列表、二叉树这些支持快速查找的动态数据结构。你可能会觉得，用散列表和二叉树也可以解决这个问题。实际上是不行的。\n虽然大部分情况下，用二分查找可以解决的问题，用散列表、二叉树都可以解决。但是，我们后面会讲，不管是散列表还是二叉树，都会需要比较多的额外的内存空间。如果用散列表或者二叉树来存储这1000万的数据，用100MB的内存肯定是存不下的。而二分查找底层依赖的是数组，除了数据本身之外，不需要额外存储其他信息，是最省内存空间的存储方式，所以刚好能在限定的内存大小下解决这个问题。\n内容小结今天我们学习了一种针对有序数据的高效查找算法，二分查找，它的时间复杂度是O(logn)。\n二分查找的核心思想理解起来非常简单，有点类似分治思想。即每次都通过跟区间中的中间元素对比，将待查找的区间缩小为一半，直到找到要查找的元素，或者区间被缩小为0。但是二分查找的代码实现比较容易写错。你需要着重掌握它的三个容易出错的地方：循环退出条件、mid的取值，low和high的更新。\n二分查找虽然性能比较优秀，但应用场景也比较有限。底层必须依赖数组，并且还要求数据是有序的。对于较小规模的数据查找，我们直接使用顺序遍历就可以了，二分查找的优势并不明显。二分查找更适合处理静态数据，也就是没有频繁的数据插入、删除操作。\n课后思考\n如何编程实现“求一个数的平方根”？要求精确到小数点后6位。\n\n我刚才说了，如果数据使用链表存储，二分查找的时间复杂就会变得很高，那查找的时间复杂度究竟是多少呢？如果你自己推导一下，你就会深刻地认识到，为何我们会选择用数组而不是链表来实现二分查找了。\n\n\n欢迎留言和我分享，我会第一时间给你反馈。\n","categories":["数据结构与算法","基础篇"]},{"title":"16 | 二分查找（下）：如何快速定位IP对应的省份地址？","url":"/2020/08/07/16%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%9A%E4%BA%8C%E5%88%86%E6%9F%A5%E6%89%BE%EF%BC%88%E4%B8%8B%EF%BC%89/","content":"通过IP地址来查找IP归属地的功能，不知道你有没有用过？没用过也没关系，你现在可以打开百度，在搜索框里随便输一个IP地址，就会看到它的归属地。\n\n这个功能并不复杂，它是通过维护一个很大的IP地址库来实现的。地址库中包括IP地址范围和归属地的对应关系。\n当我们想要查询202.102.133.13这个IP地址的归属地时，我们就在地址库中搜索，发现这个IP地址落在[202.102.133.0, 202.102.133.255]这个地址范围内，那我们就可以将这个IP地址范围对应的归属地“山东东营市”显示给用户了。\n[202.102.133.0, 202.102.133.255]  山东东营市 [202.102.135.0, 202.102.136.255]  山东烟台 [202.102.156.34, 202.102.157.255] 山东青岛 [202.102.48.0, 202.102.48.255] 江苏宿迁 [202.102.49.15, 202.102.51.251] 江苏泰州 [202.102.56.0, 202.102.56.255] 江苏连云港\n现在我的问题是，在庞大的地址库中逐一比对IP地址所在的区间，是非常耗时的。假设我们有12万条这样的IP区间与归属地的对应关系，如何快速定位出一个IP地址的归属地呢？\n是不是觉得比较难？不要紧，等学完今天的内容，你就会发现这个问题其实很简单。\n上一节我讲了二分查找的原理，并且介绍了最简单的一种二分查找的代码实现。今天我们来讲几种二分查找的变形问题。\n不知道你有没有听过这样一个说法：“十个二分九个错”。二分查找虽然原理极其简单，但是想要写出没有Bug的二分查找并不容易。\n唐纳德·克努特（Donald E.Knuth）在《计算机程序设计艺术》的第3卷《排序和查找》中说到：“尽管第一个二分查找算法于1946年出现，然而第一个完全正确的二分查找算法实现直到1962年才出现。”\n你可能会说，我们上一节学的二分查找的代码实现并不难写啊。那是因为上一节讲的只是二分查找中最简单的一种情况，在不存在重复元素的有序数组中，查找值等于给定值的元素。最简单的二分查找写起来确实不难，但是，二分查找的变形问题就没那么好写了。\n二分查找的变形问题很多，我只选择几个典型的来讲解，其他的你可以借助我今天讲的思路自己来分析。\n\n需要特别说明一点，为了简化讲解，今天的内容，我都以数据是从小到大排列为前提，如果你要处理的数据是从大到小排列的，解决思路也是一样的。同时，我希望你最好先自己动手试着写一下这4个变形问题，然后再看我的讲述，这样你就会对我说的“二分查找比较难写”有更加深的体会了。\n变体一：查找第一个值等于给定值的元素上一节中的二分查找是最简单的一种，即有序数据集合中不存在重复的数据，我们在其中查找值等于某个给定值的数据。如果我们将这个问题稍微修改下，有序数据集合中存在重复的数据，我们希望找到第一个值等于给定值的数据，这样之前的二分查找代码还能继续工作吗？\n比如下面这样一个有序数组，其中，a[5]，a[6]，a[7]的值都等于8，是重复的数据。我们希望查找第一个等于8的数据，也就是下标是5的元素。\n\n如果我们用上一节课讲的二分查找的代码实现，首先拿8与区间的中间值a[4]比较，8比6大，于是在下标5到9之间继续查找。下标5和9的中间位置是下标7，a[7]正好等于8，所以代码就返回了。\n尽管a[7]也等于8，但它并不是我们想要找的第一个等于8的元素，因为第一个值等于8的元素是数组下标为5的元素。我们上一节讲的二分查找代码就无法处理这种情况了。所以，针对这个变形问题，我们可以稍微改造一下上一节的代码。\n100个人写二分查找就会有100种写法。网上有很多关于变形二分查找的实现方法，有很多写得非常简洁，比如下面这个写法。但是，尽管简洁，理解起来却非常烧脑，也很容易写错。\npublic int bsearch(int[] a, int n, int value) &#123;    int low = 0;    int high = n - 1;    while (low &lt;= high) &#123;        int mid = low + ((high - low) &gt;&gt; 1);        if (a[mid] &gt;= value) &#123;            high = mid - 1;        &#125; else &#123;            low = mid + 1;        &#125;    &#125;    if (low &lt; n &amp;&amp; a[low]==value) return low;    else return -1;&#125;\n看完这个实现之后，你是不是觉得很不好理解？如果你只是死记硬背这个写法，我敢保证，过不了几天，你就会全都忘光，再让你写，90%的可能会写错。所以，我换了一种实现方法，你看看是不是更容易理解呢？\npublic int bsearch(int[] a, int n, int value) &#123;    int low = 0;    int high = n - 1;    while (low &lt;= high) &#123;        int mid =  low + ((high - low) &gt;&gt; 1);        if (a[mid] &gt; value) &#123;            high = mid - 1;        &#125; else if (a[mid] &lt; value) &#123;            low = mid + 1;        &#125; else &#123;            if ((mid == 0) || (a[mid - 1] != value)) return mid;            else high = mid - 1;        &#125;    &#125;    return -1;&#125;\n我来稍微解释一下这段代码。a[mid]跟要查找的value的大小关系有三种情况：大于、小于、等于。对于a[mid]&gt;value的情况，我们需要更新high= mid-1；对于a[mid]&lt;value的情况，我们需要更新low=mid+1。这两点都很好理解。那当a[mid]=value的时候应该如何处理呢？\n如果我们查找的是任意一个值等于给定值的元素，当a[mid]等于要查找的值时，a[mid]就是我们要找的元素。但是，如果我们求解的是第一个值等于给定值的元素，当a[mid]等于要查找的值时，我们就需要确认一下这个a[mid]是不是第一个值等于给定值的元素。\n我们重点看第11行代码。如果mid等于0，那这个元素已经是数组的第一个元素，那它肯定是我们要找的；如果mid不等于0，但a[mid]的前一个元素a[mid-1]不等于value，那也说明a[mid]就是我们要找的第一个值等于给定值的元素。\n如果经过检查之后发现a[mid]前面的一个元素a[mid-1]也等于value，那说明此时的a[mid]肯定不是我们要查找的第一个值等于给定值的元素。那我们就更新high=mid-1，因为要找的元素肯定出现在[low, mid-1]之间。\n对比上面的两段代码，是不是下面那种更好理解？实际上，很多人都觉得变形的二分查找很难写，主要原因是太追求第一种那样完美、简洁的写法。而对于我们做工程开发的人来说，代码易读懂、没Bug，其实更重要，所以我觉得第二种写法更好。\n变体二：查找最后一个值等于给定值的元素前面的问题是查找第一个值等于给定值的元素，我现在把问题稍微改一下，查找最后一个值等于给定值的元素，又该如何做呢？\n如果你掌握了前面的写法，那这个问题你应该很轻松就能解决。你可以先试着实现一下，然后跟我写的对比一下。\npublic int bsearch(int[] a, int n, int value) &#123;    int low = 0;    int high = n - 1;    while (low &lt;= high) &#123;        int mid =  low + ((high - low) &gt;&gt; 1);        if (a[mid] &gt; value) &#123;            high = mid - 1;        &#125; else if (a[mid] &lt; value) &#123;            low = mid + 1;        &#125; else &#123;            if ((mid == n - 1) || (a[mid + 1] != value)) return mid;            else low = mid + 1;        &#125;    &#125;    return -1;&#125;\n我们还是重点看第11行代码。如果a[mid]这个元素已经是数组中的最后一个元素了，那它肯定是我们要找的；如果a[mid]的后一个元素a[mid+1]不等于value，那也说明a[mid]就是我们要找的最后一个值等于给定值的元素。\n如果我们经过检查之后，发现a[mid]后面的一个元素a[mid+1]也等于value，那说明当前的这个a[mid]并不是最后一个值等于给定值的元素。我们就更新low=mid+1，因为要找的元素肯定出现在[mid+1, high]之间。\n变体三：查找第一个大于等于给定值的元素现在我们再来看另外一类变形问题。在有序数组中，查找第一个大于等于给定值的元素。比如，数组中存储的这样一个序列：3，4，6，7，10。如果查找第一个大于等于5的元素，那就是6。\n实际上，实现的思路跟前面的那两种变形问题的实现思路类似，代码写起来甚至更简洁。\npublic int bsearch(int[] a, int n, int value) &#123;    int low = 0;    int high = n - 1;    while (low &lt;= high) &#123;        int mid =  low + ((high - low) &gt;&gt; 1);        if (a[mid] &gt;= value) &#123;            if ((mid == 0) || (a[mid - 1] &lt; value)) return mid;            else high = mid - 1;        &#125; else &#123;            low = mid + 1;        &#125;    &#125;    return -1;&#125;\n如果a[mid]小于要查找的值value，那要查找的值肯定在[mid+1, high]之间，所以，我们更新low=mid+1。\n对于a[mid]大于等于给定值value的情况，我们要先看下这个a[mid]是不是我们要找的第一个值大于等于给定值的元素。如果a[mid]前面已经没有元素，或者前面一个元素小于要查找的值value，那a[mid]就是我们要找的元素。这段逻辑对应的代码是第7行。\n如果a[mid-1]也大于等于要查找的值value，那说明要查找的元素在[low, mid-1]之间，所以，我们将high更新为mid-1。\n变体四：查找最后一个小于等于给定值的元素现在，我们来看最后一种二分查找的变形问题，查找最后一个小于等于给定值的元素。比如，数组中存储了这样一组数据：3，5，6，8，9，10。最后一个小于等于7的元素就是6。是不是有点类似上面那一种？实际上，实现思路也是一样的。\n有了前面的基础，你完全可以自己写出来了，所以我就不详细分析了。我把代码贴出来，你可以写完之后对比一下。\npublic int bsearch7(int[] a, int n, int value) &#123;    int low = 0;    int high = n - 1;    while (low &lt;= high) &#123;        int mid =  low + ((high - low) &gt;&gt; 1);        if (a[mid] &gt; value) &#123;            high = mid - 1;        &#125; else &#123;            if ((mid == n - 1) || (a[mid + 1] &gt; value)) return mid;            else low = mid + 1;        &#125;    &#125;    return -1;&#125;\n解答开篇好了，现在我们回头来看开篇的问题：如何快速定位出一个IP地址的归属地？\n现在这个问题应该很简单了。如果IP区间与归属地的对应关系不经常更新，我们可以先预处理这12万条数据，让其按照起始IP从小到大排序。如何来排序呢？我们知道，IP地址可以转化为32位的整型数。所以，我们可以将起始地址，按照对应的整型值的大小关系，从小到大进行排序。\n然后，这个问题就可以转化为我刚讲的第四种变形问题“在有序数组中，查找最后一个小于等于某个给定值的元素”了。\n当我们要查询某个IP归属地时，我们可以先通过二分查找，找到最后一个起始IP小于等于这个IP的IP区间，然后，检查这个IP是否在这个IP区间内，如果在，我们就取出对应的归属地显示；如果不在，就返回未查找到。\n内容小结上一节我说过，凡是用二分查找能解决的，绝大部分我们更倾向于用散列表或者二叉查找树。即便是二分查找在内存使用上更节省，但是毕竟内存如此紧缺的情况并不多。那二分查找真的没什么用处了吗？\n实际上，上一节讲的求“值等于给定值”的二分查找确实不怎么会被用到，二分查找更适合用在“近似”查找问题，在这类问题上，二分查找的优势更加明显。比如今天讲的这几种变体问题，用其他数据结构，比如散列表、二叉树，就比较难实现了。\n变体的二分查找算法写起来非常烧脑，很容易因为细节处理不好而产生Bug，这些容易出错的细节有：终止条件、区间上下界更新方法、返回值选择。所以今天的内容你最好能用自己实现一遍，对锻炼编码能力、逻辑思维、写出Bug free代码，会很有帮助。\n课后思考我们今天讲的都是非常规的二分查找问题，今天的思考题也是一个非常规的二分查找问题。如果有序数组是一个循环有序数组，比如4，5，6，1，2，3。针对这种情况，如何实现一个求“值等于给定值”的二分查找算法呢？\n欢迎留言和我分享，我会第一时间给你反馈。\n","categories":["数据结构与算法","基础篇"]},{"title":"20 | 散列表（下）：为什么散列表和链表经常会一起使用？","url":"/2020/08/07/20%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%9A%E6%95%A3%E5%88%97%E8%A1%A8%EF%BC%88%E4%B8%8B%EF%BC%89/","content":"我们已经学习了20节内容，你有没有发现，有两种数据结构，散列表和链表，经常会被放在一起使用。你还记得，前面的章节中都有哪些地方讲到散列表和链表的组合使用吗？我带你一起回忆一下。\n在链表那一节，我讲到如何用链表来实现LRU缓存淘汰算法，但是链表实现的LRU缓存淘汰算法的时间复杂度是O(n)，当时我也提到了，通过散列表可以将这个时间复杂度降低到O(1)。\n在跳表那一节，我提到Redis的有序集合是使用跳表来实现的，跳表可以看作一种改进版的链表。当时我们也提到，Redis有序集合不仅使用了跳表，还用到了散列表。\n除此之外，如果你熟悉Java编程语言，你会发现LinkedHashMap这样一个常用的容器，也用到了散列表和链表两种数据结构。\n今天，我们就来看看，在这几个问题中，散列表和链表都是如何组合起来使用的，以及为什么散列表和链表会经常放到一块使用。\nLRU缓存淘汰算法在链表那一节中，我提到，借助散列表，我们可以把LRU缓存淘汰算法的时间复杂度降低为O(1)。现在，我们就来看看它是如何做到的。\n首先，我们来回顾一下当时我们是如何通过链表实现LRU缓存淘汰算法的。\n我们需要维护一个按照访问时间从大到小有序排列的链表结构。因为缓存大小有限，当缓存空间不够，需要淘汰一个数据的时候，我们就直接将链表头部的结点删除。\n当要缓存某个数据的时候，先在链表中查找这个数据。如果没有找到，则直接将数据放到链表的尾部；如果找到了，我们就把它移动到链表的尾部。因为查找数据需要遍历链表，所以单纯用链表实现的LRU缓存淘汰算法的时间复杂很高，是O(n)。\n实际上，我总结一下，一个缓存（cache）系统主要包含下面这几个操作：\n\n往缓存中添加一个数据；\n\n从缓存中删除一个数据；\n\n在缓存中查找一个数据。\n\n\n这三个操作都要涉及“查找”操作，如果单纯地采用链表的话，时间复杂度只能是O(n)。如果我们将散列表和链表两种数据结构组合使用，可以将这三个操作的时间复杂度都降低到O(1)。具体的结构就是下面这个样子：\n\n我们使用双向链表存储数据，链表中的每个结点处理存储数据（data）、前驱指针（prev）、后继指针（next）之外，还新增了一个特殊的字段hnext。这个hnext有什么作用呢？\n因为我们的散列表是通过链表法解决散列冲突的，所以每个结点会在两条链中。一个链是刚刚我们提到的双向链表，另一个链是散列表中的拉链。前驱和后继指针是为了将结点串在双向链表中，hnext指针是为了将结点串在散列表的拉链中。\n了解了这个散列表和双向链表的组合存储结构之后，我们再来看，前面讲到的缓存的三个操作，是如何做到时间复杂度是O(1)的？\n首先，我们来看如何查找一个数据。我们前面讲过，散列表中查找数据的时间复杂度接近O(1)，所以通过散列表，我们可以很快地在缓存中找到一个数据。当找到数据之后，我们还需要将它移动到双向链表的尾部。\n其次，我们来看如何删除一个数据。我们需要找到数据所在的结点，然后将结点删除。借助散列表，我们可以在O(1)时间复杂度里找到要删除的结点。因为我们的链表是双向链表，双向链表可以通过前驱指针O(1)时间复杂度获取前驱结点，所以在双向链表中，删除结点只需要O(1)的时间复杂度。\n最后，我们来看如何添加一个数据。添加数据到缓存稍微有点麻烦，我们需要先看这个数据是否已经在缓存中。如果已经在其中，需要将其移动到双向链表的尾部；如果不在其中，还要看缓存有没有满。如果满了，则将双向链表头部的结点删除，然后再将数据放到链表的尾部；如果没有满，就直接将数据放到链表的尾部。\n这整个过程涉及的查找操作都可以通过散列表来完成。其他的操作，比如删除头结点、链表尾部插入数据等，都可以在O(1)的时间复杂度内完成。所以，这三个操作的时间复杂度都是O(1)。至此，我们就通过散列表和双向链表的组合使用，实现了一个高效的、支持LRU缓存淘汰算法的缓存系统原型。\nRedis有序集合在跳表那一节，讲到有序集合的操作时，我稍微做了些简化。实际上，在有序集合中，每个成员对象有两个重要的属性，key（键值）和score（分值）。我们不仅会通过score来查找数据，还会通过key来查找数据。\n举个例子，比如用户积分排行榜有这样一个功能：我们可以通过用户的ID来查找积分信息，也可以通过积分区间来查找用户ID或者姓名信息。这里包含ID、姓名和积分的用户信息，就是成员对象，用户ID就是key，积分就是score。\n所以，如果我们细化一下Redis有序集合的操作，那就是下面这样：\n\n添加一个成员对象；\n\n按照键值来删除一个成员对象；\n\n按照键值来查找一个成员对象；\n\n按照分值区间查找数据，比如查找积分在[100, 356]之间的成员对象；\n\n按照分值从小到大排序成员变量；\n\n\n如果我们仅仅按照分值将成员对象组织成跳表的结构，那按照键值来删除、查询成员对象就会很慢，解决方法与LRU缓存淘汰算法的解决方法类似。我们可以再按照键值构建一个散列表，这样按照key来删除、查找一个成员对象的时间复杂度就变成了O(1)。同时，借助跳表结构，其他操作也非常高效。\n实际上，Redis有序集合的操作还有另外一类，也就是查找成员对象的排名（Rank）或者根据排名区间查找成员对象。这个功能单纯用刚刚讲的这种组合结构就无法高效实现了。这块内容我后面的章节再讲。\nJava LinkedHashMap前面我们讲了两个散列表和链表结合的例子，现在我们再来看另外一个，Java中的LinkedHashMap这种容器。\n如果你熟悉Java，那你几乎天天会用到这个容器。我们之前讲过，HashMap底层是通过散列表这种数据结构实现的。而LinkedHashMap前面比HashMap多了一个“Linked”，这里的“Linked”是不是说，LinkedHashMap是一个通过链表法解决散列冲突的散列表呢？\n实际上，LinkedHashMap并没有这么简单，其中的“Linked”也并不仅仅代表它是通过链表法解决散列冲突的。关于这一点，在我是初学者的时候，也误解了很久。\n我们先来看一段代码。你觉得这段代码会以什么样的顺序打印3，1，5，2这几个key呢？原因又是什么呢？\nHashMap&lt;Integer, Integer&gt; m &#x3D; new LinkedHashMap&lt;&gt;();m.put(3, 11);m.put(1, 12);m.put(5, 23);m.put(2, 22);for (Map.Entry e : m.entrySet()) &#123;  System.out.println(e.getKey());&#125;\n我先告诉你答案，上面的代码会按照数据插入的顺序依次来打印，也就是说，打印的顺序就是3，1，5，2。你有没有觉得奇怪？散列表中数据是经过散列函数打乱之后无规律存储的，这里是如何实现按照数据的插入顺序来遍历打印的呢？\n你可能已经猜到了，LinkedHashMap也是通过散列表和链表组合在一起实现的。实际上，它不仅支持按照插入顺序遍历数据，还支持按照访问顺序来遍历数据。你可以看下面这段代码：\n&#x2F;&#x2F; 10是初始大小，0.75是装载因子，true是表示按照访问时间排序HashMap&lt;Integer, Integer&gt; m &#x3D; new LinkedHashMap&lt;&gt;(10, 0.75f, true);m.put(3, 11);m.put(1, 12);m.put(5, 23);m.put(2, 22);m.put(3, 26);m.get(5);for (Map.Entry e : m.entrySet()) &#123;  System.out.println(e.getKey());&#125;\n这段代码打印的结果是1，2，3，5。我来具体分析一下，为什么这段代码会按照这样顺序来打印。\n每次调用put()函数，往LinkedHashMap中添加数据的时候，都会将数据添加到链表的尾部，所以，在前四个操作完成之后，链表中的数据是下面这样：\n\n在第8行代码中，再次将键值为3的数据放入到LinkedHashMap的时候，会先查找这个键值是否已经有了，然后，再将已经存在的(3,11)删除，并且将新的(3,26)放到链表的尾部。所以，这个时候链表中的数据就是下面这样：\n\n当第9行代码访问到key为5的数据的时候，我们将被访问到的数据移动到链表的尾部。所以，第9行代码之后，链表中的数据是下面这样：\n\n所以，最后打印出来的数据是1，2，3，5。从上面的分析，你有没有发现，按照访问时间排序的LinkedHashMap本身就是一个支持LRU缓存淘汰策略的缓存系统？实际上，它们两个的实现原理也是一模一样的。我也就不再啰嗦了。\n我现在来总结一下，实际上，LinkedHashMap是通过双向链表和散列表这两种数据结构组合实现的。LinkedHashMap中的“Linked”实际上是指的是双向链表，并非指用链表法解决散列冲突。\n解答开篇&amp;内容小结弄懂刚刚我讲的这三个例子，开篇的问题也就不言而喻了。我这里总结一下，为什么散列表和链表经常一块使用？\n散列表这种数据结构虽然支持非常高效的数据插入、删除、查找操作，但是散列表中的数据都是通过散列函数打乱之后无规律存储的。也就说，它无法支持按照某种顺序快速地遍历数据。如果希望按照顺序遍历散列表中的数据，那我们需要将散列表中的数据拷贝到数组中，然后排序，再遍历。\n因为散列表是动态数据结构，不停地有数据的插入、删除，所以每当我们希望按顺序遍历散列表中的数据的时候，都需要先排序，那效率势必会很低。为了解决这个问题，我们将散列表和链表（或者跳表）结合在一起使用。\n课后思考\n今天讲的几个散列表和链表结合使用的例子里，我们用的都是双向链表。如果把双向链表改成单链表，还能否正常工作呢？为什么呢？\n\n假设猎聘网有10万名猎头，每个猎头都可以通过做任务（比如发布职位）来积累积分，然后通过积分来下载简历。假设你是猎聘网的一名工程师，如何在内存中存储这10万个猎头ID和积分信息，让它能够支持这样几个操作：\n\n根据猎头的ID快速查找、删除、更新这个猎头的积分信息；\n\n查找积分在某个区间的猎头ID列表；\n\n查找按照积分从小到大排名在第x位到第y位之间的猎头ID列表。\n\n\n欢迎留言和我分享，我会第一时间给你反馈。\n","categories":["数据结构与算法","基础篇"]},{"title":"18 | 散列表（上）：Word文档中的单词拼写检查功能是如何实现的？","url":"/2020/08/07/18%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%9A%E6%95%A3%E5%88%97%E8%A1%A8%EF%BC%88%E4%B8%8A%EF%BC%89/","content":"Word这种文本编辑器你平时应该经常用吧，那你有没有留意过它的拼写检查功能呢？一旦我们在Word里输入一个错误的英文单词，它就会用标红的方式提示“拼写错误”。Word的这个单词拼写检查功能，虽然很小但却非常实用。你有没有想过，这个功能是如何实现的呢？\n其实啊，一点儿都不难。只要你学完今天的内容，散列表（Hash Table）。你就能像微软Office的工程师一样，轻松实现这个功能。\n散列思想散列表的英文叫“Hash Table”，我们平时也叫它“哈希表”或者“Hash表”，你一定也经常听过它，我在前面的文章里，也不止一次提到过，但是你是不是真的理解这种数据结构呢？\n散列表用的是数组支持按照下标随机访问数据的特性，所以散列表其实就是数组的一种扩展，由数组演化而来。可以说，如果没有数组，就没有散列表。\n我用一个例子来解释一下。假如我们有89名选手参加学校运动会。为了方便记录成绩，每个选手胸前都会贴上自己的参赛号码。这89名选手的编号依次是1到89。现在我们希望编程实现这样一个功能，通过编号快速找到对应的选手信息。你会怎么做呢？\n我们可以把这89名选手的信息放在数组里。编号为1的选手，我们放到数组中下标为1的位置；编号为2的选手，我们放到数组中下标为2的位置。以此类推，编号为k的选手放到数组中下标为k的位置。\n因为参赛编号跟数组下标一一对应，当我们需要查询参赛编号为x的选手的时候，我们只需要将下标为x的数组元素取出来就可以了，时间复杂度就是O(1)。这样按照编号查找选手信息，效率是不是很高？\n实际上，这个例子已经用到了散列的思想。在这个例子里，参赛编号是自然数，并且与数组的下标形成一一映射，所以利用数组支持根据下标随机访问的时候，时间复杂度是O(1)这一特性，就可以实现快速查找编号对应的选手信息。\n你可能要说了，这个例子中蕴含的散列思想还不够明显，那我来改造一下这个例子。\n假设校长说，参赛编号不能设置得这么简单，要加上年级、班级这些更详细的信息，所以我们把编号的规则稍微修改了一下，用6位数字来表示。比如051167，其中，前两位05表示年级，中间两位11表示班级，最后两位还是原来的编号1到89。这个时候我们该如何存储选手信息，才能够支持通过编号来快速查找选手信息呢？\n思路还是跟前面类似。尽管我们不能直接把编号作为数组下标，但我们可以截取参赛编号的后两位作为数组下标，来存取选手信息数据。当通过参赛编号查询选手信息的时候，我们用同样的方法，取参赛编号的后两位，作为数组下标，来读取数组中的数据。\n这就是典型的散列思想。其中，参赛选手的编号我们叫做键（key）或者关键字。我们用它来标识一个选手。我们把参赛编号转化为数组下标的映射方法就叫作散列函数（或“Hash函数”“哈希函数”），而散列函数计算得到的值就叫作散列值（或“Hash值”“哈希值”）。\n\n通过这个例子，我们可以总结出这样的规律：散列表用的就是数组支持按照下标随机访问的时候，时间复杂度是O(1)的特性。我们通过散列函数把元素的键值映射为下标，然后将数据存储在数组中对应下标的位置。当我们按照键值查询元素时，我们用同样的散列函数，将键值转化数组下标，从对应的数组下标的位置取数据。\n散列函数从上面的例子我们可以看到，散列函数在散列表中起着非常关键的作用。现在我们就来学习下散列函数。\n散列函数，顾名思义，它是一个函数。我们可以把它定义成hash(key)，其中key表示元素的键值，hash(key)的值表示经过散列函数计算得到的散列值。\n那第一个例子中，编号就是数组下标，所以hash(key)就等于key。改造后的例子，写成散列函数稍微有点复杂。我用伪代码将它写成函数就是下面这样：\nint hash(String key) &#123;    // 获取后两位字符    string lastTwoChars = key.substr(length-2, length);    // 将后两位字符转换为整数    int hashValue = convert lastTwoChas to int-type;    return hashValue;&#125;\n刚刚举的学校运动会的例子，散列函数比较简单，也比较容易想到。但是，如果参赛选手的编号是随机生成的6位数字，又或者用的是a到z之间的字符串，该如何构造散列函数呢？我总结了三点散列函数设计的基本要求：\n\n散列函数计算得到的散列值是一个非负整数；\n\n如果key1 = key2，那hash(key1) == hash(key2)；\n\n如果key1 ≠ key2，那hash(key1) ≠ hash(key2)。\n\n\n我来解释一下这三点。其中，第一点理解起来应该没有任何问题。因为数组下标是从0开始的，所以散列函数生成的散列值也要是非负整数。第二点也很好理解。相同的key，经过散列函数得到的散列值也应该是相同的。\n第三点理解起来可能会有问题，我着重说一下。这个要求看起来合情合理，但是在真实的情况下，要想找到一个不同的key对应的散列值都不一样的散列函数，几乎是不可能的。即便像业界著名的MD5、SHA、CRC等哈希算法，也无法完全避免这种散列冲突。而且，因为数组的存储空间有限，也会加大散列冲突的概率。\n所以我们几乎无法找到一个完美的无冲突的散列函数，即便能找到，付出的时间成本、计算成本也是很大的，所以针对散列冲突问题，我们需要通过其他途径来解决。\n散列冲突再好的散列函数也无法避免散列冲突。那究竟该如何解决散列冲突问题呢？我们常用的散列冲突解决方法有两类，开放寻址法（open addressing）和链表法（chaining）。\n1.开放寻址法开放寻址法的核心思想是，如果出现了散列冲突，我们就重新探测一个空闲位置，将其插入。那如何重新探测新的位置呢？我先讲一个比较简单的探测方法，线性探测（Linear Probing）。\n当我们往散列表中插入数据时，如果某个数据经过散列函数散列之后，存储位置已经被占用了，我们就从当前位置开始，依次往后查找，看是否有空闲位置，直到找到为止。\n我说的可能比较抽象，我举一个例子具体给你说明一下。这里面黄色的色块表示空闲位置，橙色的色块表示已经存储了数据。\n\n从图中可以看出，散列表的大小为10，在元素x插入散列表之前，已经6个元素插入到散列表中。x经过Hash算法之后，被散列到位置下标为7的位置，但是这个位置已经有数据了，所以就产生了冲突。于是我们就顺序地往后一个一个找，看有没有空闲的位置，遍历到尾部都没有找到空闲的位置，于是我们再从表头开始找，直到找到空闲位置2，于是将其插入到这个位置。\n在散列表中查找元素的过程有点儿类似插入过程。我们通过散列函数求出要查找元素的键值对应的散列值，然后比较数组中下标为散列值的元素和要查找的元素。如果相等，则说明就是我们要找的元素；否则就顺序往后依次查找。如果遍历到数组中的空闲位置，还没有找到，就说明要查找的元素并没有在散列表中。\n\n散列表跟数组一样，不仅支持插入、查找操作，还支持删除操作。对于使用线性探测法解决冲突的散列表，删除操作稍微有些特别。我们不能单纯地把要删除的元素设置为空。这是为什么呢？\n还记得我们刚讲的查找操作吗？在查找的时候，一旦我们通过线性探测方法，找到一个空闲位置，我们就可以认定散列表中不存在这个数据。但是，如果这个空闲位置是我们后来删除的，就会导致原来的查找算法失效。本来存在的数据，会被认定为不存在。这个问题如何解决呢？\n我们可以将删除的元素，特殊标记为deleted。当线性探测查找的时候，遇到标记为deleted的空间，并不是停下来，而是继续往下探测。\n\n你可能已经发现了，线性探测法其实存在很大问题。当散列表中插入的数据越来越多时，散列冲突发生的可能性就会越来越大，空闲位置会越来越少，线性探测的时间就会越来越久。极端情况下，我们可能需要探测整个散列表，所以最坏情况下的时间复杂度为O(n)。同理，在删除和查找时，也有可能会线性探测整张散列表，才能找到要查找或者删除的数据。\n对于开放寻址冲突解决方法，除了线性探测方法之外，还有另外两种比较经典的探测方法，二次探测（Quadratic probing）和双重散列（Double hashing）。\n所谓二次探测，跟线性探测很像，线性探测每次探测的步长是1，那它探测的下标序列就是hash(key)+0，hash(key)+1，hash(key)+2……而二次探测探测的步长就变成了原来的“二次方”，也就是说，它探测的下标序列就是hash(key)+0，hash(key)+12，hash(key)+22……\n所谓双重散列，意思就是不仅要使用一个散列函数。我们使用一组散列函数hash1(key)，hash2(key)，hash3(key)……我们先用第一个散列函数，如果计算得到的存储位置已经被占用，再用第二个散列函数，依次类推，直到找到空闲的存储位置。\n不管采用哪种探测方法，当散列表中空闲位置不多的时候，散列冲突的概率就会大大提高。为了尽可能保证散列表的操作效率，一般情况下，我们会尽可能保证散列表中有一定比例的空闲槽位。我们用装载因子（load factor）来表示空位的多少。\n装载因子的计算公式是：\n散列表的装载因子&#x3D;填入表中的元素个数&#x2F;散列表的长度\n装载因子越大，说明空闲位置越少，冲突越多，散列表的性能会下降。\n2.链表法链表法是一种更加常用的散列冲突解决办法，相比开放寻址法，它要简单很多。我们来看这个图，在散列表中，每个“桶（bucket）”或者“槽（slot）”会对应一条链表，所有散列值相同的元素我们都放到相同槽位对应的链表中。\n\n当插入的时候，我们只需要通过散列函数计算出对应的散列槽位，将其插入到对应链表中即可，所以插入的时间复杂度是O(1)。当查找、删除一个元素时，我们同样通过散列函数计算出对应的槽，然后遍历链表查找或者删除。那查找或删除操作的时间复杂度是多少呢？\n实际上，这两个操作的时间复杂度跟链表的长度k成正比，也就是O(k)。对于散列比较均匀的散列函数来说，理论上讲，k=n/m，其中n表示散列中数据的个数，m表示散列表中“槽”的个数。\n解答开篇有了前面这些基本知识储备，我们来看一下开篇的思考题：Word文档中单词拼写检查功能是如何实现的？\n常用的英文单词有20万个左右，假设单词的平均长度是10个字母，平均一个单词占用10个字节的内存空间，那20万英文单词大约占2MB的存储空间，就算放大10倍也就是20MB。对于现在的计算机来说，这个大小完全可以放在内存里面。所以我们可以用散列表来存储整个英文单词词典。\n当用户输入某个英文单词时，我们拿用户输入的单词去散列表中查找。如果查到，则说明拼写正确；如果没有查到，则说明拼写可能有误，给予提示。借助散列表这种数据结构，我们就可以轻松实现快速判断是否存在拼写错误。\n内容小结今天我讲了一些比较基础、比较偏理论的散列表知识，包括散列表的由来、散列函数、散列冲突的解决方法。\n散列表来源于数组，它借助散列函数对数组这种数据结构进行扩展，利用的是数组支持按照下标随机访问元素的特性。散列表两个核心问题是散列函数设计和散列冲突解决。散列冲突有两种常用的解决方法，开放寻址法和链表法。散列函数设计的好坏决定了散列冲突的概率，也就决定散列表的性能。\n针对散列函数和散列冲突，今天我只讲了一些基础的概念、方法，下一节我会更贴近实战、更加深入探讨这两个问题。\n课后思考\n假设我们有10万条URL访问日志，如何按照访问次数给URL排序？\n\n有两个字符串数组，每个数组大约有10万条字符串，如何快速找出两个数组中相同的字符串？\n\n\n欢迎留言和我分享，我会第一时间给你反馈。\n","categories":["数据结构与算法","基础篇"]},{"title":"23 | 二叉树基础（上）：什么样的二叉树适合用数组来存储？","url":"/2020/08/07/23%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%9A%E4%BA%8C%E5%8F%89%E6%A0%91%E5%9F%BA%E7%A1%80%EF%BC%88%E4%B8%8A%EF%BC%89/","content":"前面我们讲的都是线性表结构，栈、队列等等。今天我们讲一种非线性表结构，树。树这种数据结构比线性表的数据结构要复杂得多，内容也比较多，所以我会分四节来讲解。\n\n我反复强调过，带着问题学习，是最有效的学习方式之一，所以在正式的内容开始之前，我还是给你出一道思考题：二叉树有哪几种存储方式？什么样的二叉树适合用数组来存储？\n带着这些问题，我们就来学习今天的内容，树！\n树（Tree）我们首先来看，什么是“树”？再完备的定义，都没有图直观。所以我在图中画了几棵“树”。你来看看，这些“树”都有什么特征？\n\n你有没有发现，“树”这种数据结构真的很像我们现实生活中的“树”，这里面每个元素我们叫做“节点”；用来连接相邻节点之间的关系，我们叫做“父子关系”。\n比如下面这幅图，A节点就是B节点的父节点，B节点是A节点的子节点。B、C、D这三个节点的父节点是同一个节点，所以它们之间互称为兄弟节点。我们把没有父节点的节点叫做根节点，也就是图中的节点E。我们把没有子节点的节点叫做叶子节点或者叶节点，比如图中的G、H、I、J、K、L都是叶子节点。\n\n除此之外，关于“树”，还有三个比较相似的概念：高度（Height）、深度（Depth）、层（Level）。它们的定义是这样的：\n\n这三个概念的定义比较容易混淆，描述起来也比较空洞。我举个例子说明一下，你一看应该就能明白。\n\n记这几个概念，我还有一个小窍门，就是类比“高度”“深度”“层”这几个名词在生活中的含义。\n在我们的生活中，“高度”这个概念，其实就是从下往上度量，比如我们要度量第10层楼的高度、第13层楼的高度，起点都是地面。所以，树这种数据结构的高度也是一样，从最底层开始计数，并且计数的起点是0。\n“深度”这个概念在生活中是从上往下度量的，比如水中鱼的深度，是从水平面开始度量的。所以，树这种数据结构的深度也是类似的，从根结点开始度量，并且计数起点也是0。\n“层数”跟深度的计算类似，不过，计数起点是1，也就是说根节点位于第1层。\n二叉树（Binary Tree）树结构多种多样，不过我们最常用还是二叉树。\n二叉树，顾名思义，每个节点最多有两个“叉”，也就是两个子节点，分别是左子节点和右子节点。不过，二叉树并不要求每个节点都有两个子节点，有的节点只有左子节点，有的节点只有右子节点。我画的这几个都是二叉树。以此类推，你可以想象一下四叉树、八叉树长什么样子。\n\n这个图里面，有两个比较特殊的二叉树，分别是编号2和编号3这两个。\n其中，编号2的二叉树中，叶子节点全都在最底层，除了叶子节点之外，每个节点都有左右两个子节点，这种二叉树就叫做满二叉树。\n编号3的二叉树中，叶子节点都在最底下两层，最后一层的叶子节点都靠左排列，并且除了最后一层，其他层的节点个数都要达到最大，这种二叉树叫做完全二叉树。\n满二叉树很好理解，也很好识别，但是完全二叉树，有的人可能就分不清了。我画了几个完全二叉树和非完全二叉树的例子，你可以对比着看看。\n\n你可能会说，满二叉树的特征非常明显，我们把它单独拎出来讲，这个可以理解。但是完全二叉树的特征不怎么明显啊，单从长相上来看，完全二叉树并没有特别特殊的地方啊，更像是“芸芸众树”中的一种。\n那我们为什么还要特意把它拎出来讲呢？为什么偏偏把最后一层的叶子节点靠左排列的叫完全二叉树？如果靠右排列就不能叫完全二叉树了吗？这个定义的由来或者说目的在哪里？\n要理解完全二叉树定义的由来，我们需要先了解，如何表示（或者存储）一棵二叉树？\n想要存储一棵二叉树，我们有两种方法，一种是基于指针或者引用的二叉链式存储法，一种是基于数组的顺序存储法。\n我们先来看比较简单、直观的链式存储法。从图中你应该可以很清楚地看到，每个节点有三个字段，其中一个存储数据，另外两个是指向左右子节点的指针。我们只要拎住根节点，就可以通过左右子节点的指针，把整棵树都串起来。这种存储方式我们比较常用。大部分二叉树代码都是通过这种结构来实现的。\n\n我们再来看，基于数组的顺序存储法。我们把根节点存储在下标i = 1的位置，那左子节点存储在下标2 * i = 2的位置，右子节点存储在2 * i + 1 = 3的位置。以此类推，B节点的左子节点存储在2 * i = 2 * 2 = 4的位置，右子节点存储在2 * i + 1 = 2 * 2 + 1 = 5的位置。\n\n我来总结一下，如果节点X存储在数组中下标为i的位置，下标为2 * i 的位置存储的就是左子节点，下标为2 * i + 1的位置存储的就是右子节点。反过来，下标为i/2的位置存储就是它的父节点。通过这种方式，我们只要知道根节点存储的位置（一般情况下，为了方便计算子节点，根节点会存储在下标为1的位置），这样就可以通过下标计算，把整棵树都串起来。\n不过，我刚刚举的例子是一棵完全二叉树，所以仅仅“浪费”了一个下标为0的存储位置。如果是非完全二叉树，其实会浪费比较多的数组存储空间。你可以看我举的下面这个例子。\n\n所以，如果某棵二叉树是一棵完全二叉树，那用数组存储无疑是最节省内存的一种方式。因为数组的存储方式并不需要像链式存储法那样，要存储额外的左右子节点的指针。这也是为什么完全二叉树会单独拎出来的原因，也是为什么完全二叉树要求最后一层的子节点都靠左的原因。\n当我们讲到堆和堆排序的时候，你会发现，堆其实就是一种完全二叉树，最常用的存储方式就是数组。\n二叉树的遍历前面我讲了二叉树的基本定义和存储方法，现在我们来看二叉树中非常重要的操作，二叉树的遍历。这也是非常常见的面试题。\n如何将所有节点都遍历打印出来呢？经典的方法有三种，前序遍历、中序遍历和后序遍历。其中，前、中、后序，表示的是节点与它的左右子树节点遍历打印的先后顺序。\n\n前序遍历是指，对于树中的任意节点来说，先打印这个节点，然后再打印它的左子树，最后打印它的右子树。\n\n中序遍历是指，对于树中的任意节点来说，先打印它的左子树，然后再打印它本身，最后打印它的右子树。\n\n后序遍历是指，对于树中的任意节点来说，先打印它的左子树，然后再打印它的右子树，最后打印这个节点本身。\n\n\n\n实际上，二叉树的前、中、后序遍历就是一个递归的过程。比如，前序遍历，其实就是先打印根节点，然后再递归地打印左子树，最后递归地打印右子树。\n写递归代码的关键，就是看能不能写出递推公式，而写递推公式的关键就是，如果要解决问题A，就假设子问题B、C已经解决，然后再来看如何利用B、C来解决A。所以，我们可以把前、中、后序遍历的递推公式都写出来。\n前序遍历的递推公式：preOrder(r) &#x3D; print r-&gt;preOrder(r-&gt;left)-&gt;preOrder(r-&gt;right)中序遍历的递推公式：inOrder(r) &#x3D; inOrder(r-&gt;left)-&gt;print r-&gt;inOrder(r-&gt;right)后序遍历的递推公式：postOrder(r) &#x3D; postOrder(r-&gt;left)-&gt;postOrder(r-&gt;right)-&gt;print r\n有了递推公式，代码写起来就简单多了。这三种遍历方式的代码，我都写出来了，你可以看看。\nvoid preOrder(Node* root) &#123;  if (root &#x3D;&#x3D; null) return;  print root &#x2F;&#x2F; 此处为伪代码，表示打印root节点  preOrder(root-&gt;left);  preOrder(root-&gt;right);&#125;void inOrder(Node* root) &#123;  if (root &#x3D;&#x3D; null) return;  inOrder(root-&gt;left);  print root &#x2F;&#x2F; 此处为伪代码，表示打印root节点  inOrder(root-&gt;right);&#125;void postOrder(Node* root) &#123;  if (root &#x3D;&#x3D; null) return;  postOrder(root-&gt;left);  postOrder(root-&gt;right);  print root &#x2F;&#x2F; 此处为伪代码，表示打印root节点&#125;\n二叉树的前、中、后序遍历的递归实现是不是很简单？你知道二叉树遍历的时间复杂度是多少吗？我们一起来看看。\n从我前面画的前、中、后序遍历的顺序图，可以看出来，每个节点最多会被访问两次，所以遍历操作的时间复杂度，跟节点的个数n成正比，也就是说二叉树遍历的时间复杂度是O(n)。\n解答开篇&amp;内容小结今天，我讲了一种非线性表数据结构，树。关于树，有几个比较常用的概念你需要掌握，那就是：根节点、叶子节点、父节点、子节点、兄弟节点，还有节点的高度、深度、层数，以及树的高度。\n我们平时最常用的树就是二叉树。二叉树的每个节点最多有两个子节点，分别是左子节点和右子节点。二叉树中，有两种比较特殊的树，分别是满二叉树和完全二叉树。满二叉树又是完全二叉树的一种特殊情况。\n二叉树既可以用链式存储，也可以用数组顺序存储。数组顺序存储的方式比较适合完全二叉树，其他类型的二叉树用数组存储会比较浪费存储空间。除此之外，二叉树里非常重要的操作就是前、中、后序遍历操作，遍历的时间复杂度是O(n)，你需要理解并能用递归代码来实现。\n课后思考\n给定一组数据，比如1，3，5，6，9，10。你来算算，可以构建出多少种不同的二叉树？\n\n我们讲了三种二叉树的遍历方式，前、中、后序。实际上，还有另外一种遍历方式，也就是按层遍历，你知道如何实现吗？\n\n\n欢迎留言和我分享，我会第一时间给你反馈。\n","categories":["数据结构与算法","基础篇"]},{"title":"22 | 哈希算法（下）：哈希算法在分布式系统中有哪些应用？","url":"/2020/08/07/22%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%9A%E5%93%88%E5%B8%8C%E7%AE%97%E6%B3%95%EF%BC%88%E4%B8%8B%EF%BC%89/","content":"上一节，我讲了哈希算法的四个应用，它们分别是：安全加密、数据校验、唯一标识、散列函数。今天，我们再来看剩余三种应用：负载均衡、数据分片、分布式存储。\n你可能已经发现，这三个应用都跟分布式系统有关。没错，今天我就带你看下，哈希算法是如何解决这些分布式问题的。\n应用五：负载均衡我们知道，负载均衡算法有很多，比如轮询、随机、加权轮询等。那如何才能实现一个会话粘滞（session sticky）的负载均衡算法呢？也就是说，我们需要在同一个客户端上，在一次会话中的所有请求都路由到同一个服务器上。\n最直接的方法就是，维护一张映射关系表，这张表的内容是客户端IP地址或者会话ID与服务器编号的映射关系。客户端发出的每次请求，都要先在映射表中查找应该路由到的服务器编号，然后再请求编号对应的服务器。这种方法简单直观，但也有几个弊端：\n\n如果客户端很多，映射表可能会很大，比较浪费内存空间；\n\n客户端下线、上线，服务器扩容、缩容都会导致映射失效，这样维护映射表的成本就会很大；\n\n\n如果借助哈希算法，这些问题都可以非常完美地解决。我们可以通过哈希算法，对客户端IP地址或者会话ID计算哈希值，将取得的哈希值与服务器列表的大小进行取模运算，最终得到的值就是应该被路由到的服务器编号。 这样，我们就可以把同一个IP过来的所有请求，都路由到同一个后端服务器上。\n应用六：数据分片哈希算法还可以用于数据的分片。我这里有两个例子。\n1.如何统计“搜索关键词”出现的次数？假如我们有1T的日志文件，这里面记录了用户的搜索关键词，我们想要快速统计出每个关键词被搜索的次数，该怎么做呢？\n我们来分析一下。这个问题有两个难点，第一个是搜索日志很大，没办法放到一台机器的内存中。第二个难点是，如果只用一台机器来处理这么巨大的数据，处理时间会很长。\n针对这两个难点，我们可以先对数据进行分片，然后采用多台机器处理的方法，来提高处理速度。具体的思路是这样的：为了提高处理的速度，我们用n台机器并行处理。我们从搜索记录的日志文件中，依次读出每个搜索关键词，并且通过哈希函数计算哈希值，然后再跟n取模，最终得到的值，就是应该被分配到的机器编号。\n这样，哈希值相同的搜索关键词就被分配到了同一个机器上。也就是说，同一个搜索关键词会被分配到同一个机器上。每个机器会分别计算关键词出现的次数，最后合并起来就是最终的结果。\n实际上，这里的处理过程也是MapReduce的基本设计思想。\n2.如何快速判断图片是否在图库中？如何快速判断图片是否在图库中？上一节我们讲过这个例子，不知道你还记得吗？当时我介绍了一种方法，即给每个图片取唯一标识（或者信息摘要），然后构建散列表。\n假设现在我们的图库中有1亿张图片，很显然，在单台机器上构建散列表是行不通的。因为单台机器的内存有限，而1亿张图片构建散列表显然远远超过了单台机器的内存上限。\n我们同样可以对数据进行分片，然后采用多机处理。我们准备n台机器，让每台机器只维护某一部分图片对应的散列表。我们每次从图库中读取一个图片，计算唯一标识，然后与机器个数n求余取模，得到的值就对应要分配的机器编号，然后将这个图片的唯一标识和图片路径发往对应的机器构建散列表。\n当我们要判断一个图片是否在图库中的时候，我们通过同样的哈希算法，计算这个图片的唯一标识，然后与机器个数n求余取模。假设得到的值是k，那就去编号k的机器构建的散列表中查找。\n现在，我们来估算一下，给这1亿张图片构建散列表大约需要多少台机器。\n散列表中每个数据单元包含两个信息，哈希值和图片文件的路径。假设我们通过MD5来计算哈希值，那长度就是128比特，也就是16字节。文件路径长度的上限是256字节，我们可以假设平均长度是128字节。如果我们用链表法来解决冲突，那还需要存储指针，指针只占用8字节。所以，散列表中每个数据单元就占用152字节（这里只是估算，并不准确）。\n假设一台机器的内存大小为2GB，散列表的装载因子为0.75，那一台机器可以给大约1000万（2GB*0.75/152）张图片构建散列表。所以，如果要对1亿张图片构建索引，需要大约十几台机器。在工程中，这种估算还是很重要的，能让我们事先对需要投入的资源、资金有个大概的了解，能更好地评估解决方案的可行性。\n实际上，针对这种海量数据的处理问题，我们都可以采用多机分布式处理。借助这种分片的思路，可以突破单机内存、CPU等资源的限制。\n应用七：分布式存储现在互联网面对的都是海量的数据、海量的用户。我们为了提高数据的读取、写入能力，一般都采用分布式的方式来存储数据，比如分布式缓存。我们有海量的数据需要缓存，所以一个缓存机器肯定是不够的。于是，我们就需要将数据分布在多台机器上。\n该如何决定将哪个数据放到哪个机器上呢？我们可以借用前面数据分片的思想，即通过哈希算法对数据取哈希值，然后对机器个数取模，这个最终值就是应该存储的缓存机器编号。\n但是，如果数据增多，原来的10个机器已经无法承受了，我们就需要扩容了，比如扩到11个机器，这时候麻烦就来了。因为，这里并不是简单地加个机器就可以了。\n原来的数据是通过与10来取模的。比如13这个数据，存储在编号为3这台机器上。但是新加了一台机器中，我们对数据按照11取模，原来13这个数据就被分配到2号这台机器上了。\n\n因此，所有的数据都要重新计算哈希值，然后重新搬移到正确的机器上。这样就相当于，缓存中的数据一下子就都失效了。所有的数据请求都会穿透缓存，直接去请求数据库。这样就可能发生雪崩效应，压垮数据库。\n所以，我们需要一种方法，使得在新加入一个机器后，并不需要做大量的数据搬移。这时候，一致性哈希算法就要登场了。\n假设我们有k个机器，数据的哈希值的范围是[0, MAX]。我们将整个范围划分成m个小区间（m远大于k），每个机器负责m/k个小区间。当有新机器加入的时候，我们就将某几个小区间的数据，从原来的机器中搬移到新的机器中。这样，既不用全部重新哈希、搬移数据，也保持了各个机器上数据数量的均衡。\n一致性哈希算法的基本思想就是这么简单。除此之外，它还会借助一个虚拟的环和虚拟结点，更加优美地实现出来。这里我就不展开讲了，如果感兴趣，你可以看下这个介绍。\n除了我们上面讲到的分布式缓存，实际上，一致性哈希算法的应用非常广泛，在很多分布式存储系统中，都可以见到一致性哈希算法的影子。\n解答开篇&amp;内容小结这两节的内容理论不多，比较贴近具体的开发。今天我讲了三种哈希算法在分布式系统中的应用，它们分别是：负载均衡、数据分片、分布式存储。\n在负载均衡应用中，利用哈希算法替代映射表，可以实现一个会话粘滞的负载均衡策略。在数据分片应用中，通过哈希算法对处理的海量数据进行分片，多机分布式处理，可以突破单机资源的限制。在分布式存储应用中，利用一致性哈希算法，可以解决缓存等分布式系统的扩容、缩容导致数据大量搬移的难题。\n课后思考这两节我总共讲了七个哈希算法的应用。实际上，我讲的也只是冰山一角，哈希算法还有很多其他的应用，比如网络协议中的CRC校验、Git commit id等等。除了这些，你还能想到其他用到哈希算法的地方吗？\n欢迎留言和我分享，我会第一时间给你反馈。\n","categories":["数据结构与算法","基础篇"]},{"title":"21 | 哈希算法（上）：如何防止数据库中的用户信息被脱库？","url":"/2020/08/07/21%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%9A%E5%93%88%E5%B8%8C%E7%AE%97%E6%B3%95%EF%BC%88%E4%B8%8A%EF%BC%89/","content":"还记得2011年CSDN的“脱库”事件吗？当时，CSDN网站被黑客攻击，超过600万用户的注册邮箱和密码明文被泄露，很多网友对CSDN明文保存用户密码行为产生了不满。如果你是CSDN的一名工程师，你会如何存储用户密码这么重要的数据吗？仅仅MD5加密一下存储就够了吗？ 要想搞清楚这个问题，就要先弄明白哈希算法。\n哈希算法历史悠久，业界著名的哈希算法也有很多，比如MD5、SHA等。在我们平时的开发中，基本上都是拿现成的直接用。所以，我今天不会重点剖析哈希算法的原理，也不会教你如何设计一个哈希算法，而是从实战的角度告诉你，在实际的开发中，我们该如何用哈希算法解决问题。\n什么是哈希算法？我们前面几节讲到“散列表”“散列函数”，这里又讲到“哈希算法”，你是不是有点一头雾水？实际上，不管是“散列”还是“哈希”，这都是中文翻译的差别，英文其实就是“Hash”。所以，我们常听到有人把“散列表”叫作“哈希表”“Hash表”，把“哈希算法”叫作“Hash算法”或者“散列算法”。那到底什么是哈希算法呢？\n哈希算法的定义和原理非常简单，基本上一句话就可以概括了。将任意长度的二进制值串映射为固定长度的二进制值串，这个映射的规则就是哈希算法，而通过原始数据映射之后得到的二进制值串就是哈希值。但是，要想设计一个优秀的哈希算法并不容易，根据我的经验，我总结了需要满足的几点要求：\n\n从哈希值不能反向推导出原始数据（所以哈希算法也叫单向哈希算法）；\n\n对输入数据非常敏感，哪怕原始数据只修改了一个Bit，最后得到的哈希值也大不相同；\n\n散列冲突的概率要很小，对于不同的原始数据，哈希值相同的概率非常小；\n\n哈希算法的执行效率要尽量高效，针对较长的文本，也能快速地计算出哈希值。\n\n\n这些定义和要求都比较理论，可能还是不好理解，我拿MD5这种哈希算法来具体说明一下。\n我们分别对“今天我来讲哈希算法”和“jiajia”这两个文本，计算MD5哈希值，得到两串看起来毫无规律的字符串（MD5的哈希值是128位的Bit长度，为了方便表示，我把它们转化成了16进制编码）。可以看出来，无论要哈希的文本有多长、多短，通过MD5哈希之后，得到的哈希值的长度都是相同的，而且得到的哈希值看起来像一堆随机数，完全没有规律。\nMD5(&quot;今天我来讲哈希算法&quot;) &#x3D; bb4767201ad42c74e650c1b6c03d78faMD5(&quot;jiajia&quot;) &#x3D; cd611a31ea969b908932d44d126d195b\n我们再来看两个非常相似的文本，“我今天讲哈希算法！”和“我今天讲哈希算法”。这两个文本只有一个感叹号的区别。如果用MD5哈希算法分别计算它们的哈希值，你会发现，尽管只有一字之差，得到的哈希值也是完全不同的。\nMD5(&quot;我今天讲哈希算法！&quot;) &#x3D; 425f0d5a917188d2c3c3dc85b5e4f2cbMD5(&quot;我今天讲哈希算法&quot;) &#x3D; a1fb91ac128e6aa37fe42c663971ac3d\n我在前面也说了，通过哈希算法得到的哈希值，很难反向推导出原始数据。比如上面的例子中，我们就很难通过哈希值“a1fb91ac128e6aa37fe42c663971ac3d”反推出对应的文本“我今天讲哈希算法”。\n哈希算法要处理的文本可能是各种各样的。比如，对于非常长的文本，如果哈希算法的计算时间很长，那就只能停留在理论研究的层面，很难应用到实际的软件开发中。比如，我们把今天这篇包含4000多个汉字的文章，用MD5计算哈希值，用不了1ms的时间。\n哈希算法的应用非常非常多，我选了最常见的七个，分别是安全加密、唯一标识、数据校验、散列函数、负载均衡、数据分片、分布式存储。这节我们先来看前四个应用。\n应用一：安全加密说到哈希算法的应用，最先想到的应该就是安全加密。最常用于加密的哈希算法是MD5（MD5 Message-Digest Algorithm，MD5消息摘要算法）和SHA（Secure Hash Algorithm，安全散列算法）。\n除了这两个之外，当然还有很多其他加密算法，比如DES（Data Encryption Standard，数据加密标准）、AES（Advanced Encryption Standard，高级加密标准）。\n前面我讲到的哈希算法四点要求，对用于加密的哈希算法来说，有两点格外重要。第一点是很难根据哈希值反向推导出原始数据，第二点是散列冲突的概率要很小。\n第一点很好理解，加密的目的就是防止原始数据泄露，所以很难通过哈希值反向推导原始数据，这是一个最基本的要求。所以我着重讲一下第二点。实际上，不管是什么哈希算法，我们只能尽量减少碰撞冲突的概率，理论上是没办法做到完全不冲突的。为什么这么说呢？\n这里就基于组合数学中一个非常基础的理论，鸽巢原理（也叫抽屉原理）。这个原理本身很简单，它是说，如果有10个鸽巢，有11只鸽子，那肯定有1个鸽巢中的鸽子数量多于1个，换句话说就是，肯定有2只鸽子在1个鸽巢内。\n有了鸽巢原理的铺垫之后，我们再来看，为什么哈希算法无法做到零冲突？\n我们知道，哈希算法产生的哈希值的长度是固定且有限的。比如前面举的MD5的例子，哈希值是固定的128位二进制串，能表示的数据是有限的，最多能表示2^128个数据，而我们要哈希的数据是无穷的。基于鸽巢原理，如果我们对2^128+1个数据求哈希值，就必然会存在哈希值相同的情况。这里你应该能想到，一般情况下，哈希值越长的哈希算法，散列冲突的概率越低。\n2^128&#x3D;340282366920938463463374607431768211456\n为了让你能有个更加直观的感受，我找了两段字符串放在这里。这两段字符串经过MD5哈希算法加密之后，产生的哈希值是相同的。\n\n\n不过，即便哈希算法存在散列冲突的情况，但是因为哈希值的范围很大，冲突的概率极低，所以相对来说还是很难破解的。像MD5，有2^128个不同的哈希值，这个数据已经是一个天文数字了，所以散列冲突的概率要小于1/2^128。\n如果我们拿到一个MD5哈希值，希望通过毫无规律的穷举的方法，找到跟这个MD5值相同的另一个数据，那耗费的时间应该是个天文数字。所以，即便哈希算法存在冲突，但是在有限的时间和资源下，哈希算法还是很难被破解的。\n除此之外，没有绝对安全的加密。越复杂、越难破解的加密算法，需要的计算时间也越长。比如SHA-256比SHA-1要更复杂、更安全，相应的计算时间就会比较长。密码学界也一直致力于找到一种快速并且很难被破解的哈希算法。我们在实际的开发过程中，也需要权衡破解难度和计算时间，来决定究竟使用哪种加密算法。\n应用二：唯一标识我先来举一个例子。如果要在海量的图库中，搜索一张图是否存在，我们不能单纯地用图片的元信息（比如图片名称）来比对，因为有可能存在名称相同但图片内容不同，或者名称不同图片内容相同的情况。那我们该如何搜索呢？\n我们知道，任何文件在计算中都可以表示成二进制码串，所以，比较笨的办法就是，拿要查找的图片的二进制码串与图库中所有图片的二进制码串一一比对。如果相同，则说明图片在图库中存在。但是，每个图片小则几十KB、大则几MB，转化成二进制是一个非常长的串，比对起来非常耗时。有没有比较快的方法呢？\n我们可以给每一个图片取一个唯一标识，或者说信息摘要。比如，我们可以从图片的二进制码串开头取100个字节，从中间取100个字节，从最后再取100个字节，然后将这300个字节放到一块，通过哈希算法（比如MD5），得到一个哈希字符串，用它作为图片的唯一标识。通过这个唯一标识来判定图片是否在图库中，这样就可以减少很多工作量。\n如果还想继续提高效率，我们可以把每个图片的唯一标识，和相应的图片文件在图库中的路径信息，都存储在散列表中。当要查看某个图片是不是在图库中的时候，我们先通过哈希算法对这个图片取唯一标识，然后在散列表中查找是否存在这个唯一标识。\n如果不存在，那就说明这个图片不在图库中；如果存在，我们再通过散列表中存储的文件路径，获取到这个已经存在的图片，跟现在要插入的图片做全量的比对，看是否完全一样。如果一样，就说明已经存在；如果不一样，说明两张图片尽管唯一标识相同，但是并不是相同的图片。\n应用三：数据校验电驴这样的BT下载软件你肯定用过吧？我们知道，BT下载的原理是基于P2P协议的。我们从多个机器上并行下载一个2GB的电影，这个电影文件可能会被分割成很多文件块（比如可以分成100块，每块大约20MB）。等所有的文件块都下载完成之后，再组装成一个完整的电影文件就行了。\n我们知道，网络传输是不安全的，下载的文件块有可能是被宿主机器恶意修改过的，又或者下载过程中出现了错误，所以下载的文件块可能不是完整的。如果我们没有能力检测这种恶意修改或者文件下载出错，就会导致最终合并后的电影无法观看，甚至导致电脑中毒。现在的问题是，如何来校验文件块的安全、正确、完整呢？\n具体的BT协议很复杂，校验方法也有很多，我来说其中的一种思路。\n我们通过哈希算法，对100个文件块分别取哈希值，并且保存在种子文件中。我们在前面讲过，哈希算法有一个特点，对数据很敏感。只要文件块的内容有一丁点儿的改变，最后计算出的哈希值就会完全不同。所以，当文件块下载完成之后，我们可以通过相同的哈希算法，对下载好的文件块逐一求哈希值，然后跟种子文件中保存的哈希值比对。如果不同，说明这个文件块不完整或者被篡改了，需要再重新从其他宿主机器上下载这个文件块。\n应用四：散列函数前面讲了很多哈希算法的应用，实际上，散列函数也是哈希算法的一种应用。\n我们前两节讲到，散列函数是设计一个散列表的关键。它直接决定了散列冲突的概率和散列表的性能。不过，相对哈希算法的其他应用，散列函数对于散列算法冲突的要求要低很多。即便出现个别散列冲突，只要不是过于严重，我们都可以通过开放寻址法或者链表法解决。\n不仅如此，散列函数对于散列算法计算得到的值，是否能反向解密也并不关心。散列函数中用到的散列算法，更加关注散列后的值是否能平均分布，也就是，一组数据是否能均匀地散列在各个槽中。除此之外，散列函数执行的快慢，也会影响散列表的性能，所以，散列函数用的散列算法一般都比较简单，比较追求效率。\n解答开篇好了，有了前面的基础，现在你有没有发现开篇的问题其实很好解决？\n我们可以通过哈希算法，对用户密码进行加密之后再存储，不过最好选择相对安全的加密算法，比如SHA等（因为MD5已经号称被破解了）。不过仅仅这样加密之后存储就万事大吉了吗？\n字典攻击你听说过吗？如果用户信息被“脱库”，黑客虽然拿到是加密之后的密文，但可以通过“猜”的方式来破解密码，这是因为，有些用户的密码太简单。比如很多人习惯用00000、123456这样的简单数字组合做密码，很容易就被猜中。\n那我们就需要维护一个常用密码的字典表，把字典中的每个密码用哈希算法计算哈希值，然后拿哈希值跟脱库后的密文比对。如果相同，基本上就可以认为，这个加密之后的密码对应的明文就是字典中的这个密码。（注意，这里说是的是“基本上可以认为”，因为根据我们前面的学习，哈希算法存在散列冲突，也有可能出现，尽管密文一样，但是明文并不一样的情况。）\n针对字典攻击，我们可以引入一个盐（salt），跟用户的密码组合在一起，增加密码的复杂度。我们拿组合之后的字符串来做哈希算法加密，将它存储到数据库中，进一步增加破解的难度。不过我这里想多说一句，我认为安全和攻击是一种博弈关系，不存在绝对的安全。所有的安全措施，只是增加攻击的成本而已。\n内容小结今天的内容比较偏实战，我讲到了哈希算法的四个应用场景。我带你来回顾一下。\n第一个应用是唯一标识，哈希算法可以对大数据做信息摘要，通过一个较短的二进制编码来表示很大的数据。\n第二个应用是用于校验数据的完整性和正确性。\n第三个应用是安全加密，我们讲到任何哈希算法都会出现散列冲突，但是这个冲突概率非常小。越是复杂哈希算法越难破解，但同样计算时间也就越长。所以，选择哈希算法的时候，要权衡安全性和计算时间来决定用哪种哈希算法。\n第四个应用是散列函数，这个我们前面讲散列表的时候已经详细地讲过，它对哈希算法的要求非常特别，更加看重的是散列的平均性和哈希算法的执行效率。\n课后思考现在，区块链是一个很火的领域，它被很多人神秘化，不过其底层的实现原理并不复杂。其中，哈希算法就是它的一个非常重要的理论基础。你能讲一讲区块链使用的是哪种哈希算法吗？是为了解决什么问题而使用的呢？\n欢迎留言和我分享，我会第一时间给你反馈。\n","categories":["数据结构与算法","基础篇"]},{"title":"24 | 二叉树基础（下）：有了如此高效的散列表，为什么还需要二叉树？","url":"/2020/08/07/24%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%9A%E4%BA%8C%E5%8F%89%E6%A0%91%E5%9F%BA%E7%A1%80%EF%BC%88%E4%B8%8B%EF%BC%89/","content":"上一节我们学习了树、二叉树以及二叉树的遍历，今天我们再来学习一种特殊的二叉树，二叉查找树。二叉查找树最大的特点就是，支持动态数据集合的快速插入、删除、查找操作。\n我们之前说过，散列表也是支持这些操作的，并且散列表的这些操作比二叉查找树更高效，时间复杂度是O(1)。既然有了这么高效的散列表，使用二叉树的地方是不是都可以替换成散列表呢？有没有哪些地方是散列表做不了，必须要用二叉树来做的呢？\n带着这些问题，我们就来学习今天的内容，二叉查找树！\n二叉查找树（Binary Search Tree）二叉查找树是二叉树中最常用的一种类型，也叫二叉搜索树。顾名思义，二叉查找树是为了实现快速查找而生的。不过，它不仅仅支持快速查找一个数据，还支持快速插入、删除一个数据。它是怎么做到这些的呢？\n这些都依赖于二叉查找树的特殊结构。二叉查找树要求，在树中的任意一个节点，其左子树中的每个节点的值，都要小于这个节点的值，而右子树节点的值都大于这个节点的值。 我画了几个二叉查找树的例子，你一看应该就清楚了。\n\n前面我们讲到，二叉查找树支持快速查找、插入、删除操作，现在我们就依次来看下，这三个操作是如何实现的。\n1.二叉查找树的查找操作首先，我们看如何在二叉查找树中查找一个节点。我们先取根节点，如果它等于我们要查找的数据，那就返回。如果要查找的数据比根节点的值小，那就在左子树中递归查找；如果要查找的数据比根节点的值大，那就在右子树中递归查找。\n\n这里我把查找的代码实现了一下，贴在下面了，结合代码，理解起来会更加容易。\npublic class BinarySearchTree &#123;  private Node tree;  public Node find(int data) &#123;    Node p &#x3D; tree;    while (p !&#x3D; null) &#123;      if (data &lt; p.data) p &#x3D; p.left;      else if (data &gt; p.data) p &#x3D; p.right;      else return p;    &#125;    return null;  &#125;  public static class Node &#123;    private int data;    private Node left;    private Node right;    public Node(int data) &#123;      this.data &#x3D; data;    &#125;  &#125;&#125;\n2.二叉查找树的插入操作二叉查找树的插入过程有点类似查找操作。新插入的数据一般都是在叶子节点上，所以我们只需要从根节点开始，依次比较要插入的数据和节点的大小关系。\n如果要插入的数据比节点的数据大，并且节点的右子树为空，就将新数据直接插到右子节点的位置；如果不为空，就再递归遍历右子树，查找插入位置。同理，如果要插入的数据比节点数值小，并且节点的左子树为空，就将新数据插入到左子节点的位置；如果不为空，就再递归遍历左子树，查找插入位置。\n\n同样，插入的代码我也实现了一下，贴在下面，你可以看看。\npublic void insert(int data) &#123;  if (tree &#x3D;&#x3D; null) &#123;    tree &#x3D; new Node(data);    return;  &#125;  Node p &#x3D; tree;  while (p !&#x3D; null) &#123;    if (data &gt; p.data) &#123;      if (p.right &#x3D;&#x3D; null) &#123;        p.right &#x3D; new Node(data);        return;      &#125;      p &#x3D; p.right;    &#125; else &#123; &#x2F;&#x2F; data &lt; p.data      if (p.left &#x3D;&#x3D; null) &#123;        p.left &#x3D; new Node(data);        return;      &#125;      p &#x3D; p.left;    &#125;  &#125;&#125;\n3.二叉查找树的删除操作二叉查找树的查找、插入操作都比较简单易懂，但是它的删除操作就比较复杂了 。针对要删除节点的子节点个数的不同，我们需要分三种情况来处理。\n第一种情况是，如果要删除的节点没有子节点，我们只需要直接将父节点中，指向要删除节点的指针置为null。比如图中的删除节点55。\n第二种情况是，如果要删除的节点只有一个子节点（只有左子节点或者右子节点），我们只需要更新父节点中，指向要删除节点的指针，让它指向要删除节点的子节点就可以了。比如图中的删除节点13。\n第三种情况是，如果要删除的节点有两个子节点，这就比较复杂了。我们需要找到这个节点的右子树中的最小节点，把它替换到要删除的节点上。然后再删除掉这个最小节点，因为最小节点肯定没有左子节点（如果有左子结点，那就不是最小节点了），所以，我们可以应用上面两条规则来删除这个最小节点。比如图中的删除节点18。\n\n老规矩，我还是把删除的代码贴在这里。\npublic void delete(int data) &#123;  Node p &#x3D; tree; &#x2F;&#x2F; p指向要删除的节点，初始化指向根节点  Node pp &#x3D; null; &#x2F;&#x2F; pp记录的是p的父节点  while (p !&#x3D; null &amp;&amp; p.data !&#x3D; data) &#123;    pp &#x3D; p;    if (data &gt; p.data) p &#x3D; p.right;    else p &#x3D; p.left;  &#125;  if (p &#x3D;&#x3D; null) return; &#x2F;&#x2F; 没有找到  &#x2F;&#x2F; 要删除的节点有两个子节点  if (p.left !&#x3D; null &amp;&amp; p.right !&#x3D; null) &#123; &#x2F;&#x2F; 查找右子树中最小节点    Node minP &#x3D; p.right;    Node minPP &#x3D; p; &#x2F;&#x2F; minPP表示minP的父节点    while (minP.left !&#x3D; null) &#123;      minPP &#x3D; minP;      minP &#x3D; minP.left;    &#125;    p.data &#x3D; minP.data; &#x2F;&#x2F; 将minP的数据替换到p中    p &#x3D; minP; &#x2F;&#x2F; 下面就变成了删除minP了    pp &#x3D; minPP;  &#125;  &#x2F;&#x2F; 删除节点是叶子节点或者仅有一个子节点  Node child; &#x2F;&#x2F; p的子节点  if (p.left !&#x3D; null) child &#x3D; p.left;  else if (p.right !&#x3D; null) child &#x3D; p.right;  else child &#x3D; null;  if (pp &#x3D;&#x3D; null) tree &#x3D; child; &#x2F;&#x2F; 删除的是根节点  else if (pp.left &#x3D;&#x3D; p) pp.left &#x3D; child;  else pp.right &#x3D; child;&#125;\n实际上，关于二叉查找树的删除操作，还有个非常简单、取巧的方法，就是单纯将要删除的节点标记为“已删除”，但是并不真正从树中将这个节点去掉。这样原本删除的节点还需要存储在内存中，比较浪费内存空间，但是删除操作就变得简单了很多。而且，这种处理方法也并没有增加插入、查找操作代码实现的难度。\n4.二叉查找树的其他操作除了插入、删除、查找操作之外，二叉查找树中还可以支持快速地查找最大节点和最小节点、前驱节点和后继节点。这些操作我就不一一展示了。我会将相应的代码放到GitHub上，你可以自己先实现一下，然后再去上面看。\n二叉查找树除了支持上面几个操作之外，还有一个重要的特性，就是中序遍历二叉查找树，可以输出有序的数据序列，时间复杂度是O(n)，非常高效。因此，二叉查找树也叫作二叉排序树。\n支持重复数据的二叉查找树前面讲二叉查找树的时候，我们默认树中节点存储的都是数字。很多时候，在实际的软件开发中，我们在二叉查找树中存储的，是一个包含很多字段的对象。我们利用对象的某个字段作为键值（key）来构建二叉查找树。我们把对象中的其他字段叫作卫星数据。\n前面我们讲的二叉查找树的操作，针对的都是不存在键值相同的情况。那如果存储的两个对象键值相同，这种情况该怎么处理呢？我这里有两种解决方法。\n第一种方法比较容易。二叉查找树中每一个节点不仅会存储一个数据，因此我们通过链表和支持动态扩容的数组等数据结构，把值相同的数据都存储在同一个节点上。\n第二种方法比较不好理解，不过更加优雅。\n每个节点仍然只存储一个数据。在查找插入位置的过程中，如果碰到一个节点的值，与要插入数据的值相同，我们就将这个要插入的数据放到这个节点的右子树，也就是说，把这个新插入的数据当作大于这个节点的值来处理。\n\n当要查找数据的时候，遇到值相同的节点，我们并不停止查找操作，而是继续在右子树中查找，直到遇到叶子节点，才停止。这样就可以把键值等于要查找值的所有节点都找出来。\n\n对于删除操作，我们也需要先查找到每个要删除的节点，然后再按前面讲的删除操作的方法，依次删除。\n\n二叉查找树的时间复杂度分析好了，对于二叉查找树常用操作的实现方式，你应该掌握得差不多了。现在，我们来分析一下，二叉查找树的插入、删除、查找操作的时间复杂度。\n实际上，二叉查找树的形态各式各样。比如这个图中，对于同一组数据，我们构造了三种二叉查找树。它们的查找、插入、删除操作的执行效率都是不一样的。图中第一种二叉查找树，根节点的左右子树极度不平衡，已经退化成了链表，所以查找的时间复杂度就变成了O(n)。\n\n我刚刚其实分析了一种最糟糕的情况，我们现在来分析一个最理想的情况，二叉查找树是一棵完全二叉树（或满二叉树）。这个时候，插入、删除、查找的时间复杂度是多少呢？\n从我前面的例子、图，以及还有代码来看，不管操作是插入、删除还是查找，时间复杂度其实都跟树的高度成正比，也就是O(height)。既然这样，现在问题就转变成另外一个了，也就是，如何求一棵包含n个节点的完全二叉树的高度？\n树的高度就等于最大层数减一，为了方便计算，我们转换成层来表示。从图中可以看出，包含n个节点的完全二叉树中，第一层包含1个节点，第二层包含2个节点，第三层包含4个节点，依次类推，下面一层节点个数是上一层的2倍，第K层包含的节点个数就是2^(K-1)。\n不过，对于完全二叉树来说，最后一层的节点个数有点儿不遵守上面的规律了。它包含的节点个数在1个到2^(L-1)个之间（我们假设最大层数是L）。如果我们把每一层的节点个数加起来就是总的节点个数n。也就是说，如果节点的个数是n，那么n满足这样一个关系：\nn &gt;&#x3D; 1+2+4+8+...+2^(L-2)+1n &lt;&#x3D; 1+2+4+8+...+2^(L-2)+2^(L-1)\n借助等比数列的求和公式，我们可以计算出，L的范围是[log2(n+1), log2n +1]。完全二叉树的层数小于等于log2n +1，也就是说，完全二叉树的高度小于等于log2n。\n显然，极度不平衡的二叉查找树，它的查找性能肯定不能满足我们的需求。我们需要构建一种不管怎么删除、插入数据，在任何时候，都能保持任意节点左右子树都比较平衡的二叉查找树，这就是我们下一节课要详细讲的，一种特殊的二叉查找树，平衡二叉查找树。平衡二叉查找树的高度接近logn，所以插入、删除、查找操作的时间复杂度也比较稳定，是O(logn)。\n解答开篇我们在散列表那节中讲过，散列表的插入、删除、查找操作的时间复杂度可以做到常量级的O(1)，非常高效。而二叉查找树在比较平衡的情况下，插入、删除、查找操作时间复杂度才是O(logn)，相对散列表，好像并没有什么优势，那我们为什么还要用二叉查找树呢？\n我认为有下面几个原因：\n第一，散列表中的数据是无序存储的，如果要输出有序的数据，需要先进行排序。而对于二叉查找树来说，我们只需要中序遍历，就可以在O(n)的时间复杂度内，输出有序的数据序列。\n第二，散列表扩容耗时很多，而且当遇到散列冲突时，性能不稳定，尽管二叉查找树的性能不稳定，但是在工程中，我们最常用的平衡二叉查找树的性能非常稳定，时间复杂度稳定在O(logn)。\n第三，笼统地来说，尽管散列表的查找等操作的时间复杂度是常量级的，但因为哈希冲突的存在，这个常量不一定比logn小，所以实际的查找速度可能不一定比O(logn)快。加上哈希函数的耗时，也不一定就比平衡二叉查找树的效率高。\n第四，散列表的构造比二叉查找树要复杂，需要考虑的东西很多。比如散列函数的设计、冲突解决办法、扩容、缩容等。平衡二叉查找树只需要考虑平衡性这一个问题，而且这个问题的解决方案比较成熟、固定。\n最后，为了避免过多的散列冲突，散列表装载因子不能太大，特别是基于开放寻址法解决冲突的散列表，不然会浪费一定的存储空间。\n综合这几点，平衡二叉查找树在某些方面还是优于散列表的，所以，这两者的存在并不冲突。我们在实际的开发过程中，需要结合具体的需求来选择使用哪一个。\n内容小结今天我们学习了一种特殊的二叉树，二叉查找树。它支持快速地查找、插入、删除操作。\n二叉查找树中，每个节点的值都大于左子树节点的值，小于右子树节点的值。不过，这只是针对没有重复数据的情况。对于存在重复数据的二叉查找树，我介绍了两种构建方法，一种是让每个节点存储多个值相同的数据；另一种是，每个节点中存储一个数据。针对这种情况，我们只需要稍加改造原来的插入、删除、查找操作即可。\n在二叉查找树中，查找、插入、删除等很多操作的时间复杂度都跟树的高度成正比。两个极端情况的时间复杂度分别是O(n)和O(logn)，分别对应二叉树退化成链表的情况和完全二叉树。\n为了避免时间复杂度的退化，针对二叉查找树，我们又设计了一种更加复杂的树，平衡二叉查找树，时间复杂度可以做到稳定的O(logn)，下一节我们具体来讲。\n课后思考今天我讲了二叉树高度的理论分析方法，给出了粗略的数量级。如何通过编程，求出一棵给定二叉树的确切高度呢？\n欢迎留言和我分享，我会第一时间给你反馈。\n我已将本节内容相关的详细代码更新到GitHub，戳此即可查看。\n","categories":["数据结构与算法","基础篇"]},{"title":"25 | 红黑树（上）：为什么工程中都用红黑树这种二叉树？","url":"/2020/08/07/25%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%9A%E7%BA%A2%E9%BB%91%E6%A0%91%EF%BC%88%E4%B8%8A%EF%BC%89/","content":"上两节，我们依次讲了树、二叉树、二叉查找树。二叉查找树是最常用的一种二叉树，它支持快速插入、删除、查找操作，各个操作的时间复杂度跟树的高度成正比，理想情况下，时间复杂度是O(logn)。\n不过，二叉查找树在频繁的动态更新过程中，可能会出现树的高度远大于log2n的情况，从而导致各个操作的效率下降。极端情况下，二叉树会退化为链表，时间复杂度会退化到O(n)。我上一节说了，要解决这个复杂度退化的问题，我们需要设计一种平衡二叉查找树，也就是今天要讲的这种数据结构。\n很多书籍里，但凡讲到平衡二叉查找树，就会拿红黑树作为例子。不仅如此，如果你有一定的开发经验，你会发现，在工程中，很多用到平衡二叉查找树的地方都会用红黑树。你有没有想过，为什么工程中都喜欢用红黑树，而不是其他平衡二叉查找树呢？\n带着这个问题，让我们一起来学习今天的内容吧！\n什么是“平衡二叉查找树”？平衡二叉树的严格定义是这样的：二叉树中任意一个节点的左右子树的高度相差不能大于1。从这个定义来看，上一节我们讲的完全二叉树、满二叉树其实都是平衡二叉树，但是非完全二叉树也有可能是平衡二叉树。\n\n平衡二叉查找树不仅满足上面平衡二叉树的定义，还满足二叉查找树的特点。最先被发明的平衡二叉查找树是AVL树，它严格符合我刚讲到的平衡二叉查找树的定义，即任何节点的左右子树高度相差不超过1，是一种高度平衡的二叉查找树。\n但是很多平衡二叉查找树其实并没有严格符合上面的定义（树中任意一个节点的左右子树的高度相差不能大于1），比如我们下面要讲的红黑树，它从根节点到各个叶子节点的最长路径，有可能会比最短路径大一倍。\n我们学习数据结构和算法是为了应用到实际的开发中的，所以，我觉得没必去死抠定义。对于平衡二叉查找树这个概念，我觉得我们要从这个数据结构的由来，去理解“平衡”的意思。\n发明平衡二叉查找树这类数据结构的初衷是，解决普通二叉查找树在频繁的插入、删除等动态更新的情况下，出现时间复杂度退化的问题。\n所以，平衡二叉查找树中“平衡”的意思，其实就是让整棵树左右看起来比较“对称”、比较“平衡”，不要出现左子树很高、右子树很矮的情况。这样就能让整棵树的高度相对来说低一些，相应的插入、删除、查找等操作的效率高一些。\n所以，如果我们现在设计一个新的平衡二叉查找树，只要树的高度不比log2n大很多（比如树的高度仍然是对数量级的），尽管它不符合我们前面讲的严格的平衡二叉查找树的定义，但我们仍然可以说，这是一个合格的平衡二叉查找树。\n如何定义一棵“红黑树”？平衡二叉查找树其实有很多，比如，Splay Tree（伸展树）、Treap（树堆）等，但是我们提到平衡二叉查找树，听到的基本都是红黑树。它的出镜率甚至要高于“平衡二叉查找树”这几个字，有时候，我们甚至默认平衡二叉查找树就是红黑树，那我们现在就来看看这个“明星树”。\n红黑树的英文是“Red-Black Tree”，简称R-B Tree。它是一种不严格的平衡二叉查找树，我前面说了，它的定义是不严格符合平衡二叉查找树的定义的。那红黑树究竟是怎么定义的呢？\n顾名思义，红黑树中的节点，一类被标记为黑色，一类被标记为红色。除此之外，一棵红黑树还需要满足这样几个要求：\n\n根节点是黑色的；\n\n每个叶子节点都是黑色的空节点（NIL），也就是说，叶子节点不存储数据；\n\n任何相邻的节点都不能同时为红色，也就是说，红色节点是被黑色节点隔开的；\n\n每个节点，从该节点到达其可达叶子节点的所有路径，都包含相同数目的黑色节点；\n\n\n这里的第二点要求“叶子节点都是黑色的空节点”，稍微有些奇怪，它主要是为了简化红黑树的代码实现而设置的，下一节我们讲红黑树的实现的时候会讲到。这节我们暂时不考虑这一点，所以，在画图和讲解的时候，我将黑色的、空的叶子节点都省略掉了。\n为了让你更好地理解上面的定义，我画了两个红黑树的图例，你可以对照着看下。\n\n为什么说红黑树是“近似平衡”的？我们前面也讲到，平衡二叉查找树的初衷，是为了解决二叉查找树因为动态更新导致的性能退化问题。所以，“平衡”的意思可以等价为性能不退化。“近似平衡”就等价为性能不会退化得太严重。\n我们在上一节讲过，二叉查找树很多操作的性能都跟树的高度成正比。一棵极其平衡的二叉树（满二叉树或完全二叉树）的高度大约是log2n，所以如果要证明红黑树是近似平衡的，我们只需要分析，红黑树的高度是否比较稳定地趋近log2n就好了。\n红黑树的高度不是很好分析，我带你一步一步来推导。\n首先，我们来看，如果我们将红色节点从红黑树中去掉，那单纯包含黑色节点的红黑树的高度是多少呢？\n红色节点删除之后，有些节点就没有父节点了，它们会直接拿这些节点的祖父节点（父节点的父节点）作为父节点。所以，之前的二叉树就变成了四叉树。\n\n前面红黑树的定义里有这么一条：从任意节点到可达的叶子节点的每个路径包含相同数目的黑色节点。我们从四叉树中取出某些节点，放到叶节点位置，四叉树就变成了完全二叉树。所以，仅包含黑色节点的四叉树的高度，比包含相同节点个数的完全二叉树的高度还要小。\n上一节我们说，完全二叉树的高度近似log2n，这里的四叉“黑树”的高度要低于完全二叉树，所以去掉红色节点的“黑树”的高度也不会超过log2n。\n我们现在知道只包含黑色节点的“黑树”的高度，那我们现在把红色节点加回去，高度会变成多少呢？\n从上面我画的红黑树的例子和定义看，在红黑树中，红色节点不能相邻，也就是说，有一个红色节点就要至少有一个黑色节点，将它跟其他红色节点隔开。红黑树中包含最多黑色节点的路径不会超过log2n，所以加入红色节点之后，最长路径不会超过2log2n，也就是说，红黑树的高度近似2log2n。\n所以，红黑树的高度只比高度平衡的AVL树的高度（log2n）仅仅大了一倍，在性能上，下降得并不多。这样推导出来的结果不够精确，实际上红黑树的性能更好。\n解答开篇我们刚刚提到了很多平衡二叉查找树，现在我们就来看下，为什么在工程中大家都喜欢用红黑树这种平衡二叉查找树？\n我们前面提到Treap、Splay Tree，绝大部分情况下，它们操作的效率都很高，但是也无法避免极端情况下时间复杂度的退化。尽管这种情况出现的概率不大，但是对于单次操作时间非常敏感的场景来说，它们并不适用。\nAVL树是一种高度平衡的二叉树，所以查找的效率非常高，但是，有利就有弊，AVL树为了维持这种高度的平衡，就要付出更多的代价。每次插入、删除都要做调整，就比较复杂、耗时。所以，对于有频繁的插入、删除操作的数据集合，使用AVL树的代价就有点高了。\n红黑树只是做到了近似平衡，并不是严格的平衡，所以在维护平衡的成本上，要比AVL树要低。\n所以，红黑树的插入、删除、查找各种操作性能都比较稳定。对于工程应用来说，要面对各种异常情况，为了支撑这种工业级的应用，我们更倾向于这种性能稳定的平衡二叉查找树。\n内容小结很多同学都觉得红黑树很难，的确，它算是最难掌握的一种数据结构。其实红黑树最难的地方是它的实现，我们今天还没有涉及，下一节我会专门来讲。\n不过呢，我认为，我们其实不应该把学习的侧重点，放到它的实现上。那你可能要问了，关于红黑树，我们究竟需要掌握哪些东西呢？\n还记得我多次说过的观点吗？我们学习数据结构和算法，要学习它的由来、特性、适用的场景以及它能解决的问题。对于红黑树，也不例外。你如果能搞懂这几个问题，其实就已经足够了。\n红黑树是一种平衡二叉查找树。它是为了解决普通二叉查找树在数据更新的过程中，复杂度退化的问题而产生的。红黑树的高度近似log2n，所以它是近似平衡，插入、删除、查找操作的时间复杂度都是O(logn)。\n因为红黑树是一种性能非常稳定的二叉查找树，所以，在工程中，但凡是用到动态插入、删除、查找数据的场景，都可以用到它。不过，它实现起来比较复杂，如果自己写代码实现，难度会有些高，这个时候，我们其实更倾向用跳表来替代它。\n课后思考动态数据结构支持动态的数据插入、删除、查找操作，除了红黑树，我们前面还学习过哪些呢？能对比一下各自的优势、劣势，以及应用场景吗？\n欢迎留言和我分享，我会第一时间给你反馈。\n相关文章红黑树的根本模型：2-3树\n","categories":["数据结构与算法","基础篇"]},{"title":"27 | 递归树：如何借助树来求解递归算法的时间复杂度？","url":"/2020/08/07/27%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%9A%E9%80%92%E5%BD%92%E6%A0%91/","content":"今天，我们来讲这种数据结构的一种特殊应用，递归树。\n我们都知道，递归代码的时间复杂度分析起来很麻烦。我们在第12节《排序（下）》那里讲过，如何利用递推公式，求解归并排序、快速排序的时间复杂度，但是，有些情况，比如快排的平均时间复杂度的分析，用递推公式的话，会涉及非常复杂的数学推导。\n除了用递推公式这种比较复杂的分析方法，有没有更简单的方法呢？今天，我们就来学习另外一种方法，借助递归树来分析递归算法的时间复杂度。\n递归树与时间复杂度分析我们前面讲过，递归的思想就是，将大问题分解为小问题来求解，然后再将小问题分解为小小问题。这样一层一层地分解，直到问题的数据规模被分解得足够小，不用继续递归分解为止。\n如果我们把这个一层一层的分解过程画成图，它其实就是一棵树。我们给这棵树起一个名字，叫作递归树。我这里画了一棵斐波那契数列的递归树，你可以看看。节点里的数字表示数据的规模，一个节点的求解可以分解为左右子节点两个问题的求解。\n\n通过这个例子，你对递归树的样子应该有个感性的认识了，看起来并不复杂。现在，我们就来看，如何用递归树来求解时间复杂度。\n归并排序算法你还记得吧？它的递归实现代码非常简洁。现在我们就借助归并排序来看看，如何用递归树，来分析递归代码的时间复杂度。\n归并排序的原理我就不详细介绍了，如果你忘记了，可以回看一下第12节的内容。归并排序每次会将数据规模一分为二。我们把归并排序画成递归树，就是下面这个样子：\n\n因为每次分解都是一分为二，所以代价很低，我们把时间上的消耗记作常量$1$。归并算法中比较耗时的是归并操作，也就是把两个子数组合并为大数组。从图中我们可以看出，每一层归并操作消耗的时间总和是一样的，跟要排序的数据规模有关。我们把每一层归并操作消耗的时间记作$n$。\n现在，我们只需要知道这棵树的高度$h$，用高度$h$乘以每一层的时间消耗$n$，就可以得到总的时间复杂度$O(n*h)$。\n从归并排序的原理和递归树，可以看出来，归并排序递归树是一棵满二叉树。我们前两节中讲到，满二叉树的高度大约是$\\log_{2}n$，所以，归并排序递归实现的时间复杂度就是$O(n\\log n)$。我这里的时间复杂度都是估算的，对树的高度的计算也没有那么精确，但是这并不影响复杂度的计算结果。\n利用递归树的时间复杂度分析方法并不难理解，关键还是在实战，所以，接下来我会通过三个实际的递归算法，带你实战一下递归的复杂度分析。学完这节课之后，你应该能真正掌握递归代码的复杂度分析。\n实战一：分析快速排序的时间复杂度在用递归树推导之前，我们先来回忆一下用递推公式的分析方法。你可以回想一下，当时，我们为什么说用递推公式来求解平均时间复杂度非常复杂？\n快速排序在最好情况下，每次分区都能一分为二，这个时候用递推公式$T(n)=2T(\\frac{n}{2})+n$，很容易就能推导出时间复杂度是$O(n\\log n)$。但是，我们并不可能每次分区都这么幸运，正好一分为二。\n我们假设平均情况下，每次分区之后，两个分区的大小比例为$1:k$。当$k=9$时，如果用递推公式的方法来求解时间复杂度的话，递推公式就写成$T(n)=T(\\frac{n}{10})+T(\\frac{9n}{10})+n$。\n这个公式可以推导出时间复杂度，但是推导过程非常复杂。那我们来看看，用递归树来分析快速排序的平均情况时间复杂度，是不是比较简单呢？\n我们还是取$k$等于$9$，也就是说，每次分区都很不平均，一个分区是另一个分区的$9$倍。如果我们把递归分解的过程画成递归树，就是下面这个样子：\n\n快速排序的过程中，每次分区都要遍历待分区区间的所有数据，所以，每一层分区操作所遍历的数据的个数之和就是$n$。我们现在只要求出递归树的高度$h$，这个快排过程遍历的数据个数就是 $h * n$ ，也就是说，时间复杂度就是$O(h * n)$。\n因为每次分区并不是均匀地一分为二，所以递归树并不是满二叉树。这样一个递归树的高度是多少呢？\n我们知道，快速排序结束的条件就是待排序的小区间，大小为$1$，也就是说叶子节点里的数据规模是$1$。从根节点$n$到叶子节点$1$，递归树中最短的一个路径每次都乘以$\\frac{1}{10}$，最长的一个路径每次都乘以$\\frac{9}{10}$。通过计算，我们可以得到，从根节点到叶子节点的最短路径是$\\log_{10}n$，最长的路径是$\\log_{\\frac{10}{9}}n$。\n\n所以，遍历数据的个数总和就介于$n\\log_{10}n$和$n\\log_{\\frac{10}{9}}n$之间。根据复杂度的大O表示法，对数复杂度的底数不管是多少，我们统一写成$\\log n$，所以，当分区大小比例是$1:9$时，快速排序的时间复杂度仍然是$O(n\\log n)$。\n刚刚我们假设$k=9$，那如果$k=99$，也就是说，每次分区极其不平均，两个区间大小是$1:99$，这个时候的时间复杂度是多少呢？\n我们可以类比上面$k=9$的分析过程。当$k=99$的时候，树的最短路径就是$\\log_{100}n$，最长路径是$\\log_{\\frac{100}{99}}n$，所以总遍历数据个数介于$n\\log_{100}n$和$n\\log_{\\frac{100}{99}}n$之间。尽管底数变了，但是时间复杂度也仍然是$O(n\\log n)$。\n也就是说，对于$k$等于$9$，$99$，甚至是$999$，$9999$……，只要$k$的值不随$n$变化，是一个事先确定的常量，那快排的时间复杂度就是$O(n\\log n)$。所以，从概率论的角度来说，快排的平均时间复杂度就是$O(n\\log n)$。\n实战二：分析斐波那契数列的时间复杂度在递归那一节中，我们举了一个跨台阶的例子，你还记得吗？那个例子实际上就是一个斐波那契数列。为了方便你回忆，我把它的代码实现贴在这里。\nint f(int n) &#123;  if (n &#x3D;&#x3D; 1) return 1;  if (n &#x3D;&#x3D; 2) return 2;  return f(n-1) + f(n-2);&#125;\n这样一段代码的时间复杂度是多少呢？你可以先试着分析一下，然后再来看，我是怎么利用递归树来分析的。\n我们先把上面的递归代码画成递归树，就是下面这个样子：\n\n这棵递归树的高度是多少呢？\n$f(n)$分解为$f(n-1)$和$f(n-2)$，每次数据规模都是$-1$或者$-2$，叶子节点的数据规模是$1$或者$2$。所以，从根节点走到叶子节点，每条路径是长短不一的。如果每次都是$-1$，那最长路径大约就是$n$；如果每次都是$-2$，那最短路径大约就是$\\frac{n}{2}$。\n每次分解之后的合并操作只需要一次加法运算，我们把这次加法运算的时间消耗记作$1$。所以，从上往下，第一层的总时间消耗是$1$，第二层的总时间消耗是$2$，第三层的总时间消耗就是$2^{2}$。依次类推，第$k$层的时间消耗就是$2^{k-1}$，那整个算法的总的时间消耗就是每一层时间消耗之和。\n如果路径长度都为$n$，那这个总和就是$2^{n}-1$。\n\n如果路径长度都是$\\frac{n}{2}$ ，那整个算法的总的时间消耗就是$2^{\\frac{n}{2}}-1$。\n\n所以，这个算法的时间复杂度就介于$O(2^{n})$和$O(2^{\\frac{n}{2}})$之间。虽然这样得到的结果还不够精确，只是一个范围，但是我们也基本上知道了上面算法的时间复杂度是指数级的，非常高。\n实战三：分析全排列的时间复杂度前面两个复杂度分析都比较简单，我们再来看个稍微复杂的。\n我们在高中的时候都学过排列组合。“如何把$n$个数据的所有排列都找出来”，这就是全排列的问题。\n我来举个例子。比如，$1， 2，3$这样$3$个数据，有下面这几种不同的排列：\n1, 2, 31, 3, 22, 1, 32, 3, 13, 1, 23, 2, 1\n如何编程打印一组数据的所有排列呢？这里就可以用递归来实现。\n如果我们确定了最后一位数据，那就变成了求解剩下$n-1$个数据的排列问题。而最后一位数据可以是$n$个数据中的任意一个，因此它的取值就有$n$种情况。所以，“$n$个数据的排列”问题，就可以分解成$n$个“$n-1$个数据的排列”的子问题。\n如果我们把它写成递推公式，就是下面这个样子：\n假设数组中存储的是1，2， 3...n。        f(1,2,...n) &#x3D; &#123;最后一位是1, f(n-1)&#125; + &#123;最后一位是2, f(n-1)&#125; +...+&#123;最后一位是n, f(n-1)&#125;。\n如果我们把递推公式改写成代码，就是下面这个样子：\n&#x2F;&#x2F; 调用方式：&#x2F;&#x2F; int[]a &#x3D; a&#x3D;&#123;1, 2, 3, 4&#125;; printPermutations(a, 4, 4);&#x2F;&#x2F; k表示要处理的子数组的数据个数public void printPermutations(int[] data, int n, int k) &#123;  if (k &#x3D;&#x3D; 1) &#123;    for (int i &#x3D; 0; i &lt; n; ++i) &#123;      System.out.print(data[i] + &quot; &quot;);    &#125;    System.out.println();  &#125;  for (int i &#x3D; 0; i &lt; k; ++i) &#123;    int tmp &#x3D; data[i];    data[i] &#x3D; data[k-1];    data[k-1] &#x3D; tmp;    printPermutations(data, n, k - 1);    tmp &#x3D; data[i];    data[i] &#x3D; data[k-1];    data[k-1] &#x3D; tmp;  &#125;&#125;\n如果不用我前面讲的递归树分析方法，这个递归代码的时间复杂度会比较难分析。现在，我们来看下，如何借助递归树，轻松分析出这个代码的时间复杂度。\n首先，我们还是画出递归树。不过，现在的递归树已经不是标准的二叉树了。\n\n第一层分解有$n$次交换操作，第二层有$n$个节点，每个节点分解需要$n-1$次交换，所以第二层总的交换次数是$n(n-1)$。第三层有$n(n-1)$个节点，每个节点分解需要$n-2$次交换，所以第三层总的交换次数是$n(n-1)(n-2)$。\n以此类推，第$k$层总的交换次数就是$n * (n-1) * (n-2) * … * (n-k+1)$。最后一层的交换次数就是$n * (n-1) * (n-2) * … * 2 * 1$。每一层的交换次数之和就是总的交换次数。\nn + n*(n-1) + n*(n-1)*(n-2) +... + n*(n-1)*(n-2)*...*2*1\n这个公式的求和比较复杂，我们看最后一个数，$n * (n-1) * (n-2) * … * 2 * 1$等于$n!$，而前面的$n-1$个数都小于最后一个数，所以，总和肯定小于$n * n!$，也就是说，全排列的递归算法的时间复杂度大于$O(n!)$，小于$O(n * n!)$，虽然我们没法知道非常精确的时间复杂度，但是这样一个范围已经让我们知道，全排列的时间复杂度是非常高的。\n这里我稍微说下，掌握分析的方法很重要，思路是重点，不要纠结于精确的时间复杂度到底是多少。\n内容小结今天，我们用递归树分析了递归代码的时间复杂度。加上我们在排序那一节讲到的递推公式的时间复杂度分析方法，我们现在已经学习了两种递归代码的时间复杂度分析方法了。\n有些代码比较适合用递推公式来分析，比如归并排序的时间复杂度、快速排序的最好情况时间复杂度；有些比较适合采用递归树来分析，比如快速排序的平均时间复杂度。而有些可能两个都不怎么适合使用，比如二叉树的递归前中后序遍历。\n时间复杂度分析的理论知识并不多，也不复杂，掌握起来也不难，但是，在我们平时的工作、学习中，面对的代码千差万别，能够灵活应用学到的复杂度分析方法，来分析现有的代码，并不是件简单的事情，所以，你平时要多实战、多分析，只有这样，面对任何代码的时间复杂度分析，你才能做到游刃有余、毫不畏惧。\n课后思考$1$个细胞的生命周期是$3$小时，$1$小时分裂一次。求$n$小时后，容器内有多少细胞？请你用已经学过的递归时间复杂度的分析方法，分析一下这个递归问题的时间复杂度。\n欢迎留言和我分享，我会第一时间给你反馈。\n思考解答n 从第 0 个小时开始，\nn = 0，f(0) = 1\nn = 1，f(1) = 2*f(1)\nn = 2，f(2) = 2*f(1)\nn = 3，f(3) = 2*f(2) - f(0) ，减去存活了三个小时的细胞个数。\nn = 4，f(4) = 2*f(3) - f(1)，减去存活了三个小时的细胞个数。\n以此类推：\nf(n) = 2*f(n-1) - f(n-3)，减去存活了三个小时的细胞个数。\n与斐波那契数列的递归复杂度相同。\n","categories":["数据结构与算法","基础篇"]},{"title":"26 | 红黑树（下）：掌握这些技巧，你也可以实现一个红黑树","url":"/2020/08/07/26%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%9A%E7%BA%A2%E9%BB%91%E6%A0%91%EF%BC%88%E4%B8%8B%EF%BC%89/","content":"红黑树是一个让我又爱又恨的数据结构，“爱”是因为它稳定、高效的性能，“恨”是因为实现起来实在太难了。我今天讲的红黑树的实现，对于基础不太好的同学，理解起来可能会有些困难。但是，我觉得没必要去死磕它。\n我为什么这么说呢？因为，即便你将左右旋背得滚瓜烂熟，我保证你过不几天就忘光了。因为，学习红黑树的代码实现，对于你平时做项目开发没有太大帮助。对于绝大部分开发工程师来说，这辈子你可能都用不着亲手写一个红黑树。除此之外，它对于算法面试也几乎没什么用，一般情况下，靠谱的面试官也不会让你手写红黑树的。\n如果你对数据结构和算法很感兴趣，想要开拓眼界、训练思维，我还是很推荐你看一看这节的内容。但是如果学完今天的内容你还觉得懵懵懂懂的话，也不要纠结。我们要有的放矢去学习。你先把平时要用的、基础的东西都搞会了，如果有余力了，再来深入地研究这节内容。\n好，我们现在就进入正式的内容。上一节，我们讲到红黑树定义的时候，提到红黑树的叶子节点都是黑色的空节点。当时我只是粗略地解释了，这是为了代码实现方便，那更加确切的原因是什么呢？ 我们这节就来说一说。\n实现红黑树的基本思想不知道你有没有玩过魔方？其实魔方的复原解法是有固定算法的：遇到哪几面是什么样子，对应就怎么转几下。你只要跟着这个复原步骤，就肯定能将魔方复原。\n实际上，红黑树的平衡过程跟魔方复原非常神似，大致过程就是：遇到什么样的节点排布，我们就对应怎么去调整。只要按照这些固定的调整规则来操作，就能将一个非平衡的红黑树调整成平衡的。\n还记得我们前面讲过的红黑树的定义吗？今天的内容里，我们会频繁用到它，所以，我们现在再来回顾一下。一棵合格的红黑树需要满足这样几个要求：\n\n根节点是黑色的；\n\n每个叶子节点都是黑色的空节点（NIL），也就是说，叶子节点不存储数据；\n\n任何相邻的节点都不能同时为红色，也就是说，红色节点是被黑色节点隔开的；\n\n每个节点，从该节点到达其可达叶子节点的所有路径，都包含相同数目的黑色节点。\n\n\n在插入、删除节点的过程中，第三、第四点要求可能会被破坏，而我们今天要讲的“平衡调整”，实际上就是要把被破坏的第三、第四点恢复过来。\n在正式开始之前，我先介绍两个非常重要的操作，左旋（rotate left）、右旋（rotate right）。左旋全称其实是叫围绕某个节点的左旋，那右旋的全称估计你已经猜到了，就叫围绕某个节点的右旋。\n我们下面的平衡调整中，会一直用到这两个操作，所以我这里画了个示意图，帮助你彻底理解这两个操作。图中的a，b，r表示子树，可以为空。\n\n前面我说了，红黑树的插入、删除操作会破坏红黑树的定义，具体来说就是会破坏红黑树的平衡，所以，我们现在就来看下，红黑树在插入、删除数据之后，如何调整平衡，继续当一棵合格的红黑树的。\n插入操作的平衡调整首先，我们来看插入操作。\n红黑树规定，插入的节点必须是红色的。而且，二叉查找树中新插入的节点都是放在叶子节点上。所以，关于插入操作的平衡调整，有这样两种特殊情况，但是也都非常好处理。\n\n如果插入节点的父节点是黑色的，那我们什么都不用做，它仍然满足红黑树的定义。\n\n如果插入的节点是根节点，那我们直接改变它的颜色，把它变成黑色就可以了。\n\n\n除此之外，其他情况都会违背红黑树的定义，于是我们就需要进行调整，调整的过程包含两种基础的操作：左右旋转和改变颜色。\n红黑树的平衡调整过程是一个迭代的过程。我们把正在处理的节点叫做关注节点。关注节点会随着不停地迭代处理，而不断发生变化。最开始的关注节点就是新插入的节点。\n新节点插入之后，如果红黑树的平衡被打破，那一般会有下面三种情况。我们只需要根据每种情况的特点，不停地调整，就可以让红黑树继续符合定义，也就是继续保持平衡。\n我们下面依次来看每种情况的调整过程。提醒你注意下，为了简化描述，我把父节点的兄弟节点叫做叔叔节点，父节点的父节点叫做祖父节点。\nCASE 1：如果关注节点是a，它的叔叔节点d是红色，我们就依次执行下面的操作：\n\n将关注节点a的父节点b、叔叔节点d的颜色都设置成黑色；\n\n将关注节点a的祖父节点c的颜色设置成红色；\n\n关注节点变成a的祖父节点c；\n\n跳到CASE  2或者CASE  3。\n\n\n\nCASE 2：如果关注节点是a，它的叔叔节点d是黑色，关注节点a是其父节点b的右子节点，我们就依次执行下面的操作：\n\n关注节点变成节点a的父节点b；\n\n围绕新的关注节点\bb左旋；\n\n跳到CASE  3。\n\n\n\nCASE 3：如果关注节点是a，它的叔叔节点d是黑色，关注节点a是其父节点b的左子节点，我们就依次执行下面的操作：\n\n围绕关注节点a的祖父节点c右旋；\n\n将关注节点a的父节点b、兄弟节点c的颜色互换。\n\n调整结束。\n\n\n\n删除操作的平衡调整红黑树插入操作的平衡调整还不是很难，但是它的删除操作的平衡调整相对就要难多了。不过原理都是类似的，我们依旧只需要根据关注节点与周围节点的排布特点，按照一定的规则去调整就行了。\n删除操作的平衡调整分为两步，第一步是针对删除节点初步调整。初步调整只是保证整棵红黑树在一个节点删除之后，仍然满足最后一条定义的要求，也就是说，每个节点，从该节点到达其可达叶子节点的所有路径，都包含相同数目的黑色节点；第二步是针对关注节点进行二次调整，让它满足红黑树的第三条定义，即不存在相邻的两个红色节点。\n1.针对删除节点初步调整这里需要注意一下，红黑树的定义中“只包含红色节点和黑色节点”，经过初步调整之后，为了保证满足红黑树定义的最后一条要求，有些节点会被标记成两种颜色，“红-黑”或者“黑-黑”。如果一个节点被标记为了“黑-黑”，那在计算黑色节点个数的时候，要算成两个黑色节点。\n在下面的讲解中，如果一个节点既可以是红色，也可以是黑色，在画图的时候，我会用一半红色一半黑色来表示。如果一个节点是“红-黑”或者“黑-黑”，我会用左上角的一个小黑点来表示额外的黑色。\nCASE 1：如果要删除的节点是a，它只有一个子节点b，那我们就依次进行下面的操作：\n\n删除节点a，并且把节点b替换到节点a的位置，这一部分操作跟普通的二叉查找树的删除操作一样；\n\n节点a只能是黑色，节点b也只能是红色，其他情况均不符合红黑树的定义。这种情况下，我们把节点b改为黑色；\n\n调整结束，不需要进行二次调整。\n\n\n\nCASE 2：如果要删除的节点a有两个非空子节点，并且它的后继节点就是节点a的右子节点c。我们就依次进行下面的操作：\n\n如果节点a的后继节点就是右子节点c，那右子节点c肯定没有左子树。我们把节点a删除，并且将节点c替换到节点a的位置。这一部分操作跟普通的二叉查找树的删除操作无异；\n\n然后把节点c的颜色设置为跟节点a相同的颜色；\n\n如果节点c是黑色，为了不违反红黑树的最后一条定义，我们给节点c的右子节点d多加一个黑色，这个时候节点d就成了“红-黑”或者“黑-黑”；\n\n这个时候，关注节点变成了节点d，第二步的调整操作就会针对关注节点来做。\n\n\n\nCASE 3：如果要删除的是节点a，它有两个非空子节点，并且节点a的后继节点不是右子节点，我们就依次进行下面的操作：\n\n找到后继节点d，并将它删除，删除后继节点d的过程参照CASE  1；\n\n将节点a替换成后继节点d；\n\n把节点d的颜色设置为跟节点a相同的颜色；\n\n如果节点d是黑色，为了不违反红黑树的最后一条定义，我们给节点d的右子节点c多加一个黑色，这个时候节点c就成了“红-黑”或者“黑-黑”；\n\n这个时候，关注节点变成了节点c，第二步的调整操作就会针对关注节点来做。\n\n\n\n2.针对关注节点进行二次调整经过初步调整之后，关注节点变成了“红-黑”或者“黑-黑”节点。针对这个关注节点，我们再分四种情况来进行二次调整。二次调整是为了让红黑树中不存在相邻的红色节点。\nCASE 1：如果关注节点是a，它的兄弟节点c是红色的，我们就依次进行下面的操作：\n\n围绕关注节点a的父节点b左旋；\n\n关注节点a的父节点b和祖父节点c交换颜色；\n\n关注节点不变；\n\n继续从四种情况中选择适合的规则来调整。\n\n\n\nCASE 2：如果关注节点是a，它的兄弟节点c是黑色的，并且节点c的左右子节点d、e都是黑色的，我们就依次进行下面的操作：\n\n将关注节点a的兄弟节点c的颜色变成红色；\n\n从关注节点a中去掉一个黑色，这个时候节点a就是单纯的红色或者黑色；\n\n给关注节点a的父节点b添加一个黑色，这个时候节点b就变成了“红-黑”或者“黑-黑”；\n\n关注节点从a变成其父节点b；\n\n继续从四种情况中选择符合的规则来调整。\n\n\n\nCASE 3：如果关注节点是a，它的兄弟节点c是黑色，c的左子节点d是红色，c的右子节点e是黑色，我们就依次进行下面的操作：\n\n围绕关注节点a的兄弟节点c右旋；\n\n节点c和节点d交换颜色；\n\n关注节点不变；\n\n跳转到CASE  4，继续调整。\n\n\n\nCASE 4：如果关注节点a的兄弟节点c是黑色的，并且c的右子节点是红色的，我们就依次进行下面的操作：\n\n围绕关注节点a的父节点b左旋；\n\n将关注节点a的兄弟节点c的颜色，跟关注节点a的父节点b设置成相同的颜色；\n\n将关注节点a的父节点b的颜色设置为黑色；\n\n从关注节点a中去掉一个黑色，节点a就变成了单纯的红色或者黑色；\n\n将关注节点a的叔叔节点e设置为黑色；\n\n调整结束。\n\n\n\n解答开篇红黑树的平衡调整就讲完了，现在，你能回答开篇的问题了吗？为什么红黑树的定义中，要求叶子节点是黑色的空节点？\n要我说，之所以有这么奇怪的要求，其实就是为了实现起来方便。只要满足这一条要求，那在任何时刻，红黑树的平衡操作都可以归结为我们刚刚讲的那几种情况。\n还是有点不好理解，我通过一个例子来解释一下。假设红黑树的定义中不包含刚刚提到的那一条“叶子节点必须是黑色的空节点”，我们往一棵红黑树中插入一个数据，新插入节点的父节点也是红色的，两个红色的节点相邻，这个时候，红黑树的定义就被破坏了。那我们应该如何调整呢？\n\n你会发现，这个时候，我们前面在讲插入时，三种情况下的平衡调整规则，没有一种是适用的。但是，如果我们把黑色的空节点都给它加上，变成下面这样，你会发现，它满足CASE 2了。\n\n你可能会说，你可以调整一下平衡调整规则啊。比如把CASE 2改为“如果关注节点a的叔叔节点b是黑色或者不存在，a是父节点的右子节点，就进行某某操作”。当然可以，但是这样的话规则就没有原来简洁了。\n你可能还会说，这样给红黑树添加黑色的空的叶子节点，会不会比较浪费存储空间呢？答案是不会的。虽然我们在讲解或者画图的时候，每个黑色的、空的叶子节点都是独立画出来的。实际上，在具体实现的时候，我们只需要像下面这样，共用一个黑色的、空的叶子节点就行了。\n\n内容小结“红黑树一向都很难学”，有这种想法的人很多。但是我感觉，其实主要原因是，很多人试图去记忆它的平衡调整策略。实际上，你只需要能看懂我讲的过程，没有知识盲点，就算是掌握了这部分内容了。毕竟实际的软件开发并不是闭卷考试，当你真的需要实现一个红黑树的时候，可以对照着我讲的步骤，一点一点去实现。\n现在，我就来总结一下，如何比较轻松地看懂我今天讲的操作过程。\n第一点，把红黑树的平衡调整的过程比作魔方复原，不要过于深究这个算法的正确性。你只需要明白，只要按照固定的操作步骤，保持插入、删除的过程，不破坏平衡树的定义就行了。\n第二点，找准关注节点，不要搞丢、搞错关注节点。因为每种操作规则，都是基于关注节点来做的，只有弄对了关注节点，才能对应到正确的操作规则中。在迭代的调整过程中，关注节点在不停地改变，所以，这个过程一定要注意，不要弄丢了关注节点。\n第三点，插入操作的平衡调整比较简单，但是删除操作就比较复杂。针对删除操作，我们有两次调整，第一次是针对要删除的节点做初步调整，让调整后的红黑树继续满足第四条定义，“每个节点到可达叶子节点的路径都包含相同个数的黑色节点”。但是这个时候，第三条定义就不满足了，有可能会存在两个红色节点相邻的情况。第二次调整就是解决这个问题，让红黑树不存在相邻的红色节点。\n课后思考如果你以前了解或者学习过红黑树，关于红黑树的实现，你也可以在留言区讲讲，你是怎样来学习的？在学习的过程中，有过什么样的心得体会？有没有什么好的学习方法？\n欢迎留言和我分享，我会第一时间给你反馈。\n","categories":["数据结构与算法","基础篇"]},{"title":"31 | 深度和广度优先搜索：如何找出社交网络中的三度好友关系？","url":"/2020/08/07/31%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%9A%E6%B7%B1%E5%BA%A6%E5%92%8C%E5%B9%BF%E5%BA%A6%E4%BC%98%E5%85%88%E6%90%9C%E7%B4%A2/","content":"上一节我们讲了图的表示方法，讲到如何用有向图、无向图来表示一个社交网络。在社交网络中，有一个六度分割理论，具体是说，你与世界上的另一个人间隔的关系不会超过六度，也就是说平均只需要六步就可以联系到任何两个互不相识的人。\n一个用户的一度连接用户很好理解，就是他的好友，二度连接用户就是他好友的好友，三度连接用户就是他好友的好友的好友。在社交网络中，我们往往通过用户之间的连接关系，来实现推荐“可能认识的人”这么一个功能。今天的开篇问题就是，给你一个用户，如何找出这个用户的所有三度（其中包含一度、二度和三度）好友关系？\n这就要用到今天要讲的深度优先和广度优先搜索算法。\n什么是“搜索”算法？我们知道，算法是作用于具体数据结构之上的，深度优先搜索算法和广度优先搜索算法都是基于“图”这种数据结构的。这是因为，图这种数据结构的表达能力很强，大部分涉及搜索的场景都可以抽象成“图”。\n图上的搜索算法，最直接的理解就是，在图中找出从一个顶点出发，到另一个顶点的路径。具体方法有很多，比如今天要讲的两种最简单、最“暴力”的深度优先、广度优先搜索，还有A*、IDA*等启发式搜索算法。\n我们上一节讲过，图有两种主要存储方法，邻接表和邻接矩阵。今天我会用邻接表来存储图。\n我这里先给出图的代码实现。需要说明一下，深度优先搜索算法和广度优先搜索算法，既可以用在无向图，也可以用在有向图上。在今天的讲解中，我都针对无向图来讲解。\npublic class Graph &#123; // 无向图    private int v; // 顶点的个数    private LinkedList&lt;Integer&gt; adj[]; // 邻接表    public Graph(int v) &#123;        this.v = v;        adj = new LinkedList[v];        for (int i=0; i&lt;v; ++i) &#123;            adj[i] = new LinkedList&lt;&gt;();        &#125;    &#125;    public void addEdge(int s, int t) &#123; // 无向图一条边存两次        adj[s].add(t);        adj[t].add(s);    &#125;&#125;\n广度优先搜索（BFS）广度优先搜索（Breadth-First-Search），我们平常都简称BFS。直观地讲，它其实就是一种“地毯式”层层推进的搜索策略，即先查找离起始顶点最近的，然后是次近的，依次往外搜索。理解起来并不难，所以我画了一张示意图，你可以看下。\n\n尽管广度优先搜索的原理挺简单，但代码实现还是稍微有点复杂度。所以，我们重点讲一下它的代码实现。\n这里面，bfs()函数就是基于之前定义的，图的广度优先搜索的代码实现。其中s表示起始顶点，t表示终止顶点。我们搜索一条从s到t的路径。实际上，这样求得的路径就是从s到t的最短路径。\npublic void bfs(int s, int t) &#123;    if (s == t) return;    boolean[] visited = new boolean[v];    visited[s]=true;    Queue&lt;Integer&gt; queue = new LinkedList&lt;&gt;();    queue.add(s);    int[] prev = new int[v];    for (int i = 0; i &lt; v; ++i) &#123;        prev[i] = -1;    &#125;    while (queue.size() != 0) &#123;        int w = queue.poll();        for (int i = 0; i &lt; adj[w].size(); ++i) &#123;            int q = adj[w].get(i);            if (!visited[q]) &#123;                prev[q] = w;                if (q == t) &#123;                    print(prev, s, t);                    return;                &#125;                visited[q] = true;                queue.add(q);            &#125;        &#125;    &#125;&#125;private void print(int[] prev, int s, int t) &#123; // 递归打印s-&gt;t的路径    if (prev[t] != -1 &amp;&amp; t != s) &#123;        print(prev, s, prev[t]);    &#125;    System.out.print(t + \" \");&#125;\n这段代码不是很好理解，里面有三个重要的辅助变量visited、queue、prev。只要理解这三个变量，读懂这段代码估计就没什么问题了。\nvisited是用来记录已经被访问的顶点，用来避免顶点被重复访问。如果顶点q被访问，那相应的visited[q]会被设置为true。\nqueue是一个队列，用来存储已经被访问、但相连的顶点还没有被访问的顶点。因为广度优先搜索是逐层访问的，也就是说，我们只有把第k层的顶点都访问完成之后，才能访问第k+1层的顶点。当我们访问到第k层的顶点的时候，我们需要把第k层的顶点记录下来，稍后才能通过第k层的顶点来找第k+1层的顶点。所以，我们用这个队列来实现记录的功能。\nprev用来记录搜索路径。当我们从顶点s开始，广度优先搜索到顶点t后，prev数组中存储的就是搜索的路径。不过，这个路径是反向存储的。prev[w]存储的是，顶点w是从哪个前驱顶点遍历过来的。比如，我们通过顶点2的邻接表访问到顶点3，那prev[3]就等于2。为了正向打印出路径，我们需要递归地来打印，你可以看下print()函数的实现方式。\n为了方便你理解，我画了一个广度优先搜索的分解图，你可以结合着代码以及我的讲解一块儿看。\n))\n掌握了广度优先搜索算法的原理，我们来看下，广度优先搜索的时间、空间复杂度是多少呢？\n最坏情况下，终止顶点t离起始顶点s很远，需要遍历完整个图才能找到。这个时候，每个顶点都要进出一遍队列，每个边也都会被访问一次，所以，广度优先搜索的时间复杂度是O(V+E)，其中，V表示顶点的个数，E表示边的个数。当然，对于一个连通图来说，也就是说一个图中的所有顶点都是连通的，E肯定要大于等于V-1，所以，广度优先搜索的时间复杂度也可以简写为O(E)。\n广度优先搜索的空间消耗主要在几个辅助变量visited数组、queue队列、prev数组上。这三个存储空间的大小都不会超过顶点的个数，所以空间复杂度是O(V)。\n深度优先搜索（DFS）深度优先搜索（Depth-First-Search），简称DFS。最直观的例子就是“走迷宫”。\n假设你站在迷宫的某个岔路口，然后想找到出口。你随意选择一个岔路口来走，走着走着发现走不通的时候，你就回退到上一个岔路口，重新选择一条路继续走，直到最终找到出口。这种走法就是一种深度优先搜索策略。\n走迷宫的例子很容易能看懂，我们现在再来看下，如何在图中应用深度优先搜索，来找某个顶点到另一个顶点的路径。\n你可以看我画的这幅图。搜索的起始顶点是s，终止顶点是t，我们希望在图中寻找一条从顶点s到顶点t的路径。如果映射到迷宫那个例子，s就是你起始所在的位置，t就是出口。\n我用深度递归算法，把整个搜索的路径标记出来了。这里面实线箭头表示遍历，虚线箭头表示回退。从图中我们可以看出，深度优先搜索找出来的路径，并不是顶点s到顶点t的最短路径。\n\n实际上，深度优先搜索用的是一种比较著名的算法思想，回溯思想。这种思想解决问题的过程，非常适合用递归来实现。回溯思想我们后面会有专门的一节来讲，我们现在还回到深度优先搜索算法上。\n我把上面的过程用递归来翻译出来，就是下面这个样子。我们发现，深度优先搜索代码实现也用到了prev、visited变量以及print()函数，它们跟广度优先搜索代码实现里的作用是一样的。不过，深度优先搜索代码实现里，有个比较特殊的变量found，它的作用是，当我们已经找到终止顶点t之后，我们就不再递归地继续查找了。\nboolean found = false; // 全局变量或者类成员变量public void dfs(int s, int t) &#123;    found = false;    boolean[] visited = new boolean[v];    int[] prev = new int[v];    for (int i = 0; i &lt; v; ++i) &#123;        prev[i] = -1;    &#125;    recurDfs(s, t, visited, prev);    print(prev, s, t);&#125;private void recurDfs(int w, int t, boolean[] visited, int[] prev) &#123;    if (found == true) return;    visited[w] = true;    if (w == t) &#123;        found = true;        return;    &#125;    for (int i = 0; i &lt; adj[w].size(); ++i) &#123;        int q = adj[w].get(i);        if (!visited[q]) &#123;            prev[q] = w;            recurDfs(q, t, visited, prev);        &#125;    &#125;&#125;\n理解了深度优先搜索算法之后，我们来看，深度优先搜索的时间、空间复杂度是多少呢？\n从我前面画的图可以看出，每条边最多会被访问两次，一次是遍历，一次是回退。所以，图上的深度优先搜索算法的时间复杂度是O(E)，E表示边的个数。\n深度优先搜索算法的消耗内存主要是visited、prev数组和递归调用栈。visited、prev数组的大小跟顶点的个数V成正比，递归调用栈的最大深度不会超过顶点的个数，所以总的空间复杂度就是O(V)。\n解答开篇了解了深度优先搜索和广度优先搜索的原理之后，开篇的问题是不是变得很简单了呢？我们现在来一起看下，如何找出社交网络中某个用户的三度好友关系？\n上一节我们讲过，社交网络可以用图来表示。这个问题就非常适合用图的广度优先搜索算法来解决，因为广度优先搜索是层层往外推进的。首先，遍历与起始顶点最近的一层顶点，也就是用户的一度好友，然后再遍历与用户距离的边数为2的顶点，也就是二度好友关系，以及与用户距离的边数为3的顶点，也就是三度好友关系。\n我们只需要稍加改造一下广度优先搜索代码，用一个数组来记录每个顶点与起始顶点的距离，非常容易就可以找出三度好友关系。\n内容小结广度优先搜索和深度优先搜索是图上的两种最常用、最基本的搜索算法，比起其他高级的搜索算法，比如A*、IDA*等，要简单粗暴，没有什么优化，所以，也被叫作暴力搜索算法。所以，这两种搜索算法仅适用于状态空间不大，也就是说图不大的搜索。\n广度优先搜索，通俗的理解就是，地毯式层层推进，从起始顶点开始，依次往外遍历。广度优先搜索需要借助队列来实现，遍历得到的路径就是，起始顶点到终止顶点的最短路径。深度优先搜索用的是回溯思想，非常适合用递归实现。换种说法，深度优先搜索是借助栈来实现的。在执行效率方面，深度优先和广度优先搜索的时间复杂度都是O(E)，空间复杂度是O(V)。\n课后思考\n我们通过广度优先搜索算法解决了开篇的问题，你可以思考一下，能否用深度优先搜索来解决呢？\n\n学习数据结构最难的不是理解和掌握原理，而是能灵活地将各种场景和问题抽象成对应的数据结构和算法。今天的内容中提到，迷宫可以抽象成图，走迷宫可以抽象成搜索算法，你能具体讲讲，如何将迷宫抽象成一个图吗？或者换个说法，如何在计算机中存储一个迷宫？\n\n\n欢迎留言和我分享，也欢迎点击“请朋友读”，把今天的内容分享给你的好友，和他一起讨论、学习。\n","categories":["数据结构与算法","基础篇"]},{"title":"34 | 字符串匹配基础（下）：如何借助BM算法轻松理解KMP算法？","url":"/2020/08/07/34%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%9A%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%8C%B9%E9%85%8D%E5%9F%BA%E7%A1%80%EF%BC%88%E4%B8%8B%EF%BC%89/","content":"上一节我们讲了BM算法，尽管它很复杂，也不好理解，但却是工程中非常常用的一种高效字符串匹配算法。有统计说，它是最高效、最常用的字符串匹配算法。不过，在所有的字符串匹配算法里，要说最知名的一种的话，那就非KMP算法莫属。很多时候，提到字符串匹配，我们首先想到的就是KMP算法。\n尽管在实际的开发中，我们几乎不大可能自己亲手实现一个KMP算法。但是，学习这个算法的思想，作为让你开拓眼界、锻炼下逻辑思维，也是极好的，所以我觉得有必要拿出来给你讲一讲。不过，KMP算法是出了名的不好懂。我会尽力把它讲清楚，但是你自己也要多动动脑子。\n实际上，KMP算法跟BM算法的本质是一样的。上一节，我们讲了好后缀和坏字符规则，今天，我们就看下，如何借助上一节BM算法的讲解思路，让你能更好地理解KMP算法？\nKMP算法基本原理KMP算法是根据三位作者（D.E.Knuth，J.H.Morris和V.R.Pratt）的名字来命名的，算法的全称是Knuth Morris Pratt算法，简称为KMP算法。\nKMP算法的核心思想，跟上一节讲的BM算法非常相近。我们假设主串是a，模式串是b。在模式串与主串匹配的过程中，当遇到不可匹配的字符的时候，我们希望找到一些规律，可以将模式串往后多滑动几位，跳过那些肯定不会匹配的情况。\n还记得我们上一节讲到的好后缀和坏字符吗？这里我们可以类比一下，在模式串和主串匹配的过程中，把不能匹配的那个字符仍然叫作坏字符，把已经匹配的那段字符串叫作好前缀。\n\n当遇到坏字符的时候，我们就要把模式串往后滑动，在滑动的过程中，只要模式串和好前缀有上下重合，前面几个字符的比较，就相当于拿好前缀的后缀子串，跟模式串的前缀子串在比较。这个比较的过程能否更高效了呢？可以不用一个字符一个字符地比较了吗？\n\nKMP算法就是在试图寻找一种规律：在模式串和主串匹配的过程中，当遇到坏字符后，对于已经比对过的好前缀，能否找到一种规律，将模式串一次性滑动很多位？\n我们只需要拿好前缀本身，在它的后缀子串中，查找最长的那个可以跟好前缀的前缀子串匹配的。假设最长的可匹配的那部分前缀子串是{v}，长度是k。我们把模式串一次性往后滑动j-k位，相当于，每次遇到坏字符的时候，我们就把j更新为k，i不变，然后继续比较。\n\n为了表述起来方便，我把好前缀的所有后缀子串中，最长的可匹配前缀子串的那个后缀子串，叫作最长可匹配后缀子串；对应的前缀子串，叫作最长可匹配前缀子串。\n\n如何来求好前缀的最长可匹配前缀和后缀子串呢？我发现，这个问题其实不涉及主串，只需要通过模式串本身就能求解。所以，我就在想，能不能事先预处理计算好，在模式串和主串匹配的过程中，直接拿过来就用呢？\n类似BM算法中的bc、suffix、prefix数组，KMP算法也可以提前构建一个数组，用来存储模式串中每个前缀（这些前缀都有可能是好前缀）的最长可匹配前缀子串的结尾字符下标。我们把这个数组定义为next数组，很多书中还给这个数组起了一个名字，叫失效函数（failure function）。\n数组的下标是每个前缀结尾字符下标，数组的值是这个前缀的最长可以匹配前缀子串的结尾字符下标。这句话有点拗口，我举了一个例子，你一看应该就懂了。\n\n有了next数组，我们很容易就可以实现KMP算法了。我先假设next数组已经计算好了，先给出KMP算法的框架代码。\n// a, b分别是主串和模式串；n, m分别是主串和模式串的长度。public static int kmp(char[] a, int n, char[] b, int m) &#123;    int[] next = getNexts(b, m);    int j = 0;    for (int i = 0; i &lt; n; ++i) &#123;        while (j &gt; 0 &amp;&amp; a[i] != b[j]) &#123; // 一直找到a[i]和b[j]            j = next[j - 1] + 1;        &#125;        if (a[i] == b[j]) &#123;            ++j;        &#125;        if (j == m) &#123; // 找到匹配模式串的了            return i - m + 1;        &#125;    &#125;    return -1;&#125;\n失效函数计算方法KMP算法的基本原理讲完了，我们现在来看最复杂的部分，也就是next数组是如何计算出来的？\n当然，我们可以用非常笨的方法，比如要计算下面这个模式串b的next[4]，我们就把b[0, 4]的所有后缀子串，从长到短找出来，依次看看，是否能跟模式串的前缀子串匹配。很显然，这个方法也可以计算得到next数组，但是效率非常低。有没有更加高效的方法呢？\n\n这里的处理非常有技巧，类似于动态规划。不过，动态规划我们在后面才会讲到，所以，我这里换种方法解释，也能让你听懂。\n我们按照下标从小到大，依次计算next数组的值。当我们要计算next[i]的时候，前面的next[0]，next[1]，……，next[i-1]应该已经计算出来了。利用已经计算出来的next值，我们是否可以快速推导出next[i]的值呢？\n如果next[i-1]=k-1，也就是说，子串b[0, k-1]是b[0, i-1]的最长可匹配前缀子串。如果子串b[0, k-1]的下一个字符b[k]，与b[0, i-1]的下一个字符b[i]匹配，那子串b[0, k]就是b[0, i]的最长可匹配前缀子串。所以，next[i]等于k。但是，如果b[0, k-1]的下一字符b[k]跟b[0, i-1]的下一个字符b[i]不相等呢？这个时候就不能简单地通过next[i-1]得到next[i]了。这个时候该怎么办呢？\n\n我们假设b[0, i]的最长可匹配后缀子串是b[r, i]。如果我们把最后一个字符去掉，那b[r, i-1]肯定是b[0, i-1]的可匹配后缀子串，但不一定是最长可匹配后缀子串。所以，既然b[0, i-1]最长可匹配后缀子串对应的模式串的前缀子串的下一个字符并不等于b[i]，那么我们就可以考察b[0, i-1]的次长可匹配后缀子串b[x, i-1]对应的可匹配前缀子串b[0, i-1-x]的下一个字符b[i-x]是否等于b[i]。如果等于，那b[x, i]就是b[0, i]的最长可匹配后缀子串。\n\n可是，如何求得b[0, i-1]的次长可匹配后缀子串呢？次长可匹配后缀子串肯定被包含在最长可匹配后缀子串中，而最长可匹配后缀子串又对应最长可匹配前缀子串b[0, y]。于是，查找b[0, i-1]的次长可匹配后缀子串，这个问题就变成，查找b[0, y]的最长匹配后缀子串的问题了。\n\n按照这个思路，我们可以考察完所有的b[0, i-1]的可匹配后缀子串b[y, i-1]，直到找到一个可匹配的后缀子串，它对应的前缀子串的下一个字符等于b[i]，那这个b[y, i]就是b[0, i]的最长可匹配后缀子串。\n前面我已经给出KMP算法的框架代码了，现在我把这部分的代码也写出来了。这两部分代码合在一起，就是整个KMP算法的代码实现。\n// b表示模式串，m表示模式串的长度private static int[] getNexts(char[] b, int m) &#123;    int[] next = new int[m];    next[0] = -1;    int k = -1;    for (int i = 1; i &lt; m; ++i) &#123;        while (k != -1 &amp;&amp; b[k + 1] != b[i]) &#123;            k = next[k];        &#125;        if (b[k + 1] == b[i]) &#123;            ++k;        &#125;        next[i] = k;    &#125;    return next;&#125;\nKMP算法复杂度分析KMP算法的原理和实现我们就讲完了，我们现在来分析一下KMP算法的时间、空间复杂度是多少？\n空间复杂度很容易分析，KMP算法只需要一个额外的next数组，数组的大小跟模式串相同。所以空间复杂度是O(m)，m表示模式串的长度。\nKMP算法包含两部分，第一部分是构建next数组，第二部分才是借助next数组匹配。所以，关于时间复杂度，我们要分别从这两部分来分析。\n我们先来分析第一部分的时间复杂度。\n计算next数组的代码中，第一层for循环中i从1到m-1，也就是说，内部的代码被执行了m-1次。for循环内部代码有一个while循环，如果我们能知道每次for循环、while循环平均执行的次数，假设是k，那时间复杂度就是O(k*m)。但是，while循环执行的次数不怎么好统计，所以我们放弃这种分析方法。\n我们可以找一些参照变量，i和k。i从1开始一直增加到m，而k并不是每次for循环都会增加，所以，k累积增加的值肯定小于m。而while循环里k=next[k]，实际上是在减小k的值，k累积都没有增加超过m，所以while循环里面k=next[k]总的执行次数也不可能超过m。因此，next数组计算的时间复杂度是O(m)。\n我们再来分析第二部分的时间复杂度。分析的方法是类似的。\ni从0循环增长到n-1，j的增长量不可能超过i，所以肯定小于n。而while循环中的那条语句j=next[j-1]+1，不会让j增长的，那有没有可能让j不变呢？也没有可能。因为next[j-1]的值肯定小于j-1，所以while循环中的这条语句实际上也是在让j的值减少。而j总共增长的量都不会超过n，那减少的量也不可能超过n，所以while循环中的这条语句总的执行次数也不会超过n，所以这部分的时间复杂度是O(n)。\n所以，综合两部分的时间复杂度，KMP算法的时间复杂度就是O(m+n)。\n解答开篇&amp;内容小结KMP算法讲完了，不知道你理解了没有？如果没有，建议多看几遍，自己多思考思考。KMP算法和上一节讲的BM算法的本质非常类似，都是根据规律在遇到坏字符的时候，把模式串往后多滑动几位。\nBM算法有两个规则，坏字符和好后缀。KMP算法借鉴BM算法的思想，可以总结成好前缀规则。这里面最难懂的就是next数组的计算。如果用最笨的方法来计算，确实不难，但是效率会比较低。所以，我讲了一种类似动态规划的方法，按照下标i从小到大，依次计算next[i]，并且next[i]的计算通过前面已经计算出来的next[0]，next[1]，……，next[i-1]来推导。\nKMP算法的时间复杂度是O(n+m)，不过它的分析过程稍微需要一点技巧，不那么直观，你只要看懂就好了，并不需要掌握，在我们平常的开发中，很少会有这么难分析的代码。\n课后思考至此，我们把经典的单模式串匹配算法全部讲完了，它们分别是BF算法、RK算法、BM算法和KMP算法，关于这些算法，你觉得什么地方最难理解呢？\n欢迎留言和我分享，也欢迎点击“请朋友读”，把今天的内容分享给你的好友，和他一起讨论、学习。\n","categories":["数据结构与算法","基础篇"]},{"title":"30 | 图的表示：如何存储微博、微信等社交网络中的好友关系？","url":"/2020/08/07/30%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%9A%E5%9B%BE%E7%9A%84%E8%A1%A8%E7%A4%BA/","content":"微博、微信、LinkedIn这些社交软件我想你肯定都玩过吧。在微博中，两个人可以互相关注；在微信中，两个人可以互加好友。那你知道，如何存储微博、微信等这些社交网络的好友关系吗？\n这就要用到我们今天要讲的这种数据结构：图。实际上，涉及图的算法有很多，也非常复杂，比如图的搜索、最短路径、最小生成树、二分图等等。我们今天聚焦在图存储这一方面，后面会分好几节来依次讲解图相关的算法。\n如何理解“图”？我们前面讲过了树这种非线性表数据结构，今天我们要讲另一种非线性表数据结构，图（Graph）。和树比起来，这是一种更加复杂的非线性表结构。\n我们知道，树中的元素我们称为节点，图中的元素我们就叫做顶点（vertex）。从我画的图中可以看出来，图中的一个顶点可以与任意其他顶点建立连接关系。我们把这种建立的关系叫做边（edge）。\n\n我们生活中就有很多符合图这种结构的例子。比如，开篇问题中讲到的社交网络，就是一个非常典型的图结构。\n我们就拿微信举例子吧。我们可以把每个用户看作一个顶点。如果两个用户之间互加好友，那就在两者之间建立一条边。所以，整个微信的好友关系就可以用一张图来表示。其中，每个用户有多少个好友，对应到图中，就叫做顶点的度（degree），就是跟顶点相连接的边的条数。\n实际上，微博的社交关系跟微信还有点不一样，或者说更加复杂一点。微博允许单向关注，也就是说，用户A关注了用户B，但用户B可以不关注用户A。那我们如何用图来表示这种单向的社交关系呢？\n我们可以把刚刚讲的图结构稍微改造一下，引入边的“方向”的概念。\n如果用户A关注了用户B，我们就在图中画一条从A到B的带箭头的边，来表示边的方向。如果用户A和用户B互相关注了，那我们就画一条从A指向B的边，再画一条从B指向A的边。我们把这种边有方向的图叫做“有向图”。以此类推，我们把边没有方向的图就叫做“无向图”。\n\n我们刚刚讲过，无向图中有“度”这个概念，表示一个顶点有多少条边。在有向图中，我们把度分为入度（In-degree）和出度（Out-degree）。\n顶点的入度，表示有多少条边指向这个顶点；顶点的出度，表示有多少条边是以这个顶点为起点指向其他顶点。对应到微博的例子，入度就表示有多少粉丝，出度就表示关注了多少人。\n前面讲到了微信、微博、无向图、有向图，现在我们再来看另一种社交软件：QQ。\nQQ中的社交关系要更复杂一点。不知道你有没有留意过QQ亲密度这样一个功能。QQ不仅记录了用户之间的好友关系，还记录了两个用户之间的亲密度，如果两个用户经常往来，那亲密度就比较高；如果不经常往来，亲密度就比较低。如何在图中记录这种好友关系的亲密度呢？\n这里就要用到另一种图，带权图（weighted graph）。在带权图中，每条边都有一个权重（weight），我们可以通过这个权重来表示QQ好友间的亲密度。\n\n关于图的概念比较多，我今天也只是介绍了几个常用的，理解起来都不复杂，不知道你都掌握了没有？掌握了图的概念之后，我们再来看下，如何在内存中存储图这种数据结构呢？\n邻接矩阵存储方法图最直观的一种存储方法就是，邻接矩阵（Adjacency Matrix）。\n邻接矩阵的底层依赖一个二维数组。对于无向图来说，如果顶点i与顶点j之间有边，我们就将A[i][j]和A[j][i]标记为1；对于有向图来说，如果顶点i到顶点j之间，有一条箭头从顶点i指向顶点j的边，那我们就将A[i][j]标记为1。同理，如果有一条箭头从顶点j指向顶点i的边，我们就将A[j][i]标记为1。对于带权图，数组中就存储相应的权重。\n\n用邻接矩阵来表示一个图，虽然简单、直观，但是比较浪费存储空间。为什么这么说呢？\n对于无向图来说，如果A[i][j]等于1，那A[j][i]也肯定等于1。实际上，我们只需要存储一个就可以了。也就是说，无向图的二维数组中，如果我们将其用对角线划分为上下两部分，那我们只需要利用上面或者下面这样一半的空间就足够了，另外一半白白浪费掉了。\n还有，如果我们存储的是稀疏图（Sparse Matrix），也就是说，顶点很多，但每个顶点的边并不多，那邻接矩阵的存储方法就更加浪费空间了。比如微信有好几亿的用户，对应到图上就是好几亿的顶点。但是每个用户的好友并不会很多，一般也就三五百个而已。如果我们用邻接矩阵来存储，那绝大部分的存储空间都被浪费了。\n但这也并不是说，邻接矩阵的存储方法就完全没有优点。首先，邻接矩阵的存储方式简单、直接，因为基于数组，所以在获取两个顶点的关系时，就非常高效。其次，用邻接矩阵存储图的另外一个好处是方便计算。这是因为，用邻接矩阵的方式存储图，可以将很多图的运算转换成矩阵之间的运算。比如求解最短路径问题时会提到一个Floyd-Warshall算法，就是利用矩阵循环相乘若干次得到结果。\n邻接表存储方法针对上面邻接矩阵比较浪费内存空间的问题，我们来看另外一种图的存储方法，邻接表（Adjacency List）。\n我画了一张邻接表的图，你可以先看下。乍一看，邻接表是不是有点像散列表？每个顶点对应一条链表，链表中存储的是与这个顶点相连接的其他顶点。另外我需要说明一下，图中画的是一个有向图的邻接表存储方式，每个顶点对应的链表里面，存储的是指向的顶点。对于无向图来说，也是类似的，不过，每个顶点的链表中存储的，是跟这个顶点有边相连的顶点，你可以自己画下。\n\n还记得我们之前讲过的时间、空间复杂度互换的设计思想吗？邻接矩阵存储起来比较浪费空间，但是使用起来比较节省时间。相反，邻接表存储起来比较节省空间，但是使用起来就比较耗时间。\n就像图中的例子，如果我们要确定，是否存在一条从顶点2到顶点4的边，那我们就要遍历顶点2对应的那条链表，看链表中是否存在顶点4。而且，我们前面也讲过，链表的存储方式对缓存不友好。所以，比起邻接矩阵的存储方式，在邻接表中查询两个顶点之间的关系就没那么高效了。\n在散列表那几节里，我讲到，在基于链表法解决冲突的散列表中，如果链过长，为了提高查找效率，我们可以将链表换成其他更加高效的数据结构，比如平衡二叉查找树等。我们刚刚也讲到，邻接表长得很像散列。所以，我们也可以将邻接表同散列表一样进行“改进升级”。\n我们可以将邻接表中的链表改成平衡二叉查找树。实际开发中，我们可以选择用红黑树。这样，我们就可以更加快速地查找两个顶点之间是否存在边了。当然，这里的二叉查找树可以换成其他动态数据结构，比如跳表、散列表等。除此之外，我们还可以将链表改成有序动态数组，可以通过二分查找的方法来快速定位两个顶点之间否是存在边。\n解答开篇有了前面讲的理论知识，现在我们回过头来看开篇的问题，如何存储微博、微信等社交网络中的好友关系？\n前面我们分析了，微博、微信是两种“图”，前者是有向图，后者是无向图。在这个问题上，两者的解决思路差不多，所以我只拿微博来讲解。\n数据结构是为算法服务的，所以具体选择哪种存储方法，与期望支持的操作有关系。针对微博用户关系，假设我们需要支持下面这样几个操作：\n\n判断用户A是否关注了用户B；\n\n判断用户A是否是用户B的粉丝；\n\n用户A关注用户B；\n\n用户A取消关注用户B；\n\n根据用户名称的首字母排序，分页获取用户的粉丝列表；\n\n根据用户名称的首字母排序，分页获取用户的关注列表。\n\n\n关于如何存储一个图，前面我们讲到两种主要的存储方法，邻接矩阵和邻接表。因为社交网络是一张稀疏图，使用邻接矩阵存储比较浪费存储空间。所以，这里我们采用邻接表来存储。\n不过，用一个邻接表来存储这种有向图是不够的。我们去查找某个用户关注了哪些用户非常容易，但是如果要想知道某个用户都被哪些用户关注了，也就是用户的粉丝列表，是非常困难的。\n基于此，我们需要一个逆邻接表。邻接表中存储了用户的关注关系，逆邻接表中存储的是用户的被关注关系。对应到图上，邻接表中，每个顶点的链表中，存储的就是这个顶点指向的顶点，逆邻接表中，每个顶点的链表中，存储的是指向这个顶点的顶点。如果要查找某个用户关注了哪些用户，我们可以在邻接表中查找；如果要查找某个用户被哪些用户关注了，我们从逆邻接表中查找。\n\n基础的邻接表不适合快速判断两个用户之间是否是关注与被关注的关系，所以我们选择改进版本，将邻接表中的链表改为支持快速查找的动态数据结构。选择哪种动态数据结构呢？红黑树、跳表、有序动态数组还是散列表呢？\n因为我们需要按照用户名称的首字母排序，分页来获取用户的粉丝列表或者关注列表，用跳表这种结构再合适不过了。这是因为，跳表插入、删除、查找都非常高效，时间复杂度是O(logn)，空间复杂度上稍高，是O(n)。最重要的一点，跳表中存储的数据本来就是有序的了，分页获取粉丝列表或关注列表，就非常高效。\n如果对于小规模的数据，比如社交网络中只有几万、几十万个用户，我们可以将整个社交关系存储在内存中，上面的解决思路是没有问题的。但是如果像微博那样有上亿的用户，数据规模太大，我们就无法全部存储在内存中了。这个时候该怎么办呢？\n我们可以通过哈希算法等数据分片方式，将邻接表存储在不同的机器上。你可以看下面这幅图，我们在机器1上存储顶点1，2，3的邻接表，在机器2上，存储顶点4，5的邻接表。逆邻接表的处理方式也一样。当要查询顶点与顶点关系的时候，我们就利用同样的哈希算法，先定位顶点所在的机器，然后再在相应的机器上查找。\n\n除此之外，我们还有另外一种解决思路，就是利用外部存储（比如硬盘），因为外部存储的存储空间要比内存会宽裕很多。数据库是我们经常用来持久化存储关系数据的，所以我这里介绍一种数据库的存储方式。\n我用下面这张表来存储这样一个图。为了高效地支持前面定义的操作，我们可以在表上建立多个索引，比如第一列、第二列，给这两列都建立索引。\n\n内容小结今天我们学习了图这种非线性表数据结构，关于图，你需要理解这样几个概念：无向图、有向图、带权图、顶点、边、度、入度、出度。除此之外，我们还学习了图的两个主要的存储方式：邻接矩阵和邻接表。\n邻接矩阵存储方法的缺点是比较浪费空间，但是优点是查询效率高，而且方便矩阵运算。邻接表存储方法中每个顶点都对应一个链表，存储与其相连接的其他顶点。尽管邻接表的存储方式比较节省存储空间，但链表不方便查找，所以查询效率没有邻接矩阵存储方式高。针对这个问题，邻接表还有改进升级版，即将链表换成更加高效的动态数据结构，比如平衡二叉查找树、跳表、散列表等。\n课后思考\n关于开篇思考题，我们只讲了微博这种有向图的解决思路，那像微信这种无向图，应该怎么存储呢？你可以照着我的思路，自己做一下练习。\n\n除了我今天举的社交网络可以用图来表示之外，符合图这种结构特点的例子还有很多，比如知识图谱（Knowledge Graph）。关于图这种数据结构，你还能想到其他生活或者工作中的例子吗？\n\n\n欢迎留言和我分享，我会第一时间给你反馈。\n","categories":["数据结构与算法","基础篇"]},{"title":"36 | AC自动机：如何用多模式串匹配实现敏感词过滤功能？","url":"/2020/08/07/36%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%9AAC%E8%87%AA%E5%8A%A8%E6%9C%BA/","content":"很多支持用户发表文本内容的网站，比如BBS，大都会有敏感词过滤功能，用来过滤掉用户输入的一些淫秽、反动、谩骂等内容。你有没有想过，这个功能是怎么实现的呢？\n实际上，这些功能最基本的原理就是字符串匹配算法，也就是通过维护一个敏感词的字典，当用户输入一段文字内容之后，通过字符串匹配算法，来查找用户输入的这段文字，是否包含敏感词。如果有，就用“***”把它替代掉。\n我们前面讲过好几种字符串匹配算法了，它们都可以处理这个问题。但是，对于访问量巨大的网站来说，比如淘宝，用户每天的评论数有几亿、甚至几十亿。这时候，我们对敏感词过滤系统的性能要求就要很高。毕竟，我们也不想，用户输入内容之后，要等几秒才能发送出去吧？我们也不想，为了这个功能耗费过多的机器吧？那如何才能实现一个高性能的敏感词过滤系统呢？这就要用到今天的多模式串匹配算法。\n基于单模式串和Trie树实现的敏感词过滤我们前面几节讲了好几种字符串匹配算法，有BF算法、RK算法、BM算法、KMP算法，还有Trie树。前面四种算法都是单模式串匹配算法，只有Trie树是多模式串匹配算法。\n我说过，单模式串匹配算法，是在一个模式串和一个主串之间进行匹配，也就是说，在一个主串中查找一个模式串。多模式串匹配算法，就是在多个模式串和一个主串之间做匹配，也就是说，在一个主串中查找多个模式串。\n尽管，单模式串匹配算法也能完成多模式串的匹配工作。例如开篇的思考题，我们可以针对每个敏感词，通过单模式串匹配算法（比如KMP算法）与用户输入的文字内容进行匹配。但是，这样做的话，每个匹配过程都需要扫描一遍用户输入的内容。整个过程下来就要扫描很多遍用户输入的内容。如果敏感词很多，比如几千个，并且用户输入的内容很长，假如有上千个字符，那我们就需要扫描几千遍这样的输入内容。很显然，这种处理思路比较低效。\n与单模式匹配算法相比，多模式匹配算法在这个问题的处理上就很高效了。它只需要扫描一遍主串，就能在主串中一次性查找多个模式串是否存在，从而大大提高匹配效率。我们知道，Trie树就是一种多模式串匹配算法。那如何用Trie树实现敏感词过滤功能呢？\n我们可以对敏感词字典进行预处理，构建成Trie树结构。这个预处理的操作只需要做一次，如果敏感词字典动态更新了，比如删除、添加了一个敏感词，那我们只需要动态更新一下Trie树就可以了。\n当用户输入一个文本内容后，我们把用户输入的内容作为主串，从第一个字符（假设是字符C）开始，在Trie树中匹配。当匹配到Trie树的叶子节点，或者中途遇到不匹配字符的时候，我们将主串的开始匹配位置后移一位，也就是从字符C的下一个字符开始，重新在Trie树中匹配。\n基于Trie树的这种处理方法，有点类似单模式串匹配的BF算法。我们知道，单模式串匹配算法中，KMP算法对BF算法进行改进，引入了next数组，让匹配失败时，尽可能将模式串往后多滑动几位。借鉴单模式串的优化改进方法，能否对多模式串Trie树进行改进，进一步提高Trie树的效率呢？这就要用到AC自动机算法了。\n经典的多模式串匹配算法：AC自动机AC自动机算法，全称是Aho-Corasick算法。其实，Trie树跟AC自动机之间的关系，就像单串匹配中朴素的串匹配算法，跟KMP算法之间的关系一样，只不过前者针对的是多模式串而已。所以，AC自动机实际上就是在Trie树之上，加了类似KMP的next数组，只不过此处的next数组是构建在树上罢了。如果代码表示，就是下面这个样子：\npublic class AcNode &#123;    public char data;     public AcNode[] children = new AcNode[26]; // 字符集只包含a~z这26个字符    public boolean isEndingChar = false; // 结尾字符为true    public int length = -1; // 当isEndingChar=true时，记录模式串长度    public AcNode fail; // 失败指针    public AcNode(char data) &#123;        this.data = data;    &#125;&#125;\n所以，AC自动机的构建，包含两个操作：\n\n将多个模式串构建成Trie树；\n\n在Trie树上构建失败指针（相当于KMP中的失效函数next数组）。\n\n\n关于如何构建Trie树，我们上一节已经讲过了。所以，这里我们就重点看下，构建好Trie树之后，如何在它之上构建失败指针？\n我用一个例子给你讲解。这里有4个模式串，分别是c，bc，bcd，abcd；主串是abcd。\n\nTrie树中的每一个节点都有一个失败指针，它的作用和构建过程，跟KMP算法中的next数组极其相似。所以要想看懂这节内容，你要先理解KMP算法中next数组的构建过程。如果你还有点不清楚，建议你先回头去弄懂KMP算法。\n假设我们沿Trie树走到p节点，也就是下图中的紫色节点，那p的失败指针就是从root走到紫色节点形成的字符串abc，跟所有模式串前缀匹配的最长可匹配后缀子串，就是箭头指的bc模式串。\n这里的最长可匹配后缀子串，我稍微解释一下。字符串abc的后缀子串有两个bc，c，我们拿它们与其他模式串匹配，如果某个后缀子串可以匹配某个模式串的前缀，那我们就把这个后缀子串叫作可匹配后缀子串。\n我们从可匹配后缀子串中，找出最长的一个，就是刚刚讲到的最长可匹配后缀子串。我们将p节点的失败指针指向那个最长匹配后缀子串对应的模式串的前缀的最后一个节点，就是下图中箭头指向的节点。\n\n计算每个节点的失败指针这个过程看起来有些复杂。其实，如果我们把树中相同深度的节点放到同一层，那么某个节点的失败指针只有可能出现在它所在层的上一层。\n我们可以像KMP算法那样，当我们要求某个节点的失败指针的时候，我们通过已经求得的、深度更小的那些节点的失败指针来推导。也就是说，我们可以逐层依次来求解每个节点的失败指针。所以，失败指针的构建过程，是一个按层遍历树的过程。\n首先root的失败指针为NULL，也就是指向自己。当我们已经求得某个节点p的失败指针之后，如何寻找它的子节点的失败指针呢？\n我们假设节点p的失败指针指向节点q，我们看节点p的子节点pc对应的字符，是否也可以在节点q的子节点中找到。如果找到了节点q的一个子节点qc，对应的字符跟节点pc对应的字符相同，则将节点pc的失败指针指向节点qc。\n\n如果节点q中没有子节点的字符等于节点pc包含的字符，则令q=q-&gt;fail（fail表示失败指针，这里有没有很像KMP算法里求next的过程？），继续上面的查找，直到q是root为止，如果还没有找到相同字符的子节点，就让节点pc的失败指针指向root。\n\n我将构建失败指针的代码贴在这里，你可以对照着讲解一块看下，应该更容易理解。这里面，构建Trie树的代码我并没有贴出来，你可以参看上一节的代码，自己实现。\npublic void buildFailurePointer() &#123;    Queue&lt;AcNode&gt; queue = new LinkedList&lt;&gt;();    root.fail = null;    queue.add(root);    while (!queue.isEmpty()) &#123;        AcNode p = queue.remove();        for (int i = 0; i &lt; 26; ++i) &#123;            AcNode pc = p.children[i];            if (pc == null) continue;            if (p == root) &#123;                pc.fail = root;            &#125; else &#123;                AcNode q = p.fail;                while (q != null) &#123;                    AcNode qc = q.children[pc.data - 'a'];                    if (qc != null) &#123;                        pc.fail = qc;                        break;                    &#125;                    q = q.fail;                &#125;                if (q == null) &#123;                    pc.fail = root;                &#125;            &#125;            queue.add(pc);        &#125;    &#125;&#125;\n通过按层来计算每个节点的子节点的失效指针，刚刚举的那个例子，最后构建完成之后的AC自动机就是下面这个样子：\n\nAC自动机到此就构建完成了。我们现在来看下，如何在AC自动机上匹配主串？\n我们还是拿之前的例子来讲解。在匹配过程中，主串从i=0开始，AC自动机从指针p=root开始，假设模式串是b，主串是a。\n\n如果p指向的节点有一个等于b[i]的子节点x，我们就更新p指向x，这个时候我们需要通过失败指针，检测一系列失败指针为结尾的路径是否是模式串。这一句不好理解，你可以结合代码看。处理完之后，我们将i加一，继续这两个过程；\n\n如果p指向的节点没有等于b[i]的子节点，那失败指针就派上用场了，我们让p=p-&gt;fail，然后继续这2个过程。\n\n\n关于匹配的这部分，文字描述不如代码看得清楚，所以我把代码贴了出来，非常简短，并且添加了详细的注释，你可以对照着看下。这段代码输出的就是，在主串中每个可以匹配的模式串出现的位置。\npublic void match(char[] text) &#123; // text是主串    int n = text.length;    AcNode p = root;    for (int i = 0; i &lt; n; ++i) &#123;        int idx = text[i] - 'a';        while (p.children[idx] == null &amp;&amp; p != root) &#123;            p = p.fail; // 失败指针发挥作用的地方        &#125;        p = p.children[idx];        if (p == null) p = root; // 如果没有匹配的，从root开始重新匹配        AcNode tmp = p;        while (tmp != root) &#123; // 打印出可以匹配的模式串            if (tmp.isEndingChar == true) &#123;                int pos = i-tmp.length+1;                System.out.println(\"匹配起始下标\" + pos + \"; 长度\" + tmp.length);            &#125;            tmp = tmp.fail;        &#125;    &#125;&#125;\n解答开篇AC自动机的内容讲完了，关于开篇的问题，你应该能解答了吧？实际上，我上面贴出来的代码，已经是一个敏感词过滤的原型代码了。它可以找到所有敏感词出现的位置（在用户输入的文本中的起始下标）。你只需要稍加改造，再遍历一遍文本内容（主串），就可以将文本中的所有敏感词替换成“***”。\n所以我这里着重讲一下，AC自动机实现的敏感词过滤系统，是否比单模式串匹配方法更高效呢？\n首先，我们需要将敏感词构建成AC自动机，包括构建Trie树以及构建失败指针。\n我们上一节讲过，Trie树构建的时间复杂度是O(m*len)，其中len表示敏感词的平均长度，m表示敏感词的个数。那构建失败指针的时间复杂度是多少呢？我这里给出一个不是很紧确的上界。\n假设Trie树中总的节点个数是k，每个节点构建失败指针的时候，（你可以看下代码）最耗时的环节是while循环中的q=q-&gt;fail，每运行一次这个语句，q指向节点的深度都会减少1，而树的高度最高也不会超过len，所以每个节点构建失败指针的时间复杂度是O(len)。整个失败指针的构建过程就是O(k*len)。\n不过，AC自动机的构建过程都是预先处理好的，构建好之后，并不会频繁地更新，所以不会影响到敏感词过滤的运行效率。\n我们再来看下，用AC自动机做匹配的时间复杂度是多少？\n跟刚刚构建失败指针的分析类似，for循环依次遍历主串中的每个字符，for循环内部最耗时的部分也是while循环，而这一部分的时间复杂度也是O(len)，所以总的匹配的时间复杂度就是O(n*len)。因为敏感词并不会很长，而且这个时间复杂度只是一个非常宽泛的上限，实际情况下，可能近似于O(n)，所以AC自动机做敏感词过滤，性能非常高。\n你可以会说，从时间复杂度上看，AC自动机匹配的效率跟Trie树一样啊。实际上，因为失效指针可能大部分情况下都指向root节点，所以绝大部分情况下，在AC自动机上做匹配的效率要远高于刚刚计算出的比较宽泛的时间复杂度。只有在极端情况下，如图所示，AC自动机的性能才会退化的跟Trie树一样。\n\n内容小结今天我们讲了多模式串匹配算法，AC自动机。单模式串匹配算法是为了快速在主串中查找一个模式串，而多模式串匹配算法是为了快速在主串中查找多个模式串。\nAC自动机是基于Trie树的一种改进算法，它跟Trie树的关系，就像单模式串中，KMP算法与BF算法的关系一样。KMP算法中有一个非常关键的next数组，类比到AC自动机中就是失败指针。而且，AC自动机失败指针的构建过程，跟KMP算法中计算next数组极其相似。所以，要理解AC自动机，最好先掌握KMP算法，因为AC自动机其实就是KMP算法在多模式串上的改造。\n整个AC自动机算法包含两个部分，第一部分是将多个模式串构建成AC自动机，第二部分是在AC自动机中匹配主串。第一部分又分为两个小的步骤，一个是将模式串构建成Trie树，另一个是在Trie树上构建失败指针。\n课后思考到此为止，字符串匹配算法我们全都讲完了，你能试着分析总结一下，各个字符串匹配算法的特点和比较适合的应用场景吗？\n欢迎留言和我分享，也欢迎点击“请朋友读”，把今天的内容分享给你的好友，和他一起讨论、学习。\n","categories":["数据结构与算法","基础篇"]},{"title":"32 | 字符串匹配基础（上）：如何借助哈希算法实现高效字符串匹配？","url":"/2020/08/07/32%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%9A%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%8C%B9%E9%85%8D%E5%9F%BA%E7%A1%80%EF%BC%88%E4%B8%8A%EF%BC%89/","content":"从今天开始，我们来学习字符串匹配算法。字符串匹配这样一个功能，我想对于任何一个开发工程师来说，应该都不会陌生。我们用的最多的就是编程语言提供的字符串查找函数，比如Java中的indexOf()，Python中的find()函数等，它们底层就是依赖接下来要讲的字符串匹配算法。\n字符串匹配算法很多，我会分四节来讲解。今天我会讲两种比较简单的、好理解的，它们分别是：BF算法和RK算法。下一节，我会讲两种比较难理解、但更加高效的，它们是：BM算法和KMP算法。\n这两节讲的都是单模式串匹配的算法，也就是一个串跟一个串进行匹配。第三节、第四节，我会讲两种多模式串匹配算法，也就是在一个串中同时查找多个串，它们分别是Trie树和AC自动机。\n今天讲的两个算法中，RK算法是BF算法的改进，它巧妙借助了我们前面讲过的哈希算法，让匹配的效率有了很大的提升。那RK算法是如何借助哈希算法来实现高效字符串匹配的呢？你可以带着这个问题，来学习今天的内容。\nBF算法BF算法中的BF是Brute Force的缩写，中文叫作暴力匹配算法，也叫朴素匹配算法。从名字可以看出，这种算法的字符串匹配方式很“暴力”，当然也就会比较简单、好懂，但相应的性能也不高。\n在开始讲解这个算法之前，我先定义两个概念，方便我后面讲解。它们分别是主串和模式串。这俩概念很好理解，我举个例子你就懂了。\n比方说，我们在字符串A中查找字符串B，那字符串A就是主串，字符串B就是模式串。我们把主串的长度记作n，模式串的长度记作m。因为我们是在主串中查找模式串，所以n&gt;m。\n作为最简单、最暴力的字符串匹配算法，BF算法的思想可以用一句话来概括，那就是，我们在主串中，检查起始位置分别是0、1、2…n-m且长度为m的n-m+1个子串，看有没有跟模式串匹配的。我举一个例子给你看看，你应该可以理解得更清楚。\n\n从上面的算法思想和例子，我们可以看出，在极端情况下，比如主串是“aaaaa…aaaaaa”（省略号表示有很多重复的字符a），模式串是“aaaaab”。我们每次都比对m个字符，要比对n-m+1次，所以，这种算法的最坏情况时间复杂度是O(n*m)。\n尽管理论上，BF算法的时间复杂度很高，是O(n*m)，但在实际的开发中，它却是一个比较常用的字符串匹配算法。为什么这么说呢？原因有两点。\n第一，实际的软件开发中，大部分情况下，模式串和主串的长度都不会太长。而且每次模式串与主串中的子串匹配的时候，当中途遇到不能匹配的字符的时候，就可以就停止了，不需要把m个字符都比对一下。所以，尽管理论上的最坏情况时间复杂度是O(n*m)，但是，统计意义上，大部分情况下，算法执行效率要比这个高很多。\n第二，朴素字符串匹配算法思想简单，代码实现也非常简单。简单意味着不容易出错，如果有bug也容易暴露和修复。在工程中，在满足性能要求的前提下，简单是首选。这也是我们常说的KISS（Keep it Simple and Stupid）设计原则。\n所以，在实际的软件开发中，绝大部分情况下，朴素的字符串匹配算法就够用了。\nRK算法RK算法的全称叫Rabin-Karp算法，是由它的两位发明者Rabin和Karp的名字来命名的。这个算法理解起来也不是很难。我个人觉得，它其实就是刚刚讲的BF算法的升级版。\n我在讲BF算法的时候讲过，如果模式串长度为m，主串长度为n，那在主串中，就会有n-m+1个长度为m的子串，我们只需要暴力地对比这n-m+1个子串与模式串，就可以找出主串与模式串匹配的子串。\n但是，每次检查主串与子串是否匹配，需要依次比对每个字符，所以BF算法的时间复杂度就比较高，是O(n*m)。我们对朴素的字符串匹配算法稍加改造，引入哈希算法，时间复杂度立刻就会降低。\nRK算法的思路是这样的：我们通过哈希算法对主串中的n-m+1个子串分别求哈希值，然后逐个与模式串的哈希值比较大小。如果某个子串的哈希值与模式串相等，那就说明对应的子串和模式串匹配了（这里先不考虑哈希冲突的问题，后面我们会讲到）。因为哈希值是一个数字，数字之间比较是否相等是非常快速的，所以模式串和子串比较的效率就提高了。\n\n不过，通过哈希算法计算子串的哈希值的时候，我们需要遍历子串中的每个字符。尽管模式串与子串比较的效率提高了，但是，算法整体的效率并没有提高。有没有方法可以提高哈希算法计算子串哈希值的效率呢？\n这就需要哈希算法设计的非常有技巧了。我们假设要匹配的字符串的字符集中只包含K个字符，我们可以用一个K进制数来表示一个子串，这个K进制数转化成十进制数，作为子串的哈希值。表述起来有点抽象，我举了一个例子，看完你应该就能懂了。\n比如要处理的字符串只包含a～z这26个小写字母，那我们就用二十六进制来表示一个字符串。我们把a～z这26个字符映射到0～25这26个数字，a就表示0，b就表示1，以此类推，z表示25。\n在十进制的表示法中，一个数字的值是通过下面的方式计算出来的。对应到二十六进制，一个包含a到z这26个字符的字符串，计算哈希的时候，我们只需要把进位从10改成26就可以。\n\n这个哈希算法你应该看懂了吧？现在，为了方便解释，在下面的讲解中，我假设字符串中只包含a～z这26个小写字符，我们用二十六进制来表示一个字符串，对应的哈希值就是二十六进制数转化成十进制的结果。\n这种哈希算法有一个特点，在主串中，相邻两个子串的哈希值的计算公式有一定关系。我这有个例子，你先找一下规律，再来看我后面的讲解。\n\n从这里例子中，我们很容易就能得出这样的规律：相邻两个子串s[i-1]和s[i]（i表示子串在主串中的起始位置，子串的长度都为m），对应的哈希值计算公式有交集，也就是说，我们可以使用s[i-1]的哈希值很快的计算出s[i]的哈希值。如果用公式表示的话，就是下面这个样子：\n\n不过，这里有一个小细节需要注意，那就是26^(m-1)这部分的计算，我们可以通过查表的方法来提高效率。我们事先计算好26^0、26^1、26^2……26^(m-1)，并且存储在一个长度为m的数组中，公式中的“次方”就对应数组的下标。当我们需要计算26的x次方的时候，就可以从数组的下标为x的位置取值，直接使用，省去了计算的时间。\n\n我们开头的时候提过，RK算法的效率要比BF算法高，现在，我们就来分析一下，RK算法的时间复杂度到底是多少呢？\n整个RK算法包含两部分，计算子串哈希值和模式串哈希值与子串哈希值之间的比较。第一部分，我们前面也分析了，可以通过设计特殊的哈希算法，只需要扫描一遍主串就能计算出所有子串的哈希值了，所以这部分的时间复杂度是O(n)。\n模式串哈希值与每个子串哈希值之间的比较的时间复杂度是O(1)，总共需要比较n-m+1个子串的哈希值，所以，这部分的时间复杂度也是O(n)。所以，RK算法整体的时间复杂度就是O(n)。\n这里还有一个问题就是，模式串很长，相应的主串中的子串也会很长，通过上面的哈希算法计算得到的哈希值就可能很大，如果超过了计算机中整型数据可以表示的范围，那该如何解决呢？\n刚刚我们设计的哈希算法是没有散列冲突的，也就是说，一个字符串与一个二十六进制数一一对应，不同的字符串的哈希值肯定不一样。因为我们是基于进制来表示一个字符串的，你可以类比成十进制、十六进制来思考一下。实际上，我们为了能将哈希值落在整型数据范围内，可以牺牲一下，允许哈希冲突。这个时候哈希算法该如何设计呢？\n哈希算法的设计方法有很多，我举一个例子说明一下。假设字符串中只包含a～z这26个英文字母，那我们每个字母对应一个数字，比如a对应1，b对应2，以此类推，z对应26。我们可以把字符串中每个字母对应的数字相加，最后得到的和作为哈希值。这种哈希算法产生的哈希值的数据范围就相对要小很多了。\n不过，你也应该发现，这种哈希算法的哈希冲突概率也是挺高的。当然，我只是举了一个最简单的设计方法，还有很多更加优化的方法，比如将每一个字母从小到大对应一个素数，而不是1，2，3……这样的自然数，这样冲突的概率就会降低一些。\n那现在新的问题来了。之前我们只需要比较一下模式串和子串的哈希值，如果两个值相等，那这个子串就一定可以匹配模式串。但是，当存在哈希冲突的时候，有可能存在这样的情况，子串和模式串的哈希值虽然是相同的，但是两者本身并不匹配。\n实际上，解决方法很简单。当我们发现一个子串的哈希值跟模式串的哈希值相等的时候，我们只需要再对比一下子串和模式串本身就好了。当然，如果子串的哈希值与模式串的哈希值不相等，那对应的子串和模式串肯定也是不匹配的，就不需要比对子串和模式串本身了。\n所以，哈希算法的冲突概率要相对控制得低一些，如果存在大量冲突，就会导致RK算法的时间复杂度退化，效率下降。极端情况下，如果存在大量的冲突，每次都要再对比子串和模式串本身，那时间复杂度就会退化成O(n*m)。但也不要太悲观，一般情况下，冲突不会很多，RK算法的效率还是比BF算法高的。\n解答开篇&amp;内容小结今天我们讲了两种字符串匹配算法，BF算法和RK算法。\nBF算法是最简单、粗暴的字符串匹配算法，它的实现思路是，拿模式串与主串中是所有子串匹配，看是否有能匹配的子串。所以，时间复杂度也比较高，是O(n*m)，n、m表示主串和模式串的长度。不过，在实际的软件开发中，因为这种算法实现简单，对于处理小规模的字符串匹配很好用。\nRK算法是借助哈希算法对BF算法进行改造，即对每个子串分别求哈希值，然后拿子串的哈希值与模式串的哈希值比较，减少了比较的时间。所以，理想情况下，RK算法的时间复杂度是O(n)，跟BF算法相比，效率提高了很多。不过这样的效率取决于哈希算法的设计方法，如果存在冲突的情况下，时间复杂度可能会退化。极端情况下，哈希算法大量冲突，时间复杂度就退化为O(n*m)。\n课后思考我们今天讲的都是一维字符串的匹配方法，实际上，这两种算法都可以类比到二维空间。假设有下面这样一个二维字符串矩阵（图中的主串），借助今天讲的处理思路，如何在其中查找另一个二维字符串矩阵（图中的模式串）呢？\n\n欢迎留言和我分享，也欢迎点击“请朋友读”，把今天的内容分享给你的好友，和他一起讨论、学习。\n","categories":["数据结构与算法","基础篇"]},{"title":"39 | 回溯算法：从电影《蝴蝶效应》中学习回溯算法的核心思想","url":"/2020/08/07/39%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%9A%E5%9B%9E%E6%BA%AF%E7%AE%97%E6%B3%95/","content":"我们在第31节提到，深度优先搜索算法利用的是回溯算法思想。这个算法思想非常简单，但是应用却非常广泛。它除了用来指导像深度优先搜索这种经典的算法设计之外，还可以用在很多实际的软件开发场景中，比如正则表达式匹配、编译原理中的语法分析等。\n除此之外，很多经典的数学问题都可以用回溯算法解决，比如数独、八皇后、0-1背包、图的着色、旅行商问题、全排列等等。既然应用如此广泛，我们今天就来学习一下这个算法思想，看看它是如何指导我们解决问题的。\n如何理解“回溯算法”？在我们的一生中，会遇到很多重要的岔路口。在岔路口上，每个选择都会影响我们今后的人生。有的人在每个岔路口都能做出最正确的选择，最后生活、事业都达到了一个很高的高度；而有的人一路选错，最后碌碌无为。如果人生可以量化，那如何才能在岔路口做出最正确的选择，让自己的人生“最优”呢？\n我们可以借助前面学过的贪心算法，在每次面对岔路口的时候，都做出看起来最优的选择，期望这一组选择可以使得我们的人生达到“最优”。但是，我们前面也讲过，贪心算法并不一定能得到最优解。那有没有什么办法能得到最优解呢？\n2004年上映了一部非常著名的电影《蝴蝶效应》，讲的就是主人公为了达到自己的目标，一直通过回溯的方法，回到童年，在关键的岔路口，重新做选择。当然，这只是科幻电影，我们的人生是无法倒退的，但是这其中蕴含的思想其实就是回溯算法。\n笼统地讲，回溯算法很多时候都应用在“搜索”这类问题上。不过这里说的搜索，并不是狭义的指我们前面讲过的图的搜索算法，而是在一组可能的解中，搜索满足期望的解。\n回溯的处理思想，有点类似枚举搜索。我们枚举所有的解，找到满足期望的解。为了有规律地枚举所有可能的解，避免遗漏和重复，我们把问题求解的过程分为多个阶段。每个阶段，我们都会面对一个岔路口，我们先随意选一条路走，当发现这条路走不通的时候（不符合期望的解），就回退到上一个岔路口，另选一种走法继续走。\n理论的东西还是过于抽象，老规矩，我还是举例说明一下。我举一个经典的回溯例子，我想你可能已经猜到了，那就是八皇后问题。\n我们有一个8x8的棋盘，希望往里放8个棋子（皇后），每个棋子所在的行、列、对角线都不能有另一个棋子。你可以看我画的图，第一幅图是满足条件的一种方法，第二幅图是不满足条件的。八皇后问题就是期望找到所有满足这种要求的放棋子方式。\n\n我们把这个问题划分成8个阶段，依次将8个棋子放到第一行、第二行、第三行……第八行。在放置的过程中，我们不停地检查当前放法，是否满足要求。如果满足，则跳到下一行继续放置棋子；如果不满足，那就再换一种放法，继续尝试。\n回溯算法非常适合用递归代码实现，所以，我把八皇后的算法翻译成代码。我在代码里添加了详细的注释，你可以对比着看下。如果你之前没有接触过八皇后问题，建议你自己用熟悉的编程语言实现一遍，这对你理解回溯思想非常有帮助。\nint[] result = new int[8];//全局或成员变量,下标表示行,值表示queen存储在哪一列public void cal8queens(int row) &#123; // 调用方式：cal8queens(0);    if (row == 8) &#123; // 8个棋子都放置好了，打印结果        printQueens(result);        return; // 8行棋子都放好了，已经没法再往下递归了，所以就return    &#125;    for (int column = 0; column &lt; 8; ++column) &#123; // 每一行都有8中放法        if (isOk(row, column)) &#123; // 有些放法不满足要求            result[row] = column; // 第row行的棋子放到了column列            cal8queens(row+1); // 考察下一行        &#125;    &#125;&#125;private boolean isOk(int row, int column) &#123;//判断row行column列放置是否合适    int leftup = column - 1, rightup = column + 1;    for (int i = row-1; i &gt;= 0; --i) &#123; // 逐行往上考察每一行        if (result[i] == column) return false; // 第i行的column列有棋子吗？        if (leftup &gt;= 0) &#123; // 考察左上对角线：第i行leftup列有棋子吗？            if (result[i] == leftup) return false;        &#125;        if (rightup &lt; 8) &#123; // 考察右上对角线：第i行rightup列有棋子吗？            if (result[i] == rightup) return false;        &#125;        --leftup; ++rightup;    &#125;    return true;&#125;private void printQueens(int[] result) &#123; // 打印出一个二维矩阵    for (int row = 0; row &lt; 8; ++row) &#123;        for (int column = 0; column &lt; 8; ++column) &#123;            if (result[row] == column) System.out.print(\"Q \");            else System.out.print(\"* \");        &#125;        System.out.println();    &#125;    System.out.println();&#125;\n两个回溯算法的经典应用回溯算法的理论知识很容易弄懂。不过，对于新手来说，比较难的是用递归来实现。所以，我们再通过两个例子，来练习一下回溯算法的应用和实现。\n1.0-1背包0-1背包是非常经典的算法问题，很多场景都可以抽象成这个问题模型。这个问题的经典解法是动态规划，不过还有一种简单但没有那么高效的解法，那就是今天讲的回溯算法。动态规划的解法我下一节再讲，我们先来看下，如何用回溯法解决这个问题。\n0-1背包问题有很多变体，我这里介绍一种比较基础的。我们有一个背包，背包总的承载重量是Wkg。现在我们有n个物品，每个物品的重量不等，并且不可分割。我们现在期望选择几件物品，装载到背包中。在不超过背包所能装载重量的前提下，如何让背包中物品的总重量最大？\n实际上，背包问题我们在贪心算法那一节，已经讲过一个了，不过那里讲的物品是可以分割的，我可以装某个物品的一部分到背包里面。今天讲的这个背包问题，物品是不可分割的，要么装要么不装，所以叫0-1背包问题。显然，这个问题已经无法通过贪心算法来解决了。我们现在来看看，用回溯算法如何来解决。\n对于每个物品来说，都有两种选择，装进背包或者不装进背包。对于n个物品来说，总的装法就有2^n种，去掉总重量超过Wkg的，从剩下的装法中选择总重量最接近Wkg的。不过，我们如何才能不重复地穷举出这2^n种装法呢？\n这里就可以用回溯的方法。我们可以把物品依次排列，整个问题就分解为了n个阶段，每个阶段对应一个物品怎么选择。先对第一个物品进行处理，选择装进去或者不装进去，然后再递归地处理剩下的物品。描述起来很费劲，我们直接看代码，反而会更加清晰一些。\n这里还稍微用到了一点搜索剪枝的技巧，就是当发现已经选择的物品的重量超过Wkg之后，我们就停止继续探测剩下的物品。你可以看我写的具体的代码。\npublic int maxW = Integer.MIN_VALUE; //存储背包中物品总重量的最大值// cw表示当前已经装进去的物品的重量和；i表示考察到哪个物品了；// w背包重量；items表示每个物品的重量；n表示物品个数// 假设背包可承受重量100，物品个数10，物品重量存储在数组a中，那可以这样调用函数：// f(0, 0, a, 10, 100)public void f(int i, int cw, int[] items, int n, int w) &#123;    if (cw == w || i == n) &#123; // cw==w表示装满了;i==n表示已经考察完所有的物品        if (cw &gt; maxW) maxW = cw;        return;    &#125;    f(i+1, cw, items, n, w);    if (cw + items[i] &lt;= w) &#123;// 已经超过可以背包承受的重量的时候，就不要再装了        f(i+1,cw + items[i], items, n, w);    &#125;&#125;\n2.正则表达式看懂了0-1背包问题，我们再来看另外一个例子，正则表达式匹配。\n对于一个开发工程师来说，正则表达式你应该不陌生吧？在平时的开发中，或多或少都应该用过。实际上，正则表达式里最重要的一种算法思想就是回溯。\n正则表达式中，最重要的就是通配符，通配符结合在一起，可以表达非常丰富的语义。为了方便讲解，我假设正则表达式中只包含“*”和“?”这两种通配符，并且对这两个通配符的语义稍微做些改变，其中，“*”匹配任意多个（大于等于0个）任意字符，“?”匹配零个或者一个任意字符。基于以上背景假设，我们看下，如何用回溯算法，判断一个给定的文本，能否跟给定的正则表达式匹配？\n我们依次考察正则表达式中的每个字符，当是非通配符时，我们就直接跟文本的字符进行匹配，如果相同，则继续往下处理；如果不同，则回溯。\n如果遇到特殊字符的时候，我们就有多种处理方式了，也就是所谓的岔路口，比如“*”有多种匹配方案，可以匹配任意个文本串中的字符，我们就先随意的选择一种匹配方案，然后继续考察剩下的字符。如果中途发现无法继续匹配下去了，我们就回到这个岔路口，重新选择一种匹配方案，然后再继续匹配剩下的字符。\n有了前面的基础，是不是这个问题就好懂多了呢？我把这个过程翻译成了代码，你可以结合着一块看下，应该有助于你理解。\npublic class Pattern &#123;    private boolean matched = false;    private char[] pattern; // 正则表达式    private int plen; // 正则表达式长度    public Pattern(char[] pattern, int plen) &#123;        this.pattern = pattern;        this.plen = plen;    &#125;    public boolean match(char[] text, int tlen) &#123; // 文本串及长度        matched = false;        rmatch(0, 0, text, tlen);        return matched;    &#125;    private void rmatch(int ti, int pj, char[] text, int tlen) &#123;        if (matched) return; // 如果已经匹配了，就不要继续递归了        if (pj == plen) &#123; // 正则表达式到结尾了            if (ti == tlen) matched = true; // 文本串也到结尾了            return;        &#125;        if (pattern[pj] == '*') &#123; // *匹配任意个字符            for (int k = 0; k &lt;= tlen-ti; ++k) &#123;                rmatch(ti+k, pj+1, text, tlen);            &#125;        &#125; else if (pattern[pj] == '?') &#123; // ?匹配0个或者1个字符            rmatch(ti, pj+1, text, tlen);            rmatch(ti+1, pj+1, text, tlen);        &#125; else if (ti &lt; tlen &amp;&amp; pattern[pj] == text[ti]) &#123; // 纯字符匹配才行            rmatch(ti+1, pj+1, text, tlen);        &#125;    &#125;&#125;\n内容小结回溯算法的思想非常简单，大部分情况下，都是用来解决广义的搜索问题，也就是，从一组可能的解中，选择出一个满足要求的解。回溯算法非常适合用递归来实现，在实现的过程中，剪枝操作是提高回溯效率的一种技巧。利用剪枝，我们并不需要穷举搜索所有的情况，从而提高搜索效率。\n尽管回溯算法的原理非常简单，但是却可以解决很多问题，比如我们开头提到的深度优先搜索、八皇后、0-1背包问题、图的着色、旅行商问题、数独、全排列、正则表达式匹配等等。如果感兴趣的话，你可以自己搜索研究一下，最好还能用代码实现一下。如果这几个问题都能实现的话，你基本就掌握了回溯算法。\n课后思考现在我们对今天讲到的0-1背包问题稍加改造，如果每个物品不仅重量不同，价值也不同。如何在不超过背包重量的情况下，让背包中的总价值最大？\n欢迎留言和我分享，也欢迎点击“请朋友读”，把今天的内容分享给你的好友，和他一起讨论、学习。\n","categories":["数据结构与算法","基础篇"]},{"title":"37 | 贪心算法：如何用贪心算法实现Huffman压缩编码？","url":"/2020/08/07/37%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%9A%E8%B4%AA%E5%BF%83%E7%AE%97%E6%B3%95/","content":"基础的数据结构和算法我们基本上学完了，接下来几节，我会讲几种更加基本的算法。它们分别是贪心算法、分治算法、回溯算法、动态规划。更加确切地说，它们应该是算法思想，并不是具体的算法，常用来指导我们设计具体的算法和编码等。\n贪心、分治、回溯、动态规划这4个算法思想，原理解释起来都很简单，但是要真正掌握且灵活应用，并不是件容易的事情。所以，接下来的这4个算法思想的讲解，我依旧不会长篇大论地去讲理论，而是结合具体的问题，让你自己感受这些算法是怎么工作的，是如何解决问题的，带你在问题中体会这些算法的本质。我觉得，这比单纯记忆原理和定义要更有价值。\n今天，我们先来学习一下贪心算法（greedy algorithm）。贪心算法有很多经典的应用，比如霍夫曼编码（Huffman Coding）、Prim和Kruskal最小生成树算法、还有Dijkstra单源最短路径算法。最小生成树算法和最短路径算法我们后面会讲到，所以我们今天讲下霍夫曼编码，看看它是如何利用贪心算法来实现对数据压缩编码，有效节省数据存储空间的。\n如何理解“贪心算法”？关于贪心算法，我们先看一个例子。\n假设我们有一个可以容纳100kg物品的背包，可以装各种物品。我们有以下5种豆子，每种豆子的总量和总价值都各不相同。为了让背包中所装物品的总价值最大，我们如何选择在背包中装哪些豆子？每种豆子又该装多少呢？\n\n实际上，这个问题很简单，我估计你一下子就能想出来，没错，我们只要先算一算每个物品的单价，按照单价由高到低依次来装就好了。单价从高到低排列，依次是：黑豆、绿豆、红豆、青豆、黄豆，所以，我们可以往背包里装20kg黑豆、30kg绿豆、50kg红豆。\n这个问题的解决思路显而易见，它本质上借助的就是贪心算法。结合这个例子，我总结一下贪心算法解决问题的步骤，我们一起来看看。\n第一步，当我们看到这类问题的时候，首先要联想到贪心算法：针对一组数据，我们定义了限制值和期望值，希望从中选出几个数据，在满足限制值的情况下，期望值最大。\n类比到刚刚的例子，限制值就是重量不能超过100kg，期望值就是物品的总价值。这组数据就是5种豆子。我们从中选出一部分，满足重量不超过100kg，并且总价值最大。\n第二步，我们尝试看下这个问题是否可以用贪心算法解决：每次选择当前情况下，在对限制值同等贡献量的情况下，对期望值贡献最大的数据。\n类比到刚刚的例子，我们每次都从剩下的豆子里面，选择单价最高的，也就是重量相同的情况下，对价值贡献最大的豆子。\n第三步，我们举几个例子看下贪心算法产生的结果是否是最优的。大部分情况下，举几个例子验证一下就可以了。严格地证明贪心算法的正确性，是非常复杂的，需要涉及比较多的数学推理。而且，从实践的角度来说，大部分能用贪心算法解决的问题，贪心算法的正确性都是显而易见的，也不需要严格的数学推导证明。\n实际上，用贪心算法解决问题的思路，并不总能给出最优解。\n我来举一个例子。在一个有权图中，我们从顶点S开始，找一条到顶点T的最短路径（路径中边的权值和最小）。贪心算法的解决思路是，每次都选择一条跟当前顶点相连的权最小的边，直到找到顶点T。按照这种思路，我们求出的最短路径是S-&gt;A-&gt;E-&gt;T，路径长度是1+4+4=9。\n\n但是，这种贪心的选择方式，最终求的路径并不是最短路径，因为路径S-&gt;B-&gt;D-&gt;T才是最短路径，因为这条路径的长度是2+2+2=6。为什么贪心算法在这个问题上不工作了呢？\n在这个问题上，贪心算法不工作的主要原因是，前面的选择，会影响后面的选择。如果我们第一步从顶点S走到顶点A，那接下来面对的顶点和边，跟第一步从顶点S走到顶点B，是完全不同的。所以，即便我们第一步选择最优的走法（边最短），但有可能因为这一步选择，导致后面每一步的选择都很糟糕，最终也就无缘全局最优解了。\n贪心算法实战分析对于贪心算法，你是不是还有点懵？如果死抠理论的话，确实很难理解透彻。掌握贪心算法的关键是多练习。只要多练习几道题，自然就有感觉了。所以，我带着你分析几个具体的例子，帮助你深入理解贪心算法。\n1.分糖果我们有m个糖果和n个孩子。我们现在要把糖果分给这些孩子吃，但是糖果少，孩子多（m&lt;n），所以糖果只能分配给一部分孩子。\n每个糖果的大小不等，这m个糖果的大小分别是s1，s2，s3，……，sm。除此之外，每个孩子对糖果大小的需求也是不一样的，只有糖果的大小大于等于孩子的对糖果大小的需求的时候，孩子才得到满足。假设这n个孩子对糖果大小的需求分别是g1，g2，g3，……，gn。\n我的问题是，如何分配糖果，能尽可能满足最多数量的孩子？\n我们可以把这个问题抽象成，从n个孩子中，抽取一部分孩子分配糖果，让满足的孩子的个数（期望值）是最大的。这个问题的限制值就是糖果个数m。\n我们现在来看看如何用贪心算法来解决。对于一个孩子来说，如果小的糖果可以满足，我们就没必要用更大的糖果，这样更大的就可以留给其他对糖果大小需求更大的孩子。另一方面，对糖果大小需求小的孩子更容易被满足，所以，我们可以从需求小的孩子开始分配糖果。因为满足一个需求大的孩子跟满足一个需求小的孩子，对我们期望值的贡献是一样的。\n我们每次从剩下的孩子中，找出对糖果大小需求最小的，然后发给他剩下的糖果中能满足他的最小的糖果，这样得到的分配方案，也就是满足的孩子个数最多的方案。\n2.钱币找零这个问题在我们的日常生活中更加普遍。假设我们有1元、2元、5元、10元、20元、50元、100元这些面额的纸币，它们的张数分别是c1、c2、c5、c10、c20、c50、c100。我们现在要用这些钱来支付K元，最少要用多少张纸币呢？\n在生活中，我们肯定是先用面值最大的来支付，如果不够，就继续用更小一点面值的，以此类推，最后剩下的用1元来补齐。\n在贡献相同期望值（纸币数目）的情况下，我们希望多贡献点金额，这样就可以让纸币数更少，这就是一种贪心算法的解决思路。直觉告诉我们，这种处理方法就是最好的。实际上，要严谨地证明这种贪心算法的正确性，需要比较复杂的、有技巧的数学推导，我不建议你花太多时间在上面，不过如果感兴趣的话，可以自己去研究下。\n3.区间覆盖假设我们有n个区间，区间的起始端点和结束端点分别是[l1, r1]，[l2, r2]，[l3, r3]，……，[ln, rn]。我们从这n个区间中选出一部分区间，这部分区间满足两两不相交（端点相交的情况不算相交），最多能选出多少个区间呢？\n\n这个问题的处理思路稍微不是那么好懂，不过，我建议你最好能弄懂，因为这个处理思想在很多贪心算法问题中都有用到，比如任务调度、教师排课等等问题。\n这个问题的解决思路是这样的：我们假设这n个区间中最左端点是lmin，最右端点是rmax。这个问题就相当于，我们选择几个不相交的区间，从左到右将[lmin, rmax]覆盖上。我们按照起始端点从小到大的顺序对这n个区间排序。\n我们每次选择的时候，左端点跟前面的已经覆盖的区间不重合的，右端点又尽量小的，这样可以让剩下的未覆盖区间尽可能的大，就可以放置更多的区间。这实际上就是一种贪心的选择方法。\n\n解答开篇今天的内容就讲完了，我们现在来看开篇的问题，如何用贪心算法实现霍夫曼编码？\n假设我有一个包含1000个字符的文件，每个字符占1个byte（1byte=8bits），存储这1000个字符就一共需要8000bits，那有没有更加节省空间的存储方式呢？\n假设我们通过统计分析发现，这1000个字符中只包含6种不同字符，假设它们分别是a、b、c、d、e、f。而3个二进制位（bit）就可以表示8个不同的字符，所以，为了尽量减少存储空间，每个字符我们用3个二进制位来表示。那存储这1000个字符只需要3000bits就可以了，比原来的存储方式节省了很多空间。不过，还有没有更加节省空间的存储方式呢？\na(000)、b(001)、c(010)、d(011)、e(100)、f(101)\n霍夫曼编码就要登场了。霍夫曼编码是一种十分有效的编码方法，广泛用于数据压缩中，其压缩率通常在20%～90%之间。\n霍夫曼编码不仅会考察文本中有多少个不同字符，还会考察每个字符出现的频率，根据频率的不同，选择不同长度的编码。霍夫曼编码试图用这种不等长的编码方法，来进一步增加压缩的效率。如何给不同频率的字符选择不同长度的编码呢？根据贪心的思想，我们可以把出现频率比较多的字符，用稍微短一些的编码；出现频率比较少的字符，用稍微长一些的编码。\n对于等长的编码来说，我们解压缩起来很简单。比如刚才那个例子中，我们用3个bit表示一个字符。在解压缩的时候，我们每次从文本中读取3位二进制码，然后翻译成对应的字符。但是，霍夫曼编码是不等长的，每次应该读取1位还是2位、3位等等来解压缩呢？这个问题就导致霍夫曼编码解压缩起来比较复杂。为了避免解压缩过程中的歧义，霍夫曼编码要求各个字符的编码之间，不会出现某个编码是另一个编码前缀的情况。\n\n假设这6个字符出现的频率从高到低依次是a、b、c、d、e、f。我们把它们编码下面这个样子，任何一个字符的编码都不是另一个的前缀，在解压缩的时候，我们每次会读取尽可能长的可解压的二进制串，所以在解压缩的时候也不会歧义。经过这种编码压缩之后，这1000个字符只需要2100bits就可以了。\n\n尽管霍夫曼编码的思想并不难理解，但是如何根据字符出现频率的不同，给不同的字符进行不同长度的编码呢？这里的处理稍微有些技巧。\n我们把每个字符看作一个节点，并且附带着把频率放到优先级队列中。我们从队列中取出频率最小的两个节点A、B，然后新建一个节点C，把频率设置为两个节点的频率之和，并把这个新节点C作为节点A、B的父节点。最后再把C节点放入到优先级队列中。重复这个过程，直到队列中没有数据。\n\n现在，我们给每一条边加上画一个权值，指向左子节点的边我们统统标记为0，指向右子节点的边，我们统统标记为1，那从根节点到叶节点的路径就是叶节点对应字符的霍夫曼编码。\n\n内容小结今天我们学习了贪心算法。\n实际上，贪心算法适用的场景比较有限。这种算法思想更多的是指导设计基础算法。比如最小生成树算法、单源最短路径算法，这些算法都用到了贪心算法。从我个人的学习经验来讲，不要刻意去记忆贪心算法的原理，多练习才是最有效的学习方法。\n贪心算法的最难的一块是如何将要解决的问题抽象成贪心算法模型，只要这一步搞定之后，贪心算法的编码一般都很简单。贪心算法解决问题的正确性虽然很多时候都看起来是显而易见的，但是要严谨地证明算法能够得到最优解，并不是件容易的事。所以，很多时候，我们只需要多举几个例子，看一下贪心算法的解决方案是否真的能得到最优解就可以了。\n课后思考\n在一个非负整数a中，我们希望从中移除k个数字，让剩下的数字值最小，如何选择移除哪k个数字呢？\n\n假设有n个人等待被服务，但是服务窗口只有一个，每个人需要被服务的时间长度是不同的，如何安排被服务的先后顺序，才能让这n个人总的等待时间最短？\n\n\n欢迎留言和我分享，也欢迎点击“请朋友读”，把今天的内容分享给你的好友，和他一起讨论、学习。\n课后解析\n由最高位开始，比较低一位数字，如高位大，移除，若高位小，则向右移一位继续比较两个数字，直到高位大于低位则移除，循环k次\n由等待时间最短的开始服务\n\n","categories":["数据结构与算法","基础篇"]},{"title":"43 | 拓扑排序：如何确定代码源文件的编译依赖关系？","url":"/2020/08/07/43%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%9A%E6%8B%93%E6%89%91%E6%8E%92%E5%BA%8F/","content":"从今天开始，我们就进入了专栏的高级篇。相对基础篇，高级篇涉及的知识点，都比较零散，不是太系统。所以，我会围绕一个实际软件开发的问题，在阐述具体解决方法的过程中，将涉及的知识点给你详细讲解出来。\n所以，相较于基础篇“开篇问题-知识讲解-回答开篇-总结-课后思考”这样的文章结构，高级篇我稍作了些改变，大致分为这样几个部分：“问题阐述-算法解析-总结引申-课后思考”。\n好了，现在，我们就进入高级篇的第一节，如何确定代码源文件的编译依赖关系？\n我们知道，一个完整的项目往往会包含很多代码源文件。编译器在编译整个项目的时候，需要按照依赖关系，依次编译每个源文件。比如，A.cpp依赖B.cpp，那在编译的时候，编译器需要先编译B.cpp，才能编译A.cpp。\n编译器通过分析源文件或者程序员事先写好的编译配置文件（比如Makefile文件），来获取这种局部的依赖关系。那编译器又该如何通过源文件两两之间的局部依赖关系，确定一个全局的编译顺序呢？\n\n算法解析这个问题的解决思路与“图”这种数据结构的一个经典算法“拓扑排序算法”有关。那什么是拓扑排序呢？这个概念很好理解，我们先来看一个生活中的拓扑排序的例子。\n我们在穿衣服的时候都有一定的顺序，我们可以把这种顺序想成，衣服与衣服之间有一定的依赖关系。比如说，你必须先穿袜子才能穿鞋，先穿内裤才能穿秋裤。假设我们现在有八件衣服要穿，它们之间的两两依赖关系我们已经很清楚了，那如何安排一个穿衣序列，能够满足所有的两两之间的依赖关系？\n这就是个拓扑排序问题。从这个例子中，你应该能想到，在很多时候，拓扑排序的序列并不是唯一的。你可以看我画的这幅图，我找到了好几种满足这些局部先后关系的穿衣序列。\n\n弄懂了这个生活中的例子，开篇的关于编译顺序的问题，你应该也有思路了。开篇问题跟这个问题的模型是一样的，也可以抽象成一个拓扑排序问题。\n拓扑排序的原理非常简单，我们的重点应该放到拓扑排序的实现上面。\n我前面多次讲过，算法是构建在具体的数据结构之上的。针对这个问题，我们先来看下，如何将问题背景抽象成具体的数据结构？\n我们可以把源文件与源文件之间的依赖关系，抽象成一个有向图。每个源文件对应图中的一个顶点，源文件之间的依赖关系就是顶点之间的边。\n如果a先于b执行，也就是说b依赖于a，那么就在顶点a和顶点b之间，构建一条从a指向b的边。而且，这个图不仅要是有向图，还要是一个有向无环图，也就是不能存在像a-&gt;b-&gt;c-&gt;a这样的循环依赖关系。因为图中一旦出现环，拓扑排序就无法工作了。实际上，拓扑排序本身就是基于有向无环图的一个算法。\npublic class Graph &#123;    private int v; // 顶点的个数    private LinkedList&lt;Integer&gt; adj[]; // 邻接表    public Graph(int v) &#123;        this.v = v;        adj = new LinkedList[v];        for (int i=0; i&lt;v; ++i) &#123;            adj[i] = new LinkedList&lt;&gt;();        &#125;    &#125;    public void addEdge(int s, int t) &#123; // s先于t，边s-&gt;t        adj[s].add(t);    &#125;&#125;\n数据结构定义好了，现在，我们来看，如何在这个有向无环图上，实现拓扑排序？\n拓扑排序有两种实现方法，都不难理解。它们分别是Kahn算法和DFS深度优先搜索算法。我们依次来看下它们都是怎么工作的。\n1.Kahn算法Kahn算法实际上用的是贪心算法思想，思路非常简单、好懂。\n定义数据结构的时候，如果s需要先于t执行，那就添加一条s指向t的边。所以，如果某个顶点入度为0， 也就表示，没有任何顶点必须先于这个顶点执行，那么这个顶点就可以执行了。\n我们先从图中，找出一个入度为0的顶点，将其输出到拓扑排序的结果序列中（对应代码中就是把它打印出来），并且把这个顶点从图中删除（也就是把这个顶点可达的顶点的入度都减1）。我们循环执行上面的过程，直到所有的顶点都被输出。最后输出的序列，就是满足局部依赖关系的拓扑排序。\n我把Kahn算法用代码实现了一下，你可以结合着文字描述一块看下。不过，你应该能发现，这段代码实现更有技巧一些，并没有真正删除顶点的操作。代码中有详细的注释，你自己来看，我就不多解释了。\npublic void topoSortByKahn() &#123;    int[] inDegree = new int[v]; // 统计每个顶点的入度    for (int i = 0; i &lt; v; ++i) &#123;        for (int j = 0; j &lt; adj[i].size(); ++j) &#123;            int w = adj[i].get(j); // i-&gt;w            inDegree[w]++;        &#125;    &#125;    LinkedList&lt;Integer&gt; queue = new LinkedList&lt;&gt;();    for (int i = 0; i &lt; v; ++i) &#123;        if (inDegree[i] == 0) queue.add(i);    &#125;    while (!queue.isEmpty()) &#123;        int i = queue.remove();        System.out.print(\"-&gt;\" + i);        for (int j = 0; j &lt; adj[i].size(); ++j) &#123;            int k = adj[i].get(j);            inDegree[k]--;            if (inDegree[k] == 0) queue.add(k);        &#125;    &#125;&#125;\n2.DFS算法图上的深度优先搜索我们前面已经讲过了，实际上，拓扑排序也可以用深度优先搜索来实现。不过这里的名字要稍微改下，更加确切的说法应该是深度优先遍历，遍历图中的所有顶点，而非只是搜索一个顶点到另一个顶点的路径。\n关于这个算法的实现原理，我先把代码贴在下面，下面给你具体解释。\npublic void topoSortByDFS() &#123;    // 先构建逆邻接表，边s-&gt;t表示，s依赖于t，t先于s    LinkedList&lt;Integer&gt; inverseAdj[] = new LinkedList[v];    for (int i = 0; i &lt; v; ++i) &#123; // 申请空间        inverseAdj[i] = new LinkedList&lt;&gt;();    &#125;    for (int i = 0; i &lt; v; ++i) &#123; // 通过邻接表生成逆邻接表        for (int j = 0; j &lt; adj[i].size(); ++j) &#123;            int w = adj[i].get(j); // i-&gt;w            inverseAdj[w].add(i); // w-&gt;i        &#125;    &#125;    boolean[] visited = new boolean[v];    for (int i = 0; i &lt; v; ++i) &#123; // 深度优先遍历图        if (visited[i] == false) &#123;            visited[i] = true;            dfs(i, inverseAdj, visited);        &#125;    &#125;&#125;private void dfs(    int vertex, LinkedList&lt;Integer&gt; inverseAdj[], boolean[] visited) &#123;    for (int i = 0; i &lt; inverseAdj[vertex].size(); ++i) &#123;        int w = inverseAdj[vertex].get(i);        if (visited[w] == true) continue;        visited[w] = true;        dfs(w, inverseAdj, visited);    &#125; // 先把vertex这个顶点可达的所有顶点都打印出来之后，再打印它自己    System.out.print(\"-&gt;\" + vertex);&#125;\n这个算法包含两个关键部分。\n第一部分是通过邻接表构造逆邻接表。邻接表中，边s-&gt;t表示s先于t执行，也就是t要依赖s。在逆邻接表中，边s-&gt;t表示s依赖于t，s后于t执行。为什么这么转化呢？这个跟我们这个算法的实现思想有关。\n第二部分是这个算法的核心，也就是递归处理每个顶点。对于顶点vertex来说，我们先输出它可达的所有顶点，也就是说，先把它依赖的所有的顶点输出了，然后再输出自己。\n到这里，用Kahn算法和DFS算法求拓扑排序的原理和代码实现都讲完了。我们来看下，这两个算法的时间复杂度分别是多少呢？\n从Kahn代码中可以看出来，每个顶点被访问了一次，每个边也都被访问了一次，所以，Kahn算法的时间复杂度就是O(V+E)（V表示顶点个数，E表示边的个数）。\nDFS算法的时间复杂度我们之前分析过。每个顶点被访问两次，每条边都被访问一次，所以时间复杂度也是O(V+E)。\n注意，这里的图可能不是连通的，有可能是有好几个不连通的子图构成，所以，E并不一定大于V，两者的大小关系不确定。所以，在表示时间复杂度的时候，V、E都要考虑在内。\n总结引申在基础篇中，关于“图”，我们讲了图的定义和存储、图的广度和深度优先搜索。今天，我们又讲了一个关于图的算法，拓扑排序。\n拓扑排序应用非常广泛，解决的问题的模型也非常一致。凡是需要通过局部顺序来推导全局顺序的，一般都能用拓扑排序来解决。除此之外，拓扑排序还能检测图中环的存在。对于Kahn算法来说，如果最后输出出来的顶点个数，少于图中顶点个数，图中还有入度不是0的顶点，那就说明，图中存在环。\n关于图中环的检测，我们在递归那一节讲过一个例子，在查找最终推荐人的时候，可能会因为脏数据，造成存在循环推荐，比如，用户A推荐了用户B，用户B推荐了用户C，用户C又推荐了用户A。如何避免这种脏数据导致的无限递归？这个问题，我当时留给你思考了，现在是时候解答了。\n实际上，这就是环的检测问题。因为我们每次都只是查找一个用户的最终推荐人，所以，我们并不需要动用复杂的拓扑排序算法，而只需要记录已经访问过的用户ID，当用户ID第二次被访问的时候，就说明存在环，也就说明存在脏数据。\nHashSet&lt;Integer&gt; hashTable = new HashSet&lt;&gt;(); // 保存已经访问过的actorIdlong findRootReferrerId(long actorId) &#123;    if (hashTable.contains(actorId)) &#123; // 存在环        return;    &#125;    hashTable.add(actorId);    Long referrerId =         select referrer_id from [table] where actor_id = actorId;    if (referrerId == null) return actorId;    return findRootReferrerId(referrerId);&#125;\n如果把这个问题改一下，我们想要知道，数据库中的所有用户之间的推荐关系了，有没有存在环的情况。这个问题，就需要用到拓扑排序算法了。我们把用户之间的推荐关系，从数据库中加载到内存中，然后构建成今天讲的这种有向图数据结构，再利用拓扑排序，就可以快速检测出是否存在环了。\n课后思考\n在今天的讲解中，我们用图表示依赖关系的时候，如果a先于b执行，我们就画一条从a到b的有向边；反过来，如果a先于b，我们画一条从b到a的有向边，表示b依赖a，那今天讲的Kahn算法和DFS算法还能否正确工作呢？如果不能，应该如何改造一下呢？\n\n我们今天讲了两种拓扑排序算法的实现思路，Kahn算法和DFS深度优先搜索算法，如果换做BFS广度优先搜索算法，还可以实现吗？\n\n\n欢迎留言和我分享，也欢迎点击“请朋友读”，把今天的内容分享给你的好友，和他一起讨论、学习。\n","categories":["数据结构与算法","高级篇"]},{"title":"46 | 概率统计：如何利用朴素贝叶斯算法过滤垃圾短信？","url":"/2020/08/07/46%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%9A%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1/","content":"上一节我们讲到，如何用位图、布隆过滤器，来过滤重复的数据。今天，我们再讲一个跟过滤相关的问题，如何过滤垃圾短信？\n垃圾短信和骚扰电话，我想每个人都收到过吧？买房、贷款、投资理财、开发票，各种垃圾短信和骚扰电话，不胜其扰。如果你是一名手机应用开发工程师，让你实现一个简单的垃圾短信过滤功能以及骚扰电话拦截功能，该用什么样的数据结构和算法实现呢？\n算法解析实际上，解决这个问题并不会涉及很高深的算法。今天，我就带你一块看下，如何利用简单的数据结构和算法，解决这种看似非常复杂的问题。\n1.基于黑名单的过滤器我们可以维护一个骚扰电话号码和垃圾短信发送号码的黑名单。这个黑名单的收集，有很多途径，比如，我们可以从一些公开的网站上下载，也可以通过类似“360骚扰电话拦截”的功能，通过用户自主标记骚扰电话来收集。对于被多个用户标记，并且标记个数超过一定阈值的号码，我们就可以定义为骚扰电话，并将它加入到我们的黑名单中。\n如果黑名单中的电话号码不多的话，我们可以使用散列表、二叉树等动态数据结构来存储，对内存的消耗并不会很大。如果我们把每个号码看作一个字符串，并且假设平均长度是16个字节，那存储50万个电话号码，大约需要10MB的内存空间。即便是对于手机这样的内存有限的设备来说，这点内存的消耗也是可以接受的。\n但是，如果黑名单中的电话号码很多呢？比如有500万个。这个时候，如果再用散列表存储，就需要大约100MB的存储空间。为了实现一个拦截功能，耗费用户如此多的手机内存，这显然有点儿不合理。\n上一节我们讲了，布隆过滤器最大的特点就是比较省存储空间，所以，用它来解决这个问题再合适不过了。如果我们要存储500万个手机号码，我们把位图大小设置为10倍数据大小，也就是5000万，那也只需要使用5000万个二进制位（5000万bits），换算成字节，也就是不到7MB的存储空间。比起散列表的解决方案，内存的消耗减少了很多。\n实际上，我们还有一种时间换空间的方法，可以将内存的消耗优化到极致。\n我们可以把黑名单存储在服务器端上，把过滤和拦截的核心工作，交给服务器端来做。手机端只负责将要检查的号码发送给服务器端，服务器端通过查黑名单，判断这个号码是否应该被拦截，并将结果返回给手机端。\n用这个解决思路完全不需要占用手机内存。不过，有利就有弊。我们知道，网络通信是比较慢的，所以，网络延迟就会导致处理速度降低。而且，这个方案还有个硬性要求，那就是只有在联网的情况下，才能正常工作。\n基于黑名单的过滤器我就讲完了，不过，你可能还会说，布隆过滤器会有判错的概率呀！如果它把一个重要的电话或者短信，当成垃圾短信或者骚扰电话拦截了，对于用户来说，这是无法接受的。你说得没错，这是一个很大的问题。不过，我们现在先放一放，等三种过滤器都讲完之后，我再来解答。\n2.基于规则的过滤器刚刚讲了一种基于黑名单的垃圾短信过滤方法，但是，如果某个垃圾短信发送者的号码并不在黑名单中，那这种方法就没办法拦截了。所以，基于黑名单的过滤方式，还不够完善，我们再继续看一种基于规则的过滤方式。\n对于垃圾短信来说，我们还可以通过短信的内容，来判断某条短信是否是垃圾短信。我们预先设定一些规则，如果某条短信符合这些规则，我们就可以判定它是垃圾短信。实际上，规则可以有很多，比如下面这几个：\n\n短信中包含特殊单词（或词语），比如一些非法、淫秽、反动词语等；\n\n短信发送号码是群发号码，非我们正常的手机号码，比如+60389585；\n\n短信中包含回拨的联系方式，比如手机号码、微信、QQ、网页链接等，因为群发短信的号码一般都是无法回拨的；\n\n短信格式花哨、内容很长，比如包含各种表情、图片、网页链接等；\n\n符合已知垃圾短信的模板。垃圾短信一般都是重复群发，对于已经判定为垃圾短信的短信，我们可以抽象成模板，将获取到的短信与模板匹配，一旦匹配，我们就可以判定为垃圾短信。\n\n\n当然，如果短信只是满足其中一条规则，如果就判定为垃圾短信，那会存在比较大的误判的情况。我们可以综合多条规则进行判断。比如，满足2条以上才会被判定为垃圾短信；或者每条规则对应一个不同的得分，满足哪条规则，我们就累加对应的分数，某条短信的总得分超过某个阈值，才会被判定为垃圾短信。\n不过，我只是给出了一些制定规则的思路，具体落实到执行层面，其实还有很大的距离，还有很多细节需要处理。比如，第一条规则中，我们该如何定义特殊单词；第二条规则中，我们该如何定义什么样的号码是群发号码等等。限于篇幅，我就不一一详细展开来讲了。我这里只讲一下，如何定义特殊单词？\n如果我们只是自己拍脑袋想，哪些单词属于特殊单词，那势必有比较大的主观性，也很容易漏掉某些单词。实际上，我们可以基于概率统计的方法，借助计算机强大的计算能力，找出哪些单词最常出现在垃圾短信中，将这些最常出现的单词，作为特殊单词，用来过滤短信。\n不过这种方法的前提是，我们有大量的样本数据，也就是说，要有大量的短信（比如1000万条短信），并且我们还要求，每条短信都做好了标记，它是垃圾短信还是非垃圾短信。\n我们对这1000万条短信，进行分词处理（借助中文或者英文分词算法），去掉“的、和、是”等没有意义的停用词（Stop words），得到n个不同的单词。针对每个单词，我们统计有多少个垃圾短信出现了这个单词，有多少个非垃圾短信会出现这个单词，进而求出每个单词出现在垃圾短信中的概率，以及出现在非垃圾短信中的概率。如果某个单词出现在垃圾短信中的概率，远大于出现在非垃圾短信中的概率，那我们就把这个单词作为特殊单词，用来过滤垃圾短信。\n文字描述不好理解，我举个例子来解释一下。\n\n3.基于概率统计的过滤器基于规则的过滤器，看起来很直观，也很好理解，但是它也有一定的局限性。一方面，这些规则受人的思维方式局限，规则未免太过简单；另一方面，垃圾短信发送者可能会针对规则，精心设计短信，绕过这些规则的拦截。对此，我们再来看一种更加高级的过滤方式，基于概率统计的过滤方式。\n这种基于概率统计的过滤方式，基础理论是基于朴素贝叶斯算法。为了让你更好地理解下面的内容，我们先通过一个非常简单的例子来看下，什么是朴素贝叶斯算法？\n假设事件A是“小明不去上学”，事件B是“下雨了”。我们现在统计了一下过去10天的下雨情况和小明上学的情况，作为样本数据。\n\n我们来分析一下，这组样本有什么规律。在这10天中，有4天下雨，所以下雨的概率P(B)=4/10。10天中有3天，小明没有去上学，所以小明不去上学的概率P(A)=3/10。在4个下雨天中，小明有2天没去上学，所以下雨天不去上学的概率P(A|B)=2/4。在小明没有去上学的3天中，有2天下雨了，所以小明因为下雨而不上学的概率是P(B|A)=2/3。实际上，这4个概率值之间，有一定的关系，这个关系就是朴素贝叶斯算法，我们用公式表示出来，就是下面这个样子。\n\n朴素贝叶斯算法是不是非常简单？我们用一个公式就可以将它概括。弄懂了朴素贝叶斯算法，我们再回到垃圾短信过滤这个问题上，看看如何利用朴素贝叶斯算法，来做垃圾短信的过滤。\n基于概率统计的过滤器，是基于短信内容来判定是否是垃圾短信。而计算机没办法像人一样理解短信的含义。所以，我们需要把短信抽象成一组计算机可以理解并且方便计算的特征项，用这一组特征项代替短信本身，来做垃圾短信过滤。\n我们可以通过分词算法，把一个短信分割成n个单词。这n个单词就是一组特征项，全权代表这个短信。因此，判定一个短信是否是垃圾短信这样一个问题，就变成了，判定同时包含这几个单词的短信是否是垃圾短信。\n不过，这里我们并不像基于规则的过滤器那样，非黑即白，一个短信要么被判定为垃圾短信、要么被判定为非垃圾短息。我们使用概率，来表征一个短信是垃圾短信的可信程度。如果我们用公式将这个概率表示出来，就是下面这个样子：\n\n尽管我们有大量的短信样本，但是我们没法通过样本数据统计得到这个概率。为什么不可以呢？你可能会说，我只需要统计同时包含$W_{1}$，$W_{2}$，$W_{3}$，…，$W_{n}$这n个单词的短信有多少个（我们假设有x个），然后看这里面属于垃圾短信的有几个（我们假设有y个），那包含$W_{1}$，$W_{2}$，$W_{3}$，…，$W_{n}$这n个单词的短信是垃圾短信的概率就是y/x。\n理想很丰满，但现实往往很骨感。你忽视了非常重要的一点，那就是样本的数量再大，毕竟也是有限的，样本中不会有太多同时包含$W_{1}$，$W_{2}$，$W_{3}$，…，$W_{n}$的短信的，甚至很多时候，样本中根本不存在这样的短信。没有样本，也就无法计算概率。所以这样的推理方式虽然正确，但是实践中并不好用。\n这个时候，朴素贝叶斯公式就可以派上用场了。我们通过朴素贝叶斯公式，将这个概率的求解，分解为其他三个概率的求解。你可以看我画的图。那转化之后的三个概率是否可以通过样本统计得到呢？\n\nP（$W_{1}$，$W_{2}$，$W_{3}$，…，$W_{n}$同时出现在一条短信中 | 短信是垃圾短信）这个概率照样无法通过样本来统计得到。但是我们可以基于下面这条著名的概率规则来计算。\n\n独立事件发生的概率计算公式：P(A*B) = P(A)*P(B)\n\n\n如果事件A和事件B是独立事件，两者的发生没有相关性，事件A发生的概率P(A)等于p1，事件B发生的概率P(B)等于p2，那两个同时发生的概率P(A*B)就等于P(A)*P(B)。\n\n基于这条独立事件发生概率的计算公式，我们可以把P（W1，W2，W3，…，Wn同时出现在一条短信中 | 短信是垃圾短信）分解为下面这个公式：\n\n\n其中，P（$W_{i}$出现在短信中 | 短信是垃圾短信）表示垃圾短信中包含$W_{i}$这个单词的概率有多大。这个概率值通过统计样本很容易就能获得。我们假设垃圾短信有y个，其中包含$W_{i}$的有x个，那这个概率值就等于x/y。\nP（$W_{1}$，$W_{2}$，$W_{3}$，…，$W_{n}$同时出现在一条短信中 | 短信是垃圾短信）这个概率值，我们就计算出来了，我们再来看下剩下两个。\nP（短信是垃圾短信）表示短信是垃圾短信的概率，这个很容易得到。我们把样本中垃圾短信的个数除以总样本短信个数，就是短信是垃圾短信的概率。\n不过，P（$W_{1}$，$W_{2}$，$W_{3}$，…，$W_{n}$同时出现在一条短信中）这个概率还是不好通过样本统计得到，原因我们前面说过了，样本空间有限。不过，我们没必要非得计算这一部分的概率值。为什么这么说呢？\n实际上，我们可以分别计算同时包含$W_{1}$，$W_{2}$，$W_{3}$，…，$W_{n}$这n个单词的短信，是垃圾短信和非垃圾短信的概率。假设它们分别是p1和p2。我们并不需要单纯地基于p1值的大小来判断是否是垃圾短信，而是通过对比p1和p2值的大小，来判断一条短信是否是垃圾短信。更细化一点讲，那就是，如果p1是p2的很多倍（比如10倍），我们才确信这条短信是垃圾短信。\n\n基于这两个概率的倍数来判断是否是垃圾短信的方法，我们就可以不用计算P（$W_{1}$，$W_{2}$，$W_{3}$，…，$W_{n}$同时出现在一条短信中）这一部分的值了，因为计算p1与p2的时候，都会包含这个概率值的计算，所以在求解p1和p2倍数（p1/p2）的时候，我们也就不需要这个值。\n总结引申今天，我们讲了基于黑名单、规则、概率统计三种垃圾短信的过滤方法，实际上，今天讲的这三种方法，还可以应用到很多类似的过滤、拦截的领域，比如垃圾邮件的过滤等等。\n在讲黑名单过滤的时候，我讲到布隆过滤器可能会存在误判情况，可能会导致用户投诉。实际上，我们可以结合三种不同的过滤方式的结果，对同一个短信处理，如果三者都表明这个短信是垃圾短信，我们才把它当作垃圾短信拦截过滤，这样就会更精准。\n当然，在实际的工程中，我们还需要结合具体的场景，以及大量的实验，不断去调整策略，权衡垃圾短信判定的准确率（是否会把不是垃圾的短信错判为垃圾短信）和召回率（是否能把所有的垃圾短信都找到），来实现我们的需求。\n课后思考关于垃圾短信过滤和骚扰电话的拦截，我们可以一块儿头脑风暴一下，看看你还有没有其他方法呢？\n欢迎留言和我分享，也欢迎点击“请朋友读”，把今天的内容分享给你的好友，和他一起讨论、学习。\n","categories":["数据结构与算法","高级篇"]},{"title":"47 | 向量空间：如何实现一个简单的音乐推荐系统？","url":"/2020/08/07/47%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%9A%E5%90%91%E9%87%8F%E7%A9%BA%E9%97%B4/","content":"很多人都喜爱听歌，以前我们用MP3听歌，现在直接通过音乐App在线就能听歌。而且，各种音乐App的功能越来越强大，不仅可以自己选歌听，还可以根据你听歌的口味偏好，给你推荐可能会喜爱的音乐，而且有时候，推荐的音乐还非常适合你的口味，甚至会惊艳到你！如此智能的一个功能，你知道它是怎么实现的吗？\n算法解析实际上，要解决这个问题，并不需要特别高深的理论。解决思路的核心思想非常简单、直白，用两句话就能总结出来。\n\n找到跟你口味偏好相似的用户，把他们爱听的歌曲推荐给你；\n\n找出跟你喜爱的歌曲特征相似的歌曲，把这些歌曲推荐给你。\n\n\n接下来，我就分别讲解一下这两种思路的具体实现方法。\n1.基于相似用户做推荐如何找到跟你口味偏好相似的用户呢？或者说如何定义口味偏好相似呢？实际上，思路也很简单，我们把跟你听类似歌曲的人，看作口味相似的用户。你可以看我下面画的这个图。我用“1”表示“喜爱”，用“0”笼统地表示“不发表意见”。从图中我们可以看出，你跟小明共同喜爱的歌曲最多，有5首。于是，我们就可以说，小明跟你的口味非常相似。\n\n我们只需要遍历所有的用户，对比每个用户跟你共同喜爱的歌曲个数，并且设置一个阈值，如果你和某个用户共同喜爱的歌曲个数超过这个阈值，我们就把这个用户看作跟你口味相似的用户，把这个用户喜爱但你还没听过的歌曲，推荐给你。\n不过，刚刚的这个解决方案中有一个问题，我们如何知道用户喜爱哪首歌曲呢？也就是说，如何定义用户对某首歌曲的喜爱程度呢？\n实际上，我们可以通过用户的行为，来定义这个喜爱程度。我们给每个行为定义一个得分，得分越高表示喜爱程度越高。\n\n还是刚刚那个例子，我们如果把每个人对每首歌曲的喜爱程度表示出来，就是下面这个样子。图中，某个人对某首歌曲是否喜爱，我们不再用“1”或者“0”来表示，而是对应一个具体的分值。\n\n有了这样一个用户对歌曲的喜爱程度的对应表之后，如何来判断两个用户是否口味相似呢？\n显然，我们不能再像之前那样，采用简单的计数来统计两个用户之间的相似度。还记得我们之前讲字符串相似度度量时，提到的编辑距离吗？这里的相似度度量，我们可以使用另外一个距离，那就是欧几里得距离（Euclidean distance）。欧几里得距离是用来计算两个向量之间的距离的。这个概念中有两个关键词，向量和距离，我来给你解释一下。\n一维空间是一条线，我们用1，2，3……这样单个的数，来表示一维空间中的某个位置；二维空间是一个面，我们用（1，3）（4，2）（2，2）……这样的两个数，来表示二维空间中的某个位置；三维空间是一个立体空间，我们用（1，3，5）（3，1，7）（2，4，3）……这样的三个数，来表示三维空间中的某个位置。一维、二维、三维应该都不难理解，那更高维中的某个位置该如何表示呢？\n类比一维、二维、三维的表示方法，K维空间中的某个位置，我们可以写作（$X_{1}$，$X_{2}$，$X_{3}$，…，$X_{K}$）。这种表示方法就是向量（vector）。我们知道，二维、三维空间中，两个位置之间有距离的概念，类比到高纬空间，同样也有距离的概念，这就是我们说的两个向量之间的距离。\n那如何计算两个向量之间的距离呢？我们还是可以类比到二维、三维空间中距离的计算方法。通过类比，我们就可以得到两个向量之间距离的计算公式。这个计算公式就是欧几里得距离的计算公式：\n\n我们把每个用户对所有歌曲的喜爱程度，都用一个向量表示。我们计算出两个向量之间的欧几里得距离，作为两个用户的口味相似程度的度量。从图中的计算可以看出，小明与你的欧几里得距离距离最小，也就是说，你俩在高维空间中靠得最近，所以，我们就断定，小明跟你的口味最相似。\n\n2.基于相似歌曲做推荐刚刚我们讲了基于相似用户的歌曲推荐方法，但是，如果用户是一个新用户，我们还没有收集到足够多的行为数据，这个时候该如何推荐呢？我们现在再来看另外一种推荐方法，基于相似歌曲的推荐方法，也就是说，如果某首歌曲跟你喜爱的歌曲相似，我们就把它推荐给你。\n如何判断两首歌曲是否相似呢？对于人来说，这个事情可能会比较简单，但是对于计算机来说，判断两首歌曲是否相似，那就需要通过量化的数据来表示了。我们应该通过什么数据来量化两个歌曲之间的相似程度呢？\n最容易想到的是，我们对歌曲定义一些特征项，比如是伤感的还是愉快的，是摇滚还是民谣，是柔和的还是高亢的等等。类似基于相似用户的推荐方法，我们给每个歌曲的每个特征项打一个分数，这样每个歌曲就都对应一个特征项向量。我们可以基于这个特征项向量，来计算两个歌曲之间的欧几里得距离。欧几里得距离越小，表示两个歌曲的相似程度越大。\n但是，要实现这个方案，需要有一个前提，那就是我们能够找到足够多，并且能够全面代表歌曲特点的特征项，除此之外，我们还要人工给每首歌标注每个特征项的得分。对于收录了海量歌曲的音乐App来说，这显然是一个非常大的工程。此外，人工标注有很大的主观性，也会影响到推荐的准确性。\n既然基于歌曲特征项计算相似度不可行，那我们就换一种思路。对于两首歌，如果喜欢听的人群都是差不多的，那侧面就可以反映出，这两首歌比较相似。如图所示，每个用户对歌曲有不同的喜爱程度，我们依旧通过上一个解决方案中定义得分的标准，来定义喜爱程度。\n\n你有没有发现，这个图跟基于相似用户推荐中的图几乎一样。只不过这里把歌曲和用户主次颠倒了一下。基于相似用户的推荐方法中，针对每个用户，我们将对各个歌曲的喜爱程度作为向量。基于相似歌曲的推荐思路中，针对每个歌曲，我们将每个用户的打分作为向量。\n有了每个歌曲的向量表示，我们通过计算向量之间的欧几里得距离，来表示歌曲之间的相似度。欧几里得距离越小，表示两个歌曲越相似。然后，我们就在用户已经听过的歌曲中，找出他喜爱程度较高的歌曲。然后，我们找出跟这些歌曲相似度很高的其他歌曲，推荐给他。\n总结引申实际上，这个问题是推荐系统（Recommendation System）里最典型的一类问题。之所以讲这部分内容，主要还是想给你展示，算法的强大之处，利用简单的向量空间的欧几里得距离，就能解决如此复杂的问题。不过，今天，我只给你讲解了基本的理论，实践中遇到的问题还有很多，比如冷启动问题，产品初期积累的数据不多，不足以做推荐等等。这些更加深奥的内容，你可以之后自己在实践中慢慢探索。\n课后思考关于今天讲的推荐算法，你还能想到其他应用场景吗？\n欢迎留言和我分享，也欢迎点击“请朋友读”，把今天的内容分享给你的好友，和他一起讨论、学习。\n","categories":["数据结构与算法","高级篇"]},{"title":"48 | B+树：MySQL数据库索引是如何实现的？","url":"/2020/08/07/48%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%9AB+%E6%A0%91/","content":"作为一个软件开发工程师，你对数据库肯定再熟悉不过了。作为主流的数据存储系统，它在我们的业务开发中，有着举足轻重的地位。在工作中，为了加速数据库中数据的查找速度，我们常用的处理思路是，对表中数据创建索引。那你是否思考过，数据库索引是如何实现的呢？底层使用的是什么数据结构和算法呢？\n算法解析思考的过程比结论更重要。跟着我学习了这么多节课，很多同学已经意识到这一点，比如Jerry银银同学。我感到很开心。所以，今天的讲解，我会尽量还原这个解决方案的思考过程，让你知其然，并且知其所以然。\n1.解决问题的前提是定义清楚问题如何定义清楚问题呢？除了对问题进行详细的调研，还有一个办法，那就是，通过对一些模糊的需求进行假设，来限定要解决的问题的范围。\n如果你对数据库的操作非常了解，针对我们现在这个问题，你就能把索引的需求定义得非常清楚。但是，对于大部分软件工程师来说，我们可能只了解一小部分常用的SQL语句，所以，这里我们假设要解决的问题，只包含这样两个常用的需求：\n\n根据某个值查找数据，比如select * from user where id=1234；\n\n根据区间值来查找某些数据，比如select * from user where id &gt; 1234 and id &lt; 2345。\n\n\n除了这些功能性需求之外，这种问题往往还会涉及一些非功能性需求，比如安全、性能、用户体验等等。限于专栏要讨论的主要是数据结构和算法，对于非功能性需求，我们着重考虑性能方面的需求。性能方面的需求，我们主要考察时间和空间两方面，也就是执行效率和存储空间。\n在执行效率方面，我们希望通过索引，查询数据的效率尽可能地高；在存储空间方面，我们希望索引不要消耗太多的内存空间。\n2.尝试用学过的数据结构解决这个问题问题的需求大致定义清楚了，我们现在回想一下，能否利用已经学习过的数据结构解决这个问题呢？支持快速查询、插入等操作的动态数据结构，我们已经学习过散列表、平衡二叉查找树、跳表。\n我们先来看散列表。散列表的查询性能很好，时间复杂度是O(1)。但是，散列表不能支持按照区间快速查找数据。所以，散列表不能满足我们的需求。\n我们再来看平衡二叉查找树。尽管平衡二叉查找树查询的性能也很高，时间复杂度是O(logn)。而且，对树进行中序遍历，我们还可以得到一个从小到大有序的数据序列，但这仍然不足以支持按照区间快速查找数据。\n我们再来看跳表。跳表是在链表之上加上多层索引构成的。它支持快速地插入、查找、删除数据，对应的时间复杂度是O(logn)。并且，跳表也支持按照区间快速地查找数据。我们只需要定位到区间起点值对应在链表中的结点，然后从这个结点开始，顺序遍历链表，直到区间终点对应的结点为止，这期间遍历得到的数据就是满足区间值的数据。\n\n这样看来，跳表是可以解决这个问题。实际上，数据库索引所用到的数据结构跟跳表非常相似，叫作B+树。不过，它是通过二叉查找树演化过来的，而非跳表。为了给你还原发明B+树的整个思考过程，所以，接下来，我还要从二叉查找树讲起，看它是如何一步一步被改造成B+树的。\n3.改造二叉查找树来解决这个问题为了让二叉查找树支持按照区间来查找数据，我们可以对它进行这样的改造：树中的节点并不存储数据本身，而是只是作为索引。除此之外，我们把每个叶子节点串在一条链表上，链表中的数据是从小到大有序的。经过改造之后的二叉树，就像图中这样，看起来是不是很像跳表呢？\n\n改造之后，如果我们要求某个区间的数据。我们只需要拿区间的起始值，在树中进行查找，当查找到某个叶子节点之后，我们再顺着链表往后遍历，直到链表中的结点数据值大于区间的终止值为止。所有遍历到的数据，就是符合区间值的所有数据。\n\n但是，我们要为几千万、上亿的数据构建索引，如果将索引存储在内存中，尽管内存访问的速度非常快，查询的效率非常高，但是，占用的内存会非常多。\n比如，我们给一亿个数据构建二叉查找树索引，那索引中会包含大约1亿个节点，每个节点假设占用16个字节，那就需要大约1GB的内存空间。给一张表建立索引，我们需要1GB的内存空间。如果我们要给10张表建立索引，那对内存的需求是无法满足的。如何解决这个索引占用太多内存的问题呢？\n我们可以借助时间换空间的思路，把索引存储在硬盘中，而非内存中。我们都知道，硬盘是一个非常慢速的存储设备。通常内存的访问速度是纳秒级别的，而磁盘访问的速度是毫秒级别的。读取同样大小的数据，从磁盘中读取花费的时间，是从内存中读取所花费时间的上万倍，甚至几十万倍。\n这种将索引存储在硬盘中的方案，尽管减少了内存消耗，但是在数据查找的过程中，需要读取磁盘中的索引，因此数据查询效率就相应降低很多。\n二叉查找树，经过改造之后，支持区间查找的功能就实现了。不过，为了节省内存，如果把树存储在硬盘中，那么每个节点的读取（或者访问），都对应一次磁盘IO操作。树的高度就等于每次查询数据时磁盘IO操作的次数。\n我们前面讲到，比起内存读写操作，磁盘IO操作非常耗时，所以我们优化的重点就是尽量减少磁盘IO操作，也就是，尽量降低树的高度。那如何降低树的高度呢？\n我们来看下，如果我们把索引构建成m叉树，高度是不是比二叉树要小呢？如图所示，给16个数据构建二叉树索引，树的高度是4，查找一个数据，就需要4个磁盘IO操作（如果根节点存储在内存中，其他节点存储在磁盘中），如果对16个数据构建五叉树索引，那高度只有2，查找一个数据，对应只需要2次磁盘操作。如果m叉树中的m是100，那对一亿个数据构建索引，树的高度也只是3，最多只要3次磁盘IO就能获取到数据。磁盘IO变少了，查找数据的效率也就提高了。\n)\n如果我们将m叉树实现B+树索引，用代码实现出来，就是下面这个样子（假设我们给int类型的数据库字段添加索引，所以代码中的keywords是int类型的）：\n/** * 这是B+树非叶子节点的定义。 * * 假设keywords=[3, 5, 8, 10] * 4个键值将数据分为5个区间：(-INF,3), [3,5), [5,8), [8,10), [10,INF) * 5个区间分别对应：children[0]...children[4] * * m值是事先计算得到的，计算的依据是让所有信息的大小正好等于页的大小： * PAGE_SIZE = (m-1)*4[keywordss大小]+m*8[children大小] */public class BPlusTreeNode &#123;    public static int m = 5; // 5叉树    public int[] keywords = new int[m-1]; // 键值，用来划分数据区间    public BPlusTreeNode[] children = new BPlusTreeNode[m];//保存子节点指针&#125;/** * 这是B+树中叶子节点的定义。 * * B+树中的叶子节点跟内部节点是不一样的, * 叶子节点存储的是值，而非区间。 * 这个定义里，每个叶子节点存储3个数据行的键值及地址信息。 * * k值是事先计算得到的，计算的依据是让所有信息的大小正好等于页的大小： * PAGE_SIZE = k*4[keyw..大小]+k*8[dataAd..大小]+8[prev大小]+8[next大小] */public class BPlusTreeLeafNode &#123;    public static int k = 3;    public int[] keywords = new int[k]; // 数据的键值    public long[] dataAddress = new long[k]; // 数据地址    public BPlusTreeLeafNode prev; // 这个结点在链表中的前驱结点    public BPlusTreeLeafNode next; // 这个结点在链表中的后继结点&#125;\n我稍微解释一下这段代码。\n对于相同个数的数据构建m叉树索引，m叉树中的m越大，那树的高度就越小，那m叉树中的m是不是越大越好呢？到底多大才最合适呢？\n不管是内存中的数据，还是磁盘中的数据，操作系统都是按页（一页大小通常是4KB，这个值可以通过getconfig PAGE_SIZE命令查看）来读取的，一次会读一页的数据。如果要读取的数据量超过一页的大小，就会触发多次IO操作。所以，我们在选择m大小的时候，要尽量让每个节点的大小等于一个页的大小。读取一个节点，只需要一次磁盘IO操作。\n\n尽管索引可以提高数据库的查询效率，但是，作为一名开发工程师，你应该也知道，索引有利也有弊，它也会让写入数据的效率下降。这是为什么呢？\n数据的写入过程，会涉及索引的更新，这是索引导致写入变慢的主要原因。\n对于一个B+树来说，m值是根据页的大小事先计算好的，也就是说，每个节点最多只能有m个子节点。在往数据库中写入数据的过程中，这样就有可能使索引中某些节点的子节点个数超过m，这个节点的大小超过了一个页的大小，读取这样一个节点，就会导致多次磁盘IO操作。我们该如何解决这个问题呢？\n实际上，处理思路并不复杂。我们只需要将这个节点分裂成两个节点。但是，节点分裂之后，其上层父节点的子节点个数就有可能超过m个。不过这也没关系，我们可以用同样的方法，将父节点也分裂成两个节点。这种级联反应会从下往上，一直影响到根节点。这个分裂过程，你可以结合着下面这个图一块看，会更容易理解（图中的B+树是一个三叉树。我们限定叶子节点中，数据的个数超过2个就分裂节点；非叶子节点中，子节点的个数超过3个就分裂节点）。\n\n正是因为要时刻保证B+树索引是一个m叉树，所以，索引的存在会导致数据库写入的速度降低。实际上，不光写入数据会变慢，删除数据也会变慢。这是为什么呢？\n我们在删除某个数据的时候，也要对应地更新索引节点。这个处理思路有点类似跳表中删除数据的处理思路。频繁的数据删除，就会导致某些节点中，子节点的个数变得非常少，长此以往，如果每个节点的子节点都比较少，势必会影响索引的效率。\n我们可以设置一个阈值。在B+树中，这个阈值等于m/2。如果某个节点的子节点个数小于m/2，我们就将它跟相邻的兄弟节点合并。不过，合并之后节点的子节点个数有可能会超过m。针对这种情况，我们可以借助插入数据时候的处理方法，再分裂节点。\n文字描述不是很直观，我举了一个删除操作的例子，你可以对比着看下（图中的B+树是一个五叉树。我们限定叶子节点中，数据的个数少于2个就合并节点；非叶子节点中，子节点的个数少于3个就合并节点。）。\n\n数据库索引以及B+树的由来，到此就讲完了。你有没有发现，B+树的结构和操作，跟跳表非常类似。理论上讲，对跳表稍加改造，也可以替代B+树，作为数据库的索引实现的。\nB+树发明于1972年，跳表发明于1989年，我们可以大胆猜想下，跳表的作者有可能就是受了B+树的启发，才发明出跳表来的。不过，这个也无从考证了。\n总结引申今天，我们讲解了数据库索引实现，依赖的底层数据结构，B+树。它通过存储在磁盘的多叉树结构，做到了时间、空间的平衡，既保证了执行效率，又节省了内存。\n前面的讲解中，为了一步一步详细地给你介绍B+树的由来，内容看起来比较零散。为了方便你掌握和记忆，我这里再总结一下B+树的特点：\n\n每个节点中子节点的个数不能超过m，也不能小于m/2；\n\n根节点的子节点个数可以不超过m/2，这是一个例外；\n\nm叉树只存储索引，并不真正存储数据，这个有点儿类似跳表；\n\n通过链表将叶子节点串联在一起，这样可以方便按区间查找；\n\n一般情况，根节点会被存储在内存中，其他节点存储在磁盘中。\n\n\n除了B+树，你可能还听说过B树、B-树，我这里简单提一下。实际上，B-树就是B树，英文翻译都是B-Tree，这里的“-”并不是相对B+树中的“+”，而只是一个连接符。这个很容易误解，所以我强调下。\n而B树实际上是低级版的B+树，或者说B+树是B树的改进版。B树跟B+树的不同点主要集中在这几个地方：\n\nB+树中的节点不存储数据，只是索引，而B树中的节点存储数据；\n\nB树中的叶子节点并不需要链表来串联。\n\n\n也就是说，B树只是一个每个节点的子节点个数不能小于m/2的m叉树。\n课后思考\nB+树中，将叶子节点串起来的链表，是单链表还是双向链表？为什么？\n\n我们对平衡二叉查找树进行改造，将叶子节点串在链表中，就支持了按照区间来查找数据。我们在散列表（下）讲到，散列表也经常跟链表一块使用，如果我们把散列表中的结点，也用链表串起来，能否支持按照区间查找数据呢？\n\n\n欢迎留言和我分享，也欢迎点击“请朋友读”，把今天的内容分享给你的好友，和他一起讨论、学习。\n","categories":["数据结构与算法","高级篇"]},{"title":"38 | 分治算法：谈一谈大规模计算框架MapReduce中的分治思想","url":"/2020/08/07/38%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%9A%E5%88%86%E6%B2%BB%E7%AE%97%E6%B3%95/","content":"MapReduce是Google大数据处理的三驾马车之一，另外两个是GFS和Bigtable。它在倒排索引、PageRank计算、网页分析等搜索引擎相关的技术中都有大量的应用。\n尽管开发一个MapReduce看起来很高深，感觉跟我们遥不可及。实际上，万变不离其宗，它的本质就是我们今天要学的这种算法思想，分治算法。\n如何理解分治算法？为什么说MapRedue的本质就是分治算法呢？我们先来看，什么是分治算法？\n分治算法（divide and conquer）的核心思想其实就是四个字，分而治之 ，也就是将原问题划分成n个规模较小，并且结构与原问题相似的子问题，递归地解决这些子问题，然后再合并其结果，就得到原问题的解。\n这个定义看起来有点类似递归的定义。关于分治和递归的区别，我们在排序（下）的时候讲过，分治算法是一种处理问题的思想，递归是一种编程技巧。实际上，分治算法一般都比较适合用递归来实现。分治算法的递归实现中，每一层递归都会涉及这样三个操作：\n\n分解：将原问题分解成一系列子问题；\n\n解决：递归地求解各个子问题，若子问题足够小，则直接求解；\n\n合并：将子问题的结果合并成原问题。\n\n\n分治算法能解决的问题，一般需要满足下面这几个条件：\n\n原问题与分解成的小问题具有相同的模式；\n\n原问题分解成的子问题可以独立求解，子问题之间没有相关性，这一点是分治算法跟动态规划的明显区别，等我们讲到动态规划的时候，会详细对比这两种算法；\n\n具有分解终止条件，也就是说，当问题足够小时，可以直接求解；\n\n可以将子问题合并成原问题，而这个合并操作的复杂度不能太高，否则就起不到减小算法总体复杂度的效果了。\n\n\n分治算法应用举例分析理解分治算法的原理并不难，但是要想灵活应用并不容易。所以，接下来，我会带你用分治算法解决我们在讲排序的时候涉及的一个问题，加深你对分治算法的理解。\n还记得我们在排序算法里讲的数据的有序度、逆序度的概念吗？我当时讲到，我们用有序度来表示一组数据的有序程度，用逆序度表示一组数据的无序程度。\n假设我们有n个数据，我们期望数据从小到大排列，那完全有序的数据的有序度就是n(n-1)/2，逆序度等于0；相反，倒序排列的数据的有序度就是0，逆序度是n(n-1)/2。除了这两种极端情况外，我们通过计算有序对或者逆序对的个数，来表示数据的有序度或逆序度。\n\n我现在的问题是，如何编程求出一组数据的有序对个数或者逆序对个数呢？因为有序对个数和逆序对个数的求解方式是类似的，所以你可以只思考逆序对个数的求解方法。\n最笨的方法是，拿每个数字跟它后面的数字比较，看有几个比它小的。我们把比它小的数字个数记作k，通过这样的方式，把每个数字都考察一遍之后，然后对每个数字对应的k值求和，最后得到的总和就是逆序对个数。不过，这样操作的时间复杂度是O(n^2)。那有没有更加高效的处理方法呢？\n我们用分治算法来试试。我们套用分治的思想来求数组A的逆序对个数。我们可以将数组分成前后两半A1和A2，分别计算A1和A2的逆序对个数K1和K2，然后再计算A1与A2之间的逆序对个数K3。那数组A的逆序对个数就等于K1+K2+K3。\n我们前面讲过，使用分治算法其中一个要求是，子问题合并的代价不能太大，否则就起不了降低时间复杂度的效果了。那回到这个问题，如何快速计算出两个子问题A1与A2之间的逆序对个数呢？\n这里就要借助归并排序算法了。你可以先试着想想，如何借助归并排序算法来解决呢？\n归并排序中有一个非常关键的操作，就是将两个有序的小数组，合并成一个有序的数组。实际上，在这个合并的过程中，我们就可以计算这两个小数组的逆序对个数了。每次合并操作，我们都计算逆序对个数，把这些计算出来的逆序对个数求和，就是这个数组的逆序对个数了。\n\n尽管我画了张图来解释，但是我个人觉得，对于工程师来说，看代码肯定更好理解一些，所以我们把这个过程翻译成了代码，你可以结合着图和文字描述一起看下。\nprivate int num = 0; // 全局变量或者成员变量public int count(int[] a, int n) &#123;    num = 0;    mergeSortCounting(a, 0, n-1);    return num;&#125;private void mergeSortCounting(int[] a, int p, int r) &#123;    if (p &gt;= r) return;    int q = (p+r)/2;    mergeSortCounting(a, p, q);    mergeSortCounting(a, q+1, r);    merge(a, p, q, r);&#125;private void merge(int[] a, int p, int q, int r) &#123;    int i = p, j = q+1, k = 0;    int[] tmp = new int[r-p+1];    while (i&lt;=q &amp;&amp; j&lt;=r) &#123;        if (a[i] &lt;= a[j]) &#123;            tmp[k++] = a[i++];        &#125; else &#123;            num += (q-i+1); // 统计p-q之间，比a[j]大的元素个数            tmp[k++] = a[j++];        &#125;    &#125;    while (i &lt;= q) &#123; // 处理剩下的        tmp[k++] = a[i++];    &#125;    while (j &lt;= r) &#123; // 处理剩下的        tmp[k++] = a[j++];    &#125;    for (i = 0; i &lt;= r-p; ++i) &#123; // 从tmp拷贝回a        a[p+i] = tmp[i];    &#125;&#125;\n有很多同学经常说，某某算法思想如此巧妙，我是怎么也想不到的。实际上，确实是的。有些算法确实非常巧妙，并不是每个人短时间都能想到的。比如这个问题，并不是每个人都能想到可以借助归并排序算法来解决，不夸张地说，如果之前没接触过，绝大部分人都想不到。但是，如果我告诉你可以借助归并排序算法来解决，那你就应该要想到如何改造归并排序，来求解这个问题了，只要你能做到这一点，我觉得就很棒了。\n关于分治算法，我这还有两道比较经典的问题，你可以自己练习一下。\n\n二维平面上有n个点，如何快速计算出两个距离最近的点对？\n\n有两个n*n的矩阵A，B，如何快速求解两个矩阵的乘积C=A*B？\n\n\n分治思想在海量数据处理中的应用分治算法思想的应用是非常广泛的，并不仅限于指导编程和算法设计。它还经常用在海量数据处理的场景中。我们前面讲的数据结构和算法，大部分都是基于内存存储和单机处理。但是，如果要处理的数据量非常大，没法一次性放到内存中，这个时候，这些数据结构和算法就无法工作了。\n比如，给10GB的订单文件按照金额排序这样一个需求，看似是一个简单的排序问题，但是因为数据量大，有10GB，而我们的机器的内存可能只有2、3GB这样子，无法一次性加载到内存，也就无法通过单纯地使用快排、归并等基础算法来解决了。\n要解决这种数据量大到内存装不下的问题，我们就可以利用分治的思想。我们可以将海量的数据集合根据某种方法，划分为几个小的数据集合，每个小的数据集合单独加载到内存来解决，然后再将小数据集合合并成大数据集合。实际上，利用这种分治的处理思路，不仅仅能克服内存的限制，还能利用多线程或者多机处理，加快处理的速度。\n比如刚刚举的那个例子，给10GB的订单排序，我们就可以先扫描一遍订单，根据订单的金额，将10GB的文件划分为几个金额区间。比如订单金额为1到100元的放到一个小文件，101到200之间的放到另一个文件，以此类推。这样每个小文件都可以单独加载到内存排序，最后将这些有序的小文件合并，就是最终有序的10GB订单数据了。\n如果订单数据存储在类似GFS这样的分布式系统上，当10GB的订单被划分成多个小文件的时候，每个文件可以并行加载到多台机器上处理，最后再将结果合并在一起，这样并行处理的速度也加快了很多。不过，这里有一个点要注意，就是数据的存储与计算所在的机器是同一个或者在网络中靠的很近（比如一个局域网内，数据存取速度很快），否则就会因为数据访问的速度，导致整个处理过程不但不会变快，反而有可能变慢。\n你可能还有印象，这个就是我在讲线性排序的时候举的例子。实际上，在前面已经学习的课程中，我还讲了很多利用分治思想来解决的问题。\n解答开篇分治算法到此就讲完了，我们现在来看下开篇的问题，为什么说MapReduce的本质就是分治思想？\n我们刚刚举的订单的例子，数据有10GB大小，可能给你的感受还不强烈。那如果我们要处理的数据是1T、10T、100T这样子的，那一台机器处理的效率肯定是非常低的。而对于谷歌搜索引擎来说，网页爬取、清洗、分析、分词、计算权重、倒排索引等等各个环节中，都会面对如此海量的数据（比如网页）。所以，利用集群并行处理显然是大势所趋。\n一台机器过于低效，那我们就把任务拆分到多台机器上来处理。如果拆分之后的小任务之间互不干扰，独立计算，最后再将结果合并。这不就是分治思想吗？\n实际上，MapReduce框架只是一个任务调度器，底层依赖GFS来存储数据，依赖Borg管理机器。它从GFS中拿数据，交给Borg中的机器执行，并且时刻监控机器执行的进度，一旦出现机器宕机、进度卡壳等，就重新从Borg中调度一台机器执行。\n尽管MapReduce的模型非常简单，但是在Google内部应用非常广泛。它除了可以用来处理这种数据与数据之间存在关系的任务，比如MapReduce的经典例子，统计文件中单词出现的频率。除此之外，它还可以用来处理数据与数据之间没有关系的任务，比如对网页分析、分词等，每个网页可以独立的分析、分词，而这两个网页之间并没有关系。网页几十亿、上百亿，如果单机处理，效率低下，我们就可以利用MapReduce提供的高可靠、高性能、高容错的并行计算框架，并行地处理这几十亿、上百亿的网页。\n内容小结今天我们讲了一种应用非常广泛的算法思想，分治算法。\n分治算法用四个字概括就是“分而治之”，将原问题划分成n个规模较小而结构与原问题相似的子问题，递归地解决这些子问题，然后再合并其结果，就得到原问题的解。这个思想非常简单、好理解。\n今天我们讲了两种分治算法的典型的应用场景，一个是用来指导编码，降低问题求解的时间复杂度，另一个是解决海量数据处理问题。比如MapReduce本质上就是利用了分治思想。\n我们也时常感叹Google的创新能力如此之强，总是在引领技术的发展。实际上，创新并非离我们很远，创新的源泉来自对事物本质的认识。无数优秀架构设计的思想来源都是基础的数据结构和算法，这本身就是算法的一个魅力所在。\n课后思考我们前面讲过的数据结构、算法、解决思路，以及举的例子中，有哪些采用了分治算法的思想呢？除此之外，生活、工作中，还有没有其他用到分治算法的地方呢？你可以自己回忆、总结一下，这对你将零散的知识提炼成体系非常有帮助。\n欢迎留言和我分享，也欢迎点击“请朋友读”，把今天的内容分享给你的好友，和他一起讨论、学习。\n","categories":["数据结构与算法","基础篇"]},{"title":"50 | 索引：如何在海量数据中快速查找某个数据？","url":"/2020/08/07/50%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%9A%E7%B4%A2%E5%BC%95/","content":"在第48节中，我们讲了MySQL数据库索引的实现原理。MySQL底层依赖的是B+树这种数据结构。留言里有同学问我，那类似Redis这样的Key-Value数据库中的索引，又是怎么实现的呢？底层依赖的又是什么数据结构呢？\n今天，我就来讲一下索引这种常用的技术解决思路，底层往往会依赖哪些数据结构。同时，通过索引这个应用场景，我也带你回顾一下，之前我们学过的几种支持动态集合的数据结构。\n为什么需要索引？在实际的软件开发中，业务纷繁复杂，功能千变万化，但是，万变不离其宗。如果抛开这些业务和功能的外壳，其实它们的本质都可以抽象为“对数据的存储和计算”。对应到数据结构和算法中，那“存储”需要的就是数据结构，“计算”需要的就是算法。\n对于存储的需求，功能上无外乎增删改查。这其实并不复杂。但是，一旦存储的数据很多，那性能就成了这些系统要关注的重点，特别是在一些跟存储相关的基础系统（比如MySQL数据库、分布式文件系统等）、中间件（比如消息中间件RocketMQ等）中。\n“如何节省存储空间、如何提高数据增删改查的执行效率”，这样的问题就成了设计的重点。而这些系统的实现，都离不开一个东西，那就是索引。不夸张地说，索引设计得好坏，直接决定了这些系统是否优秀。\n索引这个概念，非常好理解。你可以类比书籍的目录来理解。如果没有目录，我们想要查找某个知识点的时候，就要一页一页翻。通过目录，我们就可以快速定位相关知识点的页数，查找的速度也会有质的提高。\n索引的需求定义索引的概念不难理解，我想你应该已经搞明白。接下来，我们就分析一下，在设计索引的过程中，需要考虑到的一些因素，换句话说就是，我们该如何定义清楚需求呢？\n对于系统设计需求，我们一般可以从功能性需求和非功能性需求两方面来分析，这个我们之前也说过。因此，这个问题也不例外。\n1.功能性需求对于功能性需求需要考虑的点，我把它们大致概括成下面这几点。\n数据是格式化数据还是非格式化数据？要构建索引的原始数据，类型有很多。我把它分为两类，一类是结构化数据，比如，MySQL中的数据；另一类是非结构化数据，比如搜索引擎中网页。对于非结构化数据，我们一般需要做预处理，提取出查询关键词，对关键词构建索引。\n数据是静态数据还是动态数据？如果原始数据是一组静态数据，也就是说，不会有数据的增加、删除、更新操作，所以，我们在构建索引的时候，只需要考虑查询效率就可以了。这样，索引的构建就相对简单些。不过，大部分情况下，我们都是对动态数据构建索引，也就是说，我们不仅要考虑到索引的查询效率，在原始数据更新的同时，我们还需要动态地更新索引。支持动态数据集合的索引，设计起来相对也要更加复杂些。\n索引存储在内存还是硬盘？如果索引存储在内存中，那查询的速度肯定要比存储在磁盘中的高。但是，如果原始数据量很大的情况下，对应的索引可能也会很大。这个时候，因为内存有限，我们可能就不得不将索引存储在磁盘中了。实际上，还有第三种情况，那就是一部分存储在内存，一部分存储在磁盘，这样就可以兼顾内存消耗和查询效率。\n单值查找还是区间查找？所谓单值查找，也就是根据查询关键词等于某个值的数据。这种查询需求最常见。所谓区间查找，就是查找关键词处于某个区间值的所有数据。你可以类比MySQL数据库的查询需求，自己想象一下。实际上，不同的应用场景，查询的需求会多种多样。\n单关键词查找还是多关键词组合查找？比如，搜索引擎中构建的索引，既要支持一个关键词的查找，比如“数据结构”，也要支持组合关键词查找，比如“数据结构 AND 算法”。对于单关键词的查找，索引构建起来相对简单些。对于多关键词查询来说，要分多种情况。像MySQL这种结构化数据的查询需求，我们可以实现针对多个关键词的组合，建立索引；对于像搜索引擎这样的非结构数据的查询需求，我们可以针对单个关键词构建索引，然后通过集合操作，比如求并集、求交集等，计算出多个关键词组合的查询结果。\n实际上，不同的场景，不同的原始数据，对于索引的需求也会千差万别。我这里只列举了一些比较有共性的需求。\n2.非功能性需求讲完了功能性需求，我们再来看，索引设计的非功能性需求。\n不管是存储在内存中还是磁盘中，索引对存储空间的消耗不能过大。如果存储在内存中，索引对占用存储空间的限制就会非常苛刻。毕竟内存空间非常有限，一个中间件启动后就占用几个GB的内存，开发者显然是无法接受的。如果存储在硬盘中，那索引对占用存储空间的限制，稍微会放宽一些。但是，我们也不能掉以轻心。因为，有时候，索引对存储空间的消耗会超过原始数据。\n在考虑索引查询效率的同时，我们还要考虑索引的维护成本。索引的目的是提高查询效率，但是，基于动态数据集合构建的索引，我们还要考虑到，索引的维护成本。因为在原始数据动态增删改的同时，我们也需要动态地更新索引。而索引的更新势必会影响到增删改操作的性能。\n构建索引常用的数据结构有哪些？我刚刚从很宏观的角度，总结了在索引设计的过程中，需要考虑的一些共性因素。现在，我们就来看，对于不同需求的索引结构，底层一般使用哪种数据结构。\n实际上，常用来构建索引的数据结构，就是我们之前讲过的几种支持动态数据集合的数据结构。比如，散列表、红黑树、跳表、B+树。除此之外，位图、布隆过滤器可以作为辅助索引，有序数组可以用来对静态数据构建索引。\n我们知道，散列表增删改查操作的性能非常好，时间复杂度是O(1)。一些键值数据库，比如Redis、Memcache，就是使用散列表来构建索引的。这类索引，一般都构建在内存中。\n红黑树作为一种常用的平衡二叉查找树，数据插入、删除、查找的时间复杂度是O(logn)，也非常适合用来构建内存索引。Ext文件系统中，对磁盘块的索引，用的就是红黑树。\nB+树比起红黑树来说，更加适合构建存储在磁盘中的索引。B+树是一个多叉树，所以，对相同个数的数据构建索引，B+树的高度要低于红黑树。当借助索引查询数据的时候，读取B+树索引，需要的磁盘IO次数会更少。所以，大部分关系型数据库的索引，比如MySQL、Oracle，都是用B+树来实现的。\n跳表也支持快速添加、删除、查找数据。而且，我们通过灵活调整索引结点个数和数据个数之间的比例，可以很好地平衡索引对内存的消耗及其查询效率。Redis中的有序集合，就是用跳表来构建的。\n除了散列表、红黑树、B+树、跳表之外，位图和布隆过滤器这两个数据结构，也可以用于索引中，辅助存储在磁盘中的索引，加速数据查找的效率。我们来看下，具体是怎么做的？\n我们知道，布隆过滤器有一定的判错率。但是，我们可以规避它的短处，发挥它的长处。尽管对于判定存在的数据，有可能并不存在，但是对于判定不存在的数据，那肯定就不存在。而且，布隆过滤器还有一个更大的特点，那就是内存占用非常少。我们可以针对数据，构建一个布隆过滤器，并且存储在内存中。当要查询数据的时候，我们可以先通过布隆过滤器，判定是否存在。如果通过布隆过滤器判定数据不存在，那我们就没有必要读取磁盘中的索引了。对于数据不存在的情况，数据查询就更加快速了。\n实际上，有序数组也可以被作为索引。如果数据是静态的，也就是不会有插入、删除、更新操作，那我们可以把数据的关键词（查询用的）抽取出来，组织成有序数组，然后利用二分查找算法来快速查找数据。\n总结引申今天这节算是一节总结课。我从索引这个非常常用的技术方案，给你展示了散列表、红黑树、跳表、位图、布隆过滤器、有序数组这些数据结构的应用场景。学习完这节课之后，不知道你对这些数据结构以及索引，有没有更加清晰的认识呢？\n从这一节内容中，你应该可以看出，架构设计离不开数据结构和算法。要想成长为一个优秀的业务架构师、基础架构师，数据结构和算法的根基一定要打稳。因为，那些看似很惊艳的架构设计思路，实际上，都是来自最常用的数据结构和算法。\n课后思考你知道基础系统、中间件、开源软件等系统中，有哪些用到了索引吗？这些系统的索引是如何实现的呢？\n欢迎留言和我分享，也欢迎点击“请朋友读”，把今天的内容分享给你的好友，和他一起讨论、学习。\n","categories":["数据结构与算法","高级篇"]},{"title":"52 | 算法实战（一）：剖析Redis常用数据类型对应的数据结构","url":"/2020/08/07/52%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%9A%E7%AE%97%E6%B3%95%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%80%EF%BC%89/","content":"到此为止，专栏前三部分我们全部讲完了。从今天开始，我们就正式进入实战篇的部分。这部分我主要通过一些开源项目、经典系统，真枪实弹地教你，如何将数据结构和算法应用到项目中。所以这部分的内容，更多的是知识点的回顾，相对于基础篇、高级篇的内容，其实这部分会更加容易看懂。\n不过，我希望你不要只是看懂就完了。你要多举一反三地思考，自己接触过的开源项目、基础框架、中间件中，都用过哪些数据结构和算法。你也可以想一想，在自己做的项目中，有哪些可以用学过的数据结构和算法进一步优化。这样的学习效果才会更好。\n好了，今天我就带你一块儿看下，经典数据库Redis中的常用数据类型，底层都是用哪种数据结构实现的？\nRedis数据库介绍Redis是一种键值（Key-Value）数据库。相对于关系型数据库（比如MySQL），Redis也被叫作非关系型数据库。\n像MySQL这样的关系型数据库，表的结构比较复杂，会包含很多字段，可以通过SQL语句，来实现非常复杂的查询需求。而Redis中只包含“键”和“值”两部分，只能通过“键”来查询“值”。正是因为这样简单的存储结构，也让Redis的读写效率非常高。\n除此之外，Redis主要是作为内存数据库来使用，也就是说，数据是存储在内存中的。尽管它经常被用作内存数据库，但是，它也支持将数据存储在硬盘中。这一点，我们后面会介绍。\nRedis中，键的数据类型是字符串，但是为了丰富数据存储的方式，方便开发者使用，值的数据类型有很多，常用的数据类型有这样几种，它们分别是字符串、列表、字典、集合、有序集合。\n“字符串（string）”这种数据类型非常简单，对应到数据结构里，就是字符串。你应该非常熟悉，这里我就不多介绍了。我们着重看下，其他四种比较复杂点的数据类型，看看它们底层都依赖了哪些数据结构。\n列表（list）我们先来看列表。列表这种数据类型支持存储一组数据。这种数据类型对应两种实现方法，一种是压缩列表（ziplist），另一种是双向循环链表。\n当列表中存储的数据量比较小的时候，列表就可以采用压缩列表的方式实现。具体需要同时满足下面两个条件：\n\n列表中保存的单个数据（有可能是字符串类型的）小于64字节；\n\n列表中数据个数少于512个。\n\n\n关于压缩列表，我这里稍微解释一下。它并不是基础数据结构，而是Redis自己设计的一种数据存储结构。它有点儿类似数组，通过一片连续的内存空间，来存储数据。不过，它跟数组不同的一点是，它允许存储的数据大小不同。具体的存储结构也非常简单，你可以看我下面画的这幅图。\n\n现在，我们来看看，压缩列表中的“压缩”两个字该如何理解？\n听到“压缩”两个字，直观的反应就是节省内存。之所以说这种存储结构节省内存，是相较于数组的存储思路而言的。我们知道，数组要求每个元素的大小相同，如果我们要存储不同长度的字符串，那我们就需要用最大长度的字符串大小作为元素的大小（假设是20个字节）。那当我们存储小于20个字节长度的字符串的时候，便会浪费部分存储空间。听起来有点儿拗口，我画个图解释一下。\n\n压缩列表这种存储结构，一方面比较节省内存，另一方面可以支持不同类型数据的存储。而且，因为数据存储在一片连续的内存空间，通过键来获取值为列表类型的数据，读取的效率也非常高。\n当列表中存储的数据量比较大的时候，也就是不能同时满足刚刚讲的两个条件的时候，列表就要通过双向循环链表来实现了。\n在链表里，我们已经讲过双向循环链表这种数据结构了，如果不记得了，你可以先回去复习一下。这里我们着重看一下Redis中双向链表的编码实现方式。\nRedis的这种双向链表的实现方式，非常值得借鉴。它额外定义一个list结构体，来组织链表的首、尾指针，还有长度等信息。这样，在使用的时候就会非常方便。\n// 以下是C语言代码，因为Redis是用C语言实现的。typedef struct listnode &#123;    struct listNode *prev;    struct listNode *next;    void *value;&#125; listNode;typedef struct list &#123;    listNode *head;    listNode *tail;    unsigned long len;    // ....省略其他定义&#125; list;\n字典（hash）字典类型用来存储一组数据对。每个数据对又包含键值两部分。字典类型也有两种实现方式。一种是我们刚刚讲到的压缩列表，另一种是散列表。\n同样，只有当存储的数据量比较小的情况下，Redis才使用压缩列表来实现字典类型。具体需要满足两个条件：\n\n字典中保存的键和值的大小都要小于64字节；\n\n字典中键值对的个数要小于512个。\n\n\n当不能同时满足上面两个条件的时候，Redis就使用散列表来实现字典类型。Redis使用MurmurHash2这种运行速度快、随机性好的哈希算法作为哈希函数。对于哈希冲突问题，Redis使用链表法来解决。除此之外，Redis还支持散列表的动态扩容、缩容。\n当数据动态增加之后，散列表的装载因子会不停地变大。为了避免散列表性能的下降，当装载因子大于1的时候，Redis会触发扩容，将散列表扩大为原来大小的2倍左右（具体值需要计算才能得到，如果感兴趣，你可以去阅读源码）。\n当数据动态减少之后，为了节省内存，当装载因子小于0.1的时候，Redis就会触发缩容，缩小为字典中数据个数的大约2倍大小（这个值也是计算得到的，如果感兴趣，你也可以去阅读源码）。\n我们前面讲过，扩容缩容要做大量的数据搬移和哈希值的重新计算，所以比较耗时。针对这个问题，Redis使用我们在散列表（中）讲的渐进式扩容缩容策略，将数据的搬移分批进行，避免了大量数据一次性搬移导致的服务停顿。\n集合（set）集合这种数据类型用来存储一组不重复的数据。这种数据类型也有两种实现方法，一种是基于有序数组，另一种是基于散列表。\n当要存储的数据，同时满足下面这样两个条件的时候，Redis就采用有序数组，来实现集合这种数据类型。\n\n存储的数据都是整数；\n\n存储的数据元素个数不超过512个。\n\n\n当不能同时满足这两个条件的时候，Redis就使用散列表来存储集合中的数据。\n有序集合（sortedset）有序集合这种数据类型，我们在跳表里已经详细讲过了。它用来存储一组数据，并且每个数据会附带一个得分。通过得分的大小，我们将数据组织成跳表这样的数据结构，以支持快速地按照得分值、得分区间获取数据。\n实际上，跟Redis的其他数据类型一样，有序集合也并不仅仅只有跳表这一种实现方式。当数据量比较小的时候，Redis会用压缩列表来实现有序集合。具体点说就是，使用压缩列表来实现有序集合的前提，有这样两个：\n\n所有数据的大小都要小于64字节；\n\n元素个数要小于128个。\n\n\n数据结构持久化尽管Redis经常会被用作内存数据库，但是，它也支持数据落盘，也就是将内存中的数据存储到硬盘中。这样，当机器断电的时候，存储在Redis中的数据也不会丢失。在机器重新启动之后，Redis只需要再将存储在硬盘中的数据，重新读取到内存，就可以继续工作了。\n刚刚我们讲到，Redis的数据格式由“键”和“值”两部分组成。而“值”又支持很多数据类型，比如字符串、列表、字典、集合、有序集合。像字典、集合等类型，底层用到了散列表，散列表中有指针的概念，而指针指向的是内存中的存储地址。 那Redis是如何将这样一个跟具体内存地址有关的数据结构存储到磁盘中的呢？\n实际上，Redis遇到的这个问题并不特殊，很多场景中都会遇到。我们把它叫作数据结构的持久化问题，或者对象的持久化问题。这里的“持久化”，你可以笼统地理解为“存储到磁盘”。\n如何将数据结构持久化到硬盘？我们主要有两种解决思路。\n第一种是清除原有的存储结构，只将数据存储到磁盘中。当我们需要从磁盘还原数据到内存的时候，再重新将数据组织成原来的数据结构。实际上，Redis采用的就是这种持久化思路。\n不过，这种方式也有一定的弊端。那就是数据从硬盘还原到内存的过程，会耗用比较多的时间。比如，我们现在要将散列表中的数据存储到磁盘。当我们从磁盘中，取出数据重新构建散列表的时候，需要重新计算每个数据的哈希值。如果磁盘中存储的是几GB的数据，那重构数据结构的耗时就不可忽视了。\n第二种方式是保留原来的存储格式，将数据按照原有的格式存储在磁盘中。我们拿散列表这样的数据结构来举例。我们可以将散列表的大小、每个数据被散列到的槽的编号等信息，都保存在磁盘中。有了这些信息，我们从磁盘中将数据还原到内存中的时候，就可以避免重新计算哈希值。\n总结引申今天，我们学习了Redis中常用数据类型底层依赖的数据结构，总结一下大概有这五种：压缩列表（可以看作一种特殊的数组）、有序数组、链表、散列表、跳表。实际上，Redis就是这些常用数据结构的封装。\n你有没有发现，有了数据结构和算法的基础之后，再去阅读Redis的源码，理解起来就容易多了？很多原来觉得很深奥的设计思想，是不是就都会觉得顺理成章了呢？\n还是那句话，夯实基础很重要。同样是看源码，有些人只能看个热闹，了解一些皮毛，无法形成自己的知识结构，不能化为己用，过不几天就忘了。而有些人基础很好，不但能知其然，还能知其所以然，从而真正理解作者设计的动机。这样不但能有助于我们理解所用的开源软件，还能为我们自己创新添砖加瓦。\n课后思考\n你有没有发现，在数据量比较小的情况下，Redis中的很多数据类型，比如字典、有序集合等，都是通过多种数据结构来实现的，为什么会这样设计呢？用一种固定的数据结构来实现，不是更加简单吗？\n\n我们讲到数据结构持久化有两种方法。对于二叉查找树这种数据结构，我们如何将它持久化到磁盘中呢？\n\n\n欢迎留言和我分享，也欢迎点击“请朋友读”，把今天的内容分享给你的好友，和他一起讨论、学习。\n","categories":["数据结构与算法","实战篇"]},{"title":"54 | 算法实战（三）：剖析高性能队列Disruptor背后的数据结构和算法","url":"/2020/08/07/54%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%9A%E7%AE%97%E6%B3%95%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%89%EF%BC%89/","content":"Disruptor你是否听说过呢？它是一种内存消息队列。从功能上讲，它其实有点儿类似Kafka。不过，和Kafka不同的是，Disruptor是线程之间用于消息传递的队列。它在Apache Storm、Camel、Log4j 2等很多知名项目中都有广泛应用。\n之所以如此受青睐，主要还是因为它的性能表现非常优秀。它比Java中另外一个非常常用的内存消息队列ArrayBlockingQueue（ABS）的性能，要高一个数量级，可以算得上是最快的内存消息队列了。它还因此获得过Oracle官方的Duke大奖。\n如此高性能的内存消息队列，在设计和实现上，必然有它独到的地方。今天，我们就来一块儿看下，Disruptor是如何做到如此高性能的？其底层依赖了哪些数据结构和算法？\n基于循环队列的“生产者-消费者模型”什么是内存消息队列？对很多业务工程师或者前端工程师来说，可能会比较陌生。不过，如果我说“生产者-消费者模型”，估计大部分人都知道。在这个模型中，“生产者”生产数据，并且将数据放到一个中心存储容器中。之后，“消费者”从中心存储容器中，取出数据消费。\n这个模型非常简单、好理解，那你有没有思考过，这里面存储数据的中心存储容器，是用什么样的数据结构来实现的呢？\n实际上，实现中心存储容器最常用的一种数据结构，就是我们在第9节讲的队列。队列支持数据的先进先出。正是这个特性，使得数据被消费的顺序性可以得到保证，也就是说，早被生产的数据就会早被消费。\n我们在第9节讲过，队列有两种实现思路。一种是基于链表实现的链式队列，另一种是基于数组实现的顺序队列。不同的需求背景下，我们会选择不同的实现方式。\n如果我们要实现一个无界队列，也就是说，队列的大小事先不确定，理论上可以支持无限大。这种情况下，我们适合选用链表来实现队列。因为链表支持快速地动态扩容。如果我们要实现一个有界队列，也就是说，队列的大小事先确定，当队列中数据满了之后，生产者就需要等待。直到消费者消费了数据，队列有空闲位置的时候，生产者才能将数据放入。\n实际上，相较于无界队列，有界队列的应用场景更加广泛。毕竟，我们的机器内存是有限的。而无界队列占用的内存数量是不可控的。对于实际的软件开发来说，这种不可控的因素，就会有潜在的风险。在某些极端情况下，无界队列就有可能因为内存持续增长，而导致OOM（Out of Memory）错误。\n在第9节中，我们还讲过一种特殊的顺序队列，循环队列。我们讲过，非循环的顺序队列在添加、删除数据的工程中，会涉及数据的搬移操作，导致性能变差。而循环队列正好可以解决这个数据搬移的问题，所以，性能更加好。所以，大部分用到顺序队列的场景中，我们都选择用顺序队列中的循环队列。\n实际上，循环队列这种数据结构，就是我们今天要讲的内存消息队列的雏形。我借助循环队列，实现了一个最简单的“生产者-消费者模型”。对应的代码我贴到这里，你可以看看。\n为了方便你理解，对于生产者和消费者之间操作的同步，我并没有用到线程相关的操作。而是采用了“当队列满了之后，生产者就轮训等待；当队列空了之后，消费者就轮训等待”这样的措施。\npublic class Queue &#123;    private Long[] data;    private int size = 0, head = 0, tail = 0;    public Queue(int size) &#123;        this.data = new Long[size];        this.size = size;    &#125;    public boolean add(Long element) &#123;        if ((tail + 1) % size == head) return false;        data[tail] = element;        tail = (tail + 1) % size;        return true;    &#125;    public Long poll() &#123;        if (head == tail) return null;        long ret = data[head];        head = (head + 1) % size;        return ret;    &#125;&#125;public class Producer &#123;    private Queue queue;    public Producer(Queue queue) &#123;        this.queue = queue;    &#125;    public void produce(Long data) throws InterruptedException &#123;        while (!queue.add(data)) &#123;            Thread.sleep(100);        &#125;    &#125;&#125;public class Consumer &#123;    private Queue queue;    public Consumer(Queue queue) &#123;        this.queue = queue;    &#125;    public void comsume() throws InterruptedException &#123;        while (true) &#123;            Long data = queue.poll();            if (data == null) &#123;                Thread.sleep(100);            &#125; else &#123;                // TODO:...消费数据的业务逻辑...            &#125;        &#125;    &#125;&#125;\n基于加锁的并发“生产者-消费者模型”实际上，刚刚的“生产者-消费者模型”实现代码，是不完善的。为什么这么说呢？\n如果我们只有一个生产者往队列中写数据，一个消费者从队列中读取数据，那上面的代码是没有问题的。但是，如果有多个生产者在并发地往队列中写入数据，或者多个消费者并发地从队列中消费数据，那上面的代码就不能正确工作了。我来给你讲讲为什么。\n在多个生产者或者多个消费者并发操作队列的情况下，刚刚的代码主要会有下面两个问题：\n\n多个生产者写入的数据可能会互相覆盖；\n\n多个消费者可能会读取重复的数据。\n\n\n因为第一个问题和第二个问题产生的原理是类似的。所以，我着重讲解第一个问题是如何产生的以及该如何解决。对于第二个问题，你可以类比我对第一个问题的解决思路自己来想一想。\n两个线程同时往队列中添加数据，也就相当于两个线程同时执行类Queue中的add()函数。我们假设队列的大小size是10，当前的tail指向下标7，head指向下标3，也就是说，队列中还有空闲空间。这个时候，线程1调用add()函数，往队列中添加一个值为12的数据；线程2调用add()函数，往队列中添加一个值为15的数据。在极端情况下，本来是往队列中添加了两个数据（12和15），最终可能只有一个数据添加成功，另一个数据会被覆盖。这是为什么呢？\n\n为了方便你查看队列Queue中的add()函数，我把它从上面的代码中摘录出来，贴在这里。\npublic boolean add(Long element) &#123;    if ((tail + 1) % size == head) return false;    data[tail] = element;    tail = (tail + 1) % size;    return true;&#125;\n从这段代码中，我们可以看到，第3行给data[tail]赋值，然后第4行才给tail的值加一。赋值和tail加一两个操作，并非原子操作。这就会导致这样的情况发生：当线程1和线程2同时执行add()函数的时候，线程1先执行完了第3行语句，将data[7]（tail等于7）的值设置为12。在线程1还未执行到第4行语句之前，也就是还未将tail加一之前，线程2执行了第3行语句，又将data[7]的值设置为15，也就是说，那线程2插入的数据覆盖了线程1插入的数据。原本应该插入两个数据（12和15）的，现在只插入了一个数据（15）。\n)\n那如何解决这种线程并发往队列中添加数据时，导致的数据覆盖、运行不正确问题呢？\n最简单的处理方法就是给这段代码加锁，同一时间只允许一个线程执行add()函数。这就相当于将这段代码的执行，由并行改成了串行，也就不存在我们刚刚说的问题了。\n不过，天下没有免费的午餐，加锁将并行改成串行，必然导致多个生产者同时生产数据的时候，执行效率的下降。当然，我们可以继续优化代码，用CAS（compare and swap，比较并交换）操作等减少加锁的粒度，但是，这不是我们这节的重点。我们直接看Disruptor的处理方法。\n基于无锁的并发“生产者-消费者模型”尽管Disruptor的源码读起来很复杂，但是基本思想其实非常简单。实际上，它是换了一种队列和“生产者-消费者模型”的实现思路。\n之前的实现思路中，队列只支持两个操作，添加数据和读取并移除数据，分别对应代码中的add()函数和poll()函数，而Disruptor采用了另一种实现思路。\n对于生产者来说，它往队列中添加数据之前，先申请可用空闲存储单元，并且是批量地申请连续的n个（n≥1）存储单元。当申请到这组连续的存储单元之后，后续往队列中添加元素，就可以不用加锁了，因为这组存储单元是这个线程独享的。不过，从刚刚的描述中，我们可以看出，申请存储单元的过程是需要加锁的。\n对于消费者来说，处理的过程跟生产者是类似的。它先去申请一批连续可读的存储单元（这个申请的过程也是需要加锁的），当申请到这批存储单元之后，后续的读取操作就可以不用加锁了。\n不过，还有一个需要特别注意的地方，那就是，如果生产者A申请到了一组连续的存储单元，假设是下标为3到6的存储单元，生产者B紧跟着申请到了下标是7到9的存储单元，那在3到6没有完全写入数据之前，7到9的数据是无法读取的。这个也是Disruptor实现思路的一个弊端。\n文字描述不好理解，我画了一个图，给你展示一下这个操作过程。\n\n实际上，Disruptor采用的是RingBuffer和AvailableBuffer这两个结构，来实现我刚刚讲的功能。不过，因为我们主要聚焦在数据结构和算法上，所以我对这两种结构做了简化，但是基本思想是一致的。如果你对Disruptor感兴趣，可以去阅读一下它的源码。\n总结引申今天，我讲了如何实现一个高性能的并发队列。这里的“并发”两个字，实际上就是多线程安全的意思。\n常见的内存队列往往采用循环队列来实现。这种实现方法，对于只有一个生产者和一个消费者的场景，已经足够了。但是，当存在多个生产者或者多个消费者的时候，单纯的循环队列的实现方式，就无法正确工作了。\n这主要是因为，多个生产者在同时往队列中写入数据的时候，在某些情况下，会存在数据覆盖的问题。而多个消费者同时消费数据，在某些情况下，会存在消费重复数据的问题。\n针对这个问题，最简单、暴力的解决方法就是，对写入和读取过程加锁。这种处理方法，相当于将原来可以并行执行的操作，强制串行执行，相应地就会导致操作性能的下降。\n为了在保证逻辑正确的前提下，尽可能地提高队列在并发情况下的性能，Disruptor采用了“两阶段写入”的方法。在写入数据之前，先加锁申请批量的空闲存储单元，之后往队列中写入数据的操作就不需要加锁了，写入的性能因此就提高了。Disruptor对消费过程的改造，跟对生产过程的改造是类似的。它先加锁申请批量的可读取的存储单元，之后从队列中读取数据的操作也就不需要加锁了，读取的性能因此也就提高了。\n你可能会觉得这个优化思路非常简单。实际上，不管架构设计还是产品设计，往往越简单的设计思路，越能更好地解决问题。正所谓“大道至简”，就是这个意思。\n课后思考为了提高存储性能，我们往往通过分库分表的方式设计数据库表。假设我们有8张表用来存储用户信息。这个时候，每张用户表中的ID字段就不能通过自增的方式来产生了。因为这样的话，就会导致不同表之间的用户ID值重复。\n为了解决这个问题，我们需要实现一个ID生成器，可以为所有的用户表生成唯一的ID号。那现在问题是，如何设计一个高性能、支持并发的、能够生成全局唯一ID的ID生成器呢？\n欢迎留言和我分享，也欢迎点击“请朋友读”，把今天的内容分享给你的好友，和他一起讨论、学习。\n","categories":["数据结构与算法","实战篇"]},{"title":"55 | 算法实战（四）：剖析微服务接口鉴权限流背后的数据结构和算法","url":"/2020/08/07/55%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%9A%E7%AE%97%E6%B3%95%E5%AE%9E%E6%88%98%EF%BC%88%E5%9B%9B%EF%BC%89/","content":"微服务是最近几年才兴起的概念。简单点讲，就是把复杂的大应用，解耦拆分成几个小的应用。这样做的好处有很多。比如，这样有利于团队组织架构的拆分，毕竟团队越大协作的难度越大；再比如，每个应用都可以独立运维，独立扩容，独立上线，各个应用之间互不影响。不用像原来那样，一个小功能上线，整个大应用都要重新发布。\n不过，有利就有弊。大应用拆分成微服务之后，服务之间的调用关系变得更复杂，平台的整体复杂熵升高，出错的概率、debug问题的难度都高了好几个数量级。所以，为了解决这些问题，服务治理便成了微服务的一个技术重点。\n所谓服务治理，简单点讲，就是管理微服务，保证平台整体正常、平稳地运行。服务治理涉及的内容比较多，比如鉴权、限流、降级、熔断、监控告警等等。这些服务治理功能的实现，底层依赖大量的数据结构和算法。今天，我就拿其中的鉴权和限流这两个功能，来带你看看，它们的实现过程中都要用到哪些数据结构和算法。\n鉴权背景介绍以防你之前可能对微服务没有太多了解，所以我对鉴权的背景做了简化。\n假设我们有一个微服务叫用户服务（User Service）。它提供很多用户相关的接口，比如获取用户信息、注册、登录等，给公司内部的其他应用使用。但是，并不是公司内部所有应用，都可以访问这个用户服务，也并不是每个有访问权限的应用，都可以访问用户服务的所有接口。\n我举了一个例子给你讲解一下，你可以看我画的这幅图。这里面，只有A、B、C、D四个应用可以访问用户服务，并且，每个应用只能访问用户服务的部分接口。\n\n要实现接口鉴权功能，我们需要事先将应用对接口的访问权限规则设置好。当某个应用访问其中一个接口的时候，我们就可以拿应用的请求URL，在规则中进行匹配。如果匹配成功，就说明允许访问；如果没有可以匹配的规则，那就说明这个应用没有这个接口的访问权限，我们就拒绝服务。\n如何实现快速鉴权？接口的格式有很多，有类似Dubbo这样的RPC接口，也有类似Spring Cloud这样的HTTP接口。不同接口的鉴权实现方式是类似的，我这里主要拿HTTP接口给你讲解。\n鉴权的原理比较简单、好理解。那具体到实现层面，我们该用什么数据结构来存储规则呢？用户请求URL在规则中快速匹配，又该用什么样的算法呢？\n实际上，不同的规则和匹配模式，对应的数据结构和匹配算法也是不一样的。所以，关于这个问题，我继续细化为三个更加详细的需求给你讲解。\n1.如何实现精确匹配规则？我们先来看最简单的一种匹配模式。只有当请求URL跟规则中配置的某个接口精确匹配时，这个请求才会被接受、处理。为了方便你理解，我举了一个例子，你可以看一下。\n\n不同的应用对应不同的规则集合。我们可以采用散列表来存储这种对应关系。我这里着重讲下，每个应用对应的规则集合，该如何存储和匹配。\n针对这种匹配模式，我们可以将每个应用对应的权限规则，存储在一个字符串数组中。当用户请求到来时，我们拿用户的请求URL，在这个字符串数组中逐一匹配，匹配的算法就是我们之前学过的字符串匹配算法（比如KMP、BM、BF等）。\n规则不会经常变动，所以，为了加快匹配速度，我们可以按照字符串的大小给规则排序，把它组织成有序数组这种数据结构。当要查找某个URL能否匹配其中某条规则的时候，我们可以采用二分查找算法，在有序数组中进行匹配。\n而二分查找算法的时间复杂度是O(logn)（n表示规则的个数），这比起时间复杂度是O(n)的顺序遍历快了很多。对于规则中接口长度比较长，并且鉴权功能调用量非常大的情况，这种优化方法带来的性能提升还是非常可观的 。\n2.如何实现前缀匹配规则？我们再来看一种稍微复杂的匹配模式。只要某条规则可以匹配请求URL的前缀，我们就说这条规则能够跟这个请求URL匹配。同样，为了方便你理解这种匹配模式，我还是举一个例子说明一下。\n\n不同的应用对应不同的规则集合。我们采用散列表来存储这种对应关系。我着重讲一下，每个应用的规则集合，最适合用什么样的数据结构来存储。\n在Trie树那节，我们讲到，Trie树非常适合用来做前缀匹配。所以，针对这个需求，我们可以将每个用户的规则集合，组织成Trie树这种数据结构。\n不过，Trie树中的每个节点不是存储单个字符，而是存储接口被“/”分割之后的子目录（比如“/user/name”被分割为“user”“name”两个子目录）。因为规则并不会经常变动，所以，在Trie树中，我们可以把每个节点的子节点们，组织成有序数组这种数据结构。在匹配的过程中，我们可以利用二分查找算法，决定从一个节点应该跳到哪一个子节点。\n\n3.如何实现模糊匹配规则？如果我们的规则更加复杂，规则中包含通配符，比如“*”表示匹配任意多个子目录，“”表示匹配任意一个子目录。只要用户请求URL可以跟某条规则模糊匹配，我们就说这条规则适用于这个请求。为了方便你理解，我举一个例子来解释一下。\n\n不同的应用对应不同的规则集合。我们还是采用散列表来存储这种对应关系。这点我们刚才讲过了，这里不再重复说了。我们着重看下，每个用户对应的规则集合，该用什么数据结构来存储？针对这种包含通配符的模糊匹配，我们又该使用什么算法来实现呢？\n还记得我们在回溯算法那节讲的正则表达式的例子吗？我们可以借助正则表达式那个例子的解决思路，来解决这个问题。我们采用回溯算法，拿请求URL跟每条规则逐一进行模糊匹配。如何用回溯算法进行模糊匹配，这部分我就不重复讲了。你如果忘记了，可以回到相应章节复习一下。\n不过，这个解决思路的时间复杂度是非常高的。我们需要拿每一个规则，跟请求URL匹配一遍。那有没有办法可以继续优化一下呢？\n实际上，我们可以结合实际情况，挖掘出这样一个隐形的条件，那就是，并不是每条规则都包含通配符，包含通配符的只是少数。于是，我们可以把不包含通配符的规则和包含通配符的规则分开处理。\n我们把不包含通配符的规则，组织成有序数组或者Trie树（具体组织成什么结构，视具体的需求而定，是精确匹配，就组织成有序数组，是前缀匹配，就组织成Trie树），而这一部分匹配就会非常高效。剩下的是少数包含通配符的规则，我们只要把它们简单存储在一个数组中就可以了。尽管匹配起来会比较慢，但是毕竟这种规则比较少，所以这种方法也是可以接受的。\n当接收到一个请求URL之后，我们可以先在不包含通配符的有序数组或者Trie树中查找。如果能够匹配，就不需要继续在通配符规则中匹配了；如果不能匹配，就继续在通配符规则中查找匹配。\n限流背景介绍讲完了鉴权的实现思路，我们再来看一下限流。\n所谓限流，顾名思义，就是对接口调用的频率进行限制。比如每秒钟不能超过100次调用，超过之后，我们就拒绝服务。限流的原理听起来非常简单，但它在很多场景中，发挥着重要的作用。比如在秒杀、大促、双11、618等场景中，限流已经成为了保证系统平稳运行的一种标配的技术解决方案。\n按照不同的限流粒度，限流可以分为很多种类型。比如给每个接口限制不同的访问频率，或者给所有接口限制总的访问频率，又或者更细粒度地限制某个应用对某个接口的访问频率等等。\n不同粒度的限流功能的实现思路都差不多，所以，我今天主要针对限制所有接口总的访问频率这样一个限流需求来讲解。其他粒度限流需求的实现思路，你可以自己思考。\n如何实现精准限流？最简单的限流算法叫固定时间窗口限流算法。这种算法是如何工作的呢？首先我们需要选定一个时间起点，之后每当有接口请求到来，我们就将计数器加一。如果在当前时间窗口内，根据限流规则（比如每秒钟最大允许100次访问请求），出现累加访问次数超过限流值的情况时，我们就拒绝后续的访问请求。当进入下一个时间窗口之后，计数器就清零重新计数。\n\n这种基于固定时间窗口的限流算法的缺点是，限流策略过于粗略，无法应对两个时间窗口临界时间内的突发流量。这是怎么回事呢？我举一个例子给你解释一下。\n假设我们的限流规则是，每秒钟不能超过100次接口请求。第一个1s时间窗口内，100次接口请求都集中在最后10ms内。在第二个1s的时间窗口内，100次接口请求都集中在最开始的10ms内。虽然两个时间窗口内流量都符合限流要求（≤100个请求），但在两个时间窗口临界的20ms内，会集中有200次接口请求。固定时间窗口限流算法并不能对这种情况做限制，所以，集中在这20ms内的200次请求就有可能压垮系统。\n\n为了解决这个问题，我们可以对固定时间窗口限流算法稍加改造。我们可以限制任意时间窗口（比如1s）内，接口请求数都不能超过某个阈值（ 比如100次）。因此，相对于固定时间窗口限流算法，这个算法叫滑动时间窗口限流算法。\n流量经过滑动时间窗口限流算法整形之后，可以保证任意一个1s的时间窗口内，都不会超过最大允许的限流值，从流量曲线上来看会更加平滑。那具体到实现层面，我们该如何来做呢？\n我们假设限流的规则是，在任意1s内，接口的请求次数都不能大于K次。我们就维护一个大小为K+1的循环队列，用来记录1s内到来的请求。注意，这里循环队列的大小等于限流次数加一，因为循环队列存储数据时会浪费一个存储单元。\n当有新的请求到来时，我们将与这个新请求的时间间隔超过1s的请求，从队列中删除。然后，我们再来看循环队列中是否有空闲位置。如果有，则把新请求存储在队列尾部（tail指针所指的位置）；如果没有，则说明这1秒内的请求次数已经超过了限流值K，所以这个请求被拒绝服务。\n为了方便你理解，我举一个例子，给你解释一下。在这个例子中，我们假设限流的规则是，任意1s内，接口的请求次数都不能大于6次。\n\n即便滑动时间窗口限流算法可以保证任意时间窗口内，接口请求次数都不会超过最大限流值，但是仍然不能防止，在细时间粒度上访问过于集中的问题。\n比如我刚刚举的那个例子，第一个1s的时间窗口内，100次请求都集中在最后10ms中，也就是说，基于时间窗口的限流算法，不管是固定时间窗口还是滑动时间窗口，只能在选定的时间粒度上限流，对选定时间粒度内的更加细粒度的访问频率不做限制。\n实际上，针对这个问题，还有很多更加平滑的限流算法，比如令牌桶算法、漏桶算法等。如果感兴趣，你可以自己去研究一下。\n总结引申今天，我们讲解了跟微服务相关的接口鉴权和限流功能的实现思路。现在，我稍微总结一下。\n关于鉴权，我们讲了三种不同的规则匹配模式。不管是哪种匹配模式，我们都可以用散列表来存储不同应用对应的不同规则集合。对于每个应用的规则集合的存储，三种匹配模式使用不同的数据结构。\n对于第一种精确匹配模式，我们利用有序数组来存储每个应用的规则集合，并且通过二分查找和字符串匹配算法，来匹配请求URL与规则。对于第二种前缀匹配模式，我们利用Trie树来存储每个应用的规则集合。对于第三种模糊匹配模式，我们采用普通的数组来存储包含通配符的规则，通过回溯算法，来进行请求URL与规则的匹配。\n关于限流，我们讲了两种限流算法，第一种是固定时间窗口限流算法，第二种是滑动时间窗口限流算法。对于滑动时间窗口限流算法，我们用了之前学习过的循环队列来实现。比起固定时间窗口限流算法，它对流量的整形效果更好，流量更加平滑。\n从今天的学习中，我们也可以看出，对于基础架构工程师来说，如果不精通数据结构和算法，我们就很难开发出性能卓越的基础架构、中间件。这其实就体现了数据结构和算法的重要性。\n课后思考\n除了用循环队列来实现滑动时间窗口限流算法之外，我们是否还可以用其他数据结构来实现呢？请对比一下这些数据结构跟循环队列在解决这个问题时的优劣之处。\n\n分析一下鉴权那部分内容中，前缀匹配算法的时间复杂度和空间复杂度。\n\n\n最后，有个消息提前通知你一下。本节是专栏的倒数第二节课了，不知道学到现在，你掌握得怎么样呢？为了帮你复习巩固，做到真正掌握这些知识，我针对专栏涉及的数据结构和算法，精心编制了一套练习题。从正月初一到初七，每天发布一篇。你要做好准备哦！\n","categories":["数据结构与算法","实战篇"]},{"title":"45 | 位图：如何实现网页爬虫中的URL去重功能？","url":"/2020/08/07/45%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%9A%E4%BD%8D%E5%9B%BE/","content":"网页爬虫是搜索引擎中的非常重要的系统，负责爬取几十亿、上百亿的网页。爬虫的工作原理是，通过解析已经爬取页面中的网页链接，然后再爬取这些链接对应的网页。而同一个网页链接有可能被包含在多个页面中，这就会导致爬虫在爬取的过程中，重复爬取相同的网页。如果你是一名负责爬虫的工程师，你会如何避免这些重复的爬取呢？\n最容易想到的方法就是，我们记录已经爬取的网页链接（也就是URL），在爬取一个新的网页之前，我们拿它的链接，在已经爬取的网页链接列表中搜索。如果存在，那就说明这个网页已经被爬取过了；如果不存在，那就说明这个网页还没有被爬取过，可以继续去爬取。等爬取到这个网页之后，我们将这个网页的链接添加到已经爬取的网页链接列表了。\n思路非常简单，我想你应该很容易就能想到。不过，我们该如何记录已经爬取的网页链接呢？需要用什么样的数据结构呢？\n算法解析关于这个问题，我们可以先回想下，是否可以用我们之前学过的数据结构来解决呢？\n这个问题要处理的对象是网页链接，也就是URL，需要支持的操作有两个，添加一个URL和查询一个URL。除了这两个功能性的要求之外，在非功能性方面，我们还要求这两个操作的执行效率要尽可能高。除此之外，因为我们处理的是上亿的网页链接，内存消耗会非常大，所以在存储效率上，我们要尽可能地高效。\n我们回想一下，满足这些条件的数据结构有哪些呢？显然，散列表、红黑树、跳表这些动态数据结构，都能支持快速地插入、查找数据，但是在内存消耗方面，是否可以接受呢？\n我们拿散列表来举例。假设我们要爬取10亿个网页（像Google、百度这样的通用搜索引擎，爬取的网页可能会更多），为了判重，我们把这10亿网页链接存储在散列表中。你来估算下，大约需要多少内存？\n假设一个URL的平均长度是64字节，那单纯存储这10亿个URL，需要大约60GB的内存空间。因为散列表必须维持较小的装载因子，才能保证不会出现过多的散列冲突，导致操作的性能下降。而且，用链表法解决冲突的散列表，还会存储链表指针。所以，如果将这10亿个URL构建成散列表，那需要的内存空间会远大于60GB，有可能会超过100GB。\n当然，对于一个大型的搜索引擎来说，即便是100GB的内存要求，其实也不算太高，我们可以采用分治的思想，用多台机器（比如20台内存是8GB的机器）来存储这10亿网页链接。这种分治的处理思路，我们讲过很多次了，这里就不详细说了。\n对于爬虫的URL去重这个问题，刚刚讲到的分治加散列表的思路，已经是可以实实在在工作的了。不过，作为一个有追求的工程师，我们应该考虑，在添加、查询数据的效率以及内存消耗方面，是否还有进一步的优化空间呢？\n你可能会说，散列表中添加、查找数据的时间复杂度已经是O(1)，还能有进一步优化的空间吗？实际上，我们前面也讲过，时间复杂度并不能完全代表代码的执行时间。大O时间复杂度表示法，会忽略掉常数、系数和低阶，并且统计的对象是语句的频度。不同的语句，执行时间也是不同的。时间复杂度只是表示执行时间随数据规模的变化趋势，并不能度量在特定的数据规模下，代码执行时间的多少。\n如果时间复杂度中原来的系数是10，我们现在能够通过优化，将系数降为1，那在时间复杂度没有变化的情况下，执行效率就提高了10倍。对于实际的软件开发来说，10倍效率的提升，显然是一个非常值得的优化。\n如果我们用基于链表的方法解决冲突问题，散列表中存储的是URL，那当查询的时候，通过哈希函数定位到某个链表之后，我们还需要依次比对每个链表中的URL。这个操作是比较耗时的，主要有两点原因。\n一方面，链表中的结点在内存中不是连续存储的，所以不能一下子加载到CPU缓存中，没法很好地利用到CPU高速缓存，所以数据访问性能方面会打折扣。\n另一方面，链表中的每个数据都是URL，而URL不是简单的数字，是平均长度为64字节的字符串。也就是说，我们要让待判重的URL，跟链表中的每个URL，做字符串匹配。显然，这样一个字符串匹配操作，比起单纯的数字比对，要慢很多。所以，基于这两点，执行效率方面肯定是有优化空间的。\n对于内存消耗方面的优化，除了刚刚这种基于散列表的解决方案，貌似没有更好的法子了。实际上，如果要想内存方面有明显的节省，那就得换一种解决方案，也就是我们今天要着重讲的这种存储结构，布隆过滤器（Bloom Filter）。\n在讲布隆过滤器前，我要先讲一下另一种存储结构，位图（BitMap）。因为，布隆过滤器本身就是基于位图的，是对位图的一种改进。\n我们先来看一个跟开篇问题非常类似、但比那个稍微简单的问题。我们有1千万个整数，整数的范围在1到1亿之间。如何快速查找某个整数是否在这1千万个整数中呢？\n当然，这个问题还是可以用散列表来解决。不过，我们可以使用一种比较“特殊”的散列表，那就是位图。我们申请一个大小为1亿、数据类型为布尔类型（true或者false）的数组。我们将这1千万个整数作为数组下标，将对应的数组值设置成true。比如，整数5对应下标为5的数组值设置为true，也就是array[5]=true。\n当我们查询某个整数K是否在这1千万个整数中的时候，我们只需要将对应的数组值array[K]取出来，看是否等于true。如果等于true，那说明1千万整数中包含这个整数K；相反，就表示不包含这个整数K。\n不过，很多语言中提供的布尔类型，大小是1个字节的，并不能节省太多内存空间。实际上，表示true和false两个值，我们只需要用一个二进制位（bit）就可以了。那如何通过编程语言，来表示一个二进制位呢？\n这里就要用到位运算了。我们可以借助编程语言中提供的数据类型，比如int、long、char等类型，通过位运算，用其中的某个位表示某个数字。文字描述起来有点儿不好理解，我把位图的代码实现写了出来，你可以对照着代码看下，应该就能看懂了。\npublic class BitMap &#123; &#x2F;&#x2F; Java中char类型占16bit，也即是2个字节  private char[] bytes;  private int nbits;    public BitMap(int nbits) &#123;    this.nbits &#x3D; nbits;    this.bytes &#x3D; new char[nbits&#x2F;16+1];  &#125;  public void set(int k) &#123;    if (k &gt; nbits) return;    int byteIndex &#x3D; k &#x2F; 16;    int bitIndex &#x3D; k % 16;    bytes[byteIndex] |&#x3D; (1 &lt;&lt; bitIndex);  &#125;  public boolean get(int k) &#123;    if (k &gt; nbits) return false;    int byteIndex &#x3D; k &#x2F; 16;    int bitIndex &#x3D; k % 16;    return (bytes[byteIndex] &amp; (1 &lt;&lt; bitIndex)) !&#x3D; 0;  &#125;&#125;\n从刚刚位图结构的讲解中，你应该可以发现，位图通过数组下标来定位数据，所以，访问效率非常高。而且，每个数字用一个二进制位来表示，在数字范围不大的情况下，所需要的内存空间非常节省。\n比如刚刚那个例子，如果用散列表存储这1千万的数据，数据是32位的整型数，也就是需要4个字节的存储空间，那总共至少需要40MB的存储空间。如果我们通过位图的话，数字范围在1到1亿之间，只需要1亿个二进制位，也就是12MB左右的存储空间就够了。\n关于位图，我们就讲完了，是不是挺简单的？不过，这里我们有个假设，就是数字所在的范围不是很大。如果数字的范围很大，比如刚刚那个问题，数字范围不是1到1亿，而是1到10亿，那位图的大小就是10亿个二进制位，也就是120MB的大小，消耗的内存空间，不降反增。\n这个时候，布隆过滤器就要出场了。布隆过滤器就是为了解决刚刚这个问题，对位图这种数据结构的一种改进。\n还是刚刚那个例子，数据个数是1千万，数据的范围是1到10亿。布隆过滤器的做法是，我们仍然使用一个1亿个二进制大小的位图，然后通过哈希函数，对数字进行处理，让它落在这1到1亿范围内。比如我们把哈希函数设计成f(x)=x%n。其中，x表示数字，n表示位图的大小（1亿），也就是，对数字跟位图的大小进行取模求余。\n不过，你肯定会说，哈希函数会存在冲突的问题啊，一亿零一和1两个数字，经过你刚刚那个取模求余的哈希函数处理之后，最后的结果都是1。这样我就无法区分，位图存储的是1还是一亿零一了。\n为了降低这种冲突概率，当然我们可以设计一个复杂点、随机点的哈希函数。除此之外，还有其他方法吗？我们来看布隆过滤器的处理方法。既然一个哈希函数可能会存在冲突，那用多个哈希函数一块儿定位一个数据，是否能降低冲突的概率呢？我来具体解释一下，布隆过滤器是怎么做的。\n我们使用K个哈希函数，对同一个数字进行求哈希值，那会得到K个不同的哈希值，我们分别记作$X_{1}$，$X_{2}$，$X_{3}$，…，$X_{K}$。我们把这K个数字作为位图中的下标，将对应的BitMap[$X_{1}$]，BitMap[$X_{2}$]，BitMap[$X_{3}$]，…，BitMap[$X_{K}$]都设置成true，也就是说，我们用K个二进制位，来表示一个数字的存在。\n当我们要查询某个数字是否存在的时候，我们用同样的K个哈希函数，对这个数字求哈希值，分别得到$Y_{1}$，$Y_{2}$，$Y_{3}$，…，$Y_{K}$。我们看这K个哈希值，对应位图中的数值是否都为true，如果都是true，则说明，这个数字存在，如果有其中任意一个不为true，那就说明这个数字不存在。\n\n对于两个不同的数字来说，经过一个哈希函数处理之后，可能会产生相同的哈希值。但是经过K个哈希函数处理之后，K个哈希值都相同的概率就非常低了。尽管采用K个哈希函数之后，两个数字哈希冲突的概率降低了，但是，这种处理方式又带来了新的问题，那就是容易误判。我们看下面这个例子。\n\n布隆过滤器的误判有一个特点，那就是，它只会对存在的情况有误判。如果某个数字经过布隆过滤器判断不存在，那说明这个数字真的不存在，不会发生误判；如果某个数字经过布隆过滤器判断存在，这个时候才会有可能误判，有可能并不存在。不过，只要我们调整哈希函数的个数、位图大小跟要存储数字的个数之间的比例，那就可以将这种误判的概率降到非常低。\n尽管布隆过滤器会存在误判，但是，这并不影响它发挥大作用。很多场景对误判有一定的容忍度。比如我们今天要解决的爬虫判重这个问题，即便一个没有被爬取过的网页，被误判为已经被爬取，对于搜索引擎来说，也并不是什么大事情，是可以容忍的，毕竟网页太多了，搜索引擎也不可能100%都爬取到。\n弄懂了布隆过滤器，我们今天的爬虫网页去重的问题，就很简单了。\n我们用布隆过滤器来记录已经爬取过的网页链接，假设需要判重的网页有10亿，那我们可以用一个10倍大小的位图来存储，也就是100亿个二进制位，换算成字节，那就是大约1.2GB。之前我们用散列表判重，需要至少100GB的空间。相比来讲，布隆过滤器在存储空间的消耗上，降低了非常多。\n那我们再来看下，利用布隆过滤器，在执行效率方面，是否比散列表更加高效呢？\n布隆过滤器用多个哈希函数对同一个网页链接进行处理，CPU只需要将网页链接从内存中读取一次，进行多次哈希计算，理论上讲这组操作是CPU密集型的。而在散列表的处理方式中，需要读取散列值相同（散列冲突）的多个网页链接，分别跟待判重的网页链接，进行字符串匹配。这个操作涉及很多内存数据的读取，所以是内存密集型的。我们知道CPU计算可能是要比内存访问更快速的，所以，理论上讲，布隆过滤器的判重方式，更加快速。\n总结引申今天，关于搜索引擎爬虫网页去重问题的解决，我们从散列表讲到位图，再讲到布隆过滤器。布隆过滤器非常适合这种不需要100%准确的、允许存在小概率误判的大规模判重场景。除了爬虫网页去重这个例子，还有比如统计一个大型网站的每天的UV数，也就是每天有多少用户访问了网站，我们就可以使用布隆过滤器，对重复访问的用户进行去重。\n我们前面讲到，布隆过滤器的误判率，主要跟哈希函数的个数、位图的大小有关。当我们往布隆过滤器中不停地加入数据之后，位图中不是true的位置就越来越少了，误判率就越来越高了。所以，对于无法事先知道要判重的数据个数的情况，我们需要支持自动扩容的功能。\n当布隆过滤器中，数据个数与位图大小的比例超过某个阈值的时候，我们就重新申请一个新的位图。后面来的新数据，会被放置到新的位图中。但是，如果我们要判断某个数据是否在布隆过滤器中已经存在，我们就需要查看多个位图，相应的执行效率就降低了一些。\n位图、布隆过滤器应用如此广泛，很多编程语言都已经实现了。比如Java中的BitSet类就是一个位图，Redis也提供了BitMap位图类，Google的Guava工具包提供了BloomFilter布隆过滤器的实现。如果你感兴趣，你可以自己去研究下这些实现的源码。\n课后思考\n假设我们有1亿个整数，数据范围是从1到10亿，如何快速并且省内存地给这1亿个数据从小到大排序？\n\n还记得我们在哈希函数（下）讲过的利用分治思想，用散列表以及哈希函数，实现海量图库中的判重功能吗？如果我们允许小概率的误判，那是否可以用今天的布隆过滤器来解决呢？你可以参照我们当时的估算方法，重新估算下，用布隆过滤器需要多少台机器？\n\n\n欢迎留言和我分享，也欢迎点击“请朋友读”，把今天的内容分享给你的好友，和他一起讨论、学习。\n","categories":["数据结构与算法","高级篇"]},{"title":"49 | 搜索：如何用A*搜索算法实现游戏中的寻路功能？","url":"/2020/08/07/49%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%9A%E6%90%9C%E7%B4%A2/","content":"魔兽世界、仙剑奇侠传这类MMRPG游戏，不知道你有没有玩过？在这些游戏中，有一个非常重要的功能，那就是人物角色自动寻路。当人物处于游戏地图中的某个位置的时候，我们用鼠标点击另外一个相对较远的位置，人物就会自动地绕过障碍物走过去。玩过这么多游戏，不知你是否思考过，这个功能是怎么实现的呢？\n算法解析实际上，这是一个非常典型的搜索问题。人物的起点就是他当下所在的位置，终点就是鼠标点击的位置。我们需要在地图中，找一条从起点到终点的路径。这条路径要绕过地图中所有障碍物，并且看起来要是一种非常聪明的走法。所谓“聪明”，笼统地解释就是，走的路不能太绕。理论上讲，最短路径显然是最聪明的走法，是这个问题的最优解。\n不过，在第44节最优出行路线规划问题中，我们也讲过，如果图非常大，那Dijkstra最短路径算法的执行耗时会很多。在真实的软件开发中，我们面对的是超级大的地图和海量的寻路请求，算法的执行效率太低，这显然是无法接受的。\n实际上，像出行路线规划、游戏寻路，这些真实软件开发中的问题，一般情况下，我们都不需要非得求最优解（也就是最短路径）。在权衡路线规划质量和执行效率的情况下，我们只需要寻求一个次优解就足够了。那如何快速找出一条接近于最短路线的次优路线呢？\n这个快速的路径规划算法，就是我们今天要学习的A*算法。实际上，A*算法是对Dijkstra算法的优化和改造。如何将Dijkstra算法改造成A*算法呢？为了更好地理解接下来要讲的内容，我建议你先温习下第44节中的Dijkstra算法的实现原理。\nDijkstra算法有点儿类似BFS算法，它每次找到跟起点最近的顶点，往外扩展。这种往外扩展的思路，其实有些盲目。为什么这么说呢？我举一个例子来给你解释一下。下面这个图对应一个真实的地图，每个顶点在地图中的位置，我们用一个二维坐标（x，y）来表示，其中，x表示横坐标，y表示纵坐标。\n\n在Dijkstra算法的实现思路中，我们用一个优先级队列，来记录已经遍历到的顶点以及这个顶点与起点的路径长度。顶点与起点路径长度越小，就越先被从优先级队列中取出来扩展，从图中举的例子可以看出，尽管我们找的是从s到t的路线，但是最先被搜索到的顶点依次是1，2，3。通过肉眼来观察，这个搜索方向跟我们期望的路线方向（s到t是从西向东）是反着的，路线搜索的方向明显“跑偏”了。\n之所以会“跑偏”，那是因为我们是按照顶点与起点的路径长度的大小，来安排出队列顺序的。与起点越近的顶点，就会越早出队列。我们并没有考虑到这个顶点到终点的距离，所以，在地图中，尽管1，2，3三个顶点离起始顶点最近，但离终点却越来越远。\n如果我们综合更多的因素，把这个顶点到终点可能还要走多远，也考虑进去，综合来判断哪个顶点该先出队列，那是不是就可以避免“跑偏”呢？\n当我们遍历到某个顶点的时候，从起点走到这个顶点的路径长度是确定的，我们记作g(i)（i表示顶点编号）。但是，从这个顶点到终点的路径长度，我们是未知的。虽然确切的值无法提前知道，但是我们可以用其他估计值来代替。\n这里我们可以通过这个顶点跟终点之间的直线距离，也就是欧几里得距离，来近似地估计这个顶点跟终点的路径长度（注意：路径长度跟直线距离是两个概念）。我们把这个距离记作h(i)（i表示这个顶点的编号），专业的叫法是启发函数（heuristic function）。因为欧几里得距离的计算公式，会涉及比较耗时的开根号计算，所以，我们一般通过另外一个更加简单的距离计算公式，那就是曼哈顿距离（Manhattan distance）。曼哈顿距离是两点之间横纵坐标的距离之和。计算的过程只涉及加减法、符号位反转，所以比欧几里得距离更加高效。\nint hManhattan(Vertex v1, Vertex v2) &#123; // Vertex表示顶点，后面有定义    return Math.abs(v1.x - v2.x) + Math.abs(v1.y - v2.y);&#125;\n原来只是单纯地通过顶点与起点之间的路径长度g(i)，来判断谁先出队列，现在有了顶点到终点的路径长度估计值，我们通过两者之和f(i)=g(i)+h(i)，来判断哪个顶点该最先出队列。综合两部分，我们就能有效避免刚刚讲的“跑偏”。这里f(i)的专业叫法是估价函数（evaluation function）。\n从刚刚的描述，我们可以发现，A*算法就是对Dijkstra算法的简单改造。实际上，代码实现方面，我们也只需要稍微改动几行代码，就能把Dijkstra算法的代码实现，改成A*算法的代码实现。\n在A*算法的代码实现中，顶点Vertex类的定义，跟Dijkstra算法中的定义，稍微有点儿区别，多了x，y坐标，以及刚刚提到的f(i)值。图Graph类的定义跟Dijkstra算法中的定义一样。为了避免重复，我这里就没有再贴出来了。\nprivate class Vertex &#123;    public int id; // 顶点编号ID    public int dist; // 从起始顶点，到这个顶点的距离，也就是g(i)    public int f; // 新增：f(i)=g(i)+h(i)    public int x, y; // 新增：顶点在地图中的坐标（x, y）    public Vertex(int id, int x, int y) &#123;        this.id = id;        this.x = x;        this.y = y;        this.f = Integer.MAX_VALUE;        this.dist = Integer.MAX_VALUE;    &#125;&#125;// Graph类的成员变量，在构造函数中初始化Vertex[] vertexes = new Vertex[this.v];// 新增一个方法，添加顶点的坐标public void addVetex(int id, int x, int y) &#123;    vertexes[id] = new Vertex(id, x, y)&#125;\nA*算法的代码实现的主要逻辑是下面这段代码。它跟Dijkstra算法的代码实现，主要有3点区别：\n\n优先级队列构建的方式不同。A*算法是根据f值（也就是刚刚讲到的f(i)=g(i)+h(i)）来构建优先级队列，而Dijkstra算法是根据dist值（也就是刚刚讲到的g(i)）来构建优先级队列；\n\nA*算法在更新顶点dist值的时候，会同步更新f值；\n\n循环结束的条件也不一样。Dijkstra算法是在终点出队列的时候才结束，A*算法是一旦遍历到终点就结束。\n\n\npublic void astar(int s, int t) &#123; // 从顶点s到顶点t的路径    int[] predecessor = new int[this.v]; // 用来还原路径    // 按照vertex的f值构建的小顶堆，而不是按照dist    PriorityQueue queue = new PriorityQueue(this.v);    boolean[] inqueue = new boolean[this.v]; // 标记是否进入过队列    vertexes[s].dist = 0;    vertexes[s].f = 0;    queue.add(vertexes[s]);    inqueue[s] = true;    while (!queue.isEmpty()) &#123;        Vertex minVertex = queue.poll(); // 取堆顶元素并删除        for (int i = 0; i &lt; adj[minVertex.id].size(); ++i) &#123;            Edge e = adj[minVertex.id].get(i); // 取出一条minVetex相连的边            Vertex nextVertex = vertexes[e.tid]; // minVertex--&gt;nextVertex            if (minVertex.dist + e.w &lt; nextVertex.dist) &#123; // 更新next的dist,f                nextVertex.dist = minVertex.dist + e.w;                nextVertex.f                     = nextVertex.dist+hManhattan(nextVertex, vertexes[t]);                predecessor[nextVertex.id] = minVertex.id;                if (inqueue[nextVertex.id] == true) &#123;                    queue.update(nextVertex);                &#125; else &#123;                    queue.add(nextVertex);                    inqueue[nextVertex.id] = true;                &#125;            &#125;            if (nextVertex.id == t) &#123; // 只要到达t就可以结束while了                queue.clear(); // 清空queue，才能推出while循环                break;             &#125;        &#125;    &#125;    // 输出路径    System.out.print(s);    print(s, t, predecessor); // print函数请参看Dijkstra算法的实现&#125;\n尽管A*算法可以更加快速地找到从起点到终点的路线，但是它并不能像Dijkstra算法那样，找到最短路线。这是为什么呢？\n要找出起点s到终点t的最短路径，最简单的方法是，通过回溯穷举所有从s到达t的不同路径，然后对比找出最短的那个。不过很显然，回溯算法的执行效率非常低，是指数级的。\n\nDijkstra算法在此基础之上，利用动态规划的思想，对回溯搜索进行了剪枝，只保留起点到某个顶点的最短路径，继续往外扩展搜索。动态规划相较于回溯搜索，只是换了一个实现思路，但它实际上也考察到了所有从起点到终点的路线，所以才能得到最优解。\n\nA*算法之所以不能像Dijkstra算法那样，找到最短路径，主要原因是两者的while循环结束条件不一样。刚刚我们讲过，Dijkstra算法是在终点出队列的时候才结束，A*算法是一旦遍历到终点就结束。对于Dijkstra算法来说，当终点出队列的时候，终点的dist值是优先级队列中所有顶点的最小值，即便再运行下去，终点的dist值也不会再被更新了。对于A*算法来说，一旦遍历到终点，我们就结束while循环，这个时候，终点的dist值未必是最小值。\nA*算法利用贪心算法的思路，每次都找f值最小的顶点出队列，一旦搜索到终点就不在继续考察其他顶点和路线了。所以，它并没有考察所有的路线，也就不可能找出最短路径了。\n搞懂了A*算法，我们再来看下，如何借助A*算法解决今天的游戏寻路问题？\n要利用A*算法解决这个问题，我们只需要把地图，抽象成图就可以了。不过，游戏中的地图跟第44节中讲的我们平常用的地图是不一样的。因为游戏中的地图并不像我们现实生活中那样，存在规划非常清晰的道路，更多的是宽阔的荒野、草坪等。所以，我们没法利用44节中讲到的抽象方法，把岔路口抽象成顶点，把道路抽象成边。\n实际上，我们可以换一种抽象的思路，把整个地图分割成一个一个的小方块。在某一个方块上的人物，只能往上下左右四个方向的方块上移动。我们可以把每个方块看作一个顶点。两个方块相邻，我们就在它们之间，连两条有向边，并且边的权值都是1。所以，这个问题就转化成了，在一个有向有权图中，找某个顶点到另一个顶点的路径问题。将地图抽象成边权值为1的有向图之后，我们就可以套用A*算法，来实现游戏中人物的自动寻路功能了。\n总结引申我们今天讲的A*算法属于一种启发式搜索算法（Heuristically Search Algorithm）。实际上，启发式搜索算法并不仅仅只有A*算法，还有很多其他算法，比如IDA*算法、蚁群算法、遗传算法、模拟退火算法等。如果感兴趣，你可以自行研究下。\n启发式搜索算法利用估价函数，避免“跑偏”，贪心地朝着最有可能到达终点的方向前进。这种算法找出的路线，并不是最短路线。但是，实际的软件开发中的路线规划问题，我们往往并不需要非得找最短路线。所以，鉴于启发式搜索算法能很好地平衡路线质量和执行效率，它在实际的软件开发中的应用更加广泛。实际上，在第44节中，我们讲到的地图App中的出行路线规划问题，也可以利用启发式搜索算法来实现。\n课后思考我们之前讲的“迷宫问题”是否可以借助A*算法来更快速地找到一个走出去的路线呢？如果可以，请具体讲讲该怎么来做；如果不可以，请说说原因。\n欢迎留言和我分享，也欢迎点击“请朋友读”，把今天的内容分享给你的好友，和他一起讨论、学习。\n","categories":["数据结构与算法","高级篇"]},{"title":"56 | 算法实战（五）：如何用学过的数据结构和算法实现一个短网址系统？","url":"/2020/08/07/56%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%9A%E7%AE%97%E6%B3%95%E5%AE%9E%E6%88%98%EF%BC%88%E4%BA%94%EF%BC%89/","content":"短网址服务你用过吗？如果我们在微博里发布一条带网址的信息，微博会把里面的网址转化成一个更短的网址。我们只要访问这个短网址，就相当于访问原始的网址。比如下面这两个网址，尽管长度不同，但是都可以跳转到我的一个GitHub开源项目里。其中，第二个网址就是通过新浪提供的短网址服务生成的。\n原始网址：https:&#x2F;&#x2F;github.com&#x2F;wangzheng0822&#x2F;ratelimiter4j短网址：http:&#x2F;&#x2F;t.cn&#x2F;EtR9QEG\n从功能上讲，短网址服务其实非常简单，就是把一个长的网址转化成一个短的网址。作为一名软件工程师，你是否思考过，这样一个简单的功能，是如何实现的呢？底层都依赖了哪些数据结构和算法呢？\n短网址服务整体介绍刚刚我们讲了，短网址服务的一个核心功能，就是把原始的长网址转化成短网址。除了这个功能之外，短网址服务还有另外一个必不可少的功能。那就是，当用户点击短网址的时候，短网址服务会将浏览器重定向为原始网址。这个过程是如何实现的呢？\n为了方便你理解，我画了一张对比图，你可以看下。\n\n从图中我们可以看出，浏览器会先访问短网址服务，通过短网址获取到原始网址，再通过原始网址访问到页面。不过这部分功能并不是我们今天要讲的重点。我们重点来看，如何将长网址转化成短网址？\n如何通过哈希算法生成短网址？我们前面学过哈希算法。哈希算法可以将一个不管多长的字符串，转化成一个长度固定的哈希值。我们可以利用哈希算法，来生成短网址。\n前面我们已经提过一些哈希算法了，比如MD5、SHA等。但是，实际上，我们并不需要这些复杂的哈希算法。在生成短网址这个问题上，毕竟，我们不需要考虑反向解密的难度，所以我们只需要关心哈希算法的计算速度和冲突概率。\n能够满足这样要求的哈希算法有很多，其中比较著名并且应用广泛的一个哈希算法，那就是MurmurHash算法。尽管这个哈希算法在2008年才被发明出来，但现在它已经广泛应用到Redis、MemCache、Cassandra、HBase、Lucene等众多著名的软件中。\nMurmurHash算法提供了两种长度的哈希值，一种是32bits，一种是128bits。为了让最终生成的短网址尽可能短，我们可以选择32bits的哈希值。对于开头那个GitHub网址，经过MurmurHash计算后，得到的哈希值就是181338494。我们再拼上短网址服务的域名，就变成了最终的短网址http://t.cn/181338494（其中，http://t.cn 是短网址服务的域名）。\n1.如何让短网址更短？不过，你可能已经看出来了，通过MurmurHash算法得到的短网址还是很长啊，而且跟我们开头那个网址的格式好像也不一样。别着急，我们只需要稍微改变一个哈希值的表示方法，就可以轻松把短网址变得更短些。\n我们可以将10进制的哈希值，转化成更高进制的哈希值，这样哈希值就变短了。我们知道，16进制中，我们用A～E，来表示10～15。在网址URL中，常用的合法字符有0～9、a～z、A～Z这样62个字符。为了让哈希值表示起来尽可能短，我们可以将10进制的哈希值转化成62进制。具体的计算过程，我写在这里了。最终用62进制表示的短网址就是http://t.cn/cgSqq。\n\n2.如何解决哈希冲突问题？不过，我们前面讲过，哈希算法无法避免的一个问题，就是哈希冲突。尽管MurmurHash算法，冲突的概率非常低。但是，一旦冲突，就会导致两个原始网址被转化成同一个短网址。当用户访问短网址的时候，我们就无从判断，用户想要访问的是哪一个原始网址了。这个问题该如何解决呢？\n一般情况下，我们会保存短网址跟原始网址之间的对应关系，以便后续用户在访问短网址的时候，可以根据对应关系，查找到原始网址。存储这种对应关系的方式有很多，比如我们自己设计存储系统或者利用现成的数据库。前面我们讲到的数据库有MySQL、Redis。我们就拿MySQL来举例。假设短网址与原始网址之间的对应关系，就存储在MySQL数据库中。\n当有一个新的原始网址需要生成短网址的时候，我们先利用MurmurHash算法，生成短网址。然后，我们拿这个新生成的短网址，在MySQL数据库中查找。\n如果没有找到相同的短网址，这也就表明，这个新生成的短网址没有冲突。于是我们就将这个短网址返回给用户（请求生成短网址的用户），然后将这个短网址与原始网址之间的对应关系，存储到MySQL数据库中。\n如果我们在数据库中，找到了相同的短网址，那也并不一定说明就冲突了。我们从数据库中，将这个短网址对应的原始网址也取出来。如果数据库中的原始网址，跟我们现在正在处理的原始网址是一样的，这就说明已经有人请求过这个原始网址的短网址了。我们就可以拿这个短网址直接用。如果数据库中记录的原始网址，跟我们正在处理的原始网址不一样，那就说明哈希算法发生了冲突。不同的原始网址，经过计算，得到的短网址重复了。这个时候，我们该怎么办呢？\n我们可以给原始网址拼接一串特殊字符，比如“[DUPLICATED]”，然后再重新计算哈希值，两次哈希计算都冲突的概率，显然是非常低的。假设出现非常极端的情况，又发生冲突了，我们可以再换一个拼接字符串，比如“[OHMYGOD]”，再计算哈希值。然后把计算得到的哈希值，跟原始网址拼接了特殊字符串之后的文本，一并存储在MySQL数据库中。\n当用户访问短网址的时候，短网址服务先通过短网址，在数据库中查找到对应的原始网址。如果原始网址有拼接特殊字符（这个很容易通过字符串匹配算法找到），我们就先将特殊字符去掉，然后再将不包含特殊字符的原始网址返回给浏览器。\n3.如何优化哈希算法生成短网址的性能？为了判断生成的短网址是否冲突，我们需要拿生成的短网址，在数据库中查找。如果数据库中存储的数据非常多，那查找起来就会非常慢，势必影响短网址服务的性能。那有没有什么优化的手段呢？\n还记得我们之前讲的MySQL数据库索引吗？我们可以给短网址字段添加B+树索引。这样通过短网址查询原始网址的速度就提高了很多。实际上，在真实的软件开发中，我们还可以通过一个小技巧，来进一步提高速度。\n在短网址生成的过程中，我们会跟数据库打两次交道，也就是会执行两条SQL语句。第一个SQL语句是通过短网址查询短网址与原始网址的对应关系，第二个SQL语句是将新生成的短网址和原始网址之间的对应关系存储到数据库。\n我们知道，一般情况下，数据库和应用服务（只做计算不存储数据的业务逻辑部分）会部署在两个独立的服务器或者虚拟服务器上。那两条SQL语句的执行就需要两次网络通信。这种IO通信耗时以及SQL语句的执行，才是整个短网址服务的性能瓶颈所在。所以，为了提高性能，我们需要尽量减少SQL语句。那又该如何减少SQL语句呢？\n我们可以给数据库中的短网址字段，添加一个唯一索引（不只是索引，还要求表中不能有重复的数据）。当有新的原始网址需要生成短网址的时候，我们并不会先拿生成的短网址，在数据库中查找判重，而是直接将生成的短网址与对应的原始网址，尝试存储到数据库中。如果数据库能够将数据正常写入，那说明并没有违反唯一索引，也就是说，这个新生成的短网址并没有冲突。\n当然，如果数据库反馈违反唯一性索引异常，那我们还得重新执行刚刚讲过的“查询、写入”过程，SQL语句执行的次数不减反增。但是，在大部分情况下，我们把新生成的短网址和对应的原始网址，插入到数据库的时候，并不会出现冲突。所以，大部分情况下，我们只需要执行一条写入的SQL语句就可以了。所以，从整体上看，总的SQL语句执行次数会大大减少。\n实际上，我们还有另外一个优化SQL语句次数的方法，那就是借助布隆过滤器。\n我们把已经生成的短网址，构建成布隆过滤器。我们知道，布隆过滤器是比较节省内存的一种存储结构，长度是10亿的布隆过滤器，也只需要125MB左右的内存空间。\n当有新的短网址生成的时候，我们先拿这个新生成的短网址，在布隆过滤器中查找。如果查找的结果是不存在，那就说明这个新生成的短网址并没有冲突。这个时候，我们只需要再执行写入短网址和对应原始网页的SQL语句就可以了。通过先查询布隆过滤器，总的SQL语句的执行次数减少了。\n到此，利用哈希算法来生成短网址的思路，我就讲完了。实际上，这种解决思路已经完全满足需求了，我们已经可以直接用到真实的软件开发中。不过，我们还有另外一种短网址的生成算法，那就是利用自增的ID生成器来生成短网址。我们接下来就看一下，这种算法是如何工作的？对于哈希算法生成短网址来说，它又有什么优势和劣势？\n如何通过ID生成器生成短网址？我们可以维护一个ID自增生成器。它可以生成1、2、3…这样自增的整数ID。当短网址服务接收到一个原始网址转化成短网址的请求之后，它先从ID生成器中取一个号码，然后将其转化成62进制表示法，拼接到短网址服务的域名（比如http://t.cn/）后面，就形成了最终的短网址。最后，我们还是会把生成的短网址和对应的原始网址存储到数据库中。\n理论非常简单好理解。不过，这里有几个细节问题需要处理。\n1.相同的原始网址可能会对应不同的短网址每次新来一个原始网址，我们就生成一个新的短网址，这种做法就会导致两个相同的原始网址生成了不同的短网址。这个该如何处理呢？实际上，我们有两种处理思路。\n第一种处理思路是不做处理。听起来有点无厘头，我稍微解释下你就明白了。实际上，相同的原始网址对应不同的短网址，这个用户是可以接受的。在大部分短网址的应用场景里，用户只关心短网址能否正确地跳转到原始网址。至于短网址长什么样子，他其实根本就不关心。所以，即便是同一个原始网址，两次生成的短网址不一样，也并不会影响到用户的使用。\n第二种处理思路是借助哈希算法生成短网址的处理思想，当要给一个原始网址生成短网址的时候，我们要先拿原始网址在数据库中查找，看数据库中是否已经存在相同的原始网址了。如果数据库中存在，那我们就取出对应的短网址，直接返回给用户。\n不过，这种处理思路有个问题，我们需要给数据库中的短网址和原始网址这两个字段，都添加索引。短网址上加索引是为了提高用户查询短网址对应的原始网页的速度，原始网址上加索引是为了加快刚刚讲的通过原始网址查询短网址的速度。这种解决思路虽然能满足“相同原始网址对应相同短网址”这样一个需求，但是是有代价的：一方面两个索引会占用更多的存储空间，另一方面索引还会导致插入、删除等操作性能的下降。\n2.如何实现高性能的ID生成器？实现ID生成器的方法有很多，比如利用数据库自增字段。当然我们也可以自己维护一个计数器，不停地加一加一。但是，一个计数器来应对频繁的短网址生成请求，显然是有点吃力的（因为计数器必须保证生成的ID不重复，笼统概念上讲，就是需要加锁）。如何提高ID生成器的性能呢？关于这个问题，实际上，有很多解决思路。我这里给出两种思路。\n第一种思路是借助第54节中讲的方法。我们可以给ID生成器装多个前置发号器。我们批量地给每个前置发号器发送ID号码。当我们接受到短网址生成请求的时候，就选择一个前置发号器来取号码。这样通过多个前置发号器，明显提高了并发发号的能力。\n\n第二种思路跟第一种差不多。不过，我们不再使用一个ID生成器和多个前置发号器这样的架构，而是，直接实现多个ID生成器同时服务。为了保证每个ID生成器生成的ID不重复。我们要求每个ID生成器按照一定的规则，来生成ID号码。比如，第一个ID生成器只能生成尾号为0的，第二个只能生成尾号为1的，以此类推。这样通过多个ID生成器同时工作，也提高了ID生成的效率。\n\n总结引申今天，我们讲了短网址服务的两种实现方法。我现在来稍微总结一下。\n第一种实现思路是通过哈希算法生成短网址。我们采用计算速度快、冲突概率小的MurmurHash算法，并将计算得到的10进制数，转化成62进制表示法，进一步缩短短网址的长度。对于哈希算法的哈希冲突问题，我们通过给原始网址添加特殊前缀字符，重新计算哈希值的方法来解决。\n第二种实现思路是通过ID生成器来生成短网址。我们维护一个ID自增的ID生成器，给每个原始网址分配一个ID号码，并且同样转成62进制表示法，拼接到短网址服务的域名之后，形成最终的短网址。\n课后思考\n如果我们还要额外支持用户自定义短网址功能（http//t.cn/{用户自定部分}），我们又该如何改造刚刚的算法呢?\n\n我们在讲通过ID生成器生成短网址这种实现思路的时候，讲到相同的原始网址可能会对应不同的短网址。针对这个问题，其中一个解决思路就是，不做处理。但是，如果每个请求都生成一个短网址，并且存储在数据库中，那这样会不会撑爆数据库呢？我们又该如何解决呢？\n\n\n今天是农历的大年三十，我们专栏的正文到这里也就全部结束了。从明天开始，我会每天发布一篇练习题，内容针对专栏涉及的数据结构和算法。从初一到初七，帮你复习巩固所学知识，拿下数据结构和算法，打响新年进步的第一枪！明天见！\n","categories":["数据结构与算法","实战篇"]},{"title":"51 | 并行算法：如何利用并行处理提高算法的执行效率？","url":"/2020/08/07/51%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%9A%E5%B9%B6%E8%A1%8C%E7%AE%97%E6%B3%95/","content":"时间复杂度是衡量算法执行效率的一种标准。但是，时间复杂度并不能跟性能划等号。在真实的软件开发中，即便在不降低时间复杂度的情况下，也可以通过一些优化手段，提升代码的执行效率。毕竟，对于实际的软件开发来说，即便是像10%、20%这样微小的性能提升，也是非常可观的。\n算法的目的就是为了提高代码执行的效率。那当算法无法再继续优化的情况下，我们该如何来进一步提高执行效率呢？我们今天就讲一种非常简单但又非常好用的优化方法，那就是并行计算。今天，我就通过几个例子，给你展示一下，如何借助并行计算的处理思想对算法进行改造？\n并行排序假设我们要给大小为8GB的数据进行排序，并且，我们机器的内存可以一次性容纳这么多数据。对于排序来说，最常用的就是时间复杂度为O(nlogn)的三种排序算法，归并排序、快速排序、堆排序。从理论上讲，这个排序问题，已经很难再从算法层面优化了。而利用并行的处理思想，我们可以很轻松地将这个给8GB数据排序问题的执行效率提高很多倍。具体的实现思路有下面两种。\n第一种是对归并排序并行化处理。我们可以将这8GB的数据划分成16个小的数据集合，每个集合包含500MB的数据。我们用16个线程，并行地对这16个500MB的数据集合进行排序。这16个小集合分别排序完成之后，我们再将这16个有序集合合并。\n第二种是对快速排序并行化处理。我们通过扫描一遍数据，找到数据所处的范围区间。我们把这个区间从小到大划分成16个小区间。我们将8GB的数据划分到对应的区间中。针对这16个小区间的数据，我们启动16个线程，并行地进行排序。等到16个线程都执行结束之后，得到的数据就是有序数据了。\n对比这两种处理思路，它们利用的都是分治的思想，对数据进行分片，然后并行处理。它们的区别在于，第一种处理思路是，先随意地对数据分片，排序之后再合并。第二种处理思路是，先对数据按照大小划分区间，然后再排序，排完序就不需要再处理了。这个跟归并和快排的区别如出一辙。\n这里我还要多说几句，如果要排序的数据规模不是8GB，而是1TB，那问题的重点就不是算法的执行效率了，而是数据的读取效率。因为1TB的数据肯定是存在硬盘中，无法一次性读取到内存中，这样在排序的过程中，就会有频繁地磁盘数据的读取和写入。如何减少磁盘的IO操作，减少磁盘数据读取和写入的总量，就变成了优化的重点。不过这个不是我们这节要讨论的重点，你可以自己思考下。\n并行查找我们知道，散列表是一种非常适合快速查找的数据结构。\n如果我们是给动态数据构建索引，在数据不断加入的时候，散列表的装载因子就会越来越大。为了保证散列表性能不下降，我们就需要对散列表进行动态扩容。对如此大的散列表进行动态扩容，一方面比较耗时，另一方面比较消耗内存。比如，我们给一个2GB大小的散列表进行扩容，扩展到原来的1.5倍，也就是3GB大小。这个时候，实际存储在散列表中的数据只有不到2GB，所以内存的利用率只有60%，有1GB的内存是空闲的。\n实际上，我们可以将数据随机分割成k份（比如16份），每份中的数据只有原来的1/k，然后我们针对这k个小数据集合分别构建散列表。这样，散列表的维护成本就变低了。当某个小散列表的装载因子过大的时候，我们可以单独对这个散列表进行扩容，而其他散列表不需要进行扩容。\n还是刚才那个例子，假设现在有2GB的数据，我们放到16个散列表中，每个散列表中的数据大约是150MB。当某个散列表需要扩容的时候，我们只需要额外增加150*0.5=75MB的内存（假设还是扩容到原来的1.5倍）。无论从扩容的执行效率还是内存的利用率上，这种多个小散列表的处理方法，都要比大散列表高效。\n当我们要查找某个数据的时候，我们只需要通过16个线程，并行地在这16个散列表中查找数据。这样的查找性能，比起一个大散列表的做法，也并不会下降，反倒有可能提高。\n当往散列表中添加数据的时候，我们可以选择将这个新数据放入装载因子最小的那个散列表中，这样也有助于减少散列冲突。\n并行字符串匹配我们前面学过，在文本中查找某个关键词这样一个功能，可以通过字符串匹配算法来实现。我们之前学过的字符串匹配算法有KMP、BM、RK、BF等。当在一个不是很长的文本中查找关键词的时候，这些字符串匹配算法中的任何一个，都可以表现得非常高效。但是，如果我们处理的是超级大的文本，那处理的时间可能就会变得很长，那有没有办法加快匹配速度呢？\n我们可以把大的文本，分割成k个小文本。假设k是16，我们就启动16个线程，并行地在这16个小文本中查找关键词，这样整个查找的性能就提高了16倍。16倍效率的提升，从理论的角度来说并不多。但是，对于真实的软件开发来说，这显然是一个非常可观的优化。\n不过，这里还有一个细节要处理，那就是原本包含在大文本中的关键词，被一分为二，分割到两个小文本中，这就会导致尽管大文本中包含这个关键词，但在这16个小文本中查找不到它。实际上，这个问题也不难解决，我们只需要针对这种特殊情况，做一些特殊处理就可以了。\n我们假设关键词的长度是m。我们在每个小文本的结尾和开始各取m个字符串。前一个小文本的末尾m个字符和后一个小文本的开头m个字符，组成一个长度是2m的字符串。我们再拿关键词，在这个长度为2m的字符串中再重新查找一遍，就可以补上刚才的漏洞了。\n并行搜索前面我们学习过好几种搜索算法，它们分别是广度优先搜索、深度优先搜索、Dijkstra最短路径算法、A*启发式搜索算法。对于广度优先搜索算法，我们也可以将其改造成并行算法。\n广度优先搜索是一种逐层搜索的搜索策略。基于当前这一层顶点，我们可以启动多个线程，并行地搜索下一层的顶点。在代码实现方面，原来广度优先搜索的代码实现，是通过一个队列来记录已经遍历到但还没有扩展的顶点。现在，经过改造之后的并行广度优先搜索算法，我们需要利用两个队列来完成扩展顶点的工作。\n假设这两个队列分别是队列A和队列B。多线程并行处理队列A中的顶点，并将扩展得到的顶点存储在队列B中。等队列A中的顶点都扩展完成之后，队列A被清空，我们再并行地扩展队列B中的顶点，并将扩展出来的顶点存储在队列A。这样两个队列循环使用，就可以实现并行广度优先搜索算法。\n总结引申上一节，我们通过实际软件开发中的“索引”这一技术点，回顾了之前学过的一些支持动态数据集合的数据结构。今天，我们又通过“并行算法”这个话题，回顾了之前学过的一些算法。\n今天的内容比较简单，没有太复杂的知识点。我通过一些例子，比如并行排序、查找、搜索、字符串匹配，给你展示了并行处理的实现思路，也就是对数据进行分片，对没有依赖关系的任务，并行地执行。\n并行计算是一个工程上的实现思路，尽管跟算法关系不大，但是，在实际的软件开发中，它确实可以非常巧妙地提高程序的运行效率，是一种非常好用的性能优化手段。\n特别是，当要处理的数据规模达到一定程度之后，我们无法通过继续优化算法，来提高执行效率 的时候，我们就需要在实现的思路上做文章，利用更多的硬件资源，来加快执行的效率。所以，在很多超大规模数据处理中，并行处理的思想，应用非常广泛，比如MapReduce实际上就是一种并行计算框架。\n课后思考假设我们有n个任务，为了提高执行的效率，我们希望能并行执行任务，但是各个任务之间又有一定的依赖关系，如何根据依赖关系找出可以并行执行的任务？\n欢迎留言和我分享，也欢迎点击“请朋友读”，把今天的内容分享给你的好友，和他一起讨论、学习。\n","categories":["数据结构与算法","高级篇"]},{"title":"Java开发规范","url":"/2019/10/24/Java%E5%BC%80%E5%8F%91%E8%A7%84%E8%8C%83/","content":"一、MyBatis不要为了多个查询条件而写1 = 1\n当遇到多个查询条件，使用where  1=1 可以很方便的解决我们的问题，但是这样很可能会造成非常大的性能损失，因为添加了 “where 1=1  ”的过滤条件之后，数据库系统就无法使用索引等查询优化策略，数据库系统将会被迫对每行数据进行扫描（即全表扫描）  以比较此行是否满足过滤条件，当表中的数据量较大时查询速度会非常慢；此外，还会存在SQL 注入的风险。\n\n反例：\n&lt;select id=\"queryBookInfo\" parameterType=\"com.tjt.platform.entity.BookInfo\" resultType=\"java.lang.Integer\"&gt;    select count(*) from t_rule_BookInfo t where 1=1    &lt;if test=\"title !=null and title !='' \"&gt;        AND title = #&#123;title&#125;    &lt;/if&gt;    &lt;if test=\"author !=null and author !='' \"&gt;        AND author = #&#123;author&#125;    &lt;/if&gt;&lt;/select&gt;\n\n正例：\n&lt;select id=\"queryBookInfo\" parameterType=\"com.tjt.platform.entity.BookInfo\" resultType=\"java.lang.Integer\"&gt;    select count(*) from t_rule_BookInfo t    &lt;where&gt;        &lt;if test=\"title !=null and title !='' \"&gt;            title = #&#123;title&#125;        &lt;/if&gt;        &lt;if test=\"author !=null and author !='' \"&gt;            AND author = #&#123;author&#125;        &lt;/if&gt;    &lt;/where&gt;&lt;/select&gt;\n\nUPDATE 操作也一样，可以用标记代替 1=1。\n二、迭代entrySet()获取Map的key和value\n当循环中只需要获取Map 的主键key时，迭代keySet() 是正确的；但是，当需要主键key 和取值value 时，迭代entrySet() 才是更高效的做法，其比先迭代keySet() 后再去通过get 取值性能更佳。\n\n反例：\n//Map 获取value 反例:HashMap&lt;String, String&gt; map = new HashMap&lt;&gt;();for (String key : map.keySet())&#123;    String value = map.get(key);&#125;\n\n正例：\n//Map 获取key &amp; value 正例:HashMap&lt;String, String&gt; map = new HashMap&lt;&gt;();for (Map.Entry&lt;String,String&gt; entry : map.entrySet())&#123;    String key = entry.getKey();    String value = entry.getValue();&#125;\n\n三、使用Collection.isEmpty()检测空\n使用Collection.size()来检测是否为空在逻辑上没有问题，但是使用Collection.isEmpty() 使得代码更易读，并且可以获得更好的性能；除此之外，任何Collection.isEmpty() 实现的时间复杂度都是O(1)  ，不需要多次循环遍历，但是某些通过Collection.size() 方法实现的时间复杂度可能是O(n)。\n\n反例：\nLinkedList&lt;Object&gt; collection = new LinkedList&lt;&gt;();if (collection.size() == 0)&#123;    System.out.println(\"collection is empty.\");&#125;\n\n正例：\nLinkedList&lt;Object&gt; collection = new LinkedList&lt;&gt;();if (collection.isEmpty())&#123;    System.out.println(\"collection is empty.\");&#125;//检测是否为null 可以使用CollectionUtils.isEmpty()if (CollectionUtils.isEmpty(collection))&#123;    System.out.println(\"collection is null.\");&#125;\n\n四、初始化集合时尽量指定其大小\n尽量在初始化时指定集合的大小，能有效减少集合的扩容次数，因为集合每次扩容的时间复杂度很可能时O(n)，耗费时间和性能。\n\n反例：\n//初始化list，往list 中添加元素反例：int[] arr = new int[]&#123;1,2,3,4&#125;;List&lt;Integer&gt; list = new ArrayList&lt;&gt;();for (int i : arr)&#123;    list.add(i);&#125;\n\n正例：\n//初始化list，往list 中添加元素正例：int[] arr = new int[]&#123;1,2,3,4&#125;;//指定集合list 的容量大小List&lt;Integer&gt; list = new ArrayList&lt;&gt;(arr.length);for (int i : arr)&#123;    list.add(i);&#125;\n\n五、使用StringBuilder拼接字符串\n一般的字符串拼接在编译期Java 会对其进行优化，但是在循环中字符串的拼接Java 编译期无法执行优化，所以需要使用StringBuilder进行替换。\n\n反例：\n//在循环中拼接字符串反例String str = \"\";for (int i = 0; i &lt; 10; i++)&#123;    //在循环中字符串拼接Java 不会对其进行优化    str += i;&#125;\n\n正例：\n//在循环中拼接字符串正例String str1 = \"Love\";String str2 = \"Courage\";String strConcat = str1 + str2;  //Java 编译器会对该普通模式的字符串拼接进行优化StringBuilder sb = new StringBuilder();for (int i = 0; i &lt; 10; i++)&#123;    //在循环中，Java 编译器无法进行优化，所以要手动使用StringBuilder    sb.append(i);&#125;\n\n六、若需频繁调用Collection.contains方法则使用Set\n在Java 集合类库中，List的contains 方法普遍时间复杂度为O(n)，若代码中需要频繁调用contains 方法查找数据则先将集合list 转换成HashSet实现，将O(n) 的时间复杂度将为O(1)。\n\n反例：\n//频繁调用Collection.contains() 反例List&lt;Object&gt; list = new ArrayList&lt;&gt;();for (int i = 0; i &lt;= Integer.MAX_VALUE; i++)&#123;    //时间复杂度为O(n)    if (list.contains(i))        System.out.println(\"list contains \"+ i);&#125;\n\n正例:\n//频繁调用Collection.contains() 正例List&lt;Object&gt; list = new ArrayList&lt;&gt;();Set&lt;Object&gt; set = new HashSet&lt;&gt;();for (int i = 0; i &lt;= Integer.MAX_VALUE; i++)&#123;    //时间复杂度为O(1)    if (set.contains(i))&#123;        System.out.println(\"list contains \"+ i);    &#125;&#125;\n\n七、使用静态代码块实现赋值静态成员变量\n对于集合类型的静态成员变量，应该使用静态代码块赋值，而不是使用集合实现来赋值。\n\n反例：\n//赋值静态成员变量反例private static Map&lt;String, Integer&gt; map = new HashMap&lt;String, Integer&gt;()&#123;    &#123;        map.put(\"Leo\",1);        map.put(\"Family-loving\",2);        map.put(\"Cold on the out side passionate on the inside\",3);    &#125;&#125;;private static List&lt;String&gt; list = new ArrayList&lt;&gt;()&#123;    &#123;        list.add(\"Sagittarius\");        list.add(\"Charming\");        list.add(\"Perfectionist\");    &#125;&#125;;\n\n正例：\n//赋值静态成员变量正例private static Map&lt;String, Integer&gt; map = new HashMap&lt;String, Integer&gt;();static &#123;    map.put(\"Leo\",1);    map.put(\"Family-loving\",2);    map.put(\"Cold on the out side passionate on the inside\",3);&#125;private static List&lt;String&gt; list = new ArrayList&lt;&gt;();static &#123;    list.add(\"Sagittarius\");    list.add(\"Charming\");    list.add(\"Perfectionist\");&#125;\n\n八、删除未使用的局部变量、方法参数、私有方法、字段和多余的括号。九、工具类中屏蔽构造函数\n工具类是一堆静态字段和函数的集合，其不应该被实例化；但是，Java 为每个没有明确定义构造函数的类添加了一个隐式公有构造函数，为了避免不必要的实例化，应该显式定义私有构造函数来屏蔽这个隐式公有构造函数。\n\n反例：\npublic class PasswordUtils &#123;    //工具类构造函数反例    private static final Logger LOG = LoggerFactory.getLogger(PasswordUtils.class);    public static final String DEFAULT_CRYPT_ALGO = \"PBEWithMD5AndDES\";    public static String encryptPassword(String aPassword) throws IOException &#123;        return new PasswordUtils(aPassword).encrypt();    &#125;\n\n正例：\npublic class PasswordUtils &#123;    //工具类构造函数正例    private static final Logger LOG = LoggerFactory.getLogger(PasswordUtils.class);    //定义私有构造函数来屏蔽这个隐式公有构造函数    private PasswordUtils()&#123;&#125;    public static final String DEFAULT_CRYPT_ALGO = \"PBEWithMD5AndDES\";    public static String encryptPassword(String aPassword) throws IOException &#123;        return new PasswordUtils(aPassword).encrypt();    &#125;\n\n十、删除多余的异常捕获并跑出\n用catch 语句捕获异常后，若什么也不进行处理，就只是让异常重新抛出，这跟不捕获异常的效果一样，可以删除这块代码或添加别的处理。\n\n反例：\n//多余异常反例private static String fileReader(String fileName)throws IOException&#123;    try (BufferedReader reader = new BufferedReader(new FileReader(fileName))) &#123;        String line;        StringBuilder builder = new StringBuilder();        while ((line = reader.readLine()) != null) &#123;            builder.append(line);        &#125;        return builder.toString();    &#125; catch (Exception e) &#123;        //仅仅是重复抛异常 未作任何处理        throw e;    &#125;&#125;\n\n正例：\n//多余异常正例private static String fileReader(String fileName)throws IOException&#123;    try (BufferedReader reader = new BufferedReader(new FileReader(fileName))) &#123;        String line;        StringBuilder builder = new StringBuilder();        while ((line = reader.readLine()) != null) &#123;            builder.append(line);        &#125;        return builder.toString();        //删除多余的抛异常，或增加其他处理：        /*catch (Exception e) &#123;            return \"fileReader exception\";        &#125;*/    &#125;&#125;\n\n十一、字符串转化使用String.valueOf(value) 代替”” + value\n把其它对象或类型转化为字符串时，使用String.valueOf(value) 比 &quot;&quot;+value 的效率更高。\n\n反例：\n//把其它对象或类型转化为字符串反例：int num = 520;// \"\" + valueString strLove = \"\" + num;\n\n正例：\n//把其它对象或类型转化为字符串正例：int num = 520;// String.valueOf() 效率更高String strLove = String.valueOf(num);\n\n十二、避免使用BigDecimal(double)\nBigDecimal(double)存在精度损失风险，在精确计算或值比较的场景中可能会导致业务逻辑异常。\n\n反例：\n// BigDecimal 反例BigDecimal bigDecimal = new BigDecimal(0.11D);\n\n正例：\n// BigDecimal 正例BigDecimal bigDecimal1 = bigDecimal.valueOf(0.11D);\n\n十三、返回空数组和集合而非 null\n若程序运行返回null，需要调用方强制检测null，否则就会抛出空指针异常；返回空数组或空集合，有效地避免了调用方因为未检测null 而抛出空指针异常的情况，还可以删除调用方检测null 的语句使代码更简洁。\n\n反例：\n//返回null 反例public static Result[] getResults() &#123;    return null;&#125;public static List&lt;Result&gt; getResultList() &#123;    return null;&#125;public static Map&lt;String, Result&gt; getResultMap() &#123;    return null;&#125;\n\n正例：\n//返回空数组和空集正例public static Result[] getResults() &#123;    return new Result[0];&#125;public static List&lt;Result&gt; getResultList() &#123;    return Collections.emptyList();&#125;public static Map&lt;String, Result&gt; getResultMap() &#123;    return Collections.emptyMap();&#125;\n\n十四、优先使用常量或确定值调用equals 方法\n对象的equals 方法容易抛空指针异常，应使用常量或确定有值的对象来调用equals 方法。\n\n反例：\n//调用 equals 方法反例private static boolean fileReader(String fileName)throws IOException&#123;    //可能抛空指针异常    return fileName.equals(\"Charming\");&#125;\n\n正例：\n//调用 equals 方法正例private static boolean fileReader(String fileName)throws IOException&#123;    //使用常量或确定有值的对象来调用 equals 方法    return \"Charming\".equals(fileName);    //或使用：java.util.Objects.equals() 方法    return Objects.equals(\"Charming\",fileName);&#125;\n\n十五、枚举的属性字段必须是私有且不可变\n枚举通常被当做常量使用，如果枚举中存在公共属性字段或设置字段方法，那么这些枚举常量的属性很容易被修改；理想情况下，枚举中的属性字段是私有的，并在私有构造函数中赋值，没有对应的Setter 方法，最好加上final 修饰符。\n\n反例：\npublic enum SwitchStatus &#123;    // 枚举的属性字段反例    DISABLED(0, \"禁用\"),    ENABLED(1, \"启用\");    public int value;    private String description;    private SwitchStatus(int value, String description) &#123;        this.value = value;        this.description = description;    &#125;    public String getDescription() &#123;        return description;    &#125;    public void setDescription(String description) &#123;        this.description = description;    &#125;&#125;\n\n正例：\npublic enum SwitchStatus &#123;    // 枚举的属性字段正例    DISABLED(0, \"禁用\"),    ENABLED(1, \"启用\");    // final 修饰    private final int value;    private final String description;    private SwitchStatus(int value, String description) &#123;        this.value = value;        this.description = description;    &#125;    // 没有Setter 方法    public int getValue() &#123;        return value;    &#125;    public String getDescription() &#123;        return description;    &#125;&#125;\n\n十六、String.split(String regex)部分关键字需要转译\n使用字符串String的split 方法时，传入的分隔字符串是正则表达式，则部分关键字（比如.[]()|等）需要转义。\n\n反例：\n// String.split(String regex) 反例String[] split = \"a.ab.abc\".split(\".\");System.out.println(Arrays.toString(split));   // 结果为[]String[] split1 = \"a|ab|abc\".split(\"|\");System.out.println(Arrays.toString(split1));  // 结果为[\"a\", \"|\", \"a\", \"b\", \"|\", \"a\", \"b\", \"c\"]\n\n正例：\n// String.split(String regex) 正例// . 需要转译String[] split2 = \"a.ab.abc\".split(\"\\\\.\");System.out.println(Arrays.toString(split2));  // 结果为[\"a\", \"ab\", \"abc\"]// | 需要转译String[] split3 = \"a|ab|abc\".split(\"\\\\|\");System.out.println(Arrays.toString(split3));  // 结果为[\"a\", \"ab\", \"abc\"]\n","tags":["Java"]},{"title":"Java网络编程介绍","url":"/2020/07/02/Java%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E4%BB%8B%E7%BB%8D/","content":"I/O的四种模型\n同步阻塞（Synchronous blocking I/O）\n\n同步非阻塞（Synchronous non-blocking I/0）\n\n异步阻塞（Asynchronous blocking I/0）\n\n异步非阻塞（Asynchronous non-blocking I/0）\n\n\nI/O多路复用I/O多路复用是指使用一个线程来检查多个文件描述符(Socket)的就绪状态，比如调用select和poll函数，传入多个文件描述符，如果有一个文件描述符就绪，则返回，否则阻塞直到超时。得到就绪状态后进行真正的操作可以在同一个线程里执行，也可以启动线程执行(比如使用线程池)。\n一般情况下，I/O 复用机制需要事件分发器。 事件分发器的作用，将那些读写事件源分发给各读写事件的处理者。\n涉及到事件分发器的两种模式称为：Reactor和Proactor。 Reactor模式是基于同步I/O的，而Proactor模式是和异步I/O相关的。本文主要介绍的就是 Reactor模式相关的知识。\nReactor模式Reactor模式也叫反应器模式\nIO的发展历史单线程阻塞while(true)&#123;    socket = accept();    handle(socket)&#125;\n\n这种方法的最大问题是无法并发，效率太低，如果当前的请求没有处理完，那么后面的请求只能被阻塞，服务器的吞吐量太低。\n多线程阻塞之后，想到了使用多线程，也就是很经典的connection per thread，每一个连接用一个线程处理，类似：\nimport java.io.IOException;import java.net.ServerSocket;import java.net.Socket;class BasicModel implements Runnable &#123;    public void run() &#123;        try &#123;            ServerSocket ss =                new ServerSocket(SystemConfig.SOCKET_SERVER_PORT);            while (!Thread.interrupted())                new Thread(new Handler(ss.accept())).start();            //创建新线程来handle            // or, single-threaded, or a thread pool        &#125; catch (IOException ex) &#123; /* ... */ &#125;    &#125;    static class Handler implements Runnable &#123;        final Socket socket;        Handler(Socket s) &#123; socket = s; &#125;        public void run() &#123;            try &#123;                byte[] input = new byte[SystemConfig.INPUT_SIZE];                socket.getInputStream().read(input);                byte[] output = process(input);                socket.getOutputStream().write(output);            &#125; catch (IOException ex) &#123; /* ... */ &#125;        &#125;        private byte[] process(byte[] input) &#123;            byte[] output=null;            /* ... */            return output;        &#125;    &#125;&#125;\n\n对于每一个请求都分发给一个线程，每个线程中都独自处理上面的流程。\ntomcat服务器的早期版本确实是这样实现的。\n优点定程度上极大地提高了服务器的吞吐量，因为之前的请求在read阻塞以后，不会影响到后续的请求，因为他们在不同的线程中。这也是为什么通常会讲“一个线程只能对应一个socket”的原因。\n缺点缺点在于资源要求太高，系统中创建线程是需要比较高的系统资源的，如果连接数太高，系统无法承受，而且，线程的反复创建-销毁也需要代价。\n改进采用基于事件驱动的设计，当有事件触发时，才会调用处理器进行数据处理。使用Reactor模式，对线程的数量进行控制，一个线程处理大量的事件。\n单线程NIO模型import java.io.IOException;import java.net.InetSocketAddress;import java.nio.ByteBuffer;import java.nio.channels.SelectionKey;import java.nio.channels.Selector;import java.nio.channels.ServerSocketChannel;import java.nio.channels.SocketChannel;import java.util.Iterator;class Server &#123;    public static final int SOCKET_SERVER_PORT = 8088;    public static void testServer() throws IOException &#123;        // 1、获取Selector选择器        Selector selector = Selector.open();        // 2、获取通道        ServerSocketChannel serverSocketChannel = ServerSocketChannel.open();        // 3.设置为非阻塞        serverSocketChannel.configureBlocking(false);        // 4、绑定连接        serverSocketChannel.bind(new InetSocketAddress(SOCKET_SERVER_PORT));        // 5、将通道注册到选择器上,并注册的操作为：“接收”操作        serverSocketChannel.register(selector, SelectionKey.OP_ACCEPT);        // 6、采用轮询的方式，查询获取“准备就绪”的注册过的操作        while (selector.select() &gt; 0) &#123;            // 7、获取当前选择器中所有注册的选择键（“已经准备就绪的操作”）            Iterator&lt;SelectionKey&gt; selectedKeys = selector.selectedKeys().iterator();            while (selectedKeys.hasNext()) &#123;                // 8、获取“准备就绪”的时间                SelectionKey selectedKey = selectedKeys.next();                // 9、判断key是具体的什么事件                if (selectedKey.isAcceptable()) &#123;                    // 10、若接受的事件是“接收就绪” 操作,就获取客户端连接                    SocketChannel socketChannel = serverSocketChannel.accept();                    // 11、切换为非阻塞模式                    socketChannel.configureBlocking(false);                    // 12、将该通道注册到selector选择器上                    socketChannel.register(selector, SelectionKey.OP_READ);                &#125; else if (selectedKey.isReadable()) &#123;                    // 13、获取该选择器上的“读就绪”状态的通道                    SocketChannel socketChannel = (SocketChannel) selectedKey.channel();                    // 14、读取数据                    ByteBuffer byteBuffer = ByteBuffer.allocate(1024);                    int length = 0;                    while ((length = socketChannel.read(byteBuffer)) != -1) &#123;                        byteBuffer.flip();                        System.out.println(new String(byteBuffer.array(), 0, length));                        byteBuffer.clear();                    &#125;                    socketChannel.close();                &#125;                // 15、移除选择键                selectedKeys.remove();            &#125;        &#125;        // 7、关闭连接        serverSocketChannel.close();    &#125;    public static void main(String[] args) throws IOException &#123;        testServer();    &#125;&#125;\n\n实际上的Reactor模式，是基于Java NIO的，在他的基础上，抽象出来两个组件——Reactor和Handler两个组件：\n（1）Reactor：负责响应IO事件，当检测到一个新的事件，将其发送给相应的Handler去处理；新的事件包含连接建立就绪、读就绪、写就绪等。\n（2）Handler:将自身（handler）与事件绑定，负责事件的处理，完成channel的读入，完成处理业务逻辑后，负责将结果写出channel。\n单线程Reactor参考import java.io.IOException;import java.net.InetSocketAddress;import java.nio.channels.SelectionKey;import java.nio.channels.Selector;import java.nio.channels.ServerSocketChannel;import java.nio.channels.SocketChannel;import java.util.Iterator;import java.util.Set;class Reactor implements Runnable &#123;    final Selector selector;    final ServerSocketChannel serverSocket;    Reactor(int port) throws IOException &#123; //Reactor初始化        selector = Selector.open();        serverSocket = ServerSocketChannel.open();        serverSocket.socket().bind(new InetSocketAddress(port));        //非阻塞        serverSocket.configureBlocking(false);        //分步处理,第一步,接收accept事件        SelectionKey sk = serverSocket.register(selector, SelectionKey.OP_ACCEPT);        //attach callback object, Acceptor        sk.attach(new Acceptor());    &#125;    public void run() &#123;        try &#123;            while (!Thread.interrupted()) &#123;                selector.select();                Set selected = selector.selectedKeys();                Iterator it = selected.iterator();                while (it.hasNext()) &#123;                    //Reactor负责dispatch收到的事件                    dispatch((SelectionKey) (it.next()));                &#125;                selected.clear();            &#125;        &#125; catch (IOException ex) &#123; /* ... */ &#125;    &#125;    void dispatch(SelectionKey k) &#123;        Runnable r = (Runnable) (k.attachment());        //调用之前注册的callback对象        if (r != null) &#123;            r.run();        &#125;    &#125;    // inner class    class Acceptor implements Runnable &#123;        public void run() &#123;            try &#123;                SocketChannel channel = serverSocket.accept();                if (channel != null)                    new Handler(selector, channel);            &#125; catch (IOException ex) &#123; /* ... */ &#125;        &#125;    &#125;&#125;\n\nimport java.io.IOException;import java.nio.ByteBuffer;import java.nio.channels.SelectionKey;import java.nio.channels.Selector;import java.nio.channels.SocketChannel;class Handler implements Runnable &#123;    public static final int INPUT_SIZE = 1024;    public static final int SEND_SIZE = 1024;    final SocketChannel channel;    final SelectionKey sk;    ByteBuffer input = ByteBuffer.allocate(INPUT_SIZE);    ByteBuffer output = ByteBuffer.allocate(SEND_SIZE);    static final int READING = 0, SENDING = 1;    int state = READING;    Handler(Selector selector, SocketChannel c) throws IOException &#123;        channel = c;        channel.configureBlocking(false);        // Optionally try first read now        sk = channel.register(selector, 0);        // 将Handler作为callback对象        sk.attach(this);        // 第二步,注册Read就绪事件        sk.interestOps(SelectionKey.OP_READ);        selector.wakeup();    &#125;    boolean inputIsComplete() &#123;        /* ... */        return false;    &#125;    boolean outputIsComplete() &#123;        /* ... */        return false;    &#125;    void process() &#123;        /* ... */        return;    &#125;    public void run() &#123;        try &#123;            if (state == READING) &#123;                read();            &#125; else if (state == SENDING) &#123;                send();            &#125;        &#125; catch (IOException ex) &#123; /* ... */ &#125;    &#125;    void read() throws IOException &#123;        channel.read(input);        if (inputIsComplete()) &#123;            process();            state = SENDING;            // Normally also do first write now            //第三步,接收write就绪事件            sk.interestOps(SelectionKey.OP_WRITE);        &#125;    &#125;    void send() throws IOException &#123;        channel.write(output);        //write完就结束了, 关闭select key        if (outputIsComplete()) &#123;            sk.cancel();        &#125;    &#125;&#125;\n\n1、 当其中某个 handler 阻塞时， 会导致其他所有的 client 的 handler 都得不到执行， 并且更严重的是， handler 的阻塞也会导致整个服务不能接收新的 client 请求(因为 acceptor 也被阻塞了)。 因为有这么多的缺陷， 因此单线程Reactor 模型用的比较少。这种单线程模型不能充分利用多核资源，所以实际使用的不多。\n2、因此，单线程模型仅仅适用于handler 中业务处理组件能快速完成的场景。\n多线程Reactor参考import nio.single.Handler;import java.io.IOException;import java.net.InetSocketAddress;import java.nio.channels.SelectionKey;import java.nio.channels.Selector;import java.nio.channels.ServerSocketChannel;import java.nio.channels.SocketChannel;import java.util.Iterator;import java.util.Set;class MthreadReactor implements Runnable &#123;    //subReactors集合, 一个selector代表一个subReactor    Selector[] selectors = new Selector[2];    int next = 0;    final ServerSocketChannel serverSocket;    MthreadReactor(int port) throws IOException &#123; //Reactor初始化        selectors[0] = Selector.open();        selectors[1] = Selector.open();        serverSocket = ServerSocketChannel.open();        serverSocket.socket().bind(new InetSocketAddress(port));        //非阻塞        serverSocket.configureBlocking(false);        //分步处理,第一步,接收accept事件        SelectionKey sk = serverSocket.register(selectors[0], SelectionKey.OP_ACCEPT);        //attach callback object, Acceptor        sk.attach(new Acceptor());    &#125;    public void run() &#123;        try &#123;            while (!Thread.interrupted()) &#123;                for (int i = 0; i &lt; 2; i++) &#123;                    selectors[i].select();                    Set selected = selectors[i].selectedKeys();                    Iterator it = selected.iterator();                    while (it.hasNext()) &#123;                        //Reactor负责dispatch收到的事件                        dispatch((SelectionKey) (it.next()));                    &#125;                    selected.clear();                &#125;            &#125;        &#125; catch (IOException ex) &#123; /* ... */ &#125;    &#125;    void dispatch(SelectionKey k) &#123;        Runnable r = (Runnable) (k.attachment());        //调用之前注册的callback对象        if (r != null) &#123;            r.run();        &#125;    &#125;    class Acceptor &#123; // ...        public synchronized void run() throws IOException &#123;            SocketChannel connection = serverSocket.accept(); //主selector负责accept            if (connection != null) &#123;                new Handler(selectors[next], connection); //选个subReactor去负责接收到的connection            &#125;            if (++next == selectors.length) next = 0;        &#125;    &#125;&#125;\n\nhttps://www.cnblogs.com/crazymakercircle/p/9833847.html\nhttps://www.cnblogs.com/winner-0715/p/8733787.html\nhttps://blog.csdn.net/weixin_37778801/article/details/86699341\n","tags":["Java","NIO"]},{"title":"Linux服务操作指南","url":"/2020/05/26/Linux%E6%9C%8D%E5%8A%A1%E6%93%8D%E4%BD%9C%E6%8C%87%E5%8D%97/","content":"\nhttps://www.freedesktop.org/software/systemd/man/systemd.service.html\nhttps://blog.csdn.net/yuesichiu/article/details/51485147\n\n服务路径systemd有系统和用户区分：\n\n系统：/etc/systemd/system/\n用户：/usr/lib/systemd/system/\n\n开机时，systemd只执行/etc/systemd/system/路径下的配置文件，建议在用户路径下创建，然后执行systemctl enabel mysqld，这个命令会在系统路径下创建一个链接，并且这个链接指向用户路径\n服务文件完整文件举例如下：\n[Unit]Description=MySQL ServerDocumentation=man:mysqld(8)Documentation=http://dev.mysql.com/doc/refman/en/using-systemd.htmlAfter=network.targetAfter=syslog.target[Install]WantedBy=multi-user.target[Service]User=mysqlGroup=mysqlType=forkingPIDFile=/var/run/mysql/mysqld.pidTimeoutSec=0ExecStart=/usr/local/mysql/bin/mysqld --daemonize --pid-file=/var/run/mysql/mysqld.pid $MYSQLD_OPTSEnvironmentFile=-/etc/sysconfig/mysqlLimitNOFILE = 10000Restart=on-failureRestartPreventExitStatus=1Environment=MYSQLD_PARENT_PID=1PrivateTmp=false\n\n\n所有的启动设置之前，都可以加上一个连词号（-），表示”抑制错误”，即发生错误的时候，不影响其他命令的执行。比如，EnvironmentFile=-/etc/sysconfig/mysql（注意等号后面的那个连词号），就表示即使/etc/sysconfig/mysql文件不存在，也不会抛出错误。\n\n文件分为三个部分：\n[Unit]\n启动顺序与依赖关系\n\nDescription字段给出当前服务的简单描述\nDocumentation给出文档位置\nBefore定义在哪些服务之前运行，Before=xxx.service，代表本服务在xxx.service启动之前启动。\nAfter定义在哪些服务之后运行\nRequires表示”强依赖”关系，即如果指定服务启动失败或异常退出，该服务也必须退出\nWants“弱依赖”关系，即如果指定服务启动失败或异常退出，该服务不受影响继续运行\n[Service]\n定义如何启动当前服务\n\nType定义启动类型。可以设置的值如下\n\nsimple（默认值）：ExecStart字段启动的进程为主进程\n\nforking：ExecStart字段将以fork()方式启动，此时父进程将会退出，子进程将成为主进程；对于常规的守护进程（daemon），除非你确定此启动方式无法满足需求，使用此类型启动即可。使用此启动类型应同时指定 PIDFile=，以便systemd能够跟踪服务的主进程。\n\noneshot：类似于simple，但只执行一次，Systemd 会等它执行完，才启动其他服务\n下面是一个oneshot的例子，笔记本电脑启动时，要把触摸板关掉，配置文件可以这样写。\n[Unit]Description=Switch-off Touchpad[Service]Type=oneshotExecStart=/usr/bin/touchpad-off[Install]WantedBy=multi-user.target\n\n如果关闭以后，将来某个时候还想打开，配置文件修改如下。\n[Unit]Description=Switch-off Touchpad[Service]Type=oneshotExecStart=/usr/bin/touchpad-off startExecStop=/usr/bin/touchpad-off stopRemainAfterExit=yes[Install]WantedBy=multi-user.target\n\ndbus：类似于simple，但会等待 D-Bus 信号后启动\n\nnotify：类似于simple，启动结束后会发出通知信号，然后 Systemd 再启动其他服务\n\nidle：类似于simple，但是要等到其他任务都执行完，才会启动该服务。一种使用场合是为让该服务的输出，不与其他服务的输出相混合\n\n\nPIDFilepid文件路径，一般路径都在/var/run/\nExecStart启动进程时执行的命令\nExecReload重启服务时执行的命令\nExecStop停止服务时执行的命令\nExecStartPre启动服务之前执行的命令\nExecStartPost启动服务之后执行的命令\nExecStopPost停止服务之后执行的命令\nEnvironmentFile指定当前服务的环境参数文件。该文件内部的key=value键值对，可以用$key的形式，在当前配置文件中获取。\n[Service]EnvironmentFile=/etc/sysconfig/sshdExecStart=/usr/sbin/sshd -D $OPTIONSExecReload=/bin/kill -HUP $MAINPID\n\nEnvironmentTimeoutSecLimitNOFILERestart定义了 服务退出后，Systemd 的重启方式。Restart字段可以设置的值如下。\n\nno（默认值）：退出后不会重启\non-success：只有正常退出时（退出状态码为0），才会重启\non-failure：非正常退出时（退出状态码非0），包括被信号终止和超时，才会重启\non-abnormal：只有被信号终止和超时，才会重启\non-abort：只有在收到没有捕捉到的信号终止时，才会重启\non-watchdog：超时退出，才会重启\nalways：不管是什么退出原因，总是重启\n\n对于守护进程，推荐设为on-failure。对于那些允许发生错误退出的服务，可以设为on-abnormal。\nRestartSec表示 Systemd 重启服务之前，需要等待的秒数\nRestartPreventExitStatusPrivateTmpTrue表示给服务分配独立的临时空间\nRemainAfterExit可设为”yes”或”no”(默认值)，表示当该服务的所有进程全部退出之后，是否依然将此服务视为活动(active)状态。这个选项只有在Type=oneshot时需要被配置\nKillMode定义 Systemd 如何停止 sshd 服务。\nKillMode字段可以设置的值如下。\n\ncontrol-group（默认值）：当前控制组里面的所有子进程，都会被杀掉\nprocess：只杀主进程\nmixed：主进程将收到 SIGTERM 信号，子进程收到 SIGKILL 信号\nnone：没有进程会被杀掉，只是执行服务的 stop 命令。\n\n[Unit]Description=OpenSSH server daemonDocumentation=man:sshd(8) man:sshd_config(5)After=network.target sshd-keygen.serviceWants=sshd-keygen.service[Service]EnvironmentFile=/etc/sysconfig/sshdExecStart=/usr/sbin/sshd -D $OPTIONSExecReload=/bin/kill -HUP $MAINPIDType=simpleKillMode=processRestart=on-failureRestartSec=42s\n\n上面这个例子中，将KillMode设为process，表示只停止主进程，不停止任何sshd 子进程，即子进程打开的 SSH session 仍然保持连接。这个设置不太常见，但对 sshd 很重要，否则你停止服务的时候，会连自己打开的 SSH session 一起杀掉。\n[Install]\n定义如何安装这个配置文件，即怎样做到开机启动\n\nWantBy表示该服务所在的target，target的含义是服务组，表示一组服务。WantedBy=multi-user.target指的是，当前服务所在的target是multi-user.target。这个设置非常重要，因为执行systemctl enable mysqld.service命令时，mysqld.service的一个符号链接，就会放在/etc/systemd/system目录下面的multi-user.target.wants子目录之中。\n\nsystemd 有默认的启动 target\n\n[root@zhang ~]# systemctl get-defaultmulti-user.target\n\n上面的结果表示，默认的启动target是multi-user.target。在这个组里的所有服务，都将开机启动。这就是为什么systemctl enable命令能设置开机启动的原因。\n\n查看 multi-user.target 包含的所有服务\n\nsystemctl list-dependencies multi-user.target\n\n\n切换到另一个 target\n\n# shutdown.target 就是关机状态sudo systemctl isolate shutdown.target\n\n  一般来说，常用的 Target 有两个：一个是multi-user.target，表示多用户命令行状态；另一个是graphical.target，表示图形用户状态，它依赖于multi-user.target。官方文档有一张非常清晰的 Target 依赖关系图。\n重新加载修改配置文件之后，需要重新加载配置文件\nsystemctl daemon-reload\n\n操作服务重载服务# 就会在/etc/systemd/system/multi-user.target.wants/目录下新建一个/usr/lib/systemd/system/mysql.service 文件的链接systemctl enable mysql.service# 相对应，可以用 disable 把它从 wants 目录给删除。systemctl disable mysql.servicesystemctl reload mysql.service\n\n启动停止重启systemctl start mysql.servicesystemctl stop mysql.servicesystemctl restart mysql.servicesystemctl kill mysql.service\n\n查看状态日志systemctl status mysql.servicejournalctl -f -u mysql.service\n","tags":["Linux"]},{"title":"Markdown数学公式语法","url":"/2020/03/21/Markdown%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F%E8%AF%AD%E6%B3%95/","content":"行内与独行\n行内公式：将公式插入到本行内，符号：$公式内容$，如：$xyz$\n独行公式：将公式插入到新的一行内，并且居中，符号：$$公式内容$$，如：$$xyz$$\n\n上标、下标与组合\n上标符号，符号：^，如：$x^4$\n下标符号，符号：_，如：$x_1$\n组合符号，符号：{}，如：${16}{8}O{2+}{2}$\n\n汉字、字体与格式\n汉字形式，符号：\\mbox{}，如：$V_{\\mbox{初始}}$\n字体控制，符号：\\displaystyle，如：$\\displaystyle \\frac{x+y}{y+z}$\n下划线符号，符号：\\underline，如：$\\underline{x+y}$\n标签，符号\\tag{数字}，如：$\\tag{11}$\n上大括号，符号：\\overbrace{算式}，如：$\\overbrace{a+b+c+d}^{2.0}$\n下大括号，符号：\\underbrace{算式}，如：$a+\\underbrace{b+c}_{1.0}+d$\n上位符号，符号：\\stacrel{上位符号}{基位符号}，如：$\\vec{x}\\stackrel{\\mathrm{def}}{=}{x_1,\\dots,x_n}$\n\n占位符\n两个quad空格，符号：\\qquad，如：$x \\qquad y$\nquad空格，符号：\\quad，如：$x \\quad y$\n大空格，符号\\，如：$x \\ y$\n中空格，符号\\:，如：$x : y$\n小空格，符号\\,，如：$x , y$\n没有空格，符号``，如：$xy$\n紧贴，符号\\!，如：$x ! y$\n\n定界符与组合\n括号，符号：（）\\big(\\big) \\Big(\\Big) \\bigg(\\bigg) \\Bigg(\\Bigg)，如：$（）\\big(\\big) \\Big(\\Big) \\bigg(\\bigg) \\Bigg(\\Bigg)$\n中括号，符号：[]，如：$[x+y]$\n大括号，符号：\\{ \\}，如：${x+y}$\n自适应括号，符号：\\left \\right，如：$\\left(x\\right)$，$\\left(x{yz}\\right)$\n组合公式，符号：{上位公式 \\choose 下位公式}，如：${n+1 \\choose k}={n \\choose k}+{n \\choose k-1}$\n组合公式，符号：{上位公式 \\atop 下位公式}，如：$\\sum_{k_0,k_1,\\ldots&gt;0 \\atop k_0+k_1+\\cdots=n}A_{k_0}A_{k_1}\\cdots$\n\n四则运算\n加法运算，符号：+，如：$x+y=z$\n减法运算，符号：-，如：$x-y=z$\n加减运算，符号：\\pm，如：$x \\pm y=z$\n减甲运算，符号：\\mp，如：$x \\mp y=z$\n乘法运算，符号：\\times，如：$x \\times y=z$\n点乘运算，符号：\\cdot，如：$x \\cdot y=z$\n星乘运算，符号：\\ast，如：$x \\ast y=z$\n除法运算，符号：\\div，如：$x \\div y=z$\n斜法运算，符号：/，如：$x/y=z$\n分式表示，符号：\\frac{分子}{分母}，如：$\\frac{x+y}{y+z}$\n分式表示，符号：{分子} \\voer {分母}，如：${x+y} \\over {y+z}$\n绝对值表示，符号：||，如：$|x+y|$\n\n高级运算\n平均数运算，符号：\\overline{算式}，如：$\\overline{xyz}$\n开二次方运算，符号：\\sqrt，如：$\\sqrt x$\n开方运算，符号：\\sqrt[开方数]{被开方数}，如：$\\sqrt[3]{x+y}$\n对数运算，符号：\\log，如：$\\log(x)$\n极限运算，符号：\\lim，如：$\\lim^{x \\to \\infty}_{y \\to 0}{\\frac{x}{y}}$\n极限运算，符号：\\displaystyle \\lim，如：$\\displaystyle \\lim^{x \\to \\infty}_{y \\to 0}{\\frac{x}{y}}$\n求和运算，符号：\\sum，如：$\\sum^{x \\to \\infty}_{y \\to 0}{\\frac{x}{y}}$\n求和运算，符号：\\displaystyle \\sum，如：$\\displaystyle \\sum^{x \\to \\infty}_{y \\to 0}{\\frac{x}{y}}$\n积分运算，符号：\\int，如：$\\int^{\\infty}_{0}{xdx}$\n积分运算，符号：\\displaystyle \\int，如：$\\displaystyle \\int^{\\infty}_{0}{xdx}$\n微分运算，符号：\\partial，如：$\\frac{\\partial x}{\\partial y}$\n矩阵表示，符号：\\begin{matrix} \\end{matrix}，如：$\\left[ \\begin{matrix} 1 &amp;2 &amp;\\cdots &amp;4\\5 &amp;6 &amp;\\cdots &amp;8\\vdots &amp;\\vdots &amp;\\ddots &amp;\\vdots\\13 &amp;14 &amp;\\cdots &amp;16\\end{matrix} \\right]$\n\n逻辑运算\n等于运算，符号：=，如：$x+y=z$\n大于运算，符号：&gt;，如：$x+y&gt;z$\n小于运算，符号：&lt;，如：$x+y&lt;z$\n大于等于运算，符号：\\geq，如：$x+y \\geq z$\n小于等于运算，符号：\\leq，如：$x+y \\leq z$\n不等于运算，符号：\\neq，如：$x+y \\neq z$\n不大于等于运算，符号：\\ngeq，如：$x+y \\ngeq z$\n不大于等于运算，符号：\\not\\geq，如：$x+y \\not\\geq z$\n不小于等于运算，符号：\\nleq，如：$x+y \\nleq z$\n不小于等于运算，符号：\\not\\leq，如：$x+y \\not\\leq z$\n约等于运算，符号：\\approx，如：$x+y \\approx z$\n恒定等于运算，符号：\\equiv，如：$x+y \\equiv z$\n\n集合运算\n属于运算，符号：\\in，如：$x \\in y$\n不属于运算，符号：\\notin，如：$x \\notin y$\n不属于运算，符号：\\not\\in，如：$x \\not\\in y$\n子集运算，符号：\\subset，如：$x \\subset y$\n子集运算，符号：\\supset，如：$x \\supset y$\n真子集运算，符号：\\subseteq，如：$x \\subseteq y$\n非真子集运算，符号：\\subsetneq，如：$x \\subsetneq y$\n真子集运算，符号：\\supseteq，如：$x \\supseteq y$\n非真子集运算，符号：\\supsetneq，如：$x \\supsetneq y$\n非子集运算，符号：\\not\\subset，如：$x \\not\\subset y$\n非子集运算，符号：\\not\\supset，如：$x \\not\\supset y$\n并集运算，符号：\\cup，如：$x \\cup y$\n交集运算，符号：\\cap，如：$x \\cap y$\n差集运算，符号：\\setminus，如：$x \\setminus y$\n同或运算，符号：\\bigodot，如：$x \\bigodot y$\n同与运算，符号：\\bigotimes，如：$x \\bigotimes y$\n实数集合，符号：\\mathbb{R}，如：\\mathbb{R}\n自然数集合，符号：\\mathbb{Z}，如：\\mathbb{Z}\n空集，符号：\\emptyset，如：$\\emptyset$\n\n数学符号\n无穷，符号：\\infty，如：$\\infty$\n虚数，符号：\\imath，如：$\\imath$\n虚数，符号：\\jmath，如：$\\jmath$\n数学符号，符号\\hat{a}，如：$\\hat{a}$\n数学符号，符号\\check{a}，如：$\\check{a}$\n数学符号，符号\\breve{a}，如：$\\breve{a}$\n数学符号，符号\\tilde{a}，如：$\\tilde{a}$\n数学符号，符号\\bar{a}，如：$\\bar{a}$\n矢量符号，符号\\vec{a}，如：$\\vec{a}$\n数学符号，符号\\acute{a}，如：$\\acute{a}$\n数学符号，符号\\grave{a}，如：$\\grave{a}$\n数学符号，符号\\mathring{a}，如：$\\mathring{a}$\n一阶导数符号，符号\\dot{a}，如：$\\dot{a}$\n二阶导数符号，符号\\ddot{a}，如：$\\ddot{a}$\n上箭头，符号：\\uparrow，如：$\\uparrow$\n上箭头，符号：\\Uparrow，如：$\\Uparrow$\n下箭头，符号：\\downarrow，如：$\\downarrow$\n下箭头，符号：\\Downarrow，如：$\\Downarrow$\n左箭头，符号：\\leftarrow，如：$\\leftarrow$\n左箭头，符号：\\Leftarrow，如：$\\Leftarrow$\n右箭头，符号：\\rightarrow，如：$\\rightarrow$\n右箭头，符号：\\Rightarrow，如：$\\Rightarrow$\n底端对齐的省略号，符号：\\ldots，如：$1,2,\\ldots,n$\n中线对齐的省略号，符号：\\cdots，如：$x_1^2 + x_2^2 + \\cdots + x_n^2$\n竖直对齐的省略号，符号：\\vdots，如：$\\vdots$\n斜对齐的省略号，符号：\\ddots，如：$\\ddots$\n\n希腊字母\n\n\n字母\n实现\n字母\n实现\n\n\n\nA\nA\nα\n\\alhpa\n\n\nB\nB\nβ\n\\beta\n\n\nΓ\n\\Gamma\nγ\n\\gamma\n\n\nΔ\n\\Delta\nδ\n\\delta\n\n\nE\nE\nϵ\n\\epsilon\n\n\nZ\nZ\nζ\n\\zeta\n\n\nH\nH\nη\n\\eta\n\n\nΘ\n\\Theta\nθ\n\\theta\n\n\nI\nI\nι\n\\iota\n\n\nK\nK\nκ\n\\kappa\n\n\nΛ\n\\Lambda\nλ\n\\lambda\n\n\nM\nM\nμ\n\\mu\n\n\nN\nN\nν\n\\nu\n\n\nΞ\n\\Xi\nξ\n\\xi\n\n\nO\nO\nο\n\\omicron\n\n\nΠ\n\\Pi\nπ\n\\pi\n\n\nP\nP\nρ\n\\rho\n\n\nΣ\n\\Sigma\nσ\n\\sigma\n\n\nT\nT\nτ\n\\tau\n\n\nΥ\n\\Upsilon\nυ\n\\upsilon\n\n\nΦ\n\\Phi\nϕ\n\\phi\n\n\nX\nX\nχ\n\\chi\n\n\nΨ\n\\Psi\nψ\n\\psi\n\n\nΩ\n\\v\nω\n\\omega\n\n\n"},{"title":"Maven常用命令","url":"/2019/12/09/Maven%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/","content":"将jar安装到本地Maven仓库\n转自：https://blog.csdn.net/ShuSheng0007/article/details/78547264/\n\n第一种方案mvn install:install-file -Dfile=&lt;path-to-file&gt; -DgroupId=&lt;group-id&gt; -DartifactId=&lt;artifact-id&gt; -Dversion=&lt;version&gt; -Dpackaging=&lt;packaging&gt;\n\n\n&lt;path-to-file&gt;: 要安装的JAR的本地路径\n&lt;group-id&gt;：要安装的JAR的Group Id\n&lt;artifact-id&gt;: 要安装的JAR的 Artificial Id\n&lt;version&gt;: JAR 版本\n&lt;packaging&gt;: 打包类型，例如JAR\n\n\n最好在pom.xml文件所在的目录运行上述命令，个人经验不在根目录运行有时会安装不成功\n\n执行上述命令后，我们就可以在pom.xml文件中引用\n&lt;dependency&gt;    &lt;groupId&gt;com.baidu.app&lt;/groupId&gt;    &lt;artifactId&gt;bdpush&lt;/artifactId&gt;    &lt;version&gt;3.0.1&lt;/version&gt;&lt;/dependency&gt;\n\n总结：这种方法弊端较大，程序的可维护性以及移植性较低。例如当你改变本地Maven仓库时需要重新安装。如果引用此JAR的项目是多人协调工作的项目，则每个人都要将其安装在自己的本地仓库。\n解决办法可以将此JAR文件放在工程的根目录下，让其随着项目走，然后在pom.xml文件中使用maven-install-plugin在Maven初始化阶段完成安装。\n&lt;plugin&gt;    &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;    &lt;artifactId&gt;maven-install-plugin&lt;/artifactId&gt;    &lt;version&gt;2.5&lt;/version&gt;    &lt;executions&gt;        &lt;execution&gt;            &lt;phase&gt;initialize&lt;/phase&gt;            &lt;goals&gt;                &lt;goal&gt;install-file&lt;/goal&gt;            &lt;/goals&gt;            &lt;configuration&gt;                &lt;groupId&gt;com.baidu.app&lt;/groupId&gt;                &lt;artifactId&gt;bdpush&lt;/artifactId&gt;                &lt;version&gt;3.0.1&lt;/version&gt;                &lt;packaging&gt;jar&lt;/packaging&gt;                &lt;file&gt;$&#123;basedir&#125;/lib/bdpush-3.0.1.ja&lt;/file&gt;            &lt;/configuration&gt;        &lt;/execution&gt;    &lt;/executions&gt;&lt;/plugin&gt;\n\n${basedir}表示pom.xml文件所在的目录\n第二种方案第二种方法比较粗暴简单，具体为将依赖设置为系统域，通过完全路径引用。例如要引用的JAR文件在 &lt;PROJECT_ROOT_FOLDER&gt;/lib下，那么使用如下方法添加依赖\n&lt;dependency&gt;    &lt;groupId&gt;com.baidu.app&lt;/groupId&gt;    &lt;artifactId&gt;bdpush&lt;/artifactId&gt;    &lt;version&gt;3.0.1&lt;/version&gt;    &lt;scope&gt;system&lt;/scope&gt;    &lt;systemPath&gt;$&#123;basedir&#125;/lib/bdpush-3.0.1.ja&lt;/systemPath&gt;&lt;/dependency&gt;\n\n${basedir}表示pom.xml文件所在的目录，例如你的JAR文件在D盘下的jarLibs里面，就将${basedir}替换为D:/jarLibs即可。\n\n这种方法我自己在SpringBoot项目中打包成war文件时，没有成功打包到里面\n\n第三种方案第三种方案与第一种差不多，不同的是JAR文件被安装在一个单独的仓库里。这个本地仓库建在你项目的根目录下，随着项目走。例如\n1：我们在${basedir}（pom.xml文件所在路径）目录下建立一个叫maven-repository的本地仓库。\n2：使用如下命令安装我们要引用的JAR到此仓库中\nmvn deploy:deploy-file -Dfile=&lt;path-to-file&gt; -DgroupId=&lt;group-id&gt; -DartifactId=&lt;artifact-id&gt; -Dversion=&lt;version&gt; -Dpackaging=jar -Durl=file:./maven-repository/ -DrepositoryId=maven-repository -DupdateReleaseInfo=true\n\n3：在pom.xml中如下使用\n\n申明仓库\n\n&lt;repositories&gt;    &lt;repository&gt;        &lt;id&gt;maven-repository&lt;/id&gt;        &lt;url&gt;file:///$&#123;project.basedir&#125;/maven-repository&lt;/url&gt;    &lt;/repository&gt;&lt;/repositories&gt;\n\n\n然后添加引用\n\n&lt;dependency&gt;    &lt;groupId&gt;com.baidu.app&lt;/groupId&gt;    &lt;artifactId&gt;bdpush&lt;/artifactId&gt;    &lt;version&gt;3.0.1&lt;/version&gt;&lt;/dependency&gt;\n\nMaven常用命令[root@zhang ~]# mvn --helpusage: mvn [options] [&lt;goal(s)&gt;] [&lt;phase(s)&gt;]Options: -am,--also-make                        If project list is specified, also                                        build projects required by the                                        list -amd,--also-make-dependents            If project list is specified, also                                        build projects that depend on                                        projects on the list -B,--batch-mode                        Run in non-interactive (batch)                                        mode (disables output color) -b,--builder &lt;arg&gt;                     The id of the build strategy to                                        use -C,--strict-checksums                  Fail the build if checksums don't                                        match -c,--lax-checksums                     Warn if checksums don't match -cpu,--check-plugin-updates            Ineffective, only kept for                                        backward compatibility -D,--define &lt;arg&gt;                      Define a system property -e,--errors                            Produce execution error messages -emp,--encrypt-master-password &lt;arg&gt;   Encrypt master security password -ep,--encrypt-password &lt;arg&gt;           Encrypt server password -f,--file &lt;arg&gt;                        Force the use of an alternate POM                                        file (or directory with pom.xml) -fae,--fail-at-end                     Only fail the build afterwards;                                        allow all non-impacted builds to                                        continue -ff,--fail-fast                        Stop at first failure in                                        reactorized builds -fn,--fail-never                       NEVER fail the build, regardless                                        of project result -gs,--global-settings &lt;arg&gt;            Alternate path for the global                                        settings file -gt,--global-toolchains &lt;arg&gt;          Alternate path for the global                                        toolchains file -h,--help                              Display help information -l,--log-file &lt;arg&gt;                    Log file where all build output                                        will go (disables output color) -llr,--legacy-local-repository         Use Maven 2 Legacy Local                                        Repository behaviour, ie no use of                                        _remote.repositories. Can also be                                        activated by using                                        -Dmaven.legacyLocalRepo=true -N,--non-recursive                     Do not recurse into sub-projects -npr,--no-plugin-registry              Ineffective, only kept for                                        backward compatibility -npu,--no-plugin-updates               Ineffective, only kept for                                        backward compatibility -nsu,--no-snapshot-updates             Suppress SNAPSHOT updates -ntp,--no-transfer-progress            Do not display transfer progress                                        when downloading or uploading -o,--offline                           Work offline -P,--activate-profiles &lt;arg&gt;           Comma-delimited list of profiles                                        to activate -pl,--projects &lt;arg&gt;                   Comma-delimited list of specified                                        reactor projects to build instead                                        of all projects. A project can be                                        specified by [groupId]:artifactId                                        or by its relative path -q,--quiet                             Quiet output - only show errors -rf,--resume-from &lt;arg&gt;                Resume reactor from specified                                        project -s,--settings &lt;arg&gt;                    Alternate path for the user                                        settings file -t,--toolchains &lt;arg&gt;                  Alternate path for the user                                        toolchains file -T,--threads &lt;arg&gt;                     Thread count, for instance 2.0C                                        where C is core multiplied -U,--update-snapshots                  Forces a check for missing                                        releases and updated snapshots on                                        remote repositories -up,--update-plugins                   Ineffective, only kept for                                        backward compatibility -v,--version                           Display version information -V,--show-version                      Display version information                                        WITHOUT stopping build -X,--debug                             Produce execution debug output\n\nmvn package \\\t-Dmaven.repo.local&#x3D;C:\\Users\\lenovo\\.m2\\Repository4 \\\t-Dmaven.test.skip&#x3D;true \\\t-f pom.xml\n\n\n-Dmaven.repo.local 强制使用本地源\n-Dmaven.test.skip 跳过单测\n-f 指定配置文件\n\n","tags":["Maven"]},{"title":"MySQL安装部署","url":"/2019/10/22/MySQL%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/","content":"Windows环境\n下载地址\n官方部署文档\n\nD:\\Develop\\mysql-8.0.18-winx64&gt;bin\\mysqld.exe --defaults-file=D:\\Develop\\mysql-8.0.18-winx64\\my.ini --initialize --console\n\n2019-10-24T08:47:21.549556Z 0 [System] [MY-013169] [Server] D:\\Develop\\mysql-8.0.18-winx64\\bin\\mysqld.exe (mysqld 8.0.18) initializing of server in progress asprocess 76122019-10-24T08:47:34.895556Z 5 [Note] [MY-010454] [Server] A temporary password is generated for root@localhost: 6#gL3pJ#qyh.\n\n# 安装服务D:\\Develop\\mysql-8.0.18-winx64&gt;bin\\mysqld.exe install\n\nnet start mysql\n\nLinux环境Yum安装\n 下载链接  参考文档\n\n下载mysql80-community-release-el7-3.noarch.rpm\n# 安装rpm源sudo rpm -Uvh mysql80-community-release-el7-3.noarch.rpm# 编辑，找到Enable to use MySQL 5.7，改为enabled=1，其他版本设置成enabled=0，# 同理这个方法可以按照别的版本vim /etc/yum.repos.d/mysql-community.repo# 检查只有MySQL 5.7启动yum repolist enabled | grep mysql# 安装MySQLsudo yum install mysql-community-server# 启动MySQL服务器sudo service mysqld start# MySQL服务器的状态sudo service mysqld status# 查看超级用户的密码sudo grep 'temporary password' /var/log/mysqld.log# 登录mysqlmysql -uroot -p# 修改密码ALTER USER 'root'@'localhost' IDENTIFIED BY 'MyNewPass4!';# 默认mysql的root用户不支持远程访问，开启访问权限GRANT ALL ON *.* TO root@'%' IDENTIFIED BY '123456' WITH GRANT OPTION;flush privileges;# 修改密码设置级别set global validate_password_policy=0;set global validate_password_length=1;# 开启3306端口firewall-cmd --add-port=3306/tcp# (a)数据库目录/var/lib/mysql/# (b)配置文件/usr/share /mysql（mysql.server命令及配置文件）/etc/my.cnf# (c)相关命令/usr/bin（mysqladmin mysqldump等命令）# (d)启动脚本/etc/rc.d/init.d/（启动脚本文件mysql的目录）\n\n二进制安装\n下载地址 参考文档\n\n下载文件： mysql-5.7.28-linux-glibc2.12-x86_64.tar\n# 查看包中内容tar -tvf mysql-5.7.28-linux-glibc2.12-x86_64.tar# mysql-5.7.28-linux-glibc2.12-x86_64.tar.gz# mysql-test-5.7.28-linux-glibc2.12-x86_64.tar.gz\n\n\n\n\nDirectory\nContents of Directory\n\n\n\nbin\nmysqld server, client and utility programs\n\n\ndocs\nMySQL manual in Info format\n\n\nman\nUnix manual pages\n\n\ninclude\nInclude (header) files\n\n\nlib\nLibraries\n\n\nshare\nError messages, dictionary, and SQL for database installation\n\n\nsupport-files\nMiscellaneous support files\n\n\n创建用户、用户组groupadd mysqluseradd -r -g mysql -s /bin/false mysql\n\n解压发行版文件cd /usr/local#tar zxvf /path/to/mysql-VERSION-OS.tar.gz 如果是.gz结尾tar xvf /path/to/mysql-VERSION-OS.tar\n\n创建软链接# 链接，这里建议使用全路径ln -s full-path-to-mysql-VERSION-OS mysql# 解压创建软链接，并且修改所属用户和组chown -R mysql:mysql mysql\n\n创建并授权cd mysqlmkdir mysql-fileschown mysql:mysql mysql-fileschmod 750 mysql-files\n\n创建配置文件cd /etctouch my.cnfchown root:root my.cnfchmod 644 my.cnf\n\n[mysqld]datadir=/usr/local/mysql/datasocket=/tmp/mysql.sockport=3306log-error=/usr/local/mysql/data/localhost.localdomain.erruser=mysqlsecure_file_priv=/usr/local/mysql/mysql-fileslocal_infile=OFF\n\n初始化数据目录cd /usr/local/mysqlmkdir datachmod 750 datachown mysql:mysql data\n\n初始化  参数： –initialize 会生成一个随机密码\nbin/mysqld --initialize --user=mysql\n\n显示下面则初始化成功：\n[root@bogon mysql]# bin/mysqld --initialize --user=mysql2019-10-23T07:41:12.611481Z 0 [Warning] TIMESTAMP with implicit DEFAULT value is deprecated. Please use --explicit_defaults_for_timestamp server option (see documentation for more details).2019-10-23T07:43:00.499738Z 0 [Warning] InnoDB: New log files created, LSN=457902019-10-23T07:43:00.819667Z 0 [Warning] InnoDB: Creating foreign key constraint system tables.2019-10-23T07:43:00.919776Z 0 [Warning] No existing UUID has been found, so we assume that this is the first time that this server has been started. Generating a new UUID: bd1531fb-f568-11e9-bc4b-46afd4d32e02.2019-10-23T07:43:00.958124Z 0 [Warning] Gtid table is not ready to be used. Table 'mysql.gtid_executed' cannot be opened.2019-10-23T07:43:02.482380Z 0 [Warning] CA certificate ca.pem is self signed.2019-10-23T07:43:02.643686Z 1 [Note] A temporary password is generated for root@localhost: Wsi!(otie8de\n\n命令添加到环境变量中vim /etc/profile# 添加PATH=$PATH:/usr/local/mysql/bin# 使生效source /etc/profile\n\n启动bin/mysqld_safe --user=mysql &amp;\n\nmkdir /var/log/mysql/touch /var/log/mysql/mysql.log# 如果日志没内容，授权chown -R mysql:mysql /var/log/mysql/mysql.log\n\n配置systemd\n参考文档 通用二进制包安装指南\n\n\n创建文件\n\n这两个路径任意一个都可以\ntouch /etc/systemd/system/mysqld.service# 建议创建到这个路径下面touch /usr/lib/systemd/system/mysqld.service\n\n这里注意pid的路径，写入下面内容\n[Unit]Description=MySQL ServerDocumentation=man:mysqld(8)Documentation=http://dev.mysql.com/doc/refman/en/using-systemd.htmlAfter=network.targetAfter=syslog.target[Install]WantedBy=multi-user.target[Service]User=mysqlGroup=mysql# Have mysqld write its state to the systemd notify socketType=forkingPIDFile=/var/run/mysql/mysqld.pid# Disable service start and stop timeout logic of systemd for mysqld service.TimeoutSec=0# Start main serviceExecStart=/usr/local/mysql/bin/mysqld --daemonize --pid-file=/var/run/mysql/mysqld.pid $MYSQLD_OPTS# Use this to switch malloc implementationEnvironmentFile=-/etc/sysconfig/mysql# Sets open_files_limitLimitNOFILE = 10000Restart=on-failureRestartPreventExitStatus=1# Set environment variable MYSQLD_PARENT_PID. This is required for restart.Environment=MYSQLD_PARENT_PID=1PrivateTmp=false\n\n\n重启\n\nsystemctl daemon-reload\n\n\n操作\n\nsystemctl &#123;start|stop|restart|status&#125; mysqld\n\n设置开机启动\n参考文档\n\ncp /usr/local/mysql/support-files/mysql.server /etc/init.d/mysqlchmod +x /etc/init.d/mysqlchkconfig --add mysql# 某些系统可能用下面命令chkconfig --level 345 mysql on\n\n相关问题\nERROR 2002 (HY000): Can&#39;t connect to local MySQL server through socket &#39;&#x2F;tmp&#x2F;mysql.sock&#39; (2)\n\n解决办法： ln -s /var/lib/mysql/mysql.sock mysql.sock\n\n重启无法创建PID\nmkdir /var/run/mysqlchown mysql:mysql -R /var/run/mysql\n\n\n相关命令# 关闭防火墙systemctl stop firewalld# 永久关闭防火墙systemctl disable firewalld# 加入开机启动systemctl enable redis.service# 查看开机是否启动成功systemctl is-enabled redis.service\n\n相关文档\n忘记初始密码怎么办\n\n创建文件mysql-init.txt，写入以下内容\nALTER USER &#39;root&#39;@&#39;localhost&#39; IDENTIFIED BY &#39;MyNewPass&#39;;\n执行\nmysqld --init-file&#x3D;C:\\\\mysql-init.txt\n\n\nMySQL同步\n\n","tags":["MySQL","数据库","部署文档"]},{"title":"MySQL各种JOIN介绍","url":"/2020/06/16/MySQL%E5%90%84%E7%A7%8DJOIN%E4%BB%8B%E7%BB%8D/","content":"前言在各种问答社区里谈及 SQL 里的各种 JOIN 之间的区别时，最被广为引用的是 CodeProject 上 C.L. Moffatt 的文章 Visual Representation of SQL Joins，他确实讲得简单明了，使用文氏图来帮助理解，效果明显。本文将沿用他的讲解方式，稍有演绎，可以视为该文较为粗糙的中译版。\n约定下文将使用两个数据库表 Table_A 和 Table_B 来进行示例讲解，其结构与数据分别如下：\nmysql&gt; SELECT * FROM Table_A ORDER BY PK ASC;+----+---------+| PK | Value   |+----+---------+|  1 | both ab ||  2 | only a  |+----+---------+2 rows in set (0.00 sec)mysql&gt; SELECT * from Table_B ORDER BY PK ASC;+----+---------+| PK | Value   |+----+---------+|  1 | both ab ||  3 | only b  |+----+---------+2 rows in set (0.00 sec)\n\n其中 PK 为 1 的记录在 Table_A 和 Table_B 中都有，2 为 Table_A 特有，3 为 Table_B 特有。\n常用的 JOININNER JOININNER JOIN 一般被译作内连接。内连接查询能将左表（表 A）和右表（表 B）中能关联起来的数据连接后返回。\n文氏图：\n示例查询：\nSELECT A.PK AS A_PK, B.PK AS B_PK,       A.Value AS A_Value, B.Value AS B_ValueFROM Table_A AINNER JOIN Table_B BON A.PK &#x3D; B.PK;\n\n查询结果：\n+------+------+---------+---------+| A_PK | B_PK | A_Value | B_Value |+------+------+---------+---------+|    1 |    1 | both ab | both ab |+------+------+---------+---------+1 row in set (0.00 sec)\n\n注：其中 A 为 Table_A 的别名，B 为 Table_B 的别名，下同。\nLEFT JOINLEFT JOIN 一般被译作左连接，也写作 LEFT OUTER JOIN。左连接查询会返回左表（表 A）中所有记录，不管右表（表 B）中有没有关联的数据。在右表中找到的关联数据列也会被一起返回。\n文氏图：\n示例查询：\nSELECT A.PK AS A_PK, B.PK AS B_PK,       A.Value AS A_Value, B.Value AS B_ValueFROM Table_A ALEFT JOIN Table_B BON A.PK &#x3D; B.PK;\n\n查询结果：\n+------+------+---------+---------+| A_PK | B_PK | A_Value | B_Value |+------+------+---------+---------+|    1 |    1 | both ab | both ba ||    2 | NULL | only a  | NULL    |+------+------+---------+---------+2 rows in set (0.00 sec)\n\nRIGHT JOINRIGHT JOIN 一般被译作右连接，也写作 RIGHT OUTER JOIN。右连接查询会返回右表（表 B）中所有记录，不管左表（表 A）中有没有关联的数据。在左表中找到的关联数据列也会被一起返回。\n文氏图：\n示例查询：\nSELECT A.PK AS A_PK, B.PK AS B_PK,       A.Value AS A_Value, B.Value AS B_ValueFROM Table_A ARIGHT JOIN Table_B BON A.PK &#x3D; B.PK;\n\n查询结果：\n+------+------+---------+---------+| A_PK | B_PK | A_Value | B_Value |+------+------+---------+---------+|    1 |    1 | both ab | both ba || NULL |    3 | NULL    | only b  |+------+------+---------+---------+2 rows in set (0.00 sec)\n\nFULL OUTER JOINFULL OUTER JOIN 一般被译作外连接、全连接，实际查询语句中可以写作 FULL OUTER JOIN 或 FULL JOIN。外连接查询能返回左右表里的所有记录，其中左右表里能关联起来的记录被连接后返回。\n文氏图：\n示例查询：\nSELECT A.PK AS A_PK, B.PK AS B_PK,       A.Value AS A_Value, B.Value AS B_ValueFROM Table_A AFULL OUTER JOIN Table_B BON A.PK &#x3D; B.PK;\n\n查询结果：\nERROR 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near &#39;FULL OUTER JOIN Table_B BON A.PK &#x3D; B.PK&#39; at line 4\n\n注：我当前示例使用的 MySQL 不支持 FULL OUTER JOIN。\n应当返回的结果（使用 UNION 模拟）：\nmysql&gt; SELECT *    -&gt; FROM Table_A    -&gt; LEFT JOIN Table_B    -&gt; ON Table_A.PK &#x3D; Table_B.PK    -&gt; UNION ALL    -&gt; SELECT *    -&gt; FROM Table_A    -&gt; RIGHT JOIN Table_B    -&gt; ON Table_A.PK &#x3D; Table_B.PK    -&gt; WHERE Table_A.PK IS NULL;+------+---------+------+---------+| PK   | Value   | PK   | Value   |+------+---------+------+---------+|    1 | both ab |    1 | both ba ||    2 | only a  | NULL | NULL    || NULL | NULL    |    3 | only b  |+------+---------+------+---------+3 rows in set (0.00 sec)\n\n小结以上四种，就是 SQL 里常见 JOIN 的种类和概念了，看一下它们的合影：\n有没有感觉少了些什么，学数学集合时完全不止这几种情况？确实如此，继续看。\n延伸用法LEFT JOIN EXCLUDING INNER JOIN返回左表有但右表没有关联数据的记录集。\n文氏图：\n示例查询：\nSELECT A.PK AS A_PK, B.PK AS B_PK,       A.Value AS A_Value, B.Value AS B_ValueFROM Table_A ALEFT JOIN Table_B BON A.PK &#x3D; B.PKWHERE B.PK IS NULL;\n\n查询结果：\n+------+------+---------+---------+| A_PK | B_PK | A_Value | B_Value |+------+------+---------+---------+|    2 | NULL | only a  | NULL    |+------+------+---------+---------+1 row in set (0.01 sec)\n\nRIGHT JOIN EXCLUDING INNER JOIN返回右表有但左表没有关联数据的记录集。\n文氏图：\n示例查询：\nSELECT A.PK AS A_PK, B.PK AS B_PK,       A.Value AS A_Value, B.Value AS B_ValueFROM Table_A ARIGHT JOIN Table_B BON A.PK &#x3D; B.PKWHERE A.PK IS NULL;\n\n查询结果：\n+------+------+---------+---------+| A_PK | B_PK | A_Value | B_Value |+------+------+---------+---------+| NULL |    3 | NULL    | only b  |+------+------+---------+---------+1 row in set (0.00 sec)\n\nFULL OUTER JOIN EXCLUDING INNER JOIN返回左表和右表里没有相互关联的记录集。\n文氏图：\n示例查询：\nSELECT A.PK AS A_PK, B.PK AS B_PK,       A.Value AS A_Value, B.Value AS B_ValueFROM Table_A AFULL OUTER JOIN Table_B BON A.PK &#x3D; B.PKWHERE A.PK IS NULLOR B.PK IS NULL;\n\n因为使用到了 FULL OUTER JOIN，MySQL 在执行该查询时再次报错。\nERROR 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near &#39;FULL OUTER JOIN Table_B BON A.PK &#x3D; B.PKWHERE A.PK IS NULLOR B.PK IS NULL&#39; at line 4\n\n应当返回的结果（用 UNION 模拟）：\nmysql&gt; SELECT *    -&gt; FROM Table_A    -&gt; LEFT JOIN Table_B    -&gt; ON Table_A.PK &#x3D; Table_B.PK    -&gt; WHERE Table_B.PK IS NULL    -&gt; UNION ALL    -&gt; SELECT *    -&gt; FROM Table_A    -&gt; RIGHT JOIN Table_B    -&gt; ON Table_A.PK &#x3D; Table_B.PK    -&gt; WHERE Table_A.PK IS NULL;+------+--------+------+--------+| PK   | Value  | PK   | Value  |+------+--------+------+--------+|    2 | only a | NULL | NULL   || NULL | NULL   |    3 | only b |+------+--------+------+--------+2 rows in set (0.00 sec)\n\n总结以上七种用法基本上可以覆盖各种 JOIN 查询了。七种用法的全家福：\n看着它们，我仿佛回到了当年学数学，求交集并集的时代……\n顺带张贴一下 C.L. Moffatt 带 SQL 语句的图片，配合学习，风味更佳：\n\n更新：更多的 JOIN除以上几种外，还有更多的 JOIN 用法，比如 CROSS JOIN（迪卡尔集）、SELF JOIN，可以参考 SQL JOINS Slide Presentation 学习。\nCROSS JOIN返回左表与右表之间符合条件的记录的迪卡尔集。\n图示：\n示例查询：\nSELECT A.PK AS A_PK, B.PK AS B_PK,       A.Value AS A_Value, B.Value AS B_ValueFROM Table_A ACROSS JOIN Table_B B;\n\n查询结果：\n+------+------+---------+---------+| A_PK | B_PK | A_Value | B_Value |+------+------+---------+---------+|    1 |    1 | both ab | both ba ||    2 |    1 | only a  | both ba ||    1 |    3 | both ab | only b  ||    2 |    3 | only a  | only b  |+------+------+---------+---------+4 rows in set (0.00 sec)\n\n上面讲过的几种 JOIN 查询的结果都可以用 CROSS JOIN 加条件模拟出来，比如 INNER JOIN 对应 CROSS JOIN ... WHERE A.PK = B.PK。\nSELF JOIN返回表与自己连接后符合条件的记录，一般用在表里有一个字段是用主键作为外键的情况。\n比如 Table_C 的结构与数据如下：\n+--------+----------+-------------+| EMP_ID | EMP_NAME | EMP_SUPV_ID |+--------+----------+-------------+|   1001 | Ma       |        NULL ||   1002 | Zhuang   |        1001 |+--------+----------+-------------+2 rows in set (0.00 sec)\n\nEMP_ID 字段表示员工 ID，EMP_NAME 字段表示员工姓名，EMP_SUPV_ID 表示主管 ID。\n示例查询：\n现在我们想查询所有有主管的员工及其对应的主管 ID 和姓名，就可以用 SELF JOIN 来实现。\nSELECT A.EMP_ID AS EMP_ID, A.EMP_NAME AS EMP_NAME,    B.EMP_ID AS EMP_SUPV_ID, B.EMP_NAME AS EMP_SUPV_NAMEFROM Table_C A, Table_C BWHERE A.EMP_SUPV_ID &#x3D; B.EMP_ID;\n\n查询结果：\n+--------+----------+-------------+---------------+| EMP_ID | EMP_NAME | EMP_SUPV_ID | EMP_SUPV_NAME |+--------+----------+-------------+---------------+|   1002 | Zhuang   |        1001 | Ma            |+--------+----------+-------------+---------------+1 row in set (0.00 sec)\n","tags":["MySQL","数据库"]},{"title":"MySQL5.7基于GTID及多线程主从复制","url":"/2020/04/17/MySQL57%E5%9F%BA%E4%BA%8EGTID%E5%8F%8A%E5%A4%9A%E7%BA%BF%E7%A8%8B%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6/","content":"\n转自：https://www.sundayle.com/mysql-gtid-relication/\n\nMySQL主从同步原理\n参考文档\n\nMySQL主从同步是在MySQL主从复制(Master-Slave Replication)基础上实现的，通过设置在Master MySQL上的binlog(使其处于打开状态)，Slave MySQL上通过一个I/O线程从Master MySQL上读取binlog，然后传输到Slave MySQL的中继日志中，然后Slave MySQL的SQL线程从中继日志中读取中继日志，然后应用到Slave MySQL的数据库中。这样实现了主从数据同步功能。\nMySQL中主从复制的优点\n横向扩展解决方案在多个从库之间扩展负载以提高性能。在这种环境中，所有写入和更新在主库上进行。但是，读取可能发生在一个或多个从库上。该模型可以提高写入的性能（由于主库专用于更新），同时在多个从库上读取，可以大大提高读取速度。\n数据安全性由于主库数据被复制到从库，从库可以暂停复制过程，可以在从库上运行备份服务，而不会破坏对应的主库数据。\n分析可以在主库上创建实时数据，而信息分析可以在从库上进行，而不会影响主服务器的性能。\n\nGTID概念从 MySQL 5.6.5 开始新增了一种基于 GTID 的复制方式。通过 GTID保证了每个在主库上提交的事务在集群中有一个唯一的ID。这种方式强化了数据库的主备一致性，故障恢复以及容错能力。在原来基于二进制日志的复制中，从库需要告知主库要从哪个偏移量进行增量同步，如果指定错误会造成数据的遗漏，从而造成数据的不一致。借助GTID，在发生主备切换的情况下，MySQL的其它从库可以自动在新主库上找到正确的复制位置，这大大简化了复杂复制拓扑下集群的维护，也减少了人为设置复制位置发生误操作的风险。另外，基于GTID的复制可以忽略已经执行过的事务，减少了数据发生不一致的风险。\n什么是GTIDGTID (Global Transaction ID) 是对于一个已提交事务的编号，并且是一个全局唯一的编号。 GTID 实际上是由UUID+TID 组成的。其中 UUID 是一个 MySQL 实例的唯一标识。TID代表了该实例上已经提交的事务数量，并且随着事务提交单调递增。下面是一个GTID的具体形式：\n3E11FA47-71CA-11E1-9E33-C80AA9429562:23，冒号分割前边为uuid，后边为TID。\nGTID 集合可以包含来自多个 MySQL 实例的事务，它们之间用逗号分隔。如果来自同一MySQL实例的事务序号有多个范围区间，各组范围之间用冒号分隔。\n例如：\ne6954592-8dba-11e6-af0e-fa163e1cf111:1-5:11-18,\ne6954592-8dba-11e6-af0e-fa163e1cf3f2:1-27\n可以使用show master status实时查看当前事务执行数。\nGTID的作用GTID采用了新的复制协议，旧协议是，首先从服务器上在一个特定的偏移量位置连接到主服务器上一个给定的二进制日志文件，然后主服务器再从给定的连接点开始发送所有的事件。新协议有所不同，支持以全局统一事务ID (GTID)为基础的复制。当在主库上提交事务或者被从库应用时，可以定位和追踪每一个事务。GTID复制是全部以事务为基础，使得检查主从一致性变得非常简单。如果所有主库上提交的事务也同样提交到从库上，一致性就得到了保证。\nGTID的工作原理①当一个事务在主库端执行并提交时，产生GTID，一同记录到binlog日志中。②binlog传输到slave,并存储到slave的relaylog后，读取这个GTID的这个值设置gtid_next变量，即告诉Slave，下一个要执行的GTID值。③sql线程从relay log中获取GTID，然后对比slave端的binlog是否有该GTID。④如果有记录，说明该GTID的事务已经执行，slave会忽略。⑤如果没有记录，slave就会执行该GTID事务，并记录该GTID到自身的binlog，在读取执行事务前会先检查其他session持有该GTID，确保不被重复执行。⑥在解析过程中会判断是否有主键，如果没有就用二级索引，如果没有就用全部扫描。\n操作环境系统：CentOS 7数据库：MySQL 5.7主库：192.168.11.31从库：192.168.11.32\n主库配置[mysqld]datadir&#x3D;&#x2F;data&#x2F;mysql&#x2F;3306socket&#x3D;&#x2F;tmp&#x2F;mysql.socksymbolic-links&#x3D;0#服务器IDserver-id&#x3D;169#二进制日志文件名log-bin&#x3D;master-bin#强烈建议，其他格式可能造成数据不一致binlog_format &#x3D; row#是否记录从服务器同步数据动作log-slave-updates &#x3D; 1#启用gitd功能gtid-mode &#x3D; on#开启强制GTID一致性enforce-gtid-consistency &#x3D; 1#记录IO线程读取已经读取到的master binlog位置，用于slave宕机后IO线程根据文件中的POS点重新拉取binlog日志master-info-repository &#x3D; TABLE#记录SQL线程读取Master binlog的位置，用于slave宕机后根据文件中记录的pos点恢复Sql线程relay-log-info-repository &#x3D; TABLE#启用确保无信息丢失；任何一个事务提交后, 将二进制日志的文件名及事件位置记录到文件中sync-master-info &#x3D; 1#设定从服务器的复制线程数；0表示关闭多线程复制功能slave-parallel-workers &#x3D; 2#设置binlog校验算法（循环冗余校验码）binlog-checksum &#x3D; CRC32#设置主服务器是否校验master-verify-checksum &#x3D; 1#设置从服务器是否校验slave-sql-verify-checksum &#x3D; 1#用于在二进制日志记录事件相关的信息，可降低故障排除的复杂度binlog-rows-query-log_events &#x3D; 1#保证master crash safe，该参数必须设置为1sync_binlog &#x3D; 1#保证master crash safe，该参数必须设置为1innodb_flush_log_at_trx_commit &#x3D; 1\n\n从库配置[mysqld]server_id &#x3D; 32log-bin&#x3D;mysql-binbinlog_format &#x3D; rowgtid-mode &#x3D; onenforce-gtid-consistency &#x3D; 1master-info-repository &#x3D; TABLErelay-log-info-repository &#x3D; TABLEsync-master-info &#x3D; 1slave-parallel-workers &#x3D; 4binlog-checksum &#x3D; CRC32master-verify-checksum &#x3D; 1slave-sql-verify-checksum &#x3D; 1binlog-rows-query-log_events &#x3D; 1#sync_binlog &#x3D; 1#innodb_flush_log_at_trx_commit &#x3D; 1log-slave-updates &#x3D; 0 \t\t\t\t# crash safe slave 5.6版本需要开启relay_log_recovery &#x3D; 1  \t\t\t# crash safe slaveread_only&#x3D;on        \t\t\t\t#设置一般用户为只读模式super_read_only&#x3D;on      \t\t\t#设置super（root）用户为只读模式#tx_read_only&#x3D;on     \t\t\t\t#设置事务为只读模式\n\n主库权限设置mysql &gt; grant replication slave on *.* to slave@&#39;192.168.11.32&#39; identified by &#39;slave123&#39;;mysql &gt; flush privileges;\n\n自动同步连接主库(方法一)适用于master也是新建不久的情况。\n\n如果你的master所有的binlog还在。可以安装slave，slave直接change master to到master端。\n原理是直接获取master所有的GTID并执行。\n优点：简单方便。\n缺点：如果binlog太多，数据完全同步需要时间较长，并且master一开始就启用了GTUD。\n\nchange master tomaster_host&#x3D;&#39;192.168.11.31&#39;,master_user&#x3D;&#39;slave&#39;,master_password&#x3D;&#39;slave123&#39;,master_port&#x3D;3306,master_auto_position&#x3D;1#master_auto_position&#x3D;1 从库自动找同步点\n\n备份导入连接主库(方法二)\nXtrabackup_binlog_info文件中，包含global.gtid_purged=&#39;XXXXXX:XXXX&#39;的信息。\n然后到slave去手工的 SET @@GLOBAL.GTID_PURGED=&#39;XXXXXX:XXXX&#39;。\n恢复备份，开启change master to 命令。\n\n备份导入连接主库(方法三)适用于拥有较大数据的情况。（推荐）\n\n通过master或者其他slave的备份搭建新的slave。\n原理：获取master的数据和这些数据对应的GTID范围，然后通过slave设置master_auto_position=1,自动同步，跳过备份包含的gtid。\n缺点：相对来说有点复杂。\n\n将主库设为只读模式注：生产环境会影响不能写入数据\nmysql&gt; flush tables with read lock;Query OK, 0 rows affected (0.00 sec)mysql&gt; set global read_only&#x3D;on;Query OK, 0 rows affected (0.00 sec)\n\n主库使用mysqldump导出可以同时导出多个数据库，如music、record\nmysqldump --databases &lt;数据库名&gt;  --single-transaction --order-by-primary -r &lt;备份文件名&gt; --routines -h&lt;服务器地址&gt;  -P&lt;端口号&gt; -u&lt;用户名&gt; -p&lt;密码&gt;mysqldump --default-character-set&#x3D;utf8mb4 --single-transaction --triggers --routines --events --hex-blob --databases muisc record &gt; music_record.sql\n\n记录GTID_PURGED\ngrep -r &quot;GLOBAL.GTID_PURGED&quot; music_record.sqlSET @@GLOBAL.GTID_PURGED&#x3D;&#39;3cdb9ce6-0d7e-11e8-abe4-001517b5a5f0:1-698887&#39;;\n\n注意：mysql服务器内置的库包括mysql库和test库不需要导出。\n\n将主库设为可读写模式数据库导出完成后将主库重新设为可读写模式。\nmysql&gt; set global read_only&#x3D;off;mysql&gt; unlock tables;\n\n从库数据导入#mysql&gt; create database &#96;music&#96;;#mysql -u root -p muisc &lt; &#x2F;root&#x2F;music.sqlmysql -u root -p &lt; &#x2F;root&#x2F;music_record.sqlmysql&gt; reset slave all;mysql&gt; reset master;mysql&gt; SET @@GLOBAL.GTID_PURGED&#x3D;&#39;3cdb9ce6-0d7e-11e8-abe4-001517b5a5f0:1-698887&#39;;\n\n从库连接主库change master tomaster_host&#x3D;&#39;192.168.11.31&#39;,master_user&#x3D;&#39;slave&#39;,master_password&#x3D;&#39;slave123&#39;,master_port&#x3D;3306,master_auto_position&#x3D;1;\n\n从库启动复制线程mysql&gt; start slave;\n\n从库查看复制状态mysql&gt; show slave status\\G;*************************** 1. row ***************************               Slave_IO_State: Waiting for master to send event                  Master_Host: 192.168.11.31                  Master_User: slave                  Master_Port: 3306                Connect_Retry: 60              Master_Log_File: master-bin.000002          Read_Master_Log_Pos: 149375983               Relay_Log_File: db2-relay-bin.000002                Relay_Log_Pos: 321        Relay_Master_Log_File: master-bin.000002             Slave_IO_Running: Yes            Slave_SQL_Running: Yes              Replicate_Do_DB:          Replicate_Ignore_DB:           Replicate_Do_Table:       Replicate_Ignore_Table:      Replicate_Wild_Do_Table:  Replicate_Wild_Ignore_Table:                   Last_Errno: 0                   Last_Error:                 Skip_Counter: 0          Exec_Master_Log_Pos: 149375983              Relay_Log_Space: 526              Until_Condition: None               Until_Log_File:                Until_Log_Pos: 0           Master_SSL_Allowed: No           Master_SSL_CA_File:           Master_SSL_CA_Path:              Master_SSL_Cert:            Master_SSL_Cipher:               Master_SSL_Key:        Seconds_Behind_Master: 0Master_SSL_Verify_Server_Cert: No                Last_IO_Errno: 0                Last_IO_Error:               Last_SQL_Errno: 0               Last_SQL_Error:  Replicate_Ignore_Server_Ids:             Master_Server_Id: 31                  Master_UUID: 834449ff-4487-11e8-8b27-000c294b06ca             Master_Info_File: mysql.slave_master_info                    SQL_Delay: 0          SQL_Remaining_Delay: NULL      Slave_SQL_Running_State: Slave has read all relay log; waiting for more updates           Master_Retry_Count: 86400                  Master_Bind:      Last_IO_Error_Timestamp:     Last_SQL_Error_Timestamp:               Master_SSL_Crl:           Master_SSL_Crlpath:           Retrieved_Gtid_Set:            Executed_Gtid_Set:                Auto_Position: 0         Replicate_Rewrite_DB:                 Channel_Name:           Master_TLS_Version:1 row in set (0.00 sec)ERROR:No query specified\n\n检查主从复制通信状态\nSlave_IO_State #从站的当前状态Slave_IO_Running： Yes #读取主程序二进制日志的I/O线程是否正在运行Slave_SQL_Running： Yes #执行读取主服务器中二进制日志事件的SQL线程是否正在运行。与I/O线程一样Seconds_Behind_Master #是否为0，0就是已经同步了\n如果再次查询状态仍然 发现Slave_IO_Running 或者Slave_SQL_Running 不同时为YES,尝试执行\nmysql&gt; stop slave;mysql&gt; reset slave;mysql&gt; start slave;\n\n主库查看状态mysql&gt; show master status;+-------------------+-----------+--------------+------------------+--------------------------------------------+| File              | Position  | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set                          |+-------------------+-----------+--------------+------------------+--------------------------------------------+| master-bin.000002 | 149375983 |              |                  | 834449ff-4487-11e8-8b27-000c294b06ca:1-254 |+-------------------+-----------+--------------+------------------+--------------------------------------------+1 row in set (0.00 sec)mysql&gt; show slave hosts;+-----------+------+------+-----------+--------------------------------------+| Server_id | Host | Port | Master_id | Slave_UUID                           |+-----------+------+------+-----------+--------------------------------------+|        32 |      | 3306 |        31 | 68303133-4489-11e8-84e9-000c293eaee6 |+-----------+------+------+-----------+--------------------------------------+1 row in set (0.00 sec)mysql&gt; show global variables like &#39;%gtid%&#39;;+----------------------------------+--------------------------------------------+| Variable_name                    | Value                                      |+----------------------------------+--------------------------------------------+| binlog_gtid_simple_recovery      | ON                                         || enforce_gtid_consistency         | ON                                         || gtid_executed                    | 834449ff-4487-11e8-8b27-000c294b06ca:1-255 || gtid_executed_compression_period | 1000                                       || gtid_mode                        | ON                                         || gtid_owned                       |                                            || gtid_purged                      |                                            || session_track_gtids              | OFF                                        |+----------------------------------+--------------------------------------------+8 rows in set (0.00 sec)\n\n其他命令mysql&gt; show binlog events;mysql&gt; show binlog events in &#39;master-bin.000001&#39;;mysql&gt; show master logs;mysql&gt; show processlistmysql&gt; show full processlist;\n\nGTID与crash safe slave查看错误\nmysql&gt; select * from performance_schema.replication_applier_status_by_worker where LAST_ERROR_NUMBER&#x3D;1007\\G;\n\nhttps://docs.azure.cn/zh-cn/mysql/mysql-database-data-replicationGTID原理和一些问题解答MySQL 5.7 Replication 相关新功能说明\n","tags":["MySQL","数据库"]},{"title":"Redis安装部署","url":"/2020/05/25/Redis%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/","content":"\n相关文章：\nhttps://www.cnblogs.com/shook/p/12883742.html\nhttps://redis.io/documentation\n\nWindow\n下载地址\n\n启动\nredis-server.exe redis.windows.conf\n\nLinux\n测试环境：Linux version 3.10.0-1062.7.1.el7.x86_64 (mockbuild@kbuilder.bsys.centos.org) (gcc version 4.8.5 20150623 (Red Hat 4.8.5-39) (GCC) ) #1 SMP Mon Dec 2 17:33:29 UTC 2019\n下载地址\n\n准备环境yum install -y gccyum install -y gcc-c++yum install -y tcl\n\n下载安装\n注意：这里选择redis-5.0.8这个版本可以编译成功，如果编译高版本失败则可能需要升级gcc版本重新编译\n\nwget http://download.redis.io/releases/redis-5.0.8.tar.gztar xzf redis-5.0.8.tar.gzcd redis-5.0.8make\n\n编译成功make[2]: 离开目录“/opt/redis-5.0.8/deps”    CC adlist.o    CC quicklist.o    CC ae.o    CC anet.o    CC dict.o    CC server.o    CC sds.o    CC zmalloc.o    CC lzf_c.o    CC lzf_d.o    CC pqsort.o    CC zipmap.o    CC sha1.o    CC ziplist.o    CC release.o    CC networking.o    CC util.o    CC object.o    CC db.o    CC replication.o    CC rdb.o    CC t_string.o    CC t_list.o    CC t_set.o    CC t_zset.o    CC t_hash.o    CC config.o    CC aof.o    CC pubsub.o    CC multi.o    CC debug.o    CC sort.o    CC intset.o    CC syncio.o    CC cluster.o    CC crc16.o    CC endianconv.o    CC slowlog.o    CC scripting.o    CC bio.o    CC rio.o    CC rand.o    CC memtest.o    CC crc64.o    CC bitops.o    CC sentinel.o    CC notify.o    CC setproctitle.o    CC blocked.o    CC hyperloglog.o    CC latency.o    CC sparkline.o    CC redis-check-rdb.o    CC redis-check-aof.o    CC geo.o    CC lazyfree.o    CC module.o    CC evict.o    CC expire.o    CC geohash.o    CC geohash_helper.o    CC childinfo.o    CC defrag.o    CC siphash.o    CC rax.o    CC t_stream.o    CC listpack.o    CC localtime.o    CC lolwut.o    CC lolwut5.o    LINK redis-server    INSTALL redis-sentinel    CC redis-cli.o    LINK redis-cli    CC redis-benchmark.o    LINK redis-benchmark    INSTALL redis-check-rdb    INSTALL redis-check-aofHint: It's a good idea to run 'make test' ;)make[1]: 离开目录“/opt/redis-5.0.8/src”\n\n编译错误erver.c:5152:44: 错误：‘struct redisServer’没有名为‘tlsfd_count’的成员         if (server.ipfd_count &gt; 0 || server.tlsfd_count &gt; 0)                                            ^server.c:5154:19: 错误：‘struct redisServer’没有名为‘sofd’的成员         if (server.sofd &gt; 0)                   ^server.c:5155:94: 错误：‘struct redisServer’没有名为‘unixsocket’的成员             serverLog(LL_NOTICE,\"The server is now ready to accept connections at %s\", server.unixsocket);                                                                                              ^server.c:5156:19: 错误：‘struct redisServer’没有名为‘supervised_mode’的成员         if (server.supervised_mode == SUPERVISED_SYSTEMD) &#123;                   ^server.c:5157:24: 错误：‘struct redisServer’没有名为‘masterhost’的成员             if (!server.masterhost) &#123;                        ^server.c:5170:15: 错误：‘struct redisServer’没有名为‘maxmemory’的成员     if (server.maxmemory &gt; 0 &amp;&amp; server.maxmemory &lt; 1024*1024) &#123;               ^server.c:5170:39: 错误：‘struct redisServer’没有名为‘maxmemory’的成员     if (server.maxmemory &gt; 0 &amp;&amp; server.maxmemory &lt; 1024*1024) &#123;                                       ^server.c:5171:176: 错误：‘struct redisServer’没有名为‘maxmemory’的成员         serverLog(LL_WARNING,\"WARNING: You specified a maxmemory value that is less than 1MB (current value is %llu bytes). Are you sure this is what you really want?\", server.maxmemory);                                                                                                                                                                                ^server.c:5174:31: 错误：‘struct redisServer’没有名为‘server_cpulist’的成员     redisSetCpuAffinity(server.server_cpulist);                               ^server.c: 在函数‘hasActiveChildProcess’中:server.c:1476:1: 警告：在有返回值的函数中，控制流程到达函数尾 [-Wreturn-type] &#125; ^server.c: 在函数‘allPersistenceDisabled’中:server.c:1482:1: 警告：在有返回值的函数中，控制流程到达函数尾 [-Wreturn-type] &#125; ^server.c: 在函数‘writeCommandsDeniedByDiskError’中:server.c:3789:1: 警告：在有返回值的函数中，控制流程到达函数尾 [-Wreturn-type] &#125; ^server.c: 在函数‘iAmMaster’中:server.c:4966:1: 警告：在有返回值的函数中，控制流程到达函数尾 [-Wreturn-type] &#125; ^make[1]: *** [server.o] 错误 1make[1]: 离开目录“/opt/redis-6.0.3/src”make: *** [all] 错误 2\n\n升级gcc版本# 查看gcc版本gcc -v# 升级到9.1版本yum -y install centos-release-scl# 临时启用yum -y install devtoolset-9-gcc devtoolset-9-gcc-c++ devtoolset-9-binutilsscl enable devtoolset-9 bash# 如果要长期使用gcc 9.1的话：echo \"source /opt/rh/devtoolset-9/enable\" &gt;&gt;/etc/profile\n\n运行测试make test\n\n正常显示如下：\nExecution time of different units:  0 seconds - unit/printver  0 seconds - unit/type/incr  1 seconds - unit/auth  1 seconds - unit/keyspace  1 seconds - unit/protocol  1 seconds - unit/quit  3 seconds - unit/multi  4 seconds - unit/type/stream-cgroups  12 seconds - unit/type/hash  13 seconds - unit/other  15 seconds - unit/expire  15 seconds - unit/type/list  17 seconds - unit/type/string  19 seconds - unit/scan  20 seconds - unit/type/set  3 seconds - integration/rdb  3 seconds - integration/convert-zipmap-hash-on-load  1 seconds - integration/logging  7 seconds - integration/aof  1 seconds - unit/pubsub  26 seconds - unit/sort  3 seconds - unit/slowlog  29 seconds - unit/type/zset  1 seconds - unit/introspection  28 seconds - integration/block-repl  1 seconds - unit/limits  11 seconds - unit/scripting  7 seconds - unit/introspection-2  41 seconds - unit/type/list-2  4 seconds - unit/bitfield  33 seconds - integration/replication-2  25 seconds - integration/psync2-reg  13 seconds - unit/bitops  2 seconds - unit/lazyfree  55 seconds - unit/dump  32 seconds - integration/psync2  57 seconds - unit/type/stream  8 seconds - unit/wait  45 seconds - integration/replication-4  11 seconds - unit/pendingquerybuf  23 seconds - unit/geo  71 seconds - integration/replication-3  99 seconds - unit/aofrw  82 seconds - unit/maxmemory  71 seconds - unit/memefficiency  118 seconds - unit/type/list-3  107 seconds - integration/replication-psync  79 seconds - unit/hyperloglog  156 seconds - integration/replication  385 seconds - unit/obuf-limits\\o/ All tests passed without errors!\n\n启动src/redis-server [redis.conf]\n\n成功启动显示：\n26876:C 26 May 2020 09:56:57.357 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo26876:C 26 May 2020 09:56:57.357 # Redis version=5.0.8, bits=64, commit=00000000, modified=0, pid=26876, just started26876:C 26 May 2020 09:56:57.357 # Warning: no config file specified, using the default config. In order to specify a config file use src/redis-server /path/to/redis.conf26876:M 26 May 2020 09:56:57.358 * Increased maximum number of open files to 10032 (it was originally set to 1024).                _._           _.-``__ ''-._      _.-``    `.  `_.  ''-._           Redis 5.0.8 (00000000/0) 64 bit  .-`` .-```.  ```\\/    _.,_ ''-._ (    '      ,       .-`  | `,    )     Running in standalone mode |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379 |    `-._   `._    /     _.-'    |     PID: 26876  `-._    `-._  `-./  _.-'    _.-' |`-._`-._    `-.__.-'    _.-'_.-'| |    `-._`-._        _.-'_.-'    |           http://redis.io  `-._    `-._`-.__.-'_.-'    _.-' |`-._`-._    `-.__.-'    _.-'_.-'| |    `-._`-._        _.-'_.-'    |  `-._    `-._`-.__.-'_.-'    _.-'      `-._    `-.__.-'    _.-'          `-._        _.-'              `-.__.-'26876:M 26 May 2020 09:56:57.362 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.26876:M 26 May 2020 09:56:57.362 # Server initialized26876:M 26 May 2020 09:56:57.362 # WARNING overcommit_memory is set to 0! Background save may fail under low memory condition. To fix this issue add 'vm.overcommit_memory = 1' to /etc/sysctl.conf and then reboot or run the command 'sysctl vm.overcommit_memory=1' for this to take effect.26876:M 26 May 2020 09:56:57.362 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.26876:M 26 May 2020 09:56:57.363 * Ready to accept connections\n\n后台启动vim redis.confdaemonize no# 改成daemonize yes\n\n启动：\n[root@zhang redis-5.0.8]# src/redis-server redis.conf29472:C 26 May 2020 10:01:41.246 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo29472:C 26 May 2020 10:01:41.246 # Redis version=5.0.8, bits=64, commit=00000000, modified=0, pid=29472, just started29472:C 26 May 2020 10:01:41.246 # Configuration loaded\n\n停止：\nps axu|grep rediskill-9 29472\n\n创建服务vim /usr/lib/systemd/system/redis.service\n\n[Unit]Description=The redis-server Process ManagerAfter=syslog.target network.target[Service]Type=forkingPIDFile=/var/run/redis_6379.pidExecStart=/opt/redis-5.0.8/src/redis-server /opt/redis-5.0.8/redis.conf[Install]WantedBy=multi-user.target\n\nsystemctl enable redis.servicesystemctl start redis.service\n\nDocker待完善\n常见问题如何远程访问修改配置文件\nbind 127.0.0.1\n","tags":["Redis"]},{"title":"二叉树详细介绍","url":"/2020/02/04/%E4%BA%8C%E5%8F%89%E6%A0%91%E8%AF%A6%E7%BB%86%E4%BB%8B%E7%BB%8D/","content":"基本概念二叉树是一种树形结构，它的特点是每个结点至多只有两棵子树（即二叉树中不存在度大于2的结点），并且二叉树的子树有左右之分，其次序不能任意颠倒。\n度：结点拥有的子树数称为结点的度。\n终端结点：度为0的结点称为叶子或者终端结点。\n深度：树中结点的最大层次称为树的深度或高度。\n完全二叉树：可以对满二叉树的结点进行连续编号，约定编号从根结点起，自上而下，从左到右。深度为$k$的，有$n$个结点的二叉树，当且仅当其每一个结点都与深度为$k$的满二叉树编号从$1$至$n$一一对应时，称之为完全二叉树。\n性质\n在二叉树的第$i$层上至多有$2^{i-1}$个结点($i \\geq 1$)。\n深度为$k$的二叉树至多有$2^k-1$个结点($k \\geq 1$)。如果是$2^k-1$则为满二叉树。\n对于任何一颗二叉树$T$，如果其终端结点树为$n_0$，度为2的结点树为$n_2$，则$n_0 = n_2 +1$\n具有$n$个结点的完全二叉树，其深度k满足$ \\log_{2}n&lt; k \\leq \\log_2n+1$，$k$的值其实就是$\\log_2n+1$向下取整\n如果对一颗有$n$个结点的完全二叉树的结点按层序编号，则对任一结点$i$($1\\leq i \\leq n$)，有\n如果$i = 1$，则结点是二叉树的根，无双亲；如果$i &gt; 1$，则其双亲结点$i/2$取整。\n如果$2i &gt; n$，则结点$i$无左孩子（结点$i$为叶子结点）；否则其左孩子是结点$2i$。\n如果$2i+1 &gt; n$，则结点无右孩子；否则其右孩子结点$2i+1$\n\n\n\n存储结构\n顺序存储结构\n链式存储结构\n\n遍历二叉树\n学习网站： https://www.cs.usfca.edu/~galles/visualization/Algorithms.html\n\n假如以L、D、R分别表示遍历左子树、访问根结点、遍历右子树，则可有DLR、LDR、LRD、DRL、RDL、RLD这6种遍历方式。若限定先左后右，则只有三种情况，分别称之为先（根）序遍历、中（根）序遍历、后（根）序遍历。基于二叉树的递归定义，可得下述遍历二叉树的递归算法定义。\n\n先序遍历二叉树的操作定义为：\n若二叉树为空，则空操作；否则\n\n\n\n访问根结点；\n先序遍历左子树；\n先序遍历右子树。\n\n\n中序遍历二叉树的操作定义为：\n若二叉树为空，则空操作；否则\n\n\n\n中序遍历左子树；\n访问根结点；\n中序遍历右子树。\n\n\n后序遍历二叉树的操作定义为：\n若二叉树为空，则空操作；否则\n\n\n\n后序遍历左子树；\n后序遍历右子树；\n访问根结点。\n\n定义二叉树public class TreeNode &#123;    int val;    TreeNode left;    TreeNode right;    TreeNode(int x) &#123;        val = x;    &#125;    public static TreeNode create(Integer[] nums, int index) &#123;        TreeNode top = null;        if (index &lt; nums.length) &#123;            Integer value = nums[index];            if (value == null) &#123;                return null;            &#125;            top = new TreeNode(value);            top.left = create(nums, index * 2 + 1);            top.right = create(nums, index * 2 + 2);            return top;        &#125;        return top;    &#125;&#125;\n\n深度优先搜索（DFS）/**  * 前序遍历 中--左--右(DLR)  * @param root  */public static void preorderTraversal(TreeNode root) &#123;    if (root != null) &#123;        System.out.println(root.getData() + \"-&gt;\");        preorderTraversal(root.getLeftNode());        preorderTraversal(root.getRightNode());    &#125;&#125;/**  * 中序遍历 左--中--右(LDR)  * @param root  */public static void inorderTraversal(TreeNode root) &#123;    if (root != null) &#123;        inorderTraversal(root.getLeftNode());        System.out.println(root.getData() + \"-&gt;\");        inorderTraversal(root.getRightNode());    &#125;&#125;/**  * 后序遍历 左--右--中(LRD)  * @param root  */public static void postorderTraversal(TreeNode root) &#123;    if (root != null) &#123;        postorderTraversal(root.getLeftNode());        postorderTraversal(root.getRightNode());        System.out.println(root.getData() + \"-&gt;\");    &#125;&#125;\n\n宽度优先搜索（BFS）\n相关：https://leetcode-cn.com/problems/binary-tree-level-order-traversal/solution/er-cha-shu-de-ceng-ci-bian-li-by-leetcode/\n\nclass Solution &#123;    List&lt;List&lt;Integer&gt;&gt; levels = new ArrayList&lt;List&lt;Integer&gt;&gt;();    public void helper(TreeNode node, int level) &#123;        // start the current level        if (levels.size() == level)            levels.add(new ArrayList&lt;Integer&gt;());        // fulfil the current level        levels.get(level).add(node.val);        // process child nodes for the next level        if (node.left != null)            helper(node.left, level + 1);        if (node.right != null)            helper(node.right, level + 1);    &#125;    public List&lt;List&lt;Integer&gt;&gt; levelOrder(TreeNode root) &#123;        if (root == null) return levels;        helper(root, 0);        return levels;    &#125;&#125;\n\nclass Solution &#123;    public List&lt;List&lt;Integer&gt;&gt; levelOrder(TreeNode root) &#123;        List&lt;List&lt;Integer&gt;&gt; levels = new ArrayList&lt;List&lt;Integer&gt;&gt;();        if (root == null) return levels;        Queue&lt;TreeNode&gt; queue = new LinkedList&lt;TreeNode&gt;();        queue.add(root);        int level = 0;        while ( !queue.isEmpty() ) &#123;            // start the current level            levels.add(new ArrayList&lt;Integer&gt;());            // number of elements in the current level            int level_length = queue.size();            for(int i = 0; i &lt; level_length; ++i) &#123;                TreeNode node = queue.remove();                // fulfill the current level                levels.get(level).add(node.val);                // add child nodes of the current level                // in the queue for the next level                if (node.left != null) queue.add(node.left);                if (node.right != null) queue.add(node.right);            &#125;            // go to next level            level++;        &#125;        return levels;    &#125;&#125;\n\n遍历图解\n\n旋转什么是旋转\n\n\n左旋：是以节点的”右分支”为轴，进行逆时针旋转。我们将左旋操作定义为 left_rotate.\n右旋：是以节点的“左分支”为轴，进行顺时针旋转。我们将右旋操作定义为 right_rotate.\n\n为什么要旋转在解释这个道理之前，我们先看看执行旋转后，二叉树中节点的深度有什么变化。在上图中，二叉树执行左旋后，a 分支所有节点的深度比以前多 1，b 分支保持不变，c 分支所有节点比以前少 1.\n这就意味着，通过合适的左旋和右旋操作，我们可以调整二叉树的深度。另一方面，通过合适的左旋和右旋，我们可以把二叉树变换成任意的形状！\n\n\n如上图，如何把二叉树通过若干次左旋和右旋操作变换成链，答案：\nleft_rotate(4);right_rotate(10);right_rotate(8);right_rotate(5);right_rotate(4);right_rotate(2);\n\n旋转算法\n定义二叉树结构\n\npublic class TreeNode &#123;    int val;    TreeNode left;    TreeNode right;    TreeNode parent;&#125;\n\n\n旋转\n\npackage example;public class TreeUtil &#123;    public static TreeNode leftRotate(TreeNode node) &#123;        TreeNode root = node.parent;        TreeNode x = node;        TreeNode y = node.right;        TreeNode b = node.right.left;        if (root.left == node) &#123;            root.left = y;        &#125; else &#123;            root.right = y;        &#125;        y.parent = root;        x.right = b;        b.parent = x;        y.left = x;        x.parent = y;        return y;    &#125;    public static TreeNode rightRotate(TreeNode node) &#123;        TreeNode root = node.parent;        TreeNode y = node;        TreeNode x = node.left;        TreeNode b = node.left.right;        if (root.left == node) &#123;            root.left = x;        &#125; else &#123;            root.right = x;        &#125;        x.parent = root;        y.left = b;        b.parent = y;        x.right = y;        y.parent = x;        return x;    &#125;    public static void main(String[] args) &#123;        TreeNode xx = new TreeNode(\"xx\");        TreeNode x = new TreeNode(\"x\");        TreeNode y = new TreeNode(\"y\");        TreeNode a = new TreeNode(\"a\");        TreeNode b = new TreeNode(\"b\");        TreeNode c = new TreeNode(\"c\");        xx.left = x;        x.parent = xx;        x.left = a;        a.parent = x;        x.right = y;        y.parent = x;        y.left = b;        b.parent = y;        y.right = c;        c.parent = y;        x = TreeUtil.leftRotate(x);        x = TreeUtil.rightRotate(x);        System.out.println(\"end\");    &#125;&#125;\n","tags":["算法相关"]},{"title":"Spring源码详解","url":"/2020/05/25/Spring%E6%BA%90%E7%A0%81%E8%AF%A6%E8%A7%A3/","content":"Spring源码详解AOP\n相关文章：https://blog.csdn.net/wyl6019/article/details/80136000\n\nXML注册\n@Aspect注册\n\ncontextInitialized:103, ContextLoaderListener\ninitWebApplicationContext:291, ContextLoader\nconfigureAndRefreshWebApplicationContext:400, ContextLoader\nrefresh:549, AbstractApplicationContext\nfinishBeanFactoryInitialization:877, AbstractApplicationContext\npreInstantiateSingletons:830, DefaultListableBeanFactory\ngetBean:199, AbstractBeanFactory\ndoGetBean:318, AbstractBeanFactory\ngetSingleton:222, DefaultSingletonBeanRegistry\ngetObject:-1, 889007824\nlambda$doGetBean$0:320, AbstractBeanFactory\ncreateBean:515, AbstractAutowireCapableBeanFactory\ndoCreateBean:593, AbstractAutowireCapableBeanFactory\ninitializeBean:1766, AbstractAutowireCapableBeanFactory\napplyBeanPostProcessorsAfterInitialization:429, AbstractAutowireCapableBeanFactory\npostProcessAfterInitialization:296, AbstractAutoProxyCreator\nwrapIfNecessary:335, AbstractAutoProxyCreator\ncreateProxy:471, AbstractAutoProxyCreator\ncreateProxy:471, AbstractAutoProxyCreator\ngetProxy:160, CglibAopProxy\n\npublic Object getProxy(@Nullable ClassLoader classLoader) &#123;    if (logger.isTraceEnabled()) &#123;        logger.trace(\"Creating CGLIB proxy: \" + this.advised.getTargetSource());    &#125;    try &#123;        Class&lt;?&gt; rootClass = this.advised.getTargetClass();        Assert.state(rootClass != null, \"Target class must be available for creating a CGLIB proxy\");        Class&lt;?&gt; proxySuperClass = rootClass;        if (ClassUtils.isCglibProxyClass(rootClass)) &#123;            proxySuperClass = rootClass.getSuperclass();            Class&lt;?&gt;[] additionalInterfaces = rootClass.getInterfaces();            for (Class&lt;?&gt; additionalInterface : additionalInterfaces) &#123;                this.advised.addInterface(additionalInterface);            &#125;        &#125;        // Validate the class, writing log messages as necessary.        validateClassIfNecessary(proxySuperClass, classLoader);        // Configure CGLIB Enhancer...        Enhancer enhancer = createEnhancer();        if (classLoader != null) &#123;            enhancer.setClassLoader(classLoader);            if (classLoader instanceof SmartClassLoader &amp;&amp;                    ((SmartClassLoader) classLoader).isClassReloadable(proxySuperClass)) &#123;                enhancer.setUseCache(false);            &#125;        &#125;        enhancer.setSuperclass(proxySuperClass);        enhancer.setInterfaces(AopProxyUtils.completeProxiedInterfaces(this.advised));        enhancer.setNamingPolicy(SpringNamingPolicy.INSTANCE);        enhancer.setStrategy(new ClassLoaderAwareUndeclaredThrowableStrategy(classLoader));        // 获取Callback派生类，包含@After、@Before等        Callback[] callbacks = getCallbacks(rootClass);        Class&lt;?&gt;[] types = new Class&lt;?&gt;[callbacks.length];        for (int x = 0; x &lt; types.length; x++) &#123;            types[x] = callbacks[x].getClass();        &#125;        // fixedInterceptorMap only populated at this point, after getCallbacks call above        enhancer.setCallbackFilter(new ProxyCallbackFilter(                this.advised.getConfigurationOnlyCopy(), this.fixedInterceptorMap, this.fixedInterceptorOffset));        enhancer.setCallbackTypes(types);        // Generate the proxy class and create a proxy instance.        return createProxyClassAndInstance(enhancer, callbacks);    &#125;    catch (CodeGenerationException | IllegalArgumentException ex) &#123;        throw new AopConfigException(\"Could not generate CGLIB subclass of \" + this.advised.getTargetClass() +                \": Common causes of this problem include using a final class or a non-visible class\",                ex);    &#125;    catch (Throwable ex) &#123;        // TargetSource.getTarget() failed        throw new AopConfigException(\"Unexpected AOP exception\", ex);    &#125;&#125;\n\n// Create proxy if we have advice.Object[] specificInterceptors = getAdvicesAndAdvisorsForBean(bean.getClass(), beanName, null);if (specificInterceptors != DO_NOT_PROXY) &#123;    this.advisedBeans.put(cacheKey, Boolean.TRUE);    Object proxy = createProxy(            bean.getClass(), beanName, specificInterceptors, new SingletonTargetSource(bean));    this.proxyTypes.put(cacheKey, proxy.getClass());    return proxy;&#125;\n\n执行\nintercept:688, CglibAopProxy$DynamicAdvisedInterceptor\nproceed:162, ReflectiveMethodInvocation\n\npublic Object proceed() throws Throwable &#123;    //\tWe start with an index of -1 and increment early.    if (this.currentInterceptorIndex == this.interceptorsAndDynamicMethodMatchers.size() - 1) &#123;        return invokeJoinpoint();    &#125;    Object interceptorOrInterceptionAdvice =            this.interceptorsAndDynamicMethodMatchers.get(++this.currentInterceptorIndex);    if (interceptorOrInterceptionAdvice instanceof InterceptorAndDynamicMethodMatcher) &#123;        // Evaluate dynamic method matcher here: static part will already have        // been evaluated and found to match.        InterceptorAndDynamicMethodMatcher dm =                (InterceptorAndDynamicMethodMatcher) interceptorOrInterceptionAdvice;        Class&lt;?&gt; targetClass = (this.targetClass != null ? this.targetClass : this.method.getDeclaringClass());        if (dm.methodMatcher.matches(this.method, targetClass, this.arguments)) &#123;            return dm.interceptor.invoke(this);        &#125;        else &#123;            // Dynamic matching failed.            // Skip this interceptor and invoke the next in the chain.            return proceed();        &#125;    &#125;    else &#123;        // It's an interceptor, so we just invoke it: The pointcut will have        // been evaluated statically before this object was constructed.        // 调用机制相当于实现了一个chain，把this传给下一个Callback，注意获取的时候++this.currentInterceptorIndex        return ((MethodInterceptor) interceptorOrInterceptionAdvice).invoke(this);    &#125;&#125;\n\n处理请求注册\nrefresh:549, AbstractApplicationContext\nfinishBeanFactoryInitialization:877, AbstractApplicationContext\npreInstantiateSingletons:849, DefaultListableBeanFactory\ngetBean:199, AbstractBeanFactory\ndoGetBean:318, AbstractBeanFactory\ngetSingleton:222, DefaultSingletonBeanRegistry\ngetObject:-1, 911637678\nlambda$doGetBean$0:320, AbstractBeanFactory\ncreateBean:515, AbstractAutowireCapableBeanFactory\ndoCreateBean:593, AbstractAutowireCapableBeanFactory\ninitializeBean:1758, AbstractAutowireCapableBeanFactory\ninvokeInitMethods:1821, AbstractAutowireCapableBeanFactory\nafterPropertiesSet:164, RequestMappingHandlerMapping\nafterPropertiesSet:199, AbstractHandlerMethodMapping\ninitHandlerMethods:211, AbstractHandlerMethodMapping\n\n\ngetCandidateBeanNames()获取到需要初始化的bean,processCandidateBean(beanName)方法对这些bean进行过滤\n\nprotected void initHandlerMethods() &#123;    for (String beanName : getCandidateBeanNames()) &#123;        if (!beanName.startsWith(SCOPED_TARGET_NAME_PREFIX)) &#123;            processCandidateBean(beanName);        &#125;    &#125;    handlerMethodsInitialized(getHandlerMethods());&#125;\n\nprocessCandidateBean:250, AbstractHandlerMethodMapping\nisHandler(beanType)主要过滤条件,detectHandlerMethods(beanName);注册\n\nprotected void processCandidateBean(String beanName) &#123;    Class&lt;?&gt; beanType = null;    try &#123;        beanType = obtainApplicationContext().getType(beanName);    &#125;    catch (Throwable ex) &#123;        // An unresolvable bean type, probably from a lazy bean - let's ignore it.        if (logger.isTraceEnabled()) &#123;            logger.trace(\"Could not resolve type for bean '\" + beanName + \"'\", ex);        &#125;    &#125;    if (beanType != null &amp;&amp; isHandler(beanType)) &#123;        detectHandlerMethods(beanName);    &#125;&#125;\n\n符合@Controller或者@RequestMapping\n\nprotected boolean isHandler(Class&lt;?&gt; beanType) &#123;    return (AnnotatedElementUtils.hasAnnotation(beanType, Controller.class) ||            AnnotatedElementUtils.hasAnnotation(beanType, RequestMapping.class));&#125;\n\n\n\n请求\norg.springframework.web.servlet.DispatcherServlet\nservice:882, FrameworkServlet\nservice:742, HttpServlet\nservice:635, HttpServlet\n这个方法具体判断是什么请求类型\n\nprotected void service(HttpServletRequest req, HttpServletResponse resp)        throws ServletException, IOException&#123;    String method = req.getMethod();    if (method.equals(METHOD_GET)) &#123;        long lastModified = getLastModified(req);        if (lastModified == -1) &#123;            // servlet doesn't support if-modified-since, no reason            // to go through further expensive logic            doGet(req, resp);        &#125; else &#123;            long ifModifiedSince = req.getDateHeader(HEADER_IFMODSINCE);            if (ifModifiedSince &lt; lastModified) &#123;                // If the servlet mod time is later, call doGet()                // Round down to the nearest second for a proper compare                // A ifModifiedSince of -1 will always be less                maybeSetLastModified(resp, lastModified);                doGet(req, resp);            &#125; else &#123;                resp.setStatus(HttpServletResponse.SC_NOT_MODIFIED);            &#125;        &#125;    &#125; else if (method.equals(METHOD_HEAD)) &#123;        long lastModified = getLastModified(req);        maybeSetLastModified(resp, lastModified);        doHead(req, resp);    &#125; else if (method.equals(METHOD_POST)) &#123;        doPost(req, resp);    &#125; else if (method.equals(METHOD_PUT)) &#123;        doPut(req, resp);    &#125; else if (method.equals(METHOD_DELETE)) &#123;        doDelete(req, resp);    &#125; else if (method.equals(METHOD_OPTIONS)) &#123;        doOptions(req,resp);    &#125; else if (method.equals(METHOD_TRACE)) &#123;        doTrace(req,resp);    &#125; else &#123;        //        // Note that this means NO servlet supports whatever        // method was requested, anywhere on this server.        //        String errMsg = lStrings.getString(\"http.method_not_implemented\");        Object[] errArgs = new Object[1];        errArgs[0] = method;        errMsg = MessageFormat.format(errMsg, errArgs);        resp.sendError(HttpServletResponse.SC_NOT_IMPLEMENTED, errMsg);    &#125;&#125;\ndoGet:897, FrameworkServlet\nprocessRequest:1005, FrameworkServlet\ndoService:942, DispatcherServlet\ndoDispatch:1014, DispatcherServlet\ngetHandler:1231, DispatcherServlet\ngetHandler:401, AbstractHandlerMapping\ngetHandlerInternal:365, AbstractHandlerMethodMapping\nAbstractHandlerMethodMapping内部使用了读写锁\nlookupHandlerMethod(lookupPath, request); 从注册好的集合中查找返回HandlerMethod调用\nmappingRegistry.urlLookup这个map保存了url和对应的方法\n\nprotected HandlerMethod getHandlerInternal(HttpServletRequest request) throws Exception &#123;    String lookupPath = getUrlPathHelper().getLookupPathForRequest(request);    this.mappingRegistry.acquireReadLock();    try &#123;        HandlerMethod handlerMethod = lookupHandlerMethod(lookupPath, request);        return (handlerMethod != null ? handlerMethod.createWithResolvedBean() : null);    &#125;    finally &#123;        this.mappingRegistry.releaseReadLock();    &#125;&#125;\n\n\n\n事务\nSpring并不直接管理事务，而是提供了事务管理接口是PlatformTransactionManager，通过这个 接口，Spring 为各个平台如JDBC、Hibernate等都提供了对应的事务管理器，但是具体的实现就是各个平台自己的 事情了。\n\n\n\n\n平台\n实现类\n\n\n\nJDBC\nDataSourceTransactionManager\n\n\nJTA\nJtaTransactionManager\n\n\nHibernate\nHibernateTransactionManager\n\n\n注册执行","tags":["Java","Spring"]},{"title":"二进制基础","url":"/2019/11/14/%E4%BA%8C%E8%BF%9B%E5%88%B6%E5%9F%BA%E7%A1%80/","content":"一. 机器数和真值在学习原码, 反码和补码之前, 需要先了解机器数和真值的概念.\n1、机器数一个数在计算机中的二进制表示形式,  叫做这个数的机器数。机器数是带符号的，在计算机用一个数的最高位存放符号, 正数为0, 负数为1.\n比如，十进制中的数 +3 ，计算机字长为8位，转换成二进制就是00000011。如果是 -3 ，就是 10000011 。\n那么，这里的 00000011 和 10000011 就是机器数。\n2、真值\n因为第一位是符号位，所以机器数的形式值就不等于真正的数值。例如上面的有符号数 10000011，其最高位1代表负，其真正数值是 -3 而不是形式值131（10000011转换成十进制等于131）。所以，为区别起见，将带符号位的机器数对应的真正数值称为机器数的真值。\n\n例：0000 0001的真值 = +000 0001 = +1，1000 0001的真值 = –000 0001 = –1\n二. 原码, 反码, 补码的基础概念和计算方法在探求为何机器要使用补码之前, 让我们先了解原码, 反码和补码的概念.对于一个数, 计算机要使用一定的编码方式进行存储. 原码, 反码, 补码是机器存储一个具体数字的编码方式.\n1. 原码原码就是符号位加上真值的绝对值, 即用第一位表示符号, 其余位表示值. 比如如果是8位二进制:\n\n[+1]原 = 0000 0001\n[-1]原 = 1000 0001\n\n第一位是符号位. 因为第一位是符号位, 所以8位二进制数的取值范围就是:\n\n[1111 1111 , 0111 1111]\n\n即\n\n[-127 , 127]\n\n原码是人脑最容易理解和计算的表示方式.\n2. 反码反码的表示方法是:\n正数的反码是其本身\n负数的反码是在其原码的基础上, 符号位不变，其余各个位取反.\n\n[+1] = [00000001]原 = [00000001]反\n[-1] = [10000001]原 = [11111110]反\n\n可见如果一个反码表示的是负数, 人脑无法直观的看出来它的数值. 通常要将其转换成原码再计算.\n3. 补码补码的表示方法是:\n正数的补码就是其本身\n负数的补码是在其原码的基础上, 符号位不变, 其余各位取反, 最后+1. (即在反码的基础上+1)\n\n[+1] = [00000001]原 = [00000001]反 = [00000001]补\n[-1] = [10000001]原 = [11111110]反 = [11111111]补\n\n对于负数, 补码表示方式也是人脑无法直观看出其数值的. 通常也需要转换成原码在计算其数值.\n4. 理解一个字节8位* 譬如一个10进制数 27* 转成二进制 00011011-27 的二进制* 先计算原码： 00011011* 反码：      11100100* 补码：      11100101\n\n三. 为何要使用原码, 反码和补码在开始深入学习前, 我的学习建议是先”死记硬背”上面的原码, 反码和补码的表示方式以及计算方法.\n现在我们知道了计算机可以有三种编码方式表示一个数. 对于正数因为三种编码方式的结果都相同:\n\n[+1] = [00000001]原 = [00000001]反 = [00000001]补\n\n所以不需要过多解释. 但是对于负数:\n\n[-1] = [10000001]原 = [11111110]反 = [11111111]补\n\n可见原码, 反码和补码是完全不同的. 既然原码才是被人脑直接识别并用于计算表示方式, 为何还会有反码和补码呢?\n首先, 因为人脑可以知道第一位是符号位, 在计算的时候我们会根据符号位, 选择对真值区域的加减. (真值的概念在本文最开头). 但是对于计算机, 加减乘数已经是最基础的运算, 要设计的尽量简单. 计算机辨别”符号位”显然会让计算机的基础电路设计变得十分复杂! 于是人们想出了将符号位也参与运算的方法. 我们知道, 根据运算法则减去一个正数等于加上一个负数, 即: 1-1 = 1 + (-1) = 0 , 所以机器可以只有加法而没有减法, 这样计算机运算的设计就更简单了.\n于是人们开始探索 将符号位参与运算, 并且只保留加法的方法. 首先来看原码:\n计算十进制的表达式: 1-1=0\n\n1 - 1 = 1 + (-1) = [00000001]原 + [10000001]原 = [10000010]原 = -2\n\n如果用原码表示, 让符号位也参与计算, 显然对于减法来说, 结果是不正确的.这也就是为何计算机内部不使用原码表示一个数.\n为了解决原码做减法的问题, 出现了反码:\n计算十进制的表达式: 1-1=0\n\n1 - 1 = 1 + (-1) = [0000 0001]原 + [1000 0001]原= [0000 0001]反 + [1111 1110]反 = [1111 1111]反 = [1000 0000]原 = -0\n\n发现用反码计算减法, 结果的真值部分是正确的. 而唯一的问题其实就出现在”0”这个特殊的数值上. 虽然人们理解上+0和-0是一样的, 但是0带符号是没有任何意义的. 而且会有[0000 0000]原和[1000 0000]原两个编码表示0.\n于是补码的出现, 解决了0的符号以及两个编码的问题:\n\n1-1 = 1 + (-1) = [0000 0001]原 + [1000 0001]原 = [0000 0001]补 + [1111 1111]补 = [0000 0000]补=[0000 0000]原\n\n这样0用[0000 0000]表示, 而以前出现问题的-0则不存在了.而且可以用[1000 0000]表示-128:\n\n(-1) + (-127) = [1000 0001]原 + [1111 1111]原 = [1111 1111]补 + [1000 0001]补 = [1000 0000]补\n\n-1-127的结果应该是-128, 在用补码运算的结果中, [1000 0000]补 就是-128. 但是注意因为实际上是使用以前的-0的补码来表示-128, 所以-128并没有原码和反码表示.(对-128的补码表示[1000 0000]补算出来的原码是[0000 0000]原, 这是不正确的)\n使用补码, 不仅仅修复了0的符号以及存在两个编码的问题, 而且还能够多表示一个最低数. 这就是为什么8位二进制, 使用原码或反码表示的范围为[-127, +127], 而使用补码表示的范围为[-128, 127].\n因为机器使用补码, 所以对于编程中常用到的32位int类型, 可以表示范围是: [-231, 231-1] 因为第一位表示的是符号位.而使用补码表示时又可以多保存一个最小值.\n四. 原码, 反码, 补码 再深入计算机巧妙地把符号位参与运算, 并且将减法变成了加法, 背后蕴含了怎样的数学原理呢?\n将钟表想象成是一个1位的12进制数. 如果当前时间是6点, 我希望将时间设置成4点, 需要怎么做呢?我们可以:\n\n\n往回拨2个小时: 6 - 2 = 4\n\n往前拨10个小时: (6 + 10) mod 12 = 4\n\n往前拨10+12=22个小时: (6+22) mod 12 =4\n\n\n\n2,3方法中的mod是指取模操作, 16 mod 12 =4 即用16除以12后的余数是4.\n所以钟表往回拨(减法)的结果可以用往前拨(加法)替代!\n现在的焦点就落在了如何用一个正数, 来替代一个负数. 上面的例子我们能感觉出来一些端倪, 发现一些规律. 但是数学是严谨的. 不能靠感觉.\n首先介绍一个数学中相关的概念: 同余\n同余的概念两个整数a，b，若它们除以整数m所得的余数相等，则称a，b对于模m同余\n记作 a ≡ b (mod m)\n读作 a 与 b 关于模 m 同余。\n举例说明:\n\n4 mod 12 = 4\n16 mod 12 = 4\n28 mod 12 = 4\n\n所以4, 16, 28关于模 12 同余.\n负数取模正数进行mod运算是很简单的. 但是负数呢?\n下面是关于mod运算的数学定义:\n\n上面是截图, “取下界”符号找不到如何输入(word中粘贴过来后乱码). 下面是使用”L”和”J”替换上图的”取下界”符号:\n\nx mod y = x - y L x / y J\n\n上面公式的意思是:\nx mod y等于 x 减去 y 乘上 x与y的商的下界.\n以 -3 mod 2 举例:\n\n-3 mod 2\n= -3 - 2xL -3/2 J\n= -3 - 2xL-1.5J\n= -3 - 2x(-2)\n= -3 + 4 = 1\n\n所以:\n\n(-2) mod 12 = 12-2=10\n(-4) mod 12 = 12-4 = 8\n(-5) mod 12 = 12 - 5 = 7\n\n开始证明再回到时钟的问题上:\n\n回拨2小时 = 前拨10小时\n回拨4小时 = 前拨8小时\n回拨5小时= 前拨7小时\n\n注意, 这里发现的规律!\n结合上面学到的同余的概念.实际上:\n\n(-2) mod 12 = 10\n10 mod 12 = 10\n\n-2与10是同余的.\n\n(-4) mod 12 = 8\n8 mod 12 = 8\n\n-4与8是同余的.\n距离成功越来越近了. 要实现用正数替代负数, 只需要运用同余数的两个定理:\n反身性:\n\na ≡ a (mod m)\n\n这个定理是很显而易见的.\n线性运算定理:\n\n如果a ≡ b (mod m)，c ≡ d (mod m) 那么:\n(1)a ± c ≡ b ± d (mod m)\n(2)a * c ≡ b * d (mod m)\n\n如果想看这个定理的证明, 请看:http://baike.baidu.com/view/79282.htm\n所以:\n\n7 ≡ 7 (mod 12)\n(-2) ≡ 10 (mod 12)\n7 -2 ≡ 7 + 10 (mod 12)\n\n现在我们为一个负数, 找到了它的正数同余数. 但是并不是7-2 = 7+10, 而是 7 -2 ≡ 7 + 10 (mod 12) , 即计算结果的余数相等.\n接下来回到二进制的问题上, 看一下: 2-1=1的问题.\n\n2-1=2+(-1) = [0000 0010]原 + [1000 0001]原= [0000 0010]反 + [1111 1110]反\n\n先到这一步, -1的反码表示是1111 1110. 如果这里将[1111 1110]认为是原码, 则[1111 1110]原 = -126, 这里将符号位除去, 即认为是126.\n发现有如下规律:\n\n(-1) mod 127 = 126\n126 mod 127 = 126\n\n即:\n\n(-1) ≡ 126 (mod 127)\n2-1 ≡ 2+126 (mod 127)\n\n2-1 与 2+126的余数结果是相同的! 而这个余数, 正式我们的期望的计算结果: 2-1=1\n所以说一个数的反码, 实际上是这个数对于一个膜的同余数. 而这个膜并不是我们的二进制, 而是所能表示的最大值! 这就和钟表一样, 转了一圈后总能找到在可表示范围内的一个正确的数值!\n而2+126很显然相当于钟表转过了一轮, 而因为符号位是参与计算的, 正好和溢出的最高位形成正确的运算结果.\n既然反码可以将减法变成加法, 那么现在计算机使用的补码呢? 为什么在反码的基础上加1, 还能得到正确的结果?\n\n2-1=2+(-1) = [0000 0010]原 + [1000 0001]原 = [0000 0010]补 + [1111 1111]补\n\n如果把[1111 1111]当成原码, 去除符号位, 则:\n\n[0111 1111]原 = 127\n\n其实, 在反码的基础上+1, 只是相当于增加了膜的值:\n\n(-1) mod 128 = 127\n127 mod 128 = 127\n2-1 ≡ 2+127 (mod 128)\n\n此时, 表盘相当于每128个刻度转一轮. 所以用补码表示的运算结果最小值和最大值应该是[-128, 128].\n但是由于0的特殊情况, 没有办法表示128, 所以补码的取值范围是[-128, 127]\n"},{"title":"数据结构与算法具体实现","url":"/2020/06/03/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/","content":"\n\n\n\nC\nC++\nJava\n\n\n\n线性结构\n1. 数组、单链表和双链表2. Linux内核中双向链表的经典实现\n数组、单链表和双链表\n数组、单链表和双链表\n\n\n\n栈\n栈\n栈\n\n\n\n队列\n队列\n队列\n\n\n树形结构\n二叉查找树\n二叉查找树\n二叉查找树 \n\n\n\nAVL树\nAVL树\nAVL树\n\n\n\n1. 红黑树(一)之原理和算法详细介绍2. 红黑树(二)之C语言的实现3. 红黑树(三)之Linux内核中红黑树的经典实现4. 红黑树(六)之 参考资料\n1. 红黑树(一)之原理和算法详细介绍2. 红黑树(四)之C++的实现 3. 红黑树(六)之参考资料\n1. 红黑树(一)之原理和算法详细介绍2. 红黑树(五)之Java的实现3. 红黑树(六)之参考资料\n\n\n\n哈夫曼树\n哈夫曼树\n哈夫曼树\n\n\n堆\n二叉堆\n二叉堆\n二叉堆\n\n\n\n左倾堆\n左倾堆\n左倾堆\n\n\n\n斜堆\n斜堆\n斜堆\n\n\n\n二项堆\n二项堆\n二项堆\n\n\n\n斐波那契堆\n斐波那契堆\n斐波那契堆\n\n\n图\n图的理论基础\n图的理论基础\n图的理论基础\n\n\n\n1. 邻接矩阵无向图2. 邻接表无向图3. 邻接矩阵有向图4. 邻接表有向图\n1. 邻接矩阵无向图2. 邻接表无向图3. 邻接矩阵有向图4. 邻接表有向图\n1. 邻接矩阵无向图2. 邻接表无向图3. 邻接矩阵有向图4. 邻接表有向图\n\n\n\n深度优先搜索和广度优先搜索\n深度优先搜索和广度优先搜索\n深度优先搜索和广度优先搜索\n\n\n\n拓扑排序\n拓扑排序\n拓扑排序\n\n\n\nKruskal算法\nKruskal算法\nKruskal算法\n\n\n\nPrim算法\nPrim算法\nPrim算法\n\n\n\nDijkstra算法\n\n\n\n\n排序算法\n冒泡排序\n冒泡排序\n冒泡排序\n\n\n\n快速排序\n快速排序\n快速排序\n\n\n\n直接插入排序\n直接插入排序\n直接插入排序\n\n\n\n希尔排序\n希尔排序\n希尔排序\n\n\n\n选择排序\n选择排序\n选择排序\n\n\n\n堆排序\n堆排序\n堆排序\n\n\n\n归并排序\n归并排序\n归并排序\n\n\n\n桶排序\n桶排序\n桶排序\n\n\n\n基数排序\n基数排序\n基数排序\n\n\n"},{"title":"零拷贝","url":"/2020/05/19/%E9%9B%B6%E6%8B%B7%E8%B4%9D/","content":"引言传统的Linux操作系统的标准I/O接口是基于数据拷贝操作的，即I/O操作会导致数据在操作系统内核地址空间的缓冲区和应用程序地址空间定义的缓冲区之间进行传输。这样做最大的好处是可以减少磁盘I/O的操作，因为如果所请求的数据已经存放在操作系统的高速缓冲存储器中，那么就不需要再进行实际的物理磁盘I/O操作。但是数据传输过程中的数据拷贝操作却导致了极大的CPU开销，限制了操作系统有效进行数据传输操作的能力。\n零拷贝（zero-copy）这种技术可以有效地改善数据传输的性能，在内核驱动程序（比如网络堆栈或者磁盘存储驱动程序）处理I/O数据的时候，零拷贝技术可以在某种程度上减少甚至完全避免不必要CPU数据拷贝操作。现代的CPU和存储体系结构提供了很多特征可以有效地实现零拷贝技术，但是因为存储体系结构非常复杂，而且网络协议栈有时需要对数据进行必要的处理，所以零拷贝技术有可能会产生很多负面的影响，甚至会导致零拷贝技术自身的优点完全丧失。\n为什么需要零拷贝技术如今，很多网络服务器都是基于客户端-服务器这一模型的。在这种模型中，客户端向服务器端请求数据或者服务；服务器端则需要响应客户端发出的请求，并为客户端提供它所需要的数据。随着网络服务的逐渐普及，video这类应用程序发展迅速。当今的计算机系统已经具备足够的能力去处理video这类应用程序对客户端所造成的重负荷，但是对于服务器端来说，它应付由video这类应用程序引起的网络通信量就显得捉襟见肘了。而且，客户端的数量增长迅速，那么服务器端就更容易成为性能瓶颈。而对于负荷很重的服务器来说，操作系统通常都是引起性能瓶颈的罪魁祸首。举个例子来说，当数据“写”操作或者数据“发送”操作的系统调用发出时，操作系统通常都会将数据从应用程序地址空间的缓冲区拷贝到操作系统内核的缓冲区中去。操作系统这样做的好处是接口简单，但是却在很大程度上损失了系统性能，因为这种数据拷贝操作不单需要占用CPU时间片，同时也需要占用额外的内存带宽。\n一般来说，客户端通过网络接口卡向服务器端发送请求，操作系统将这些客户端的请求传递给服务器端应用程序，服务器端应用程序会处理这些请求，请求处理完成以后，操作系统还需要将处理得到的结果通过网络适配器传递回去。\n下边这一小节会跟读者简单介绍一下传统的服务器是如何进行数据传输的，以及这种数据传输的处理过程存在哪些问题有可能会造成服务器的性能损失。\nLinux中传统服务器进行数据传输的流程Linux中传统的I/O操作是一种缓冲I/O，I/O过程中产生的数据传输通常需要在缓冲区中进行多次的拷贝操作。一般来说，在传输数据的时候，用户应用程序需要分配一块大小合适的缓冲区用来存放需要传输的数据。应用程序从文件中读取一块数据，然后把这块数据通过网络发送到接收端去。用户应用程序只是需要调用两个系统调用read()和write()就可以完成这个数据传输操作，应用程序并不知晓在这个数据传输的过程中操作系统所做的数据拷贝操作。对于Linux操作系统来说，基于数据排序或者校验等各方面因素的考虑，操作系统内核会在处理数据传输的过程中进行多次拷贝操作。在某些情况下，这些数据拷贝操作会极大地降低数据传输的性能。\n当应用程序需要访问某块数据的时候，操作系统内核会先检查这块数据是不是因为前一次对相同文件的访问而已经被存放在操作系统内核地址空间的缓冲区内，如果在内核缓冲区中找不到这块数据，Linux操作系统内核会先将这块数据从磁盘读出来放到操作系统内核的缓冲区里去。如果这个数据读取操作是由DMA完成的，那么在DMA进行数据读取的这一过程中，CPU只是需要进行缓冲区管理，以及创建和处理DMA，除此之外，CPU不需要再做更多的事情，DMA执行完数据读取操作之后，会通知操作系统做进一步的处理。Linux操作系统会根据read()系统调用指定的应用程序地址空间的地址，把这块数据存放到请求这块数据的应用程序的地址空间中去，在接下来的处理过程中，操作系统需要将数据再一次从用户应用程序地址空间的缓冲区拷贝到与网络堆栈相关的内核缓冲区中去，这个过程也是需要占用CPU的。数据拷贝操作结束以后，数据会被打包，然后发送到网络接口卡上去。在数据传输的过程中，应用程序可以先返回进而执行其他的操作。之后，在调用write()系统调用的时候，用户应用程序缓冲区中的数据内容可以被安全的丢弃或者更改，因为操作系统已经在内核缓冲区中保留了一份数据拷贝，当数据被成功传送到硬件上之后，这份数据拷贝就可以被丢弃。\n从上面的描述可以看出，在这种传统的数据传输过程中，数据至少发生了四次拷贝操作，即便是使用了DMA来进行与硬件的通讯，CPU仍然需要访问数据两次。在read()读数据的过程中，数据并不是直接来自于硬盘，而是必须先经过操作系统的文件系统层。在write()写数据的过程中，为了和要传输的数据包的大小相吻合，数据必须要先被分割成块，而且还要预先考虑包头，并且要进行数据校验和操作。\n\n图1.传统使用read和write系统调用的数据传输\n\n\n零拷贝（zero-copy）技术概述什么是零拷贝？简单一点来说，零拷贝就是一种避免CPU将数据从一块存储拷贝到另外一块存储的技术。针对操作系统中的设备驱动程序、文件系统以及网络协议堆栈而出现的各种零拷贝技术极大地提升了特定应用程序的性能，并且使得这些应用程序可以更加有效地利用系统资源。这种性能的提升就是通过在数据拷贝进行的同时，允许CPU执行其他的任务来实现的。零拷贝技术可以减少数据拷贝和共享总线操作的次数，消除传输数据在存储器之间不必要的中间拷贝次数，从而有效地提高数据传输效率。而且，零拷贝技术减少了用户应用程序地址空间和操作系统内核地址空间之间因为上下文切换而带来的开销。进行大量的数据拷贝操作其实是一件简单的任务，从操作系统的角度来说，如果CPU一直被占用着去执行这项简单的任务，那么这将会是很浪费资源的；如果有其他比较简单的系统部件可以代劳这件事情，从而使得CPU解脱出来可以做别的事情，那么系统资源的利用则会更加有效。综上所述，零拷贝技术的目标可以概括如下：\n避免数据拷贝\n\n避免操作系统内核缓冲区之间进行数据拷贝操作。\n避免操作系统内核和用户应用程序地址空间这两者之间进行数据拷贝操作。\n用户应用程序可以避开操作系统直接访问硬件存储。\n数据传输尽量让DMA来做。\n\n将多种操作结合在一起\n\n避免不必要的系统调用和上下文切换。\n需要拷贝的数据可以先被缓存起来。\n对数据进行处理尽量让硬件来做。\n\n前文提到过，对于高速网络来说，零拷贝技术是非常重要的。这是因为高速网络的网络链接能力与CPU的处理能力接近，甚至会超过CPU的处理能力。如果是这样的话，那么CPU就有可能需要花费几乎所有的时间去拷贝要传输的数据，而没有能力再去做别的事情，这就产生了性能瓶颈，限制了通讯速率，从而降低了网络链接的能力。一般来说，一个CPU时钟周期可以处理一位的数据。举例来说，一个1GHz的处理器可以对1Gbit/s的网络链接进行传统的数据拷贝操作，但是如果是10Gbit/s的网络，那么对于相同的处理器来说，零拷贝技术就变得非常重要了。对于超过1Gbit/s的网络链接来说，零拷贝技术在超级计算机集群以及大型的商业数据中心中都有所应用。然而，随着信息技术的发展，1Gbit/s，10Gbit/s以及100Gbit/s的网络会越来越普及，那么零拷贝技术也会变得越来越普及，这是因为网络链接的处理能力比CPU的处理能力的增长要快得多。传统的数据拷贝受限于传统的操作系统或者通信协议，这就限制了数据传输性能。零拷贝技术通过减少数据拷贝次数，简化协议处理的层次，在应用程序和网络之间提供更快的数据传输方法，从而可以有效地降低通信延迟，提高网络吞吐率。零拷贝技术是实现主机或者路由器等设备高速网络接口的主要技术之一。\n现代的CPU和存储体系结构提供了很多相关的功能来减少或避免I/O操作过程中产生的不必要的CPU数据拷贝操作，但是，CPU和存储体系结构的这种优势经常被过高估计。存储体系结构的复杂性以及网络协议中必需的数据传输可能会产生问题，有时甚至会导致零拷贝这种技术的优点完全丧失。在下一章中，我们会介绍几种Linux操作系统中出现的零拷贝技术，简单描述一下它们的实现方法，并对它们的弱点进行分析。\n零拷贝技术分类零拷贝技术的发展很多样化，现有的零拷贝技术种类也非常多，而当前并没有一个适合于所有场景的零拷贝技术的出现。对于Linux来说，现存的零拷贝技术也比较多，这些零拷贝技术大部分存在于不同的Linux内核版本，有些旧的技术在不同的Linux内核版本间得到了很大的发展或者已经渐渐被新的技术所代替。本文针对这些零拷贝技术所适用的不同场景对它们进行了划分。概括起来，Linux中的零拷贝技术主要有下面这几种：\n\n直接I/O：对于这种数据传输方式来说，应用程序可以直接访问硬件存储，操作系统内核只是辅助数据传输：这类零拷贝技术针对的是操作系统内核并不需要对数据进行直接处理的情况，数据可以在应用程序地址空间的缓冲区和磁盘之间直接进行传输，完全不需要Linux操作系统内核提供的页缓存的支持。\n在数据传输的过程中，避免数据在操作系统内核地址空间的缓冲区和用户应用程序地址空间的缓冲区之间进行拷贝。有的时候，应用程序在数据进行传输的过程中不需要对数据进行访问，那么，将数据从Linux的页缓存拷贝到用户进程的缓冲区中就可以完全避免，传输的数据在页缓存中就可以得到处理。在某些特殊的情况下，这种零拷贝技术可以获得较好的性能。Linux中提供类似的系统调用主要有mmap()，sendfile()以及splice()。\n对数据在Linux的页缓存和用户进程的缓冲区之间的传输过程进行优化。该零拷贝技术侧重于灵活地处理数据在用户进程的缓冲区和操作系统的页缓存之间的拷贝操作。这种方法延续了传统的通信方式，但是更加灵活。在Linux中，该方法主要利用了写时复制技术。\n\n前两类方法的目的主要是为了避免应用程序地址空间和操作系统内核地址空间这两者之间的缓冲区拷贝操作。这两类零拷贝技术通常适用在某些特殊的情况下，比如要传送的数据不需要经过操作系统内核的处理或者不需要经过应用程序的处理。第三类方法则继承了传统的应用程序地址空间和操作系统内核地址空间之间数据传输的概念，进而针对数据传输本身进行优化。我们知道，硬件和软件之间的数据传输可以通过使用DMA来进行，DMA进行数据传输的过程中几乎不需要CPU参与，这样就可以把CPU解放出来去做更多其他的事情，但是当数据需要在用户地址空间的缓冲区和Linux操作系统内核的页缓存之间进行传输的时候，并没有类似DMA这种工具可以使用，CPU需要全程参与到这种数据拷贝操作中，所以这第三类方法的目的是可以有效地改善数据在用户地址空间和操作系统内核地址空间之间传递的效率。\nJava通过零拷贝实现高效的数据传输在 Linux 和 Unix 系统中 Java 类库通过java.nio.channels.FileChannel 的transgerTo方法支持零拷贝。\n"},{"title":"03 | 复杂度分析（上）：如何分析、统计算法的执行效率和资源消耗？","url":"/2020/08/07/03%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%9A%E5%A4%8D%E6%9D%82%E5%BA%A6%E5%88%86%E6%9E%90%EF%BC%88%E4%B8%8A%EF%BC%89/","content":"我们都知道，数据结构和算法本身解决的是“快”和“省”的问题，即如何让代码运行得更快，如何让代码更省存储空间。所以，执行效率是算法一个非常重要的考量指标。那如何来衡量你编写的算法代码的执行效率呢？这里就要用到我们今天要讲的内容：时间、空间复杂度分析。\n其实，只要讲到数据结构与算法，就一定离不开时间、空间复杂度分析。而且，我个人认为，复杂度分析是整个算法学习的精髓，只要掌握了它，数据结构和算法的内容基本上就掌握了一半。\n复杂度分析实在太重要了，因此我准备用两节内容来讲。希望你学完这个内容之后，无论在任何场景下，面对任何代码的复杂度分析，你都能做到“庖丁解牛”般游刃有余。\n为什么需要复杂度分析？你可能会有些疑惑，我把代码跑一遍，通过统计、监控，就能得到算法执行的时间和占用的内存大小。为什么还要做时间、空间复杂度分析呢？这种分析方法能比我实实在在跑一遍得到的数据更准确吗？\n首先，我可以肯定地说，你这种评估算法执行效率的方法是正确的。很多数据结构和算法书籍还给这种方法起了一个名字，叫事后统计法。但是，这种统计方法有非常大的局限性。\n1.测试结果非常依赖测试环境测试环境中硬件的不同会对测试结果有很大的影响。比如，我们拿同样一段代码，分别用Intel Core i9处理器和Intel Core i3处理器来运行，不用说，i9处理器要比i3处理器执行的速度快很多。还有，比如原本在这台机器上a代码执行的速度比b代码要快，等我们换到另一台机器上时，可能会有截然相反的结果。\n2.测试结果受数据规模的影响很大后面我们会讲排序算法，我们先拿它举个例子。对同一个排序算法，待排序数据的有序度不一样，排序的执行时间就会有很大的差别。极端情况下，如果数据已经是有序的，那排序算法不需要做任何操作，执行时间就会非常短。除此之外，如果测试数据规模太小，测试结果可能无法真实地反映算法的性能。比如，对于小规模的数据排序，插入排序可能反倒会比快速排序要快！\n所以，我们需要一个不用具体的测试数据来测试，就可以粗略地估计算法的执行效率的方法。这就是我们今天要讲的时间、空间复杂度分析方法。\n大O复杂度表示法算法的执行效率，粗略地讲，就是算法代码执行的时间。但是，如何在不运行代码的情况下，用“肉眼”得到一段代码的执行时间呢？\n这里有段非常简单的代码，求1,2,3…n的累加和。现在，我就带你一块来估算一下这段代码的执行时间。\nint cal(int n) &#123;    int sum = 0;    int i = 1;    for (; i &lt;= n; ++i) &#123;        sum = sum + i;    &#125;    return sum;&#125;\n从CPU的角度来看，这段代码的每一行都执行着类似的操作：读数据-运算-写数据。尽管每行代码对应的CPU执行的个数、执行的时间都不一样，但是，我们这里只是粗略估计，所以可以假设每行代码执行的时间都一样，为unit_time。在这个假设的基础之上，这段代码的总执行时间是多少呢？\n第2、3行代码分别需要1个unit_time的执行时间，第4、5行都运行了n遍，所以需要2n*unit_time的执行时间，所以这段代码总的执行时间就是(2n+2)*unit_time。可以看出来，所有代码的执行时间T(n)与每行代码的执行次数成正比。\n按照这个分析思路，我们再来看这段代码。\nint cal(int n) &#123;    int sum = 0;    int i = 1;    int j = 1;    for (; i &lt;= n; ++i) &#123;        j = 1;        for (; j &lt;= n; ++j) &#123;            sum = sum +  i * j;        &#125;    &#125;&#125;\n我们依旧假设每个语句的执行时间是unit_time。那这段代码的总执行时间T(n)是多少呢？\n第2、3、4行代码，每行都需要1个unit_time的执行时间，第5、6行代码循环执行了n遍，需要2n * unit_time的执行时间，第7、8行代码循环执行了n2遍，所以需要2n2 * unit_time的执行时间。所以，整段代码总的执行时间T(n) = (2n2+2n+3)*unit_time。\n尽管我们不知道unit_time的具体值，但是通过这两段代码执行时间的推导过程，我们可以得到一个非常重要的规律，那就是，所有代码的执行时间T(n)与每行代码的执行次数n成正比。\n我们可以把这个规律总结成一个公式。注意，大O就要登场了！\n\n我来具体解释一下这个公式。其中，T(n)我们已经讲过了，它表示代码执行的时间；n表示数据规模的大小；f(n)表示每行代码执行的次数总和。因为这是一个公式，所以用f(n)来表示。公式中的O，表示代码的执行时间T(n)与f(n)表达式成正比。\n所以，第一个例子中的T(n) = O(2n+2)，第二个例子中的T(n) = O(2n2+2n+3)。这就是大O时间复杂度表示法。大O时间复杂度实际上并不具体表示代码真正的执行时间，而是表示代码执行时间随数据规模增长的变化趋势，所以，也叫作渐进时间复杂度（asymptotic time complexity），简称时间复杂度。\n当n很大时，你可以把它想象成10000、100000。而公式中的低阶、常量、系数三部分并不左右增长趋势，所以都可以忽略。我们只需要记录一个最大量级就可以了，如果用大O表示法表示刚讲的那两段代码的时间复杂度，就可以记为：T(n) = O(n)； T(n) = O(n2)。\n时间复杂度分析前面介绍了大O时间复杂度的由来和表示方法。现在我们来看下，如何分析一段代码的时间复杂度？我这儿有三个比较实用的方法可以分享给你。\n1.只关注循环执行次数最多的一段代码我刚才说了，大O这种复杂度表示方法只是表示一种变化趋势。我们通常会忽略掉公式中的常量、低阶、系数，只需要记录一个最大阶的量级就可以了。所以，我们在分析一个算法、一段代码的时间复杂度的时候，也只关注循环执行次数最多的那一段代码就可以了。这段核心代码执行次数的n的量级，就是整段要分析代码的时间复杂度。\n为了便于你理解，我还是拿前面的例子来说明。\nint cal(int n) &#123;    int sum = 0;    int i = 1;    for (; i &lt;= n; ++i) &#123;        sum = sum + i;    &#125;    return sum;&#125;\n其中第2、3行代码都是常量级的执行时间，与n的大小无关，所以对于复杂度并没有影响。循环执行次数最多的是第4、5行代码，所以这块代码要重点分析。前面我们也讲过，这两行代码被执行了n次，所以总的时间复杂度就是O(n)。\n2.加法法则：总复杂度等于量级最大的那段代码的复杂度我这里还有一段代码。你可以先试着分析一下，然后再往下看跟我的分析思路是否一样。\nint cal(int n) &#123;    int sum_1 = 0;    int p = 1;    for (; p &lt; 100; ++p) &#123;        sum_1 = sum_1 + p;    &#125;    int sum_2 = 0;    int q = 1;    for (; q &lt; n; ++q) &#123;        sum_2 = sum_2 + q;    &#125;    int sum_3 = 0;    int i = 1;    int j = 1;    for (; i &lt;= n; ++i) &#123;        j = 1;         for (; j &lt;= n; ++j) &#123;            sum_3 = sum_3 +  i * j;        &#125;    &#125;    return sum_1 + sum_2 + sum_3;&#125;\n这个代码分为三部分，分别是求sum_1、sum_2、sum_3。我们可以分别分析每一部分的时间复杂度，然后把它们放到一块儿，再取一个量级最大的作为整段代码的复杂度。\n第一段的时间复杂度是多少呢？这段代码循环执行了100次，所以是一个常量的执行时间，跟n的规模无关。\n这里我要再强调一下，即便这段代码循环10000次、100000次，只要是一个已知的数，跟n无关，照样也是常量级的执行时间。当n无限大的时候，就可以忽略。尽管对代码的执行时间会有很大影响，但是回到时间复杂度的概念来说，它表示的是一个算法执行效率与数据规模增长的变化趋势，所以不管常量的执行时间多大，我们都可以忽略掉。因为它本身对增长趋势并没有影响。\n那第二段代码和第三段代码的时间复杂度是多少呢？答案是O(n)和O(n2)，你应该能容易就分析出来，我就不啰嗦了。\n综合这三段代码的时间复杂度，我们取其中最大的量级。所以，整段代码的时间复杂度就为O(n2)。也就是说：总的时间复杂度就等于量级最大的那段代码的时间复杂度。那我们将这个规律抽象成公式就是：\n如果T1(n)=O(f(n))，T2(n)=O(g(n))；那么T(n)=T1(n)+T2(n)=max(O(f(n)), O(g(n))) =O(max(f(n), g(n))).\n3.乘法法则：嵌套代码的复杂度等于嵌套内外代码复杂度的乘积我刚讲了一个复杂度分析中的加法法则，这儿还有一个乘法法则。类比一下，你应该能“猜到”公式是什么样子的吧？\n如果T1(n)=O(f(n))，T2(n)=O(g(n))；那么T(n)=T1(n)*T2(n)=O(f(n))*O(g(n))=O(f(n)*g(n)).\n也就是说，假设T1(n) = O(n)，T2(n) = O(n2)，则T1(n) * T2(n) = O(n3)。落实到具体的代码上，我们可以把乘法法则看成是嵌套循环，我举个例子给你解释一下。\nint cal(int n) &#123;    int ret = 0;     int i = 1;    for (; i &lt; n; ++i) &#123;        ret = ret + f(i);    &#125; &#125; int f(int n) &#123;    int sum = 0;    int i = 1;    for (; i &lt; n; ++i) &#123;        sum = sum + i;    &#125;     return sum;&#125;\n我们单独看cal()函数。假设f()只是一个普通的操作，那第4～6行的时间复杂度就是，T1(n) = O(n)。但f()函数本身不是一个简单的操作，它的时间复杂度是T2(n) = O(n)，所以，整个cal()函数的时间复杂度就是，T(n) = T1(n) * T2(n) = O(n*n) = O(n2)。\n我刚刚讲了三种复杂度的分析技巧。不过，你并不用刻意去记忆。实际上，复杂度分析这个东西关键在于“熟练”。你只要多看案例，多分析，就能做到“无招胜有招”。\n几种常见时间复杂度实例分析虽然代码千差万别，但是常见的复杂度量级并不多。我稍微总结了一下，这些复杂度量级几乎涵盖了你今后可以接触的所有代码的复杂度量级。\n\n对于刚罗列的复杂度量级，我们可以粗略地分为两类，多项式量级和非多项式量级。其中，非多项式量级只有两个：O(2n)和O(n!)。\n我们把时间复杂度为非多项式量级的算法问题叫作NP（Non-Deterministic Polynomial，非确定多项式）问题。\n当数据规模n越来越大时，非多项式量级算法的执行时间会急剧增加，求解问题的执行时间会无限增长。所以，非多项式时间复杂度的算法其实是非常低效的算法。因此，关于NP时间复杂度我就不展开讲了。我们主要来看几种常见的多项式时间复杂度。\n1. O(1)首先你必须明确一个概念，O(1)只是常量级时间复杂度的一种表示方法，并不是指只执行了一行代码。比如这段代码，即便有3行，它的时间复杂度也是O(1），而不是O(3)。\nint i = 8;int j = 6;int sum = i + j;\n我稍微总结一下，只要代码的执行时间不随n的增大而增长，这样代码的时间复杂度我们都记作O(1)。或者说，一般情况下，只要算法中不存在循环语句、递归语句，即使有成千上万行的代码，其时间复杂度也是Ο(1)。\n2.  O(logn)、O(nlogn)对数阶时间复杂度非常常见，同时也是最难分析的一种时间复杂度。我通过一个例子来说明一下。\ni=1;while (i &lt;= n)  &#123;    i = i * 2;&#125;\n根据我们前面讲的复杂度分析方法，第三行代码是循环执行次数最多的。所以，我们只要能计算出这行代码被执行了多少次，就能知道整段代码的时间复杂度。\n从代码中可以看出，变量i的值从1开始取，每循环一次就乘以2。当大于n时，循环结束。还记得我们高中学过的等比数列吗？实际上，变量i的取值就是一个等比数列。如果我把它一个一个列出来，就应该是这个样子的：\n\n所以，我们只要知道x值是多少，就知道这行代码执行的次数了。通过2x=n求解x这个问题我们想高中应该就学过了，我就不多说了。x=log2n，所以，这段代码的时间复杂度就是O(log2n)。\n现在，我把代码稍微改下，你再看看，这段代码的时间复杂度是多少？\ni=1;while (i &lt;= n)  &#123;    i = i * 3;&#125;\n根据我刚刚讲的思路，很简单就能看出来，这段代码的时间复杂度为O(log3n)。\n实际上，不管是以2为底、以3为底，还是以10为底，我们可以把所有对数阶的时间复杂度都记为O(logn)。为什么呢？\n我们知道，对数之间是可以互相转换的，log3n就等于log32 * log2n，所以O(log3n) = O(C *  log2n)，其中C=log32是一个常量。基于我们前面的一个理论：在采用大O标记复杂度的时候，可以忽略系数，即O(Cf(n)) = O(f(n))。所以，O(log2n) 就等于O(log3n)。因此，在对数阶时间复杂度的表示方法里，我们忽略对数的“底”，统一表示为O(logn)。\n如果你理解了我前面讲的O(logn)，那O(nlogn)就很容易理解了。还记得我们刚讲的乘法法则吗？如果一段代码的时间复杂度是O(logn)，我们循环执行n遍，时间复杂度就是O(nlogn)了。而且，O(nlogn)也是一种非常常见的算法时间复杂度。比如，归并排序、快速排序的时间复杂度都是O(nlogn)。\n3.  O(m+n)、O(m*n)我们再来讲一种跟前面都不一样的时间复杂度，代码的复杂度由两个数据的规模来决定。老规矩，先看代码！\nint cal(int m, int n) &#123;    int sum_1 = 0;    int i = 1;    for (; i &lt; m; ++i) &#123;        sum_1 = sum_1 + i;    &#125;    int sum_2 = 0;    int j = 1;    for (; j &lt; n; ++j) &#123;        sum_2 = sum_2 + j;    &#125;    return sum_1 + sum_2;&#125;\n从代码中可以看出，m和n是表示两个数据规模。我们无法事先评估m和n谁的量级大，所以我们在表示复杂度的时候，就不能简单地利用加法法则，省略掉其中一个。所以，上面代码的时间复杂度就是O(m+n)。\n针对这种情况，原来的加法法则就不正确了，我们需要将加法规则改为：T1(m) + T2(n) = O(f(m) + g(n))。但是乘法法则继续有效：T1(m)*T2(n) = O(f(m) * f(n))。\n空间复杂度分析前面，咱们花了很长时间讲大O表示法和时间复杂度分析，理解了前面讲的内容，空间复杂度分析方法学起来就非常简单了。\n前面我讲过，时间复杂度的全称是渐进时间复杂度，表示算法的执行时间与数据规模之间的增长关系。类比一下，空间复杂度全称就是渐进空间复杂度（asymptotic space complexity），表示算法的存储空间与数据规模之间的增长关系。\n我还是拿具体的例子来给你说明。（这段代码有点“傻”，一般没人会这么写，我这么写只是为了方便给你解释。）\nvoid print(int n) &#123;    int i = 0;    int[] a = new int[n];    for (i; i &lt;n; ++i) &#123;        a[i] = i * i;    &#125;    for (i = n-1; i &gt;= 0; --i) &#123;        print out a[i]    &#125;&#125;\n跟时间复杂度分析一样，我们可以看到，第2行代码中，我们申请了一个空间存储变量i，但是它是常量阶的，跟数据规模n没有关系，所以我们可以忽略。第3行申请了一个大小为n的int类型数组，除此之外，剩下的代码都没有占用更多的空间，所以整段代码的空间复杂度就是O(n)。\n我们常见的空间复杂度就是O(1)、O(n)、O(n2 )，像O(logn)、O(nlogn)这样的对数阶复杂度平时都用不到。而且，空间复杂度分析比时间复杂度分析要简单很多。所以，对于空间复杂度，掌握刚我说的这些内容已经足够了。\n内容小结基础复杂度分析的知识到此就讲完了，我们来总结一下。\n复杂度也叫渐进复杂度，包括时间复杂度和空间复杂度，用来分析算法执行效率与数据规模之间的增长关系，可以粗略地表示，越高阶复杂度的算法，执行效率越低。常见的复杂度并不多，从低阶到高阶有：O(1)、O(logn)、O(n)、O(nlogn)、O(n2 )。等你学完整个专栏之后，你就会发现几乎所有的数据结构和算法的复杂度都跑不出这几个。\n\n复杂度分析并不难，关键在于多练。 之后讲后面的内容时，我还会带你详细地分析每一种数据结构和算法的时间、空间复杂度。只要跟着我的思路学习、练习，你很快就能和我一样，每次看到代码的时候，简单的一眼就能看出其复杂度，难的稍微分析一下就能得出答案。\n课后思考有人说，我们项目之前都会进行性能测试，再做代码的时间复杂度、空间复杂度分析，是不是多此一举呢？而且，每段代码都分析一下时间复杂度、空间复杂度，是不是很浪费时间呢？你怎么看待这个问题呢？\n欢迎留言和我分享，我会第一时间给你反馈。\n","categories":["数据结构与算法","入门篇"]},{"title":"10 |  递归：如何用三行代码找到“最终推荐人”？","url":"/2020/08/07/10%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%9A%E9%80%92%E5%BD%92/","content":"推荐注册返佣金的这个功能我想你应该不陌生吧？现在很多App都有这个功能。这个功能中，用户A推荐用户B来注册，用户B又推荐了用户C来注册。我们可以说，用户C的“最终推荐人”为用户A，用户B的“最终推荐人”也为用户A，而用户A没有“最终推荐人”。\n一般来说，我们会通过数据库来记录这种推荐关系。在数据库表中，我们可以记录两行数据，其中actor_id表示用户id，referrer_id表示推荐人id。\n\n基于这个背景，我的问题是，给定一个用户ID，如何查找这个用户的“最终推荐人”？ 带着这个问题，我们来学习今天的内容，递归（Recursion）！\n如何理解“递归”？从我自己学习数据结构和算法的经历来看，我个人觉得，有两个最难理解的知识点，一个是动态规划，另一个就是递归。\n递归是一种应用非常广泛的算法（或者编程技巧）。之后我们要讲的很多数据结构和算法的编码实现都要用到递归，比如DFS深度优先搜索、前中后序二叉树遍历等等。所以，搞懂递归非常重要，否则，后面复杂一些的数据结构和算法学起来就会比较吃力。\n不过，别看我说了这么多，递归本身可是一点儿都不“高冷”，咱们生活中就有很多用到递归的例子。\n周末你带着女朋友去电影院看电影，女朋友问你，咱们现在坐在第几排啊？电影院里面太黑了，看不清，没法数，现在你怎么办？\n别忘了你是程序员，这个可难不倒你，递归就开始排上用场了。于是你就问前面一排的人他是第几排，你想只要在他的数字上加一，就知道自己在哪一排了。但是，前面的人也看不清啊，所以他也问他前面的人。就这样一排一排往前问，直到问到第一排的人，说我在第一排，然后再这样一排一排再把数字传回来。直到你前面的人告诉你他在哪一排，于是你就知道答案了。\n这就是一个非常标准的递归求解问题的分解过程，去的过程叫“递”，回来的过程叫“归”。基本上，所有的递归问题都可以用递推公式来表示。刚刚这个生活中的例子，我们用递推公式将它表示出来就是这样的：\nf(n)&#x3D;f(n-1)+1 其中，f(1)&#x3D;1\nf(n)表示你想知道自己在哪一排，f(n-1)表示前面一排所在的排数，f(1)=1表示第一排的人知道自己在第一排。有了这个递推公式，我们就可以很轻松地将它改为递归代码，如下：\nint f(int n) &#123;    if (n == 1) return 1;    return f(n-1) + 1;&#125;\n递归需要满足的三个条件刚刚这个例子是非常典型的递归，那究竟什么样的问题可以用递归来解决呢？我总结了三个条件，只要同时满足以下三个条件，就可以用递归来解决。\n1.一个问题的解可以分解为几个子问题的解\n何为子问题？子问题就是数据规模更小的问题。比如，前面讲的电影院的例子，你要知道，“自己在哪一排”的问题，可以分解为“前一排的人在哪一排”这样一个子问题。\n2.这个问题与分解之后的子问题，除了数据规模不同，求解思路完全一样\n比如电影院那个例子，你求解“自己在哪一排”的思路，和前面一排人求解“自己在哪一排”的思路，是一模一样的。\n3.存在递归终止条件\n把问题分解为子问题，把子问题再分解为子子问题，一层一层分解下去，不能存在无限循环，这就需要有终止条件。\n还是电影院的例子，第一排的人不需要再继续询问任何人，就知道自己在哪一排，也就是f(1)=1，这就是递归的终止条件。\n如何编写递归代码？刚刚铺垫了这么多，现在我们来看，如何来写递归代码？我个人觉得，写递归代码最关键的是写出递推公式，找到终止条件，剩下将递推公式转化为代码就很简单了。\n你先记住这个理论。我举一个例子，带你一步一步实现一个递归代码，帮你理解。\n假如这里有n个台阶，每次你可以跨1个台阶或者2个台阶，请问走这n个台阶有多少种走法？如果有7个台阶，你可以2，2，2，1这样子上去，也可以1，2，1，1，2这样子上去，总之走法有很多，那如何用编程求得总共有多少种走法呢？\n我们仔细想下，实际上，可以根据第一步的走法把所有走法分为两类，第一类是第一步走了1个台阶，另一类是第一步走了2个台阶。所以n个台阶的走法就等于先走1阶后，n-1个台阶的走法 加上先走2阶后，n-2个台阶的走法。用公式表示就是：\nf(n) &#x3D; f(n-1)+f(n-2)\n有了递推公式，递归代码基本上就完成了一半。我们再来看下终止条件。当有一个台阶时，我们不需要再继续递归，就只有一种走法。所以f(1)=1。这个递归终止条件足够吗？我们可以用n=2，n=3这样比较小的数试验一下。\nn=2时，f(2)=f(1)+f(0)。如果递归终止条件只有一个f(1)=1，那f(2)就无法求解了。所以除了f(1)=1这一个递归终止条件外，还要有f(0)=1，表示走0个台阶有一种走法，不过这样子看起来就不符合正常的逻辑思维了。所以，我们可以把f(2)=2作为一种终止条件，表示走2个台阶，有两种走法，一步走完或者分两步来走。\n所以，递归终止条件就是f(1)=1，f(2)=2。这个时候，你可以再拿n=3，n=4来验证一下，这个终止条件是否足够并且正确。\n我们把递归终止条件和刚刚得到的递推公式放到一起就是这样的：\nf(1) = 1;f(2) = 2;f(n) = f(n-1)+f(n-2)\n有了这个公式，我们转化成递归代码就简单多了。最终的递归代码是这样的：\nint f(int n) &#123;    if (n == 1) return 1;    if (n == 2) return 2;    return f(n-1) + f(n-2);&#125;\n我总结一下，写递归代码的关键就是找到如何将大问题分解为小问题的规律，并且基于此写出递推公式，然后再推敲终止条件，最后将递推公式和终止条件翻译成代码。\n虽然我讲了这么多方法，但是作为初学者的你，现在是不是还是有种想不太清楚的感觉呢？实际上，我刚学递归的时候，也有这种感觉，这也是文章开头我说递归代码比较难理解的地方。\n刚讲的电影院的例子，我们的递归调用只有一个分支，也就是说“一个问题只需要分解为一个子问题”，我们很容易能够想清楚“递“和”归”的每一个步骤，所以写起来、理解起来都不难。\n但是，当我们面对的是一个问题要分解为多个子问题的情况，递归代码就没那么好理解了。\n像我刚刚讲的第二个例子，人脑几乎没办法把整个“递”和“归”的过程一步一步都想清楚。\n计算机擅长做重复的事情，所以递归正合它的胃口。而我们人脑更喜欢平铺直叙的思维方式。当我们看到递归时，我们总想把递归平铺展开，脑子里就会循环，一层一层往下调，然后再一层一层返回，试图想搞清楚计算机每一步都是怎么执行的，这样就很容易被绕进去。\n对于递归代码，这种试图想清楚整个递和归过程的做法，实际上是进入了一个思维误区。很多时候，我们理解起来比较吃力，主要原因就是自己给自己制造了这种理解障碍。那正确的思维方式应该是怎样的呢？\n如果一个问题A可以分解为若干子问题B、C、D，你可以假设子问题B、C、D已经解决，在此基础上思考如何解决问题A。而且，你只需要思考问题A与子问题B、C、D两层之间的关系即可，不需要一层一层往下思考子问题与子子问题，子子问题与子子子问题之间的关系。屏蔽掉递归细节，这样子理解起来就简单多了。\n因此，编写递归代码的关键是，只要遇到递归，我们就把它抽象成一个递推公式，不用想一层层的调用关系，不要试图用人脑去分解递归的每个步骤。\n递归代码要警惕堆栈溢出在实际的软件开发中，编写递归代码时，我们会遇到很多问题，比如堆栈溢出。而堆栈溢出会造成系统性崩溃，后果会非常严重。为什么递归代码容易造成堆栈溢出呢？我们又该如何预防堆栈溢出呢？\n我在“栈”那一节讲过，函数调用会使用栈来保存临时变量。每调用一个函数，都会将临时变量封装为栈帧压入内存栈，等函数执行完成返回时，才出栈。系统栈或者虚拟机栈空间一般都不大。如果递归求解的数据规模很大，调用层次很深，一直压入栈，就会有堆栈溢出的风险。\n比如前面的讲到的电影院的例子，如果我们将系统栈或者JVM堆栈大小设置为1KB，在求解f(19999)时便会出现如下堆栈报错：\nException in thread &quot;main&quot; java.lang.StackOverflowError\n那么，如何避免出现堆栈溢出呢？\n我们可以通过在代码中限制递归调用的最大深度的方式来解决这个问题。递归调用超过一定深度（比如1000）之后，我们就不继续往下再递归了，直接返回报错。还是电影院那个例子，我们可以改造成下面这样子，就可以避免堆栈溢出了。不过，我写的代码是伪代码，为了代码简洁，有些边界条件没有考虑，比如x&lt;=0。\n// 全局变量，表示递归的深度。int depth = 0;int f(int n) &#123;    ++depth；        if (depth &gt; 1000) throw exception;    if (n == 1) return 1;    return f(n-1) + 1;&#125;\n但这种做法并不能完全解决问题，因为最大允许的递归深度跟当前线程剩余的栈空间大小有关，事先无法计算。如果实时计算，代码过于复杂，就会影响代码的可读性。所以，如果最大深度比较小，比如10、50，就可以用这种方法，否则这种方法并不是很实用。\n递归代码要警惕重复计算除此之外，使用递归时还会出现重复计算的问题。刚才我讲的第二个递归代码的例子，如果我们把整个递归过程分解一下的话，那就是这样的：\n\n从图中，我们可以直观地看到，想要计算f(5)，需要先计算f(4)和f(3)，而计算f(4)还需要计算f(3)，因此，f(3)就被计算了很多次，这就是重复计算问题。\n为了避免重复计算，我们可以通过一个数据结构（比如散列表）来保存已经求解过的f(k)。当递归调用到f(k)时，先看下是否已经求解过了。如果是，则直接从散列表中取值返回，不需要重复计算，这样就能避免刚讲的问题了。\n按照上面的思路，我们来改造一下刚才的代码：\npublic int f(int n) &#123;    if (n == 1) return 1;    if (n == 2) return 2;    // hasSolvedList可以理解成一个Map，key是n，value是f(n)    if (hasSolvedList.containsKey(n)) &#123;        return hasSolvedList.get(n);    &#125;    int ret = f(n-1) + f(n-2);    hasSolvedList.put(n, ret);    return ret;&#125;\n除了堆栈溢出、重复计算这两个常见的问题。递归代码还有很多别的问题。\n在时间效率上，递归代码里多了很多函数调用，当这些函数调用的数量较大时，就会积聚成一个可观的时间成本。在空间复杂度上，因为递归调用一次就会在内存栈中保存一次现场数据，所以在分析递归代码空间复杂度时，需要额外考虑这部分的开销，比如我们前面讲到的电影院递归代码，空间复杂度并不是O(1)，而是O(n)。\n怎么将递归代码改写为非递归代码？我们刚说了，递归有利有弊，利是递归代码的表达力很强，写起来非常简洁；而弊就是空间复杂度高、有堆栈溢出的风险、存在重复计算、过多的函数调用会耗时较多等问题。所以，在开发过程中，我们要根据实际情况来选择是否需要用递归的方式来实现。\n那我们是否可以把递归代码改写为非递归代码呢？比如刚才那个电影院的例子，我们抛开场景，只看f(x) =f(x-1)+1这个递推公式。我们这样改写看看：\nint f(int n) &#123;    int ret = 1;    for (int i = 2; i &lt;= n; ++i) &#123;        ret = ret + 1;    &#125;    return ret;&#125;\n同样，第二个例子也可以改为非递归的实现方式。\nint f(int n) &#123;    if (n == 1) return 1;    if (n == 2) return 2;    int ret = 0;    int pre = 2;    int prepre = 1;    for (int i = 3; i &lt;= n; ++i) &#123;        ret = pre + prepre;        prepre = pre;        pre = ret;    &#125;    return ret;&#125;\n那是不是所有的递归代码都可以改为这种迭代循环的非递归写法呢？\n笼统地讲，是的。因为递归本身就是借助栈来实现的，只不过我们使用的栈是系统或者虚拟机本身提供的，我们没有感知罢了。如果我们自己在内存堆上实现栈，手动模拟入栈、出栈过程，这样任何递归代码都可以改写成看上去不是递归代码的样子。\n但是这种思路实际上是将递归改为了“手动”递归，本质并没有变，而且也并没有解决前面讲到的某些问题，徒增了实现的复杂度。\n解答开篇到此为止，递归相关的基础知识已经讲完了，咱们来看一下开篇的问题：如何找到“最终推荐人”？我的解决方案是这样的：\nlong findRootReferrerId(long actorId) &#123;    Long referrerId = select referrer_id from [table] where actor_id = actorId;    if (referrerId == null) return actorId;    return findRootReferrerId(referrerId);&#125;\n是不是非常简洁？用三行代码就能搞定了，不过在实际项目中，上面的代码并不能工作，为什么呢？这里面有两个问题。\n第一，如果递归很深，可能会有堆栈溢出的问题。\n第二，如果数据库里存在脏数据，我们还需要处理由此产生的无限递归问题。比如demo环境下数据库中，测试工程师为了方便测试，会人为地插入一些数据，就会出现脏数据。如果A的推荐人是B，B的推荐人是C，C的推荐人是A，这样就会发生死循环。\n第一个问题，我前面已经解答过了，可以用限制递归深度来解决。第二个问题，也可以用限制递归深度来解决。不过，还有一个更高级的处理方法，就是自动检测A-B-C-A这种“环”的存在。如何来检测环的存在呢？这个我暂时不细说，你可以自己思考下，后面的章节我们还会讲。\n内容小结关于递归的知识，到这里就算全部讲完了。我来总结一下。\n递归是一种非常高效、简洁的编码技巧。只要是满足“三个条件”的问题就可以通过递归代码来解决。\n不过递归代码也比较难写、难理解。编写递归代码的关键就是不要把自己绕进去，正确姿势是写出递推公式，找出终止条件，然后再翻译成递归代码。\n递归代码虽然简洁高效，但是，递归代码也有很多弊端。比如，堆栈溢出、重复计算、函数调用耗时多、空间复杂度高等，所以，在编写递归代码的时候，一定要控制好这些副作用。\n课后思考我们平时调试代码喜欢使用IDE的单步跟踪功能，像规模比较大、递归层次很深的递归代码，几乎无法使用这种调试方式。对于递归代码，你有什么好的调试方法呢？\n欢迎留言和我分享，我会第一时间给你反馈。\n","categories":["数据结构与算法","基础篇"]},{"title":"11 | 排序（上）：为什么插入排序比冒泡排序更受欢迎？","url":"/2020/08/07/11%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%9A%E6%8E%92%E5%BA%8F%EF%BC%88%E4%B8%8A%EF%BC%89/","content":"排序对于任何一个程序员来说，可能都不会陌生。你学的第一个算法，可能就是排序。大部分编程语言中，也都提供了排序函数。在平常的项目中，我们也经常会用到排序。排序非常重要，所以我会花多一点时间来详细讲一讲经典的排序算法。\n排序算法太多了，有很多可能你连名字都没听说过，比如猴子排序、睡眠排序、面条排序等。我只讲众多排序算法中的一小撮，也是最经典的、最常用的：冒泡排序、插入排序、选择排序、归并排序、快速排序、计数排序、基数排序、桶排序。我按照时间复杂度把它们分成了三类，分三节课来讲解。\n\n带着问题去学习，是最有效的学习方法。所以按照惯例，我还是先给你出一个思考题：插入排序和冒泡排序的时间复杂度相同，都是O(n2)，在实际的软件开发里，为什么我们更倾向于使用插入排序算法而不是冒泡排序算法呢？\n你可以先思考一两分钟，带着这个问题，我们开始今天的内容！\n如何分析一个“排序算法”？学习排序算法，我们除了学习它的算法原理、代码实现之外，更重要的是要学会如何评价、分析一个排序算法。那分析一个排序算法，要从哪几个方面入手呢？\n排序算法的执行效率对于排序算法执行效率的分析，我们一般会从这几个方面来衡量：\n1.最好情况、最坏情况、平均情况时间复杂度我们在分析排序算法的时间复杂度时，要分别给出最好情况、最坏情况、平均情况下的时间复杂度。除此之外，你还要说出最好、最坏时间复杂度对应的要排序的原始数据是什么样的。\n为什么要区分这三种时间复杂度呢？第一，有些排序算法会区分，为了好对比，所以我们最好都做一下区分。第二，对于要排序的数据，有的接近有序，有的完全无序。有序度不同的数据，对于排序的执行时间肯定是有影响的，我们要知道排序算法在不同数据下的性能表现。\n2.时间复杂度的系数、常数 、低阶我们知道，时间复杂度反映的是数据规模n很大的时候的一个增长趋势，所以它表示的时候会忽略系数、常数、低阶。但是实际的软件开发中，我们排序的可能是10个、100个、1000个这样规模很小的数据，所以，在对同一阶时间复杂度的排序算法性能对比的时候，我们就要把系数、常数、低阶也考虑进来。\n3.比较次数和交换（或移动）次数这一节和下一节讲的都是基于比较的排序算法。基于比较的排序算法的执行过程，会涉及两种操作，一种是元素比较大小，另一种是元素交换或移动。所以，如果我们在分析排序算法的执行效率的时候，应该把比较次数和交换（或移动）次数也考虑进去。\n排序算法的内存消耗我们前面讲过，算法的内存消耗可以通过空间复杂度来衡量，排序算法也不例外。不过，针对排序算法的空间复杂度，我们还引入了一个新的概念，原地排序（Sorted in place）。原地排序算法，就是特指空间复杂度是O(1)的排序算法。我们今天讲的三种排序算法，都是原地排序算法。\n排序算法的稳定性仅仅用执行效率和内存消耗来衡量排序算法的好坏是不够的。针对排序算法，我们还有一个重要的度量指标，稳定性。这个概念是说，如果待排序的序列中存在值相等的元素，经过排序之后，相等元素之间原有的先后顺序不变。\n我通过一个例子来解释一下。比如我们有一组数据2，9，3，4，8，3，按照大小排序之后就是2，3，3，4，8，9。\n这组数据里有两个3。经过某种排序算法排序之后，如果两个3的前后顺序没有改变，那我们就把这种排序算法叫作稳定的排序算法；如果前后顺序发生变化，那对应的排序算法就叫作不稳定的排序算法。\n你可能要问了，两个3哪个在前，哪个在后有什么关系啊，稳不稳定又有什么关系呢？为什么要考察排序算法的稳定性呢？\n很多数据结构和算法课程，在讲排序的时候，都是用整数来举例，但在真正软件开发中，我们要排序的往往不是单纯的整数，而是一组对象，我们需要按照对象的某个key来排序。\n比如说，我们现在要给电商交易系统中的“订单”排序。订单有两个属性，一个是下单时间，另一个是订单金额。如果我们现在有10万条订单数据，我们希望按照金额从小到大对订单数据排序。对于金额相同的订单，我们希望按照下单时间从早到晚有序。对于这样一个排序需求，我们怎么来做呢？\n最先想到的方法是：我们先按照金额对订单数据进行排序，然后，再遍历排序之后的订单数据，对于每个金额相同的小区间再按照下单时间排序。这种排序思路理解起来不难，但是实现起来会很复杂。\n借助稳定排序算法，这个问题可以非常简洁地解决。解决思路是这样的：我们先按照下单时间给订单排序，注意是按照下单时间，不是金额。排序完成之后，我们用稳定排序算法，按照订单金额重新排序。两遍排序之后，我们得到的订单数据就是按照金额从小到大排序，金额相同的订单按照下单时间从早到晚排序的。为什么呢？\n稳定排序算法可以保持金额相同的两个对象，在排序之后的前后顺序不变。第一次排序之后，所有的订单按照下单时间从早到晚有序了。在第二次排序中，我们用的是稳定的排序算法，所以经过第二次排序之后，相同金额的订单仍然保持下单时间从早到晚有序。\n\n冒泡排序（Bubble Sort）我们从冒泡排序开始，学习今天的三种排序算法。\n冒泡排序只会操作相邻的两个数据。每次冒泡操作都会对相邻的两个元素进行比较，看是否满足大小关系要求。如果不满足就让它俩互换。一次冒泡会让至少一个元素移动到它应该在的位置，重复n次，就完成了n个数据的排序工作。\n我用一个例子，带你看下冒泡排序的整个过程。我们要对一组数据4，5，6，3，2，1，从小到大进行排序。第一次冒泡操作的详细过程就是这样：\n\n可以看出，经过一次冒泡操作之后，6这个元素已经存储在正确的位置上。要想完成所有数据的排序，我们只要进行6次这样的冒泡操作就行了。\n\n实际上，刚讲的冒泡过程还可以优化。当某次冒泡操作已经没有数据交换时，说明已经达到完全有序，不用再继续执行后续的冒泡操作。我这里还有另外一个例子，这里面给6个元素排序，只需要4次冒泡操作就可以了。\n\n冒泡排序算法的原理比较容易理解，具体的代码我贴到下面，你可以结合着代码来看我前面讲的原理。\n// 冒泡排序，a表示数组，n表示数组大小public void bubbleSort(int[] a, int n) &#123;    if (n &lt;= 1) return;    for (int i = 0; i &lt; n; ++i) &#123;        // 提前退出冒泡循环的标志位        boolean flag = false;        for (int j = 0; j &lt; n - i - 1; ++j) &#123;            if (a[j] &gt; a[j+1]) &#123; // 交换                int tmp = a[j];                a[j] = a[j+1];                a[j+1] = tmp;                flag = true;  // 表示有数据交换                  &#125;        &#125;        if (!flag) break;  // 没有数据交换，提前退出    &#125;&#125;\n现在，结合刚才我分析排序算法的三个方面，我有三个问题要问你。\n第一，冒泡排序是原地排序算法吗？\n冒泡的过程只涉及相邻数据的交换操作，只需要常量级的临时空间，所以它的空间复杂度为O(1)，是一个原地排序算法。\n第二，冒泡排序是稳定的排序算法吗？\n在冒泡排序中，只有交换才可以改变两个元素的前后顺序。为了保证冒泡排序算法的稳定性，当有相邻的两个元素大小相等的时候，我们不做交换，相同大小的数据在排序前后不会改变顺序，所以冒泡排序是稳定的排序算法。\n第三，冒泡排序的时间复杂度是多少？\n最好情况下，要排序的数据已经是有序的了，我们只需要进行一次冒泡操作，就可以结束了，所以最好情况时间复杂度是O(n)。而最坏的情况是，要排序的数据刚好是倒序排列的，我们需要进行n次冒泡操作，所以最坏情况时间复杂度为O(n2)。\n\n最好、最坏情况下的时间复杂度很容易分析，那平均情况下的时间复杂是多少呢？我们前面讲过，平均时间复杂度就是加权平均期望时间复杂度，分析的时候要结合概率论的知识。\n对于包含n个数据的数组，这n个数据就有n!种排列方式。不同的排列方式，冒泡排序执行的时间肯定是不同的。比如我们前面举的那两个例子，其中一个要进行6次冒泡，而另一个只需要4次。如果用概率论方法定量分析平均时间复杂度，涉及的数学推理和计算就会很复杂。我这里还有一种思路，通过“有序度”和“逆序度”这两个概念来进行分析。\n有序度是数组中具有有序关系的元素对的个数。有序元素对用数学表达式表示就是这样：\n有序元素对：a[i] &lt;&#x3D; a[j], 如果i &lt; j。\n\n同理，对于一个倒序排列的数组，比如6，5，4，3，2，1，有序度是0；对于一个完全有序的数组，比如1，2，3，4，5，6，有序度就是n*(n-1)/2，也就是15。我们把这种完全有序的数组的有序度叫作满有序度。\n逆序度的定义正好跟有序度相反（默认从小到大为有序），我想你应该已经想到了。关于逆序度，我就不举例子讲了。你可以对照我讲的有序度的例子自己看下。\n逆序元素对：a[i] &gt; a[j], 如果i &lt; j。\n关于这三个概念，我们还可以得到一个公式：逆序度=满有序度-有序度。我们排序的过程就是一种增加有序度，减少逆序度的过程，最后达到满有序度，就说明排序完成了。\n我还是拿前面举的那个冒泡排序的例子来说明。要排序的数组的初始状态是4，5，6，3，2，1 ，其中，有序元素对有(4，5) (4，6)(5，6)，所以有序度是3。n=6，所以排序完成之后终态的满有序度为n*(n-1)/2=15。\n\n冒泡排序包含两个操作原子，比较和交换。每交换一次，有序度就加1。不管算法怎么改进，交换次数总是确定的，即为逆序度，也就是n*(n-1)/2–初始有序度。此例中就是15–3=12，要进行12次交换操作。\n对于包含n个数据的数组进行冒泡排序，平均交换次数是多少呢？最坏情况下，初始状态的有序度是0，所以要进行n*(n-1)/2次交换。最好情况下，初始状态的有序度是n*(n-1)/2，就不需要进行交换。我们可以取个中间值n*(n-1)/4，来表示初始有序度既不是很高也不是很低的平均情况。\n换句话说，平均情况下，需要n*(n-1)/4次交换操作，比较操作肯定要比交换操作多，而复杂度的上限是O(n2)，所以平均情况下的时间复杂度就是O(n2)。\n这个平均时间复杂度推导过程其实并不严格，但是很多时候很实用，毕竟概率论的定量分析太复杂，不太好用。等我们讲到快排的时候，我还会再次用这种“不严格”的方法来分析平均时间复杂度。\n插入排序（Insertion Sort）我们先来看一个问题。一个有序的数组，我们往里面添加一个新的数据后，如何继续保持数据有序呢？很简单，我们只要遍历数组，找到数据应该插入的位置将其插入即可。\n\n这是一个动态排序的过程，即动态地往有序集合中添加数据，我们可以通过这种方法保持集合中的数据一直有序。而对于一组静态数据，我们也可以借鉴上面讲的插入方法，来进行排序，于是就有了插入排序算法。\n那插入排序具体是如何借助上面的思想来实现排序的呢？\n首先，我们将数组中的数据分为两个区间，已排序区间和未排序区间。初始已排序区间只有一个元素，就是数组的第一个元素。插入算法的核心思想是取未排序区间中的元素，在已排序区间中找到合适的插入位置将其插入，并保证已排序区间数据一直有序。重复这个过程，直到未排序区间中元素为空，算法结束。\n如图所示，要排序的数据是4，5，6，1，3，2，其中左侧为已排序区间，右侧是未排序区间。\n\n插入排序也包含两种操作，一种是元素的比较，一种是元素的移动。当我们需要将一个数据a插入到已排序区间时，需要拿a与已排序区间的元素依次比较大小，找到合适的插入位置。找到插入点之后，我们还需要将插入点之后的元素顺序往后移动一位，这样才能腾出位置给元素a插入。\n对于不同的查找插入点方法（从头到尾、从尾到头），元素的比较次数是有区别的。但对于一个给定的初始序列，移动操作的次数总是固定的，就等于逆序度。\n为什么说移动次数就等于逆序度呢？我拿刚才的例子画了一个图表，你一看就明白了。满有序度是n*(n-1)/2=15，初始序列的有序度是5，所以逆序度是10。插入排序中，数据移动的个数总和也等于10=3+3+4。\n\n插入排序的原理也很简单吧？我也将代码实现贴在这里，你可以结合着代码再看下。\n// 插入排序，a表示数组，n表示数组大小public void insertionSort(int[] a, int n) &#123;    if (n &lt;= 1) return;    for (int i = 1; i &lt; n; ++i) &#123;        int value = a[i];        int j = i - 1;        // 查找插入的位置        for (; j &gt;= 0; --j) &#123;            if (a[j] &gt; value) &#123;                a[j+1] = a[j];  // 数据移动            &#125; else &#123;                break;            &#125;        &#125;        a[j+1] = value; // 插入数据    &#125;&#125;\n现在，我们来看点稍微复杂的东西。我这里还是有三个问题要问你。\n第一，插入排序是原地排序算法吗？\n从实现过程可以很明显地看出，插入排序算法的运行并不需要额外的存储空间，所以空间复杂度是O(1)，也就是说，这是一个原地排序算法。\n第二，插入排序是稳定的排序算法吗？\n在插入排序中，对于值相同的元素，我们可以选择将后面出现的元素，插入到前面出现元素的后面，这样就可以保持原有的前后顺序不变，所以插入排序是稳定的排序算法。\n第三，插入排序的时间复杂度是多少？\n如果要排序的数据已经是有序的，我们并不需要搬移任何数据。如果我们从尾到头在有序数据组里面查找插入位置，每次只需要比较一个数据就能确定插入的位置。所以这种情况下，最好是时间复杂度为O(n)。注意，这里是从尾到头遍历已经有序的数据。\n如果数组是倒序的，每次插入都相当于在数组的第一个位置插入新的数据，所以需要移动大量的数据，所以最坏情况时间复杂度为O(n2)。\n还记得我们在数组中插入一个数据的平均时间复杂度是多少吗？没错，是O(n)。所以，对于插入排序来说，每次插入操作都相当于在数组中插入一个数据，循环执行n次插入操作，所以平均时间复杂度为O(n2)。\n选择排序（Selection Sort）选择排序算法的实现思路有点类似插入排序，也分已排序区间和未排序区间。但是选择排序每次会从未排序区间中找到最小的元素，将其放到已排序区间的末尾。\n\n照例，也有三个问题需要你思考，不过前面两种排序算法我已经分析得很详细了，这里就直接公布答案了。\n首先，选择排序空间复杂度为O(1)，是一种原地排序算法。选择排序的最好情况时间复杂度、最坏情况和平均情况时间复杂度都为O(n2)。你可以自己来分析看看。\n那选择排序是稳定的排序算法吗？这个问题我着重来说一下。\n答案是否定的，选择排序是一种不稳定的排序算法。从我前面画的那张图中，你可以看出来，选择排序每次都要找剩余未排序元素中的最小值，并和前面的元素交换位置，这样破坏了稳定性。\n比如5，8，5，2，9这样一组数据，使用选择排序算法来排序的话，第一次找到最小元素2，与第一个5交换位置，那第一个5和中间的5顺序就变了，所以就不稳定了。正是因此，相对于冒泡排序和插入排序，选择排序就稍微逊色了。\n解答开篇基本的知识都讲完了，我们来看开篇的问题：冒泡排序和插入排序的时间复杂度都是O(n2)，都是原地排序算法，为什么插入排序要比冒泡排序更受欢迎呢？\n我们前面分析冒泡排序和插入排序的时候讲到，冒泡排序不管怎么优化，元素交换的次数是一个固定值，是原始数据的逆序度。插入排序是同样的，不管怎么优化，元素移动的次数也等于原始数据的逆序度。\n但是，从代码实现上来看，冒泡排序的数据交换要比插入排序的数据移动要复杂，冒泡排序需要3个赋值操作，而插入排序只需要1个。我们来看这段操作：\n冒泡排序中数据的交换操作：if (a[j] &gt; a[j+1]) &#123; &#x2F;&#x2F; 交换   int tmp &#x3D; a[j];   a[j] &#x3D; a[j+1];   a[j+1] &#x3D; tmp;   flag &#x3D; true;&#125;插入排序中数据的移动操作：if (a[j] &gt; value) &#123;  a[j+1] &#x3D; a[j];  &#x2F;&#x2F; 数据移动&#125; else &#123;  break;&#125;\n我们把执行一个赋值语句的时间粗略地计为单位时间（unit_time），然后分别用冒泡排序和插入排序对同一个逆序度是K的数组进行排序。用冒泡排序，需要K次交换操作，每次需要3个赋值语句，所以交换操作总耗时就是3*K单位时间。而插入排序中数据移动操作只需要K个单位时间。\n这个只是我们非常理论的分析，为了实验，针对上面的冒泡排序和插入排序的Java代码，我写了一个性能对比测试程序，随机生成10000个数组，每个数组中包含200个数据，然后在我的机器上分别用冒泡和插入排序算法来排序，冒泡排序算法大约700ms才能执行完成，而插入排序只需要100ms左右就能搞定！\n所以，虽然冒泡排序和插入排序在时间复杂度上是一样的，都是O(n2)，但是如果我们希望把性能优化做到极致，那肯定首选插入排序。插入排序的算法思路也有很大的优化空间，我们只是讲了最基础的一种。如果你对插入排序的优化感兴趣，可以自行学习一下希尔排序。\n内容小结要想分析、评价一个排序算法，需要从执行效率、内存消耗和稳定性三个方面来看。因此，这一节，我带你分析了三种时间复杂度是O(n2)的排序算法，冒泡排序、插入排序、选择排序。你需要重点掌握的是它们的分析方法。\n\n这三种时间复杂度为O(n2)的排序算法中，冒泡排序、选择排序，可能就纯粹停留在理论的层面了，学习的目的也只是为了开拓思维，实际开发中应用并不多，但是插入排序还是挺有用的。后面讲排序优化的时候，我会讲到，有些编程语言中的排序函数的实现原理会用到插入排序算法。\n今天讲的这三种排序算法，实现代码都非常简单，对于小规模数据的排序，用起来非常高效。但是在大规模数据排序的时候，这个时间复杂度还是稍微有点高，所以我们更倾向于用下一节要讲的时间复杂度为O(nlogn)的排序算法。\n课后思考我们讲过，特定算法是依赖特定的数据结构的。我们今天讲的几种排序算法，都是基于数组实现的。如果数据存储在链表中，这三种排序算法还能工作吗？如果能，那相应的时间、空间复杂度又是多少呢？\n欢迎留言和我分享，我会第一时间给你反馈。\n我已将本节内容相关的详细代码更新到GitHub，戳此即可查看。\n\n","categories":["数据结构与算法","基础篇"]},{"title":"13 | 线性排序：如何根据年龄给100万用户数据排序？","url":"/2020/08/07/13%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%9A%E7%BA%BF%E6%80%A7%E6%8E%92%E5%BA%8F/","content":"上两节中，我带你着重分析了几种常用排序算法的原理、时间复杂度、空间复杂度、稳定性等。今天，我会讲三种时间复杂度是O(n)的排序算法：桶排序、计数排序、基数排序。因为这些排序算法的时间复杂度是线性的，所以我们把这类排序算法叫作线性排序（Linear sort）。之所以能做到线性的时间复杂度，主要原因是，这三个算法是非基于比较的排序算法，都不涉及元素之间的比较操作。\n这几种排序算法理解起来都不难，时间、空间复杂度分析起来也很简单，但是对要排序的数据要求很苛刻，所以我们今天的学习重点是掌握这些排序算法的适用场景。\n按照惯例，我先给你出一道思考题：如何根据年龄给100万用户排序？ 你可能会说，我用上一节课讲的归并、快排就可以搞定啊！是的，它们也可以完成功能，但是时间复杂度最低也是O(nlogn)。有没有更快的排序方法呢？让我们一起进入今天的内容！\n桶排序（Bucket sort）首先，我们来看桶排序。桶排序，顾名思义，会用到“桶”，核心思想是将要排序的数据分到几个有序的桶里，每个桶里的数据再单独进行排序。桶内排完序之后，再把每个桶里的数据按照顺序依次取出，组成的序列就是有序的了。\n\n桶排序的时间复杂度为什么是O(n)呢？我们一块儿来分析一下。\n如果要排序的数据有n个，我们把它们均匀地划分到m个桶内，每个桶里就有k=n/m个元素。每个桶内部使用快速排序，时间复杂度为O(k * logk)。m个桶排序的时间复杂度就是O(m * k * logk)，因为k=n/m，所以整个桶排序的时间复杂度就是O(n*log(n/m))。当桶的个数m接近数据个数n时，log(n/m)就是一个非常小的常量，这个时候桶排序的时间复杂度接近O(n)。\n桶排序看起来很优秀，那它是不是可以替代我们之前讲的排序算法呢？\n答案当然是否定的。为了让你轻松理解桶排序的核心思想，我刚才做了很多假设。实际上，桶排序对要排序数据的要求是非常苛刻的。\n首先，要排序的数据需要很容易就能划分成m个桶，并且，桶与桶之间有着天然的大小顺序。这样每个桶内的数据都排序完之后，桶与桶之间的数据不需要再进行排序。\n其次，数据在各个桶之间的分布是比较均匀的。如果数据经过桶的划分之后，有些桶里的数据非常多，有些非常少，很不平均，那桶内数据排序的时间复杂度就不是常量级了。在极端情况下，如果数据都被划分到一个桶里，那就退化为O(nlogn)的排序算法了。\n桶排序比较适合用在外部排序中。所谓的外部排序就是数据存储在外部磁盘中，数据量比较大，内存有限，无法将数据全部加载到内存中。\n比如说我们有10GB的订单数据，我们希望按订单金额（假设金额都是正整数）进行排序，但是我们的内存有限，只有几百MB，没办法一次性把10GB的数据都加载到内存中。这个时候该怎么办呢？\n现在我来讲一下，如何借助桶排序的处理思想来解决这个问题。\n我们可以先扫描一遍文件，看订单金额所处的数据范围。假设经过扫描之后我们得到，订单金额最小是1元，最大是10万元。我们将所有订单根据金额划分到100个桶里，第一个桶我们存储金额在1元到1000元之内的订单，第二桶存储金额在1001元到2000元之内的订单，以此类推。每一个桶对应一个文件，并且按照金额范围的大小顺序编号命名（00，01，02…99）。\n理想的情况下，如果订单金额在1到10万之间均匀分布，那订单会被均匀划分到100个文件中，每个小文件中存储大约100MB的订单数据，我们就可以将这100个小文件依次放到内存中，用快排来排序。等所有文件都排好序之后，我们只需要按照文件编号，从小到大依次读取每个小文件中的订单数据，并将其写入到一个文件中，那这个文件中存储的就是按照金额从小到大排序的订单数据了。\n不过，你可能也发现了，订单按照金额在1元到10万元之间并不一定是均匀分布的 ，所以10GB订单数据是无法均匀地被划分到100个文件中的。有可能某个金额区间的数据特别多，划分之后对应的文件就会很大，没法一次性读入内存。这又该怎么办呢？\n针对这些划分之后还是比较大的文件，我们可以继续划分，比如，订单金额在1元到1000元之间的比较多，我们就将这个区间继续划分为10个小区间，1元到100元，101元到200元，201元到300元…901元到1000元。如果划分之后，101元到200元之间的订单还是太多，无法一次性读入内存，那就继续再划分，直到所有的文件都能读入内存为止。\n计数排序（Counting sort）我个人觉得，计数排序其实是桶排序的一种特殊情况。当要排序的n个数据，所处的范围并不大的时候，比如最大值是k，我们就可以把数据划分成k个桶。每个桶内的数据值都是相同的，省掉了桶内排序的时间。\n我们都经历过高考，高考查分数系统你还记得吗？我们查分数的时候，系统会显示我们的成绩以及所在省的排名。如果你所在的省有50万考生，如何通过成绩快速排序得出名次呢？\n考生的满分是900分，最小是0分，这个数据的范围很小，所以我们可以分成901个桶，对应分数从0分到900分。根据考生的成绩，我们将这50万考生划分到这901个桶里。桶内的数据都是分数相同的考生，所以并不需要再进行排序。我们只需要依次扫描每个桶，将桶内的考生依次输出到一个数组中，就实现了50万考生的排序。因为只涉及扫描遍历操作，所以时间复杂度是O(n)。\n计数排序的算法思想就是这么简单，跟桶排序非常类似，只是桶的大小粒度不一样。不过，为什么这个排序算法叫“计数”排序呢？“计数”的含义来自哪里呢？\n想弄明白这个问题，我们就要来看计数排序算法的实现方法。我还拿考生那个例子来解释。为了方便说明，我对数据规模做了简化。假设只有8个考生，分数在0到5分之间。这8个考生的成绩我们放在一个数组A[8]中，它们分别是：2，5，3，0，2，3，0，3。\n考生的成绩从0到5分，我们使用大小为6的数组C[6]表示桶，其中下标对应分数。不过，C[6]内存储的并不是考生，而是对应的考生个数。像我刚刚举的那个例子，我们只需要遍历一遍考生分数，就可以得到C[6]的值。\n\n从图中可以看出，分数为3分的考生有3个，小于3分的考生有4个，所以，成绩为3分的考生在排序之后的有序数组R[8]中，会保存下标4，5，6的位置。\n\n那我们如何快速计算出，每个分数的考生在有序数组中对应的存储位置呢？这个处理方法非常巧妙，很不容易想到。\n思路是这样的：我们对C[6]数组顺序求和，C[6]存储的数据就变成了下面这样子。C[k]里存储小于等于分数k的考生个数。\n\n有了前面的数据准备之后，现在我就要讲计数排序中最复杂、最难理解的一部分了，请集中精力跟着我的思路！\n我们从后到前依次扫描数组A。比如，当扫描到3时，我们可以从数组C中取出下标为3的值7，也就是说，到目前为止，包括自己在内，分数小于等于3的考生有7个，也就是说3是数组R中的第7个元素（也就是数组R中下标为6的位置）。当3放入到数组R中后，小于等于3的元素就只剩下了6个了，所以相应的C[3]要减1，变成6。\n以此类推，当我们扫描到第2个分数为3的考生的时候，就会把它放入数组R中的第6个元素的位置（也就是下标为5的位置）。当我们扫描完整个数组A后，数组R内的数据就是按照分数从小到大有序排列的了。\n\n上面的过程有点复杂，我写成了代码，你可以对照着看下。\n// 计数排序，a是数组，n是数组大小。假设数组中存储的都是非负整数。public void countingSort(int[] a, int n) &#123;    if (n &lt;= 1) return;    // 查找数组中数据的范围    int max = a[0];    for (int i = 1; i &lt; n; ++i) &#123;        if (max &lt; a[i]) &#123;            max = a[i];        &#125;    &#125;    int[] c = new int[max + 1]; // 申请一个计数数组c，下标大小[0,max]    for (int i = 0; i &lt;= max; ++i) &#123;        c[i] = 0;    &#125;    // 计算每个元素的个数，放入c中    for (int i = 0; i &lt; n; ++i) &#123;        c[a[i]]++;    &#125;    // 依次累加    for (int i = 1; i &lt;= max; ++i) &#123;        c[i] = c[i-1] + c[i];    &#125;    // 临时数组r，存储排序之后的结果    int[] r = new int[n];    // 计算排序的关键步骤，有点难理解    for (int i = n - 1; i &gt;= 0; --i) &#123;        int index = c[a[i]]-1;        r[index] = a[i];        c[a[i]]--;    &#125;    // 将结果拷贝给a数组    for (int i = 0; i &lt; n; ++i) &#123;        a[i] = r[i];    &#125;&#125;\n这种利用另外一个数组来计数的实现方式是不是很巧妙呢？这也是为什么这种排序算法叫计数排序的原因。不过，你千万不要死记硬背上面的排序过程，重要的是理解和会用。\n我总结一下，计数排序只能用在数据范围不大的场景中，如果数据范围k比要排序的数据n大很多，就不适合用计数排序了。而且，计数排序只能给非负整数排序，如果要排序的数据是其他类型的，要将其在不改变相对大小的情况下，转化为非负整数。\n比如，还是拿考生这个例子。如果考生成绩精确到小数后一位，我们就需要将所有的分数都先乘以10，转化成整数，然后再放到9010个桶内。再比如，如果要排序的数据中有负数，数据的范围是[-1000, 1000]，那我们就需要先对每个数据都加1000，转化成非负整数。\n基数排序（Radix sort）我们再来看这样一个排序问题。假设我们有10万个手机号码，希望将这10万个手机号码从小到大排序，你有什么比较快速的排序方法呢？\n我们之前讲的快排，时间复杂度可以做到O(nlogn)，还有更高效的排序算法吗？桶排序、计数排序能派上用场吗？手机号码有11位，范围太大，显然不适合用这两种排序算法。针对这个排序问题，有没有时间复杂度是O(n)的算法呢？现在我就来介绍一种新的排序算法，基数排序。\n刚刚这个问题里有这样的规律：假设要比较两个手机号码a，b的大小，如果在前面几位中，a手机号码已经比b手机号码大了，那后面的几位就不用看了。\n借助稳定排序算法，这里有一个巧妙的实现思路。还记得我们第11节中，在阐述排序算法的稳定性的时候举的订单的例子吗？我们这里也可以借助相同的处理思路，先按照最后一位来排序手机号码，然后，再按照倒数第二位重新排序，以此类推，最后按照第一位重新排序。经过11次排序之后，手机号码就都有序了。\n手机号码稍微有点长，画图比较不容易看清楚，我用字符串排序的例子，画了一张基数排序的过程分解图，你可以看下。\n\n注意，这里按照每位来排序的排序算法要是稳定的，否则这个实现思路就是不正确的。因为如果是非稳定排序算法，那最后一次排序只会考虑最高位的大小顺序，完全不管其他位的大小关系，那么低位的排序就完全没有意义了。\n根据每一位来排序，我们可以用刚讲过的桶排序或者计数排序，它们的时间复杂度可以做到O(n)。如果要排序的数据有k位，那我们就需要k次桶排序或者计数排序，总的时间复杂度是O(k*n)。当k不大的时候，比如手机号码排序的例子，k最大就是11，所以基数排序的时间复杂度就近似于O(n)。\n实际上，有时候要排序的数据并不都是等长的，比如我们排序牛津字典中的20万个英文单词，最短的只有1个字母，最长的我特意去查了下，有45个字母，中文翻译是尘肺病。对于这种不等长的数据，基数排序还适用吗？\n实际上，我们可以把所有的单词补齐到相同长度，位数不够的可以在后面补“0”，因为根据ASCII值，所有字母都大于“0”，所以补“0”不会影响到原有的大小顺序。这样就可以继续用基数排序了。\n我来总结一下，基数排序对要排序的数据是有要求的，需要可以分割出独立的“位”来比较，而且位之间有递进的关系，如果a数据的高位比b数据大，那剩下的低位就不用比较了。除此之外，每一位的数据范围不能太大，要可以用线性排序算法来排序，否则，基数排序的时间复杂度就无法做到O(n)了。\n解答开篇今天的内容学完了。我们再回过头来看看开篇的思考题：如何根据年龄给100万用户排序？现在思考题是不是变得非常简单了呢？我来说一下我的解决思路。\n实际上，根据年龄给100万用户排序，就类似按照成绩给50万考生排序。我们假设年龄的范围最小1岁，最大不超过120岁。我们可以遍历这100万用户，根据年龄将其划分到这120个桶里，然后依次顺序遍历这120个桶中的元素。这样就得到了按照年龄排序的100万用户数据。\n内容小结今天，我们学习了3种线性时间复杂度的排序算法，有桶排序、计数排序、基数排序。它们对要排序的数据都有比较苛刻的要求，应用不是非常广泛。但是如果数据特征比较符合这些排序算法的要求，应用这些算法，会非常高效，线性时间复杂度可以达到O(n)。\n桶排序和计数排序的排序思想是非常相似的，都是针对范围不大的数据，将数据划分成不同的桶来实现排序。基数排序要求数据可以划分成高低位，位之间有递进关系。比较两个数，我们只需要比较高位，高位相同的再比较低位。而且每一位的数据范围不能太大，因为基数排序算法需要借助桶排序或者计数排序来完成每一个位的排序工作。\n课后思考我们今天讲的都是针对特殊数据的排序算法。实际上，还有很多看似是排序但又不需要使用排序算法就能处理的排序问题。\n假设我们现在需要对D，a，F，B，c，A，z这个字符串进行排序，要求将其中所有小写字母都排在大写字母的前面，但小写字母内部和大写字母内部不要求有序。比如经过排序之后为a，c，z，D，F，B，A，这个如何来实现呢？如果字符串中存储的不仅有大小写字母，还有数字。要将小写字母的放到前面，大写字母放在最后，数字放在中间，不用排序算法，又该怎么解决呢？\n欢迎留言和我分享，我会第一时间给你反馈。\n我已将本节内容相关的详细代码更新到GitHub，戳此即可查看。\n\n","categories":["数据结构与算法","基础篇"]},{"title":"12 | 排序（下）：如何用快排思想在O(n)内查找第K大元素？","url":"/2020/08/07/12%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%9A%E6%8E%92%E5%BA%8F%EF%BC%88%E4%B8%8B%EF%BC%89/","content":"上一节我讲了冒泡排序、插入排序、选择排序这三种排序算法，它们的时间复杂度都是O(n2)，比较高，适合小规模数据的排序。今天，我讲两种时间复杂度为O(nlogn)的排序算法，归并排序和快速排序。这两种排序算法适合大规模的数据排序，比上一节讲的那三种排序算法要更常用。\n归并排序和快速排序都用到了分治思想，非常巧妙。我们可以借鉴这个思想，来解决非排序的问题，比如：如何在O(n)的时间复杂度内查找一个无序数组中的第K大元素？ 这就要用到我们今天要讲的内容。\n归并排序的原理我们先来看归并排序（Merge Sort）。\n归并排序的核心思想还是蛮简单的。如果要排序一个数组，我们先把数组从中间分成前后两部分，然后对前后两部分分别排序，再将排好序的两部分合并在一起，这样整个数组就都有序了。\n\n归并排序使用的就是分治思想。分治，顾名思义，就是分而治之，将一个大问题分解成小的子问题来解决。小的子问题解决了，大问题也就解决了。\n从我刚才的描述，你有没有感觉到，分治思想跟我们前面讲的递归思想很像。是的，分治算法一般都是用递归来实现的。分治是一种解决问题的处理思想，递归是一种编程技巧，这两者并不冲突。分治算法的思想我后面会有专门的一节来讲，现在不展开讨论，我们今天的重点还是排序算法。\n前面我通过举例让你对归并有了一个感性的认识，又告诉你，归并排序用的是分治思想，可以用递归来实现。我们现在就来看看如何用递归代码来实现归并排序。\n我在第10节讲的递归代码的编写技巧你还记得吗？写递归代码的技巧就是，分析得出递推公式，然后找到终止条件，最后将递推公式翻译成递归代码。所以，要想写出归并排序的代码，我们先写出归并排序的递推公式。\n递推公式：merge_sort(p…r) &#x3D; merge(merge_sort(p…q), merge_sort(q+1…r))终止条件：p &gt;&#x3D; r 不用再继续分解\n我来解释一下这个递推公式。\nmerge_sort(p…r)表示，给下标从p到r之间的数组排序。我们将这个排序问题转化为了两个子问题，merge_sort(p…q)和merge_sort(q+1…r)，其中下标q等于p和r的中间位置，也就是(p+r)/2。当下标从p到q和从q+1到r这两个子数组都排好序之后，我们再将两个有序的子数组合并在一起，这样下标从p到r之间的数据就也排好序了。\n有了递推公式，转化成代码就简单多了。为了阅读方便，我这里只给出伪代码，你可以翻译成你熟悉的编程语言。\n&#x2F;&#x2F; 归并排序算法, A是数组，n表示数组大小merge_sort(A, n) &#123;  merge_sort_c(A, 0, n-1)&#125;&#x2F;&#x2F; 递归调用函数merge_sort_c(A, p, r) &#123;  &#x2F;&#x2F; 递归终止条件  if p &gt;&#x3D; r  then return  &#x2F;&#x2F; 取p到r之间的中间位置q  q &#x3D; (p+r) &#x2F; 2  &#x2F;&#x2F; 分治递归  merge_sort_c(A, p, q)  merge_sort_c(A, q+1, r)  &#x2F;&#x2F; 将A[p...q]和A[q+1...r]合并为A[p...r]  merge(A[p...r], A[p...q], A[q+1...r])&#125;\n你可能已经发现了，merge(A[p…r], A[p…q], A[q+1…r])这个函数的作用就是，将已经有序的A[p…q]和A[q+1…r]合并成一个有序的数组，并且放入A[p…r]。那这个过程具体该如何做呢？\n如图所示，我们申请一个临时数组tmp，大小与A[p…r]相同。我们用两个游标i和j，分别指向A[p…q]和A[q+1…r]的第一个元素。比较这两个元素A[i]和A[j]，如果A[i]&lt;=A[j]，我们就把A[i]放入到临时数组tmp，并且i后移一位，否则将A[j]放入到数组tmp，j后移一位。\n继续上述比较过程，直到其中一个子数组中的所有数据都放入临时数组中，再把另一个数组中的数据依次加入到临时数组的末尾，这个时候，临时数组中存储的就是两个子数组合并之后的结果了。最后再把临时数组tmp中的数据拷贝到原数组A[p…r]中。\n\n我们把merge()函数写成伪代码，就是下面这样：\nmerge(A[p...r], A[p...q], A[q+1...r]) &#123;  var i :&#x3D; p，j :&#x3D; q+1，k :&#x3D; 0 &#x2F;&#x2F; 初始化变量i, j, k  var tmp :&#x3D; new array[0...r-p] &#x2F;&#x2F; 申请一个大小跟A[p...r]一样的临时数组  while i&lt;&#x3D;q AND j&lt;&#x3D;r do &#123;    if A[i] &lt;&#x3D; A[j] &#123;      tmp[k++] &#x3D; A[i++] &#x2F;&#x2F; i++等于i:&#x3D;i+1    &#125; else &#123;      tmp[k++] &#x3D; A[j++]    &#125;  &#125;    &#x2F;&#x2F; 判断哪个子数组中有剩余的数据  var start :&#x3D; i，end :&#x3D; q  if j&lt;&#x3D;r then start :&#x3D; j, end:&#x3D;r    &#x2F;&#x2F; 将剩余的数据拷贝到临时数组tmp  while start &lt;&#x3D; end do &#123;    tmp[k++] &#x3D; A[start++]  &#125;    &#x2F;&#x2F; 将tmp中的数组拷贝回A[p...r]  for i:&#x3D;0 to r-p do &#123;    A[p+i] &#x3D; tmp[i]  &#125;&#125;\n你还记得第7讲讲过的利用哨兵简化编程的处理技巧吗？merge()合并函数如果借助哨兵，代码就会简洁很多，这个问题留给你思考。\n归并排序的性能分析这样跟着我一步一步分析，归并排序是不是没那么难啦？还记得上节课我们分析排序算法的三个问题吗？接下来，我们来看归并排序的三个问题。\n第一，归并排序是稳定的排序算法吗？\n结合我前面画的那张图和归并排序的伪代码，你应该能发现，归并排序稳不稳定关键要看merge()函数，也就是两个有序子数组合并成一个有序数组的那部分代码。\n在合并的过程中，如果A[p…q]和A[q+1…r]之间有值相同的元素，那我们可以像伪代码中那样，先把A[p…q]中的元素放入tmp数组。这样就保证了值相同的元素，在合并前后的先后顺序不变。所以，归并排序是一个稳定的排序算法。\n第二，归并排序的时间复杂度是多少？\n归并排序涉及递归，时间复杂度的分析稍微有点复杂。我们正好借此机会来学习一下，如何分析递归代码的时间复杂度。\n在递归那一节我们讲过，递归的适用场景是，一个问题a可以分解为多个子问题b、c，那求解问题a就可以分解为求解问题b、c。问题b、c解决之后，我们再把b、c的结果合并成a的结果。\n如果我们定义求解问题a的时间是T(a)，求解问题b、c的时间分别是T(b)和 T( c)，那我们就可以得到这样的递推关系式：\nT(a) &#x3D; T(b) + T(c) + K\n其中K等于将两个子问题b、c的结果合并成问题a的结果所消耗的时间。\n从刚刚的分析，我们可以得到一个重要的结论：不仅递归求解的问题可以写成递推公式，递归代码的时间复杂度也可以写成递推公式。\n套用这个公式，我们来分析一下归并排序的时间复杂度。\n我们假设对n个元素进行归并排序需要的时间是T(n)，那分解成两个子数组排序的时间都是T(n/2)。我们知道，merge()函数合并两个有序子数组的时间复杂度是O(n)。所以，套用前面的公式，归并排序的时间复杂度的计算公式就是：\nT(1) &#x3D; C；   n&#x3D;1时，只需要常量级的执行时间，所以表示为C。T(n) &#x3D; 2*T(n&#x2F;2) + n； n&gt;1\n通过这个公式，如何来求解T(n)呢？还不够直观？那我们再进一步分解一下计算过程。\nT(n) &#x3D; 2*T(n&#x2F;2) + n     &#x3D; 2*(2*T(n&#x2F;4) + n&#x2F;2) + n &#x3D; 4*T(n&#x2F;4) + 2*n     &#x3D; 4*(2*T(n&#x2F;8) + n&#x2F;4) + 2*n &#x3D; 8*T(n&#x2F;8) + 3*n     &#x3D; 8*(2*T(n&#x2F;16) + n&#x2F;8) + 3*n &#x3D; 16*T(n&#x2F;16) + 4*n     ......     &#x3D; 2^k * T(n&#x2F;2^k) + k * n     ......\n通过这样一步一步分解推导，我们可以得到T(n) = 2^kT(n/2^k)+kn。当T(n/2^k)=T(1)时，也就是n/2^k=1，我们得到k=log2n 。我们将k值代入上面的公式，得到T(n)=Cn+nlog2n 。如果我们用大O标记法来表示的话，T(n)就等于O(nlogn)。所以归并排序的时间复杂度是O(nlogn)。\n从我们的原理分析和伪代码可以看出，归并排序的执行效率与要排序的原始数组的有序程度无关，所以其时间复杂度是非常稳定的，不管是最好情况、最坏情况，还是平均情况，时间复杂度都是O(nlogn)。\n第三，归并排序的空间复杂度是多少？\n归并排序的时间复杂度任何情况下都是O(nlogn)，看起来非常优秀。（待会儿你会发现，即便是快速排序，最坏情况下，时间复杂度也是O(n2)。）但是，归并排序并没有像快排那样，应用广泛，这是为什么呢？因为它有一个致命的“弱点”，那就是归并排序不是原地排序算法。\n这是因为归并排序的合并函数，在合并两个有序数组为一个有序数组时，需要借助额外的存储空间。这一点你应该很容易理解。那我现在问你，归并排序的空间复杂度到底是多少呢？是O(n)，还是O(nlogn)，应该如何分析呢？\n如果我们继续按照分析递归时间复杂度的方法，通过递推公式来求解，那整个归并过程需要的空间复杂度就是O(nlogn)。不过，类似分析时间复杂度那样来分析空间复杂度，这个思路对吗？\n实际上，递归代码的空间复杂度并不能像时间复杂度那样累加。刚刚我们忘记了最重要的一点，那就是，尽管每次合并操作都需要申请额外的内存空间，但在合并完成之后，临时开辟的内存空间就被释放掉了。在任意时刻，CPU只会有一个函数在执行，也就只会有一个临时的内存空间在使用。临时内存空间最大也不会超过n个数据的大小，所以空间复杂度是O(n)。\n快速排序的原理我们再来看快速排序算法（Quicksort），我们习惯性把它简称为“快排”。快排利用的也是分治思想。乍看起来，它有点像归并排序，但是思路其实完全不一样。我们待会会讲两者的区别。现在，我们先来看下快排的核心思想。\n快排的思想是这样的：如果要排序数组中下标从p到r之间的一组数据，我们选择p到r之间的任意一个数据作为pivot（分区点）。\n我们遍历p到r之间的数据，将小于pivot的放到左边，将大于pivot的放到右边，将pivot放到中间。经过这一步骤之后，数组p到r之间的数据就被分成了三个部分，前面p到q-1之间都是小于pivot的，中间是pivot，后面的q+1到r之间是大于pivot的。\n\n根据分治、递归的处理思想，我们可以用递归排序下标从p到q-1之间的数据和下标从q+1到r之间的数据，直到区间缩小为1，就说明所有的数据都有序了。\n如果我们用递推公式来将上面的过程写出来的话，就是这样：\n递推公式：quick_sort(p…r) &#x3D; quick_sort(p…q-1) + quick_sort(q+1… r)终止条件：p &gt;&#x3D; r\n我将递推公式转化成递归代码。跟归并排序一样，我还是用伪代码来实现，你可以翻译成你熟悉的任何语言。\n&#x2F;&#x2F; 快速排序，A是数组，n表示数组的大小quick_sort(A, n) &#123;  quick_sort_c(A, 0, n-1)&#125;&#x2F;&#x2F; 快速排序递归函数，p,r为下标quick_sort_c(A, p, r) &#123;  if p &gt;&#x3D; r then return    q &#x3D; partition(A, p, r) &#x2F;&#x2F; 获取分区点  quick_sort_c(A, p, q-1)  quick_sort_c(A, q+1, r)&#125;\n归并排序中有一个merge()合并函数，我们这里有一个partition()分区函数。partition()分区函数实际上我们前面已经讲过了，就是随机选择一个元素作为pivot（一般情况下，可以选择p到r区间的最后一个元素），然后对A[p…r]分区，函数返回pivot的下标。\n如果我们不考虑空间消耗的话，partition()分区函数可以写得非常简单。我们申请两个临时数组X和Y，遍历A[p…r]，将小于pivot的元素都拷贝到临时数组X，将大于pivot的元素都拷贝到临时数组Y，最后再将数组X和数组Y中数据顺序拷贝到A[p…r]。\n\n但是，如果按照这种思路实现的话，partition()函数就需要很多额外的内存空间，所以快排就不是原地排序算法了。如果我们希望快排是原地排序算法，那它的空间复杂度得是O(1)，那partition()分区函数就不能占用太多额外的内存空间，我们就需要在A[p…r]的原地完成分区操作。\n原地分区函数的实现思路非常巧妙，我写成了伪代码，我们一起来看一下。\npartition(A, p, r) &#123;  pivot :&#x3D; A[r]  i :&#x3D; p  for j :&#x3D; p to r-1 do &#123;    if A[j] &lt; pivot &#123;      swap A[i] with A[j]      i :&#x3D; i+1    &#125;  &#125;  swap A[i] with A[r]  return i\n这里的处理有点类似选择排序。我们通过游标i把A[p…r-1]分成两部分。A[p…i-1]的元素都是小于pivot的，我们暂且叫它“已处理区间”，A[i…r-1]是“未处理区间”。我们每次都从未处理的区间A[i…r-1]中取一个元素A[j]，与pivot对比，如果小于pivot，则将其加入到已处理区间的尾部，也就是A[i]的位置。\n数组的插入操作还记得吗？在数组某个位置插入元素，需要搬移数据，非常耗时。当时我们也讲了一种处理技巧，就是交换，在O(1)的时间复杂度内完成插入操作。这里我们也借助这个思想，只需要将A[i]与A[j]交换，就可以在O(1)时间复杂度内将A[j]放到下标为i的位置。\n文字不如图直观，所以我画了一张图来展示分区的整个过程。\n\n因为分区的过程涉及交换操作，如果数组中有两个相同的元素，比如序列6，8，7，6，3，5，9，4，在经过第一次分区操作之后，两个6的相对先后顺序就会改变。所以，快速排序并不是一个稳定的排序算法。\n到此，快速排序的原理你应该也掌握了。现在，我再来看另外一个问题：快排和归并用的都是分治思想，递推公式和递归代码也非常相似，那它们的区别在哪里呢？\n\n可以发现，归并排序的处理过程是由下到上的，先处理子问题，然后再合并。而快排正好相反，它的处理过程是由上到下的，先分区，然后再处理子问题。归并排序虽然是稳定的、时间复杂度为O(nlogn)的排序算法，但是它是非原地排序算法。我们前面讲过，归并之所以是非原地排序算法，主要原因是合并函数无法在原地执行。快速排序通过设计巧妙的原地分区函数，可以实现原地排序，解决了归并排序占用太多内存的问题。\n快速排序的性能分析现在，我们来分析一下快速排序的性能。我在讲解快排的实现原理的时候，已经分析了稳定性和空间复杂度。快排是一种原地、不稳定的排序算法。现在，我们集中精力来看快排的时间复杂度。\n快排也是用递归来实现的。对于递归代码的时间复杂度，我前面总结的公式，这里也还是适用的。如果每次分区操作，都能正好把数组分成大小接近相等的两个小区间，那快排的时间复杂度递推求解公式跟归并是相同的。所以，快排的时间复杂度也是O(nlogn)。\nT(1) &#x3D; C;   n&#x3D;1时，只需要常量级的执行时间，所以表示为C。T(n) &#x3D; 2*T(n&#x2F;2) + n; n&gt;1\n但是，公式成立的前提是每次分区操作，我们选择的pivot都很合适，正好能将大区间对等地一分为二。但实际上这种情况是很难实现的。\n我举一个比较极端的例子。如果数组中的数据原来已经是有序的了，比如1，3，5，6，8。如果我们每次选择最后一个元素作为pivot，那每次分区得到的两个区间都是不均等的。我们需要进行大约n次分区操作，才能完成快排的整个过程。每次分区我们平均要扫描大约n/2个元素，这种情况下，快排的时间复杂度就从O(nlogn)退化成了O(n2)。\n我们刚刚讲了两个极端情况下的时间复杂度，一个是分区极其均衡，一个是分区极其不均衡。它们分别对应快排的最好情况时间复杂度和最坏情况时间复杂度。那快排的平均情况时间复杂度是多少呢？\n我们假设每次分区操作都将区间分成大小为9:1的两个小区间。我们继续套用递归时间复杂度的递推公式，就会变成这样：\nT(1) &#x3D; C;   n&#x3D;1时，只需要常量级的执行时间，所以表示为C。T(n) &#x3D; T(n&#x2F;10) + T(9*n&#x2F;10) + n; n&gt;1\n这个公式的递推求解的过程非常复杂，虽然可以求解，但我不推荐用这种方法。实际上，递归的时间复杂度的求解方法除了递推公式之外，还有递归树，在树那一节我再讲，这里暂时不说。我这里直接给你结论：T(n)在大部分情况下的时间复杂度都可以做到O(nlogn)，只有在极端情况下，才会退化到O(n2)。而且，我们也有很多方法将这个概率降到很低，如何来做？我们后面章节再讲。\n解答开篇快排核心思想就是分治和分区，我们可以利用分区的思想，来解答开篇的问题：O(n)时间复杂度内求无序数组中的第K大元素。比如，4， 2， 5， 12， 3这样一组数据，第3大元素就是4。\n我们选择数组区间A[0…n-1]的最后一个元素A[n-1]作为pivot，对数组A[0…n-1]原地分区，这样数组就分成了三部分，A[0…p-1]、A[p]、A[p+1…n-1]。\n如果p+1=K，那A[p]就是要求解的元素；如果K&gt;p+1, 说明第K大元素出现在A[p+1…n-1]区间，我们再按照上面的思路递归地在A[p+1…n-1]这个区间内查找。同理，如果K&lt;p+1，那我们就在A[0…p-1]区间查找。\n\n我们再来看，为什么上述解决思路的时间复杂度是O(n)？\n第一次分区查找，我们需要对大小为n的数组执行分区操作，需要遍历n个元素。第二次分区查找，我们只需要对大小为n/2的数组执行分区操作，需要遍历n/2个元素。依次类推，分区遍历元素的个数分别为、n/2、n/4、n/8、n/16.……直到区间缩小为1。\n如果我们把每次分区遍历的元素个数加起来，就是：n+n/2+n/4+n/8+…+1。这是一个等比数列求和，最后的和等于2n-1。所以，上述解决思路的时间复杂度就为O(n)。\n你可能会说，我有个很笨的办法，每次取数组中的最小值，将其移动到数组的最前面，然后在剩下的数组中继续找最小值，以此类推，执行K次，找到的数据不就是第K大元素了吗？\n不过，时间复杂度就并不是O(n)了，而是O(K * n)。你可能会说，时间复杂度前面的系数不是可以忽略吗？O(K * n)不就等于O(n)吗？\n这个可不能这么简单地划等号。当K是比较小的常量时，比如1、2，那最好时间复杂度确实是O(n)；但当K等于n/2或者n时，这种最坏情况下的时间复杂度就是O(n2)了。\n内容小结归并排序和快速排序是两种稍微复杂的排序算法，它们用的都是分治的思想，代码都通过递归来实现，过程非常相似。理解归并排序的重点是理解递推公式和merge()合并函数。同理，理解快排的重点也是理解递推公式，还有partition()分区函数。\n归并排序算法是一种在任何情况下时间复杂度都比较稳定的排序算法，这也使它存在致命的缺点，即归并排序不是原地排序算法，空间复杂度比较高，是O(n)。正因为此，它也没有快排应用广泛。\n快速排序算法虽然最坏情况下的时间复杂度是O(n2)，但是平均情况下时间复杂度都是O(nlogn)。不仅如此，快速排序算法时间复杂度退化到O(n2)的概率非常小，我们可以通过合理地选择pivot来避免这种情况。\n课后思考现在你有10个接口访问日志文件，每个日志文件大小约300MB，每个文件里的日志都是按照时间戳从小到大排序的。你希望将这10个较小的日志文件，合并为1个日志文件，合并之后的日志仍然按照时间戳从小到大排列。如果处理上述排序任务的机器内存只有1GB，你有什么好的解决思路，能“快速”地将这10个日志文件合并吗？\n欢迎留言和我分享，我会第一时间给你反馈。\n我已将本节内容相关的详细代码更新到GitHub，戳此即可查看。\n","categories":["数据结构与算法","基础篇"]},{"title":"19 | 散列表（中）：如何打造一个工业级水平的散列表？","url":"/2020/08/07/19%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%9A%E6%95%A3%E5%88%97%E8%A1%A8%EF%BC%88%E4%B8%AD%EF%BC%89/","content":"通过上一节的学习，我们知道，散列表的查询效率并不能笼统地说成是O(1)。它跟散列函数、装载因子、散列冲突等都有关系。如果散列函数设计得不好，或者装载因子过高，都可能导致散列冲突发生的概率升高，查询效率下降。\n在极端情况下，有些恶意的攻击者，还有可能通过精心构造的数据，使得所有的数据经过散列函数之后，都散列到同一个槽里。如果我们使用的是基于链表的冲突解决方法，那这个时候，散列表就会退化为链表，查询的时间复杂度就从O(1)急剧退化为O(n)。\n如果散列表中有10万个数据，退化后的散列表查询的效率就下降了10万倍。更直接点说，如果之前运行100次查询只需要0.1秒，那现在就需要1万秒。这样就有可能因为查询操作消耗大量CPU或者线程资源，导致系统无法响应其他请求，从而达到拒绝服务攻击（DoS）的目的。这也就是散列表碰撞攻击的基本原理。\n今天，我们就来学习一下，如何设计一个可以应对各种异常情况的工业级散列表，来避免在散列冲突的情况下，散列表性能的急剧下降，并且能抵抗散列碰撞攻击？\n如何设计散列函数？散列函数设计的好坏，决定了散列表冲突的概率大小，也直接决定了散列表的性能。那什么才是好的散列函数呢？\n首先，散列函数的设计不能太复杂。过于复杂的散列函数，势必会消耗很多计算时间，也就间接地影响到散列表的性能。其次，散列函数生成的值要尽可能随机并且均匀分布，这样才能避免或者最小化散列冲突，而且即便出现冲突，散列到每个槽里的数据也会比较平均，不会出现某个槽内数据特别多的情况。\n实际工作中，我们还需要综合考虑各种因素。这些因素有关键字的长度、特点、分布、还有散列表的大小等。散列函数各式各样，我举几个常用的、简单的散列函数的设计方法，让你有个直观的感受。\n第一个例子就是我们上一节的学生运动会的例子，我们通过分析参赛编号的特征，把编号中的后两位作为散列值。我们还可以用类似的散列函数处理手机号码，因为手机号码前几位重复的可能性很大，但是后面几位就比较随机，我们可以取手机号的后四位作为散列值。这种散列函数的设计方法，我们一般叫做“数据分析法”。\n第二个例子就是上一节的开篇思考题，如何实现Word拼写检查功能。这里面的散列函数，我们就可以这样设计：将单词中每个字母的ASCll码值“进位”相加，然后再跟散列表的大小求余、取模，作为散列值。比如，英文单词nice，我们转化出来的散列值就是下面这样：\nhash(&quot;nice&quot;)&#x3D;((&quot;n&quot; - &quot;a&quot;) * 26*26*26 + (&quot;i&quot; - &quot;a&quot;)*26*26 + (&quot;c&quot; - &quot;a&quot;)*26+ (&quot;e&quot;-&quot;a&quot;)) &#x2F; 78978\n实际上，散列函数的设计方法还有很多，比如直接寻址法、平方取中法、折叠法、随机数法等，这些你只要了解就行了，不需要全都掌握。\n装载因子过大了怎么办？我们上一节讲到散列表的装载因子的时候说过，装载因子越大，说明散列表中的元素越多，空闲位置越少，散列冲突的概率就越大。不仅插入数据的过程要多次寻址或者拉很长的链，查找的过程也会因此变得很慢。\n对于没有频繁插入和删除的静态数据集合来说，我们很容易根据数据的特点、分布等，设计出完美的、极少冲突的散列函数，因为毕竟之前数据都是已知的。\n对于动态散列表来说，数据集合是频繁变动的，我们事先无法预估将要加入的数据个数，所以我们也无法事先申请一个足够大的散列表。随着数据慢慢加入，装载因子就会慢慢变大。当装载因子大到一定程度之后，散列冲突就会变得不可接受。这个时候，我们该如何处理呢？\n还记得我们前面多次讲的“动态扩容”吗？你可以回想一下，我们是如何做数组、栈、队列的动态扩容的。\n针对散列表，当装载因子过大时，我们也可以进行动态扩容，重新申请一个更大的散列表，将数据搬移到这个新散列表中。假设每次扩容我们都申请一个原来散列表大小两倍的空间。如果原来散列表的装载因子是0.8，那经过扩容之后，新散列表的装载因子就下降为原来的一半，变成了0.4。\n针对数组的扩容，数据搬移操作比较简单。但是，针对散列表的扩容，数据搬移操作要复杂很多。因为散列表的大小变了，数据的存储位置也变了，所以我们需要通过散列函数重新计算每个数据的存储位置。\n你可以看我图里这个例子。在原来的散列表中，21这个元素原来存储在下标为0的位置，搬移到新的散列表中，存储在下标为7的位置。\n\n对于支持动态扩容的散列表，插入操作的时间复杂度是多少呢？前面章节我已经多次分析过支持动态扩容的数组、栈等数据结构的时间复杂度了。所以，这里我就不啰嗦了，你要是还不清楚的话，可以回去复习一下。\n插入一个数据，最好情况下，不需要扩容，最好时间复杂度是O(1)。最坏情况下，散列表装载因子过高，启动扩容，我们需要重新申请内存空间，重新计算哈希位置，并且搬移数据，所以时间复杂度是O(n)。用摊还分析法，均摊情况下，时间复杂度接近最好情况，就是O(1)。\n实际上，对于动态散列表，随着数据的删除，散列表中的数据会越来越少，空闲空间会越来越多。如果我们对空间消耗非常敏感，我们可以在装载因子小于某个值之后，启动动态缩容。当然，如果我们更加在意执行效率，能够容忍多消耗一点内存空间，那就可以不用费劲来缩容了。\n我们前面讲到，当散列表的装载因子超过某个阈值时，就需要进行扩容。装载因子阈值需要选择得当。如果太大，会导致冲突过多；如果太小，会导致内存浪费严重。\n装载因子阈值的设置要权衡时间、空间复杂度。如果内存空间不紧张，对执行效率要求很高，可以降低负载因子的阈值；相反，如果内存空间紧张，对执行效率要求又不高，可以增加负载因子的值，甚至可以大于1。\n如何避免低效的扩容？我们刚刚分析得到，大部分情况下，动态扩容的散列表插入一个数据都很快，但是在特殊情况下，当装载因子已经到达阈值，需要先进行扩容，再插入数据。这个时候，插入数据就会变得很慢，甚至会无法接受。\n我举一个极端的例子，如果散列表当前大小为1GB，要想扩容为原来的两倍大小，那就需要对1GB的数据重新计算哈希值，并且从原来的散列表搬移到新的散列表，听起来就很耗时，是不是？\n如果我们的业务代码直接服务于用户，尽管大部分情况下，插入一个数据的操作都很快，但是，极个别非常慢的插入操作，也会让用户崩溃。这个时候，“一次性”扩容的机制就不合适了。\n为了解决一次性扩容耗时过多的情况，我们可以将扩容操作穿插在插入操作的过程中，分批完成。当装载因子触达阈值之后，我们只申请新空间，但并不将老的数据搬移到新散列表中。\n当有新数据要插入时，我们将新数据插入新散列表中，并且从老的散列表中拿出一个数据放入到新散列表。每次插入一个数据到散列表，我们都重复上面的过程。经过多次插入操作之后，老的散列表中的数据就一点一点全部搬移到新散列表中了。这样没有了集中的一次性数据搬移，插入操作就都变得很快了。\n\n这期间的查询操作怎么来做呢？对于查询操作，为了兼容了新、老散列表中的数据，我们先从新散列表中查找，如果没有找到，再去老的散列表中查找。\n通过这样均摊的方法，将一次性扩容的代价，均摊到多次插入操作中，就避免了一次性扩容耗时过多的情况。这种实现方式，任何情况下，插入一个数据的时间复杂度都是O(1)。\n如何选择冲突解决方法？上一节我们讲了两种主要的散列冲突的解决办法，开放寻址法和链表法。这两种冲突解决办法在实际的软件开发中都非常常用。比如，Java中LinkedHashMap就采用了链表法解决冲突，ThreadLocalMap是通过线性探测的开放寻址法来解决冲突。那你知道，这两种冲突解决方法各有什么优势和劣势，又各自适用哪些场景吗？\n1.开放寻址法我们先来看看，开放寻址法的优点有哪些。\n开放寻址法不像链表法，需要拉很多链表。散列表中的数据都存储在数组中，可以有效地利用CPU缓存加快查询速度。而且，这种方法实现的散列表，序列化起来比较简单。链表法包含指针，序列化起来就没那么容易。你可不要小看序列化，很多场合都会用到的。我们后面就有一节会讲什么是数据结构序列化、如何序列化，以及为什么要序列化。\n我们再来看下，开放寻址法有哪些缺点。\n上一节我们讲到，用开放寻址法解决冲突的散列表，删除数据的时候比较麻烦，需要特殊标记已经删除掉的数据。而且，在开放寻址法中，所有的数据都存储在一个数组中，比起链表法来说，冲突的代价更高。所以，使用开放寻址法解决冲突的散列表，装载因子的上限不能太大。这也导致这种方法比链表法更浪费内存空间。\n所以，我总结一下，当数据量比较小、装载因子小的时候，适合采用开放寻址法。这也是Java中的ThreadLocalMap使用开放寻址法解决散列冲突的原因。\n2.链表法首先，链表法对内存的利用率比开放寻址法要高。因为链表结点可以在需要的时候再创建，并不需要像开放寻址法那样事先申请好。实际上，这一点也是我们前面讲过的链表优于数组的地方。\n链表法比起开放寻址法，对大装载因子的容忍度更高。开放寻址法只能适用装载因子小于1的情况。接近1时，就可能会有大量的散列冲突，导致大量的探测、再散列等，性能会下降很多。但是对于链表法来说，只要散列函数的值随机均匀，即便装载因子变成10，也就是链表的长度变长了而已，虽然查找效率有所下降，但是比起顺序查找还是快很多。\n还记得我们之前在链表那一节讲的吗？链表因为要存储指针，所以对于比较小的对象的存储，是比较消耗内存的，还有可能会让内存的消耗翻倍。而且，因为链表中的结点是零散分布在内存中的，不是连续的，所以对CPU缓存是不友好的，这方面对于执行效率也有一定的影响。\n当然，如果我们存储的是大对象，也就是说要存储的对象的大小远远大于一个指针的大小（4个字节或者8个字节），那链表中指针的内存消耗在大对象面前就可以忽略了。\n实际上，我们对链表法稍加改造，可以实现一个更加高效的散列表。那就是，我们将链表法中的链表改造为其他高效的动态数据结构，比如跳表、红黑树。这样，即便出现散列冲突，极端情况下，所有的数据都散列到同一个桶内，那最终退化成的散列表的查找时间也只不过是O(logn)。这样也就有效避免了前面讲到的散列碰撞攻击。\n\n所以，我总结一下，基于链表的散列冲突处理方法比较适合存储大对象、大数据量的散列表，而且，比起开放寻址法，它更加灵活，支持更多的优化策略，比如用红黑树代替链表。\n工业级散列表举例分析刚刚我讲了实现一个工业级散列表需要涉及的一些关键技术，现在，我就拿一个具体的例子，Java中的HashMap这样一个工业级的散列表，来具体看下，这些技术是怎么应用的。\n1.初始大小HashMap默认的初始大小是16，当然这个默认值是可以设置的，如果事先知道大概的数据量有多大，可以通过修改默认初始大小，减少动态扩容的次数，这样会大大提高HashMap的性能。\n2.装载因子和动态扩容最大装载因子默认是0.75，当HashMap中元素个数超过0.75*capacity（capacity表示散列表的容量）的时候，就会启动扩容，每次扩容都会扩容为原来的两倍大小。\n3.散列冲突解决方法HashMap底层采用链表法来解决冲突。即使负载因子和散列函数设计得再合理，也免不了会出现拉链过长的情况，一旦出现拉链过长，则会严重影响HashMap的性能。\n于是，在JDK1.8版本中，为了对HashMap做进一步优化，我们引入了红黑树。而当链表长度太长（默认超过8）时，链表就转换为红黑树。我们可以利用红黑树快速增删改查的特点，提高HashMap的性能。当红黑树结点个数少于8个的时候，又会将红黑树转化为链表。因为在数据量较小的情况下，红黑树要维护平衡，比起链表来，性能上的优势并不明显。\n4.散列函数散列函数的设计并不复杂，追求的是简单高效、分布均匀。我把它摘抄出来，你可以看看。\nint hash(Object key) &#123;    int h &#x3D; key.hashCode()；    return (h ^ (h &gt;&gt;&gt; 16)) &amp; (capicity -1); &#x2F;&#x2F;capicity表示散列表的大小&#125;\n其中，hashCode()返回的是Java对象的hash code。比如String类型的对象的hashCode()就是下面这样：\npublic int hashCode() &#123;  int var1 &#x3D; this.hash;  if(var1 &#x3D;&#x3D; 0 &amp;&amp; this.value.length &gt; 0) &#123;    char[] var2 &#x3D; this.value;    for(int var3 &#x3D; 0; var3 &lt; this.value.length; ++var3) &#123;      var1 &#x3D; 31 * var1 + var2[var3];    &#125;    this.hash &#x3D; var1;  &#125;  return var1;&#125;\n解答开篇今天的内容就讲完了，我现在来分析一下开篇的问题：如何设计一个工业级的散列函数？如果这是一道面试题或者是摆在你面前的实际开发问题，你会从哪几个方面思考呢？\n首先，我会思考，何为一个工业级的散列表？工业级的散列表应该具有哪些特性？\n结合已经学习过的散列知识，我觉得应该有这样几点要求：\n\n支持快速地查询、插入、删除操作；\n\n内存占用合理，不能浪费过多的内存空间；\n\n性能稳定，极端情况下，散列表的性能也不会退化到无法接受的情况。\n\n\n如何实现这样一个散列表呢？根据前面讲到的知识，我会从这三个方面来考虑设计思路：\n\n设计一个合适的散列函数；\n\n定义装载因子阈值，并且设计动态扩容策略；\n\n选择合适的散列冲突解决方法。\n\n\n关于散列函数、装载因子、动态扩容策略，还有散列冲突的解决办法，我们前面都讲过了，具体如何选择，还要结合具体的业务场景、具体的业务数据来具体分析。不过只要我们朝这三个方向努力，就离设计出工业级的散列表不远了。\n内容小结上一节的内容比较偏理论，今天的内容侧重实战。我主要讲了如何设计一个工业级的散列表，以及如何应对各种异常情况，防止在极端情况下，散列表的性能退化过于严重。我分了三部分来讲解这些内容，分别是：如何设计散列函数，如何根据装载因子动态扩容，以及如何选择散列冲突解决方法。\n关于散列函数的设计，我们要尽可能让散列后的值随机且均匀分布，这样会尽可能地减少散列冲突，即便冲突之后，分配到每个槽内的数据也比较均匀。除此之外，散列函数的设计也不能太复杂，太复杂就会太耗时间，也会影响散列表的性能。\n关于散列冲突解决方法的选择，我对比了开放寻址法和链表法两种方法的优劣和适应的场景。大部分情况下，链表法更加普适。而且，我们还可以通过将链表法中的链表改造成其他动态查找数据结构，比如红黑树，来避免散列表时间复杂度退化成O(n)，抵御散列碰撞攻击。但是，对于小规模数据、装载因子不高的散列表，比较适合用开放寻址法。\n对于动态散列表来说，不管我们如何设计散列函数，选择什么样的散列冲突解决方法。随着数据的不断增加，散列表总会出现装载因子过高的情况。这个时候，我们就需要启动动态扩容。\n课后思考在你熟悉的编程语言中，哪些数据类型底层是基于散列表实现的？散列函数是如何设计的？散列冲突是通过哪种方法解决的？是否支持动态扩容呢？\n欢迎留言和我分享，我会第一时间给你反馈。\n","categories":["数据结构与算法","基础篇"]},{"title":"06 | 链表（上）：如何实现LRU缓存淘汰算法?","url":"/2020/08/07/06%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%9A%E9%93%BE%E8%A1%A8%EF%BC%88%E4%B8%8A%EF%BC%89/","content":"今天我们来聊聊“链表（Linked list）”这个数据结构。学习链表有什么用呢？为了回答这个问题，我们先来讨论一个经典的链表应用场景，那就是LRU缓存淘汰算法。\n缓存是一种提高数据读取性能的技术，在硬件设计、软件开发中都有着非常广泛的应用，比如常见的CPU缓存、数据库缓存、浏览器缓存等等。\n缓存的大小有限，当缓存被用满时，哪些数据应该被清理出去，哪些数据应该被保留？这就需要缓存淘汰策略来决定。常见的策略有三种：先进先出策略FIFO（First In，First Out）、最少使用策略LFU（Least Frequently Used）、最近最少使用策略LRU（Least Recently Used）。\n这些策略你不用死记，我打个比方你很容易就明白了。假如说，你买了很多本技术书，但有一天你发现，这些书太多了，太占书房空间了，你要做个大扫除，扔掉一些书籍。那这个时候，你会选择扔掉哪些书呢？对应一下，你的选择标准是不是和上面的三种策略神似呢？\n好了，回到正题，我们今天的开篇问题就是：如何用链表来实现LRU缓存淘汰策略呢？ 带着这个问题，我们开始今天的内容吧！\n五花八门的链表结构相比数组，链表是一种稍微复杂一点的数据结构。对于初学者来说，掌握起来也要比数组稍难一些。这两个非常基础、非常常用的数据结构，我们常常会放到一块儿来比较。所以我们先来看，这两者有什么区别。\n我们先从底层的存储结构上来看一看。\n为了直观地对比，我画了一张图。从图中我们看到，数组需要一块连续的内存空间来存储，对内存的要求比较高。如果我们申请一个100MB大小的数组，当内存中没有连续的、足够大的存储空间时，即便内存的剩余总可用空间大于100MB，仍然会申请失败。\n而链表恰恰相反，它并不需要一块连续的内存空间，它通过“指针”将一组零散的内存块串联起来使用，所以如果我们申请的是100MB大小的链表，根本不会有问题。\n\n链表结构五花八门，今天我重点给你介绍三种最常见的链表结构，它们分别是：单链表、双向链表和循环链表。我们首先来看最简单、最常用的单链表。\n我们刚刚讲到，链表通过指针将一组零散的内存块串联在一起。其中，我们把内存块称为链表的“结点”。为了将所有的结点串起来，每个链表的结点除了存储数据之外，还需要记录链上的下一个结点的地址。如图所示，我们把这个记录下个结点地址的指针叫作后继指针next。\n\n从我画的单链表图中，你应该可以发现，其中有两个结点是比较特殊的，它们分别是第一个结点和最后一个结点。我们习惯性地把第一个结点叫作头结点，把最后一个结点叫作尾结点。其中，头结点用来记录链表的基地址。有了它，我们就可以遍历得到整条链表。而尾结点特殊的地方是：指针不是指向下一个结点，而是指向一个空地址NULL，表示这是链表上最后一个结点。\n与数组一样，链表也支持数据的查找、插入和删除操作。\n我们知道，在进行数组的插入、删除操作时，为了保持内存数据的连续性，需要做大量的数据搬移，所以时间复杂度是O(n)。而在链表中插入或者删除一个数据，我们并不需要为了保持内存的连续性而搬移结点，因为链表的存储空间本身就不是连续的。所以，在链表中插入和删除一个数据是非常快速的。\n为了方便你理解，我画了一张图，从图中我们可以看出，针对链表的插入和删除操作，我们只需要考虑相邻结点的指针改变，所以对应的时间复杂度是O(1)。\n\n但是，有利就有弊。链表要想随机访问第k个元素，就没有数组那么高效了。因为链表中的数据并非连续存储的，所以无法像数组那样，根据首地址和下标，通过寻址公式就能直接计算出对应的内存地址，而是需要根据指针一个结点一个结点地依次遍历，直到找到相应的结点。\n你可以把链表想象成一个队伍，队伍中的每个人都只知道自己后面的人是谁，所以当我们希望知道排在第k位的人是谁的时候，我们就需要从第一个人开始，一个一个地往下数。所以，链表随机访问的性能没有数组好，需要O(n)的时间复杂度。\n好了，单链表我们就简单介绍完了，接着来看另外两个复杂的升级版，循环链表和双向链表。\n循环链表是一种特殊的单链表。实际上，循环链表也很简单。它跟单链表唯一的区别就在尾结点。我们知道，单链表的尾结点指针指向空地址，表示这就是最后的结点了。而循环链表的尾结点指针是指向链表的头结点。从我画的循环链表图中，你应该可以看出来，它像一个环一样首尾相连，所以叫作“循环”链表。\n\n和单链表相比，循环链表的优点是从链尾到链头比较方便。当要处理的数据具有环型结构特点时，就特别适合采用循环链表。比如著名的约瑟夫问题。尽管用单链表也可以实现，但是用循环链表实现的话，代码就会简洁很多。\n单链表和循环链表是不是都不难？接下来我们再来看一个稍微复杂的，在实际的软件开发中，也更加常用的链表结构：双向链表。\n单向链表只有一个方向，结点只有一个后继指针next指向后面的结点。而双向链表，顾名思义，它支持两个方向，每个结点不止有一个后继指针next指向后面的结点，还有一个前驱指针prev指向前面的结点。\n\n从我画的图中可以看出来，双向链表需要额外的两个空间来存储后继结点和前驱结点的地址。所以，如果存储同样多的数据，双向链表要比单链表占用更多的内存空间。虽然两个指针比较浪费存储空间，但可以支持双向遍历，这样也带来了双向链表操作的灵活性。那相比单链表，双向链表适合解决哪种问题呢？\n从结构上来看，双向链表可以支持O(1)时间复杂度的情况下找到前驱结点，正是这样的特点，也使双向链表在某些情况下的插入、删除等操作都要比单链表简单、高效。\n你可能会说，我刚讲到单链表的插入、删除操作的时间复杂度已经是O(1)了，双向链表还能再怎么高效呢？别着急，刚刚的分析比较偏理论，很多数据结构和算法书籍中都会这么讲，但是这种说法实际上是不准确的，或者说是有先决条件的。我再来带你分析一下链表的两个操作。\n我们先来看删除操作。\n在实际的软件开发中，从链表中删除一个数据无外乎这两种情况：\n\n删除结点中“值等于某个给定值”的结点；\n\n删除给定指针指向的结点。\n\n\n对于第一种情况，不管是单链表还是双向链表，为了查找到值等于给定值的结点，都需要从头结点开始一个一个依次遍历对比，直到找到值等于给定值的结点，然后再通过我前面讲的指针操作将其删除。\n尽管单纯的删除操作时间复杂度是O(1)，但遍历查找的时间是主要的耗时点，对应的时间复杂度为O(n)。根据时间复杂度分析中的加法法则，删除值等于给定值的结点对应的链表操作的总时间复杂度为O(n)。\n对于第二种情况，我们已经找到了要删除的结点，但是删除某个结点q需要知道其前驱结点，而单链表并不支持直接获取前驱结点，所以，为了找到前驱结点，我们还是要从头结点开始遍历链表，直到p-&gt;next=q，说明p是q的前驱结点。\n但是对于双向链表来说，这种情况就比较有优势了。因为双向链表中的结点已经保存了前驱结点的指针，不需要像单链表那样遍历。所以，针对第二种情况，单链表删除操作需要O(n)的时间复杂度，而双向链表只需要在O(1)的时间复杂度内就搞定了！\n同理，如果我们希望在链表的某个指定结点前面插入一个结点，双向链表比单链表有很大的优势。双向链表可以在O(1)时间复杂度搞定，而单向链表需要O(n)的时间复杂度。你可以参照我刚刚讲过的删除操作自己分析一下。\n除了插入、删除操作有优势之外，对于一个有序链表，双向链表的按值查询的效率也要比单链表高一些。因为，我们可以记录上次查找的位置p，每次查询时，根据要查找的值与p的大小关系，决定是往前还是往后查找，所以平均只需要查找一半的数据。\n现在，你有没有觉得双向链表要比单链表更加高效呢？这就是为什么在实际的软件开发中，双向链表尽管比较费内存，但还是比单链表的应用更加广泛的原因。如果你熟悉Java语言，你肯定用过LinkedHashMap这个容器。如果你深入研究LinkedHashMap的实现原理，就会发现其中就用到了双向链表这种数据结构。\n实际上，这里有一个更加重要的知识点需要你掌握，那就是用空间换时间的设计思想。当内存空间充足的时候，如果我们更加追求代码的执行速度，我们就可以选择空间复杂度相对较高、但时间复杂度相对很低的算法或者数据结构。相反，如果内存比较紧缺，比如代码跑在手机或者单片机上，这个时候，就要反过来用时间换空间的设计思路。\n还是开篇缓存的例子。缓存实际上就是利用了空间换时间的设计思想。如果我们把数据存储在硬盘上，会比较节省内存，但每次查找数据都要询问一次硬盘，会比较慢。但如果我们通过缓存技术，事先将数据加载在内存中，虽然会比较耗费内存空间，但是每次数据查询的速度就大大提高了。\n所以我总结一下，对于执行较慢的程序，可以通过消耗更多的内存（空间换时间）来进行优化；而消耗过多内存的程序，可以通过消耗更多的时间（时间换空间）来降低内存的消耗。你还能想到其他时间换空间或者空间换时间的例子吗？\n了解了循环链表和双向链表，如果把这两种链表整合在一起就是一个新的版本：双向循环链表。我想不用我多讲，你应该知道双向循环链表长什么样子了吧？你可以自己试着在纸上画一画。\n\n链表VS数组性能大比拼通过前面内容的学习，你应该已经知道，数组和链表是两种截然不同的内存组织方式。正是因为内存存储的区别，它们插入、删除、随机访问操作的时间复杂度正好相反。\n\n不过，数组和链表的对比，并不能局限于时间复杂度。而且，在实际的软件开发中，不能仅仅利用复杂度分析就决定使用哪个数据结构来存储数据。\n数组简单易用，在实现上使用的是连续的内存空间，可以借助CPU的缓存机制，预读数组中的数据，所以访问效率更高。而链表在内存中并不是连续存储，所以对CPU缓存不友好，没办法有效预读。\n数组的缺点是大小固定，一经声明就要占用整块连续内存空间。如果声明的数组过大，系统可能没有足够的连续内存空间分配给它，导致“内存不足（out of memory）”。如果声明的数组过小，则可能出现不够用的情况。这时只能再申请一个更大的内存空间，把原数组拷贝进去，非常费时。链表本身没有大小的限制，天然地支持动态扩容，我觉得这也是它与数组最大的区别。\n你可能会说，我们Java中的ArrayList容器，也可以支持动态扩容啊？我们上一节课讲过，当我们往支持动态扩容的数组中插入一个数据时，如果数组中没有空闲空间了，就会申请一个更大的空间，将数据拷贝过去，而数据拷贝的操作是非常耗时的。\n我举一个稍微极端的例子。如果我们用ArrayList存储了了1GB大小的数据，这个时候已经没有空闲空间了，当我们再插入数据的时候，ArrayList会申请一个1.5GB大小的存储空间，并且把原来那1GB的数据拷贝到新申请的空间上。听起来是不是就很耗时？\n除此之外，如果你的代码对内存的使用非常苛刻，那数组就更适合你。因为链表中的每个结点都需要消耗额外的存储空间去存储一份指向下一个结点的指针，所以内存消耗会翻倍。而且，对链表进行频繁的插入、删除操作，还会导致频繁的内存申请和释放，容易造成内存碎片，如果是Java语言，就有可能会导致频繁的GC（Garbage Collection，垃圾回收）。\n所以，在我们实际的开发中，针对不同类型的项目，要根据具体情况，权衡究竟是选择数组还是链表。\n解答开篇好了，关于链表的知识我们就讲完了。我们现在回过头来看下开篇留给你的思考题。如何基于链表实现LRU缓存淘汰算法？\n我的思路是这样的：我们维护一个有序单链表，越靠近链表尾部的结点是越早之前访问的。当有一个新的数据被访问时，我们从链表头开始顺序遍历链表。\n1.如果此数据之前已经被缓存在链表中了，我们遍历得到这个数据对应的结点，并将其从原来的位置删除，然后再插入到链表的头部。\n2.如果此数据没有在缓存链表中，又可以分为两种情况：\n\n如果此时缓存未满，则将此结点直接插入到链表的头部；\n\n如果此时缓存已满，则链表尾结点删除，将新的数据结点插入链表的头部。\n\n\n这样我们就用链表实现了一个LRU缓存，是不是很简单？\n现在我们来看下缓存访问的时间复杂度是多少。因为不管缓存有没有满，我们都需要遍历一遍链表，所以这种基于链表的实现思路，缓存访问的时间复杂度为O(n)。\n实际上，我们可以继续优化这个实现思路，比如引入散列表（Hash table）来记录每个数据的位置，将缓存访问的时间复杂度降到O(1)。因为要涉及我们还没有讲到的数据结构，所以这个优化方案，我现在就不详细说了，等讲到散列表的时候，我会再拿出来讲。\n除了基于链表的实现思路，实际上还可以用数组来实现LRU缓存淘汰策略。如何利用数组实现LRU缓存淘汰策略呢？我把这个问题留给你思考。\n内容小结今天我们讲了一种跟数组“相反”的数据结构，链表。它跟数组一样，也是非常基础、非常常用的数据结构。不过链表要比数组稍微复杂，从普通的单链表衍生出来好几种链表结构，比如双向链表、循环链表、双向循环链表。\n和数组相比，链表更适合插入、删除操作频繁的场景，查询的时间复杂度较高。不过，在具体软件开发中，要对数组和链表的各种性能进行对比，综合来选择使用两者中的哪一个。\n课后思考如何判断一个字符串是否是回文字符串的问题，我想你应该听过，我们今天的题目就是基于这个问题的改造版本。如果字符串是通过单链表来存储的，那该如何来判断是一个回文串呢？你有什么好的解决思路呢？相应的时间空间复杂度又是多少呢？\n欢迎留言和我分享，我会第一时间给你反馈。\n我已将本节内容相关的详细代码更新到GitHub，戳此即可查看。\n","categories":["数据结构与算法","基础篇"]},{"title":"28 | 堆和堆排序：为什么说堆排序没有快速排序快？","url":"/2020/08/07/28%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%9A%E5%A0%86%E5%92%8C%E5%A0%86%E6%8E%92%E5%BA%8F/","content":"我们今天讲另外一种特殊的树，“堆”（$Heap$）。堆这种数据结构的应用场景非常多，最经典的莫过于堆排序了。堆排序是一种原地的、时间复杂度为$O(n\\log n)$的排序算法。\n前面我们学过快速排序，平均情况下，它的时间复杂度为$O(n\\log n)$。尽管这两种排序算法的时间复杂度都是$O(n\\log n)$，甚至堆排序比快速排序的时间复杂度还要稳定，但是，在实际的软件开发中，快速排序的性能要比堆排序好，这是为什么呢？\n现在，你可能还无法回答，甚至对问题本身还有点疑惑。没关系，带着这个问题，我们来学习今天的内容。等你学完之后，或许就能回答出来了。\n如何理解“堆”？前面我们提到，堆是一种特殊的树。我们现在就来看看，什么样的树才是堆。我罗列了两点要求，只要满足这两点，它就是一个堆。\n\n堆是一个完全二叉树；\n\n堆中每一个节点的值都必须大于等于（或小于等于）其子树中每个节点的值。\n\n\n我分别解释一下这两点。\n第一点，堆必须是一个完全二叉树。还记得我们之前讲的完全二叉树的定义吗？完全二叉树要求，除了最后一层，其他层的节点个数都是满的，最后一层的节点都靠左排列。\n第二点，堆中的每个节点的值必须大于等于（或者小于等于）其子树中每个节点的值。实际上，我们还可以换一种说法，堆中每个节点的值都大于等于（或者小于等于）其左右子节点的值。这两种表述是等价的。\n对于每个节点的值都大于等于子树中每个节点值的堆，我们叫做“大顶堆”。对于每个节点的值都小于等于子树中每个节点值的堆，我们叫做“小顶堆”。\n定义解释清楚了，你来看看，下面这几个二叉树是不是堆？\n\n其中第$1$个和第$2$个是大顶堆，第$3$个是小顶堆，第$4$个不是堆。除此之外，从图中还可以看出来，对于同一组数据，我们可以构建多种不同形态的堆。\n如何实现一个堆？要实现一个堆，我们先要知道，堆都支持哪些操作以及如何存储一个堆。\n我之前讲过，完全二叉树比较适合用数组来存储。用数组来存储完全二叉树是非常节省存储空间的。因为我们不需要存储左右子节点的指针，单纯地通过数组的下标，就可以找到一个节点的左右子节点和父节点。\n我画了一个用数组存储堆的例子，你可以先看下。\n\n从图中我们可以看到，数组中下标为$i$的节点的左子节点，就是下标为$i*2$的节点，右子节点就是下标为$i*2+1$的节点，父节点就是下标为$\\frac{i}{2}$的节点。\n知道了如何存储一个堆，那我们再来看看，堆上的操作有哪些呢？我罗列了几个非常核心的操作，分别是往堆中插入一个元素和删除堆顶元素。（如果没有特殊说明，我下面都是拿大顶堆来讲解）。\n1.往堆中插入一个元素往堆中插入一个元素后，我们需要继续满足堆的两个特性。\n如果我们把新插入的元素放到堆的最后，你可以看我画的这个图，是不是不符合堆的特性了？于是，我们就需要进行调整，让其重新满足堆的特性，这个过程我们起了一个名字，就叫做堆化（heapify）。\n堆化实际上有两种，从下往上和从上往下。这里我先讲从下往上的堆化方法。\n\n堆化非常简单，就是顺着节点所在的路径，向上或者向下，对比，然后交换。\n我这里画了一张堆化的过程分解图。我们可以让新插入的节点与父节点对比大小。如果不满足子节点小于等于父节点的大小关系，我们就互换两个节点。一直重复这个过程，直到父子节点之间满足刚说的那种大小关系。\n\n我将上面讲的往堆中插入数据的过程，翻译成了代码，你可以结合着一块看。\npublic class Heap &#123;  private int[] a; &#x2F;&#x2F; 数组，从下标1开始存储数据  private int n;  &#x2F;&#x2F; 堆可以存储的最大数据个数  private int count; &#x2F;&#x2F; 堆中已经存储的数据个数  public Heap(int capacity) &#123;    a &#x3D; new int[capacity + 1];    n &#x3D; capacity;    count &#x3D; 0;  &#125;  public void insert(int data) &#123;    if (count &gt;&#x3D; n) return; &#x2F;&#x2F; 堆满了    ++count;    a[count] &#x3D; data;    int i &#x3D; count;    while (i&#x2F;2 &gt; 0 &amp;&amp; a[i] &gt; a[i&#x2F;2]) &#123; &#x2F;&#x2F; 自下往上堆化      swap(a, i, i&#x2F;2); &#x2F;&#x2F; swap()函数作用：交换下标为i和i&#x2F;2的两个元素      i &#x3D; i&#x2F;2;    &#125;  &#125; &#125;\n2.删除堆顶元素从堆的定义的第二条中，任何节点的值都大于等于（或小于等于）子树节点的值，我们可以发现，堆顶元素存储的就是堆中数据的最大值或者最小值。\n假设我们构造的是大顶堆，堆顶元素就是最大的元素。当我们删除堆顶元素之后，就需要把第二大的元素放到堆顶，那第二大元素肯定会出现在左右子节点中。然后我们再迭代地删除第二大节点，以此类推，直到叶子节点被删除。\n这里我也画了一个分解图。不过这种方法有点问题，就是最后堆化出来的堆并不满足完全二叉树的特性。\n\n实际上，我们稍微改变一下思路，就可以解决这个问题。你看我画的下面这幅图。我们把最后一个节点放到堆顶，然后利用同样的父子节点对比方法。对于不满足父子节点大小关系的，互换两个节点，并且重复进行这个过程，直到父子节点之间满足大小关系为止。这就是从上往下的堆化方法。\n因为我们移除的是数组中的最后一个元素，而在堆化的过程中，都是交换操作，不会出现数组中的“空洞”，所以这种方法堆化之后的结果，肯定满足完全二叉树的特性。\n\n我把上面的删除过程同样也翻译成了代码，贴在这里，你可以结合着看。\npublic void removeMax() &#123;  if (count &#x3D;&#x3D; 0) return -1; &#x2F;&#x2F; 堆中没有数据  a[1] &#x3D; a[count];  --count;  heapify(a, count, 1);&#125;private void heapify(int[] a, int n, int i) &#123; &#x2F;&#x2F; 自上往下堆化  while (true) &#123;    int maxPos &#x3D; i;    if (i*2 &lt;&#x3D; n &amp;&amp; a[i] &lt; a[i*2]) maxPos &#x3D; i*2;    if (i*2+1 &lt;&#x3D; n &amp;&amp; a[maxPos] &lt; a[i*2+1]) maxPos &#x3D; i*2+1;    if (maxPos &#x3D;&#x3D; i) break;    swap(a, i, maxPos);    i &#x3D; maxPos;  &#125;&#125;\n我们知道，一个包含$n$个节点的完全二叉树，树的高度不会超过$\\log_{2}n$。堆化的过程是顺着节点所在路径比较交换的，所以堆化的时间复杂度跟树的高度成正比，也就是$O(\\log n)$。插入数据和删除堆顶元素的主要逻辑就是堆化，所以，往堆中插入一个元素和删除堆顶元素的时间复杂度都是$O(\\log n)$。\n如何基于堆实现排序？前面我们讲过好几种排序算法，我们再来回忆一下，有时间复杂度是$O(n^{2})$的冒泡排序、插入排序、选择排序，有时间复杂度是$O(n\\log n)$的归并排序、快速排序，还有线性排序。\n这里我们借助于堆这种数据结构实现的排序算法，就叫做堆排序。这种排序方法的时间复杂度非常稳定，是$O(n\\log n)$，并且它还是原地排序算法。如此优秀，它是怎么做到的呢？\n我们可以把堆排序的过程大致分解成两个大的步骤，建堆和排序。\n1.建堆我们首先将数组原地建成一个堆。所谓“原地”就是，不借助另一个数组，就在原数组上操作。建堆的过程，有两种思路。\n第一种是借助我们前面讲的，在堆中插入一个元素的思路。尽管数组中包含$n$个数据，但是我们可以假设，起初堆中只包含一个数据，就是下标为$1$的数据。然后，我们调用前面讲的插入操作，将下标从$2$到$n$的数据依次插入到堆中。这样我们就将包含$n$个数据的数组，组织成了堆。\n第二种实现思路，跟第一种截然相反，也是我这里要详细讲的。第一种建堆思路的处理过程是从前往后处理数组数据，并且每个数据插入堆中时，都是从下往上堆化。而第二种实现思路，是从后往前处理数组，并且每个数据都是从上往下堆化。\n我举了一个例子，并且画了一个第二种实现思路的建堆分解步骤图，你可以看下。因为叶子节点往下堆化只能自己跟自己比较，所以我们直接从第一个非叶子节点开始，依次堆化就行了。\n\n对于程序员来说，看代码可能更好理解一些，所以，我将第二种实现思路翻译成了代码，你可以看下。\nprivate static void buildHeap(int[] a, int n) &#123;  for (int i &#x3D; n&#x2F;2; i &gt;&#x3D; 1; --i) &#123;    heapify(a, n, i);  &#125;&#125;private static void heapify(int[] a, int n, int i) &#123;  while (true) &#123;    int maxPos &#x3D; i;    if (i*2 &lt;&#x3D; n &amp;&amp; a[i] &lt; a[i*2]) maxPos &#x3D; i*2;    if (i*2+1 &lt;&#x3D; n &amp;&amp; a[maxPos] &lt; a[i*2+1]) maxPos &#x3D; i*2+1;    if (maxPos &#x3D;&#x3D; i) break;    swap(a, i, maxPos);    i &#x3D; maxPos;  &#125;&#125;\n你可能已经发现了，在这段代码中，我们对下标从$\\frac{n}{2}$ 开始到$1$的数据进行堆化，下标是$\\frac{n}{2}+1$到$n$的节点是叶子节点，我们不需要堆化。实际上，对于完全二叉树来说，下标从$\\frac{n}{2}+1$到$n$的节点都是叶子节点。\n现在，我们来看，建堆操作的时间复杂度是多少呢？\n每个节点堆化的时间复杂度是$O(\\log n)$，那$\\frac{n}{2}+1$个节点堆化的总时间复杂度是不是就是$O(n\\log n)$呢？这个答案虽然也没错，但是这个值还是不够精确。实际上，堆排序的建堆过程的时间复杂度是$O(n)$。我带你推导一下。\n因为叶子节点不需要堆化，所以需要堆化的节点从倒数第二层开始。每个节点堆化的过程中，需要比较和交换的节点个数，跟这个节点的高度$k$成正比。\n我把每一层的节点个数和对应的高度画了出来，你可以看看。我们只需要将每个节点的高度求和，得出的就是建堆的时间复杂度。\n\n我们将每个非叶子节点的高度求和，就是下面这个公式：\n\n这个公式的求解稍微有点技巧，不过我们高中应该都学过：把公式左右都乘以$2$，就得到另一个公式$S2$。我们将$S2$错位对齐，并且用$S2$减去$S1$，可以得到$S$。\n\n$S$的中间部分是一个等比数列，所以最后可以用等比数列的求和公式来计算，最终的结果就是下面图中画的这个样子。\n\n因为$h=\\log_{2}n$，代入公式$S$，就能得到$S=O(n)$，所以，建堆的时间复杂度就是$O(n)$。\n2.排序建堆结束之后，数组中的数据已经是按照大顶堆的特性来组织的。数组中的第一个元素就是堆顶，也就是最大的元素。我们把它跟最后一个元素交换，那最大元素就放到了下标为$n$的位置。\n这个过程有点类似上面讲的“删除堆顶元素”的操作，当堆顶元素移除之后，我们把下标为$n$的元素放到堆顶，然后再通过堆化的方法，将剩下的$n-1$个元素重新构建成堆。堆化完成之后，我们再取堆顶的元素，放到下标是$n-1$的位置，一直重复这个过程，直到最后堆中只剩下标为$1$的一个元素，排序工作就完成了。\n\n堆排序的过程，我也翻译成了代码。结合着代码看，你理解起来应该会更加容易。\n&#x2F;&#x2F; n表示数据的个数，数组a中的数据从下标1到n的位置。public static void sort(int[] a, int n) &#123;  buildHeap(a, n);  int k &#x3D; n;  while (k &gt; 1) &#123;    swap(a, 1, k);    --k;    heapify(a, k, 1);  &#125;&#125;\n现在，我们再来分析一下堆排序的时间复杂度、空间复杂度以及稳定性。\n整个堆排序的过程，都只需要极个别临时存储空间，所以堆排序是原地排序算法。堆排序包括建堆和排序两个操作，建堆过程的时间复杂度是$O(n)$，排序过程的时间复杂度是$O(n\\log n)$，所以，堆排序整体的时间复杂度是$O(n\\log n)$。\n堆排序不是稳定的排序算法，因为在排序的过程，存在将堆的最后一个节点跟堆顶节点互换的操作，所以就有可能改变值相同数据的原始相对顺序。\n今天的内容到此就讲完了。我这里要稍微解释一下，在前面的讲解以及代码中，我都假设，堆中的数据是从数组下标为1的位置开始存储。那如果从$0$开始存储，实际上处理思路是没有任何变化的，唯一变化的，可能就是，代码实现的时候，计算子节点和父节点的下标的公式改变了。\n如果节点的下标是$i$，那左子节点的下标就是$2*i+1$，右子节点的下标就是$2*i+2$，父节点的下标就是$\\frac{i-1}{2}$。\n解答开篇现在我们来看开篇的问题，在实际开发中，为什么快速排序要比堆排序性能好？\n我觉得主要有两方面的原因。\n第一点，堆排序数据访问的方式没有快速排序友好。\n对于快速排序来说，数据是顺序访问的。而对于堆排序来说，数据是跳着访问的。 比如，堆排序中，最重要的一个操作就是数据的堆化。比如下面这个例子，对堆顶节点进行堆化，会依次访问数组下标是$1，2，4，8$的元素，而不是像快速排序那样，局部顺序访问，所以，这样对CPU缓存是不友好的。\n\n第二点，对于同样的数据，在排序过程中，堆排序算法的数据交换次数要多于快速排序。\n我们在讲排序的时候，提过两个概念，有序度和逆序度。对于基于比较的排序算法来说，整个排序过程就是由两个基本的操作组成的，比较和交换（或移动）。快速排序数据交换的次数不会比逆序度多。\n但是堆排序的第一步是建堆，建堆的过程会打乱数据原有的相对先后顺序，导致原数据的有序度降低。比如，对于一组已经有序的数据来说，经过建堆之后，数据反而变得更无序了。\n\n对于第二点，你可以自己做个试验看下。我们用一个记录交换次数的变量，在代码中，每次交换的时候，我们就对这个变量加一，排序完成之后，这个变量的值就是总的数据交换次数。这样你就能很直观地理解我刚刚说的，堆排序比快速排序交换次数多。\n内容小结今天我们讲了堆这种数据结构。堆是一种完全二叉树。它最大的特性是：每个节点的值都大于等于（或小于等于）其子树节点的值。因此，堆被分成了两类，大顶堆和小顶堆。\n堆中比较重要的两个操作是插入一个数据和删除堆顶元素。这两个操作都要用到堆化。插入一个数据的时候，我们把新插入的数据放到数组的最后，然后从下往上堆化；删除堆顶数据的时候，我们把数组中的最后一个元素放到堆顶，然后从上往下堆化。这两个操作时间复杂度都是$O(\\log n)$。\n除此之外，我们还讲了堆的一个经典应用，堆排序。堆排序包含两个过程，建堆和排序。我们将下标从$\\frac{n}{2}$到$1$的节点，依次进行从上到下的堆化操作，然后就可以将数组中的数据组织成堆这种数据结构。接下来，我们迭代地将堆顶的元素放到堆的末尾，并将堆的大小减一，然后再堆化，重复这个过程，直到堆中只剩下一个元素，整个数组中的数据就都有序排列了。\n课后思考\n在讲堆排序建堆的时候，我说到，对于完全二叉树来说，下标从$\\frac{n}{2}+1$到$n$的都是叶子节点，这个结论是怎么推导出来的呢？\n\n我们今天讲了堆的一种经典应用，堆排序。关于堆，你还能想到它的其他应用吗？\n\n\n欢迎留言和我分享，我会第一时间给你反馈。\n","categories":["数据结构与算法","基础篇"]},{"title":"29 | 堆的应用：如何快速获取到Top 10最热门的搜索关键词？","url":"/2020/08/07/29%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%9A%E5%A0%86%E7%9A%84%E5%BA%94%E7%94%A8/","content":"搜索引擎的热门搜索排行榜功能你用过吗？你知道这个功能是如何实现的吗？实际上，它的实现并不复杂。搜索引擎每天会接收大量的用户搜索请求，它会把这些用户输入的搜索关键词记录下来，然后再离线地统计分析，得到最热门的Top 10搜索关键词。\n那请你思考下，假设现在我们有一个包含10亿个搜索关键词的日志文件，如何能快速获取到热门榜Top 10的搜索关键词呢？\n这个问题就可以用堆来解决，这也是堆这种数据结构一个非常典型的应用。上一节我们讲了堆和堆排序的一些理论知识，今天我们就来讲一讲，堆这种数据结构几个非常重要的应用：优先级队列、求Top K和求中位数。\n堆的应用一：优先级队列首先，我们来看第一个应用场景：优先级队列。\n优先级队列，顾名思义，它首先应该是一个队列。我们前面讲过，队列最大的特性就是先进先出。不过，在优先级队列中，数据的出队顺序不是先进先出，而是按照优先级来，优先级最高的，最先出队。\n如何实现一个优先级队列呢？方法有很多，但是用堆来实现是最直接、最高效的。这是因为，堆和优先级队列非常相似。一个堆就可以看作一个优先级队列。很多时候，它们只是概念上的区分而已。往优先级队列中插入一个元素，就相当于往堆中插入一个元素；从优先级队列中取出优先级最高的元素，就相当于取出堆顶元素。\n你可别小看这个优先级队列，它的应用场景非常多。我们后面要讲的很多数据结构和算法都要依赖它。比如，赫夫曼编码、图的最短路径、最小生成树算法等等。不仅如此，很多语言中，都提供了优先级队列的实现，比如，Java的PriorityQueue，C++的priority_queue等。\n只讲这些应用场景比较空泛，现在，我举两个具体的例子，让你感受一下优先级队列具体是怎么用的。\n1.合并有序小文件假设我们有100个小文件，每个文件的大小是100MB，每个文件中存储的都是有序的字符串。我们希望将这些100个小文件合并成一个有序的大文件。这里就会用到优先级队列。\n整体思路有点像归并排序中的合并函数。我们从这100个文件中，各取第一个字符串，放入数组中，然后比较大小，把最小的那个字符串放入合并后的大文件中，并从数组中删除。\n假设，这个最小的字符串来自于13.txt这个小文件，我们就再从这个小文件取下一个字符串，放到数组中，重新比较大小，并且选择最小的放入合并后的大文件，将它从数组中删除。依次类推，直到所有的文件中的数据都放入到大文件为止。\n这里我们用数组这种数据结构，来存储从小文件中取出来的字符串。每次从数组中取最小字符串，都需要循环遍历整个数组，显然，这不是很高效。有没有更加高效方法呢？\n这里就可以用到优先级队列，也可以说是堆。我们将从小文件中取出来的字符串放入到小顶堆中，那堆顶的元素，也就是优先级队列队首的元素，就是最小的字符串。我们将这个字符串放入到大文件中，并将其从堆中删除。然后再从小文件中取出下一个字符串，放入到堆中。循环这个过程，就可以将100个小文件中的数据依次放入到大文件中。\n我们知道，删除堆顶数据和往堆中插入数据的时间复杂度都是O(logn)，n表示堆中的数据个数，这里就是100。是不是比原来数组存储的方式高效了很多呢？\n2.高性能定时器假设我们有一个定时器，定时器中维护了很多定时任务，每个任务都设定了一个要触发执行的时间点。定时器每过一个很小的单位时间（比如1秒），就扫描一遍任务，看是否有任务到达设定的执行时间。如果到达了，就拿出来执行。\n\n但是，这样每过1秒就扫描一遍任务列表的做法比较低效，主要原因有两点：第一，任务的约定执行时间离当前时间可能还有很久，这样前面很多次扫描其实都是徒劳的；第二，每次都要扫描整个任务列表，如果任务列表很大的话，势必会比较耗时。\n针对这些问题，我们就可以用优先级队列来解决。我们按照任务设定的执行时间，将这些任务存储在优先级队列中，队列首部（也就是小顶堆的堆顶）存储的是最先执行的任务。\n这样，定时器就不需要每隔1秒就扫描一遍任务列表了。它拿队首任务的执行时间点，与当前时间点相减，得到一个时间间隔T。\n这个时间间隔T就是，从当前时间开始，需要等待多久，才会有第一个任务需要被执行。这样，定时器就可以设定在T秒之后，再来执行任务。从当前时间点到（T-1）秒这段时间里，定时器都不需要做任何事情。\n当T秒时间过去之后，定时器取优先级队列中队首的任务执行。然后再计算新的队首任务的执行时间点与当前时间点的差值，把这个值作为定时器执行下一个任务需要等待的时间。\n这样，定时器既不用间隔1秒就轮询一次，也不用遍历整个任务列表，性能也就提高了。\n堆的应用二：利用堆求Top K刚刚我们学习了优先级队列，我们现在来看，堆的另外一个非常重要的应用场景，那就是“求Top K问题”。\n我把这种求Top K的问题抽象成两类。一类是针对静态数据集合，也就是说数据集合事先确定，不会再变。另一类是针对动态数据集合，也就是说数据集合事先并不确定，有数据动态地加入到集合中。\n针对静态数据，如何在一个包含n个数据的数组中，查找前K大数据呢？我们可以维护一个大小为K的小顶堆，顺序遍历数组，从数组中取出数据与堆顶元素比较。如果比堆顶元素大，我们就把堆顶元素删除，并且将这个元素插入到堆中；如果比堆顶元素小，则不做处理，继续遍历数组。这样等数组中的数据都遍历完之后，堆中的数据就是前K大数据了。\n遍历数组需要O(n)的时间复杂度，一次堆化操作需要O(logK)的时间复杂度，所以最坏情况下，n个元素都入堆一次，时间复杂度就是O(nlogK)。\n针对动态数据求得Top K就是实时Top K。怎么理解呢？我举一个例子。一个数据集合中有两个操作，一个是添加数据，另一个询问当前的前K大数据。\n如果每次询问前K大数据，我们都基于当前的数据重新计算的话，那时间复杂度就是O(nlogK)，n表示当前的数据的大小。实际上，我们可以一直都维护一个K大小的小顶堆，当有数据被添加到集合中时，我们就拿它与堆顶的元素对比。如果比堆顶元素大，我们就把堆顶元素删除，并且将这个元素插入到堆中；如果比堆顶元素小，则不做处理。这样，无论任何时候需要查询当前的前K大数据，我们都可以立刻返回给他。\n堆的应用三：利用堆求中位数前面我们讲了如何求Top K的问题，现在我们来讲下，如何求动态数据集合中的中位数。\n中位数，顾名思义，就是处在中间位置的那个数。如果数据的个数是奇数，把数据从小到大排列，那第$\\frac{n}{2}+1$个数据就是中位数（注意：假设数据是从0开始编号的）；如果数据的个数是偶数的话，那处于中间位置的数据有两个，第$\\frac{n}{2}$个和第$\\frac{n}{2}+1$个数据，这个时候，我们可以随意取一个作为中位数，比如取两个数中靠前的那个，就是第$\\frac{n}{2}$个数据。\n\n对于一组静态数据，中位数是固定的，我们可以先排序，第$\\frac{n}{2}$个数据就是中位数。每次询问中位数的时候，我们直接返回这个固定的值就好了。所以，尽管排序的代价比较大，但是边际成本会很小。但是，如果我们面对的是动态数据集合，中位数在不停地变动，如果再用先排序的方法，每次询问中位数的时候，都要先进行排序，那效率就不高了。\n借助堆这种数据结构，我们不用排序，就可以非常高效地实现求中位数操作。我们来看看，它是如何做到的？\n我们需要维护两个堆，一个大顶堆，一个小顶堆。大顶堆中存储前半部分数据，小顶堆中存储后半部分数据，且小顶堆中的数据都大于大顶堆中的数据。\n也就是说，如果有n个数据，n是偶数，我们从小到大排序，那前$\\frac{n}{2}$个数据存储在大顶堆中，后$\\frac{n}{2}$个数据存储在小顶堆中。这样，大顶堆中的堆顶元素就是我们要找的中位数。如果n是奇数，情况是类似的，大顶堆就存储$\\frac{n}{2}+1$个数据，小顶堆中就存储$\\frac{n}{2}$个数据。\n\n我们前面也提到，数据是动态变化的，当新添加一个数据的时候，我们如何调整两个堆，让大顶堆中的堆顶元素继续是中位数呢？\n如果新加入的数据小于等于大顶堆的堆顶元素，我们就将这个新数据插入到大顶堆；否则，我们就将这个新数据插入到小顶堆。\n这个时候就有可能出现，两个堆中的数据个数不符合前面约定的情况：如果n是偶数，两个堆中的数据个数都是$\\frac{n}{2}$；如果n是奇数，大顶堆有$\\frac{n}{2}+1$个数据，小顶堆有$\\frac{n}{2}$个数据。这个时候，我们可以从一个堆中不停地将堆顶元素移动到另一个堆，通过这样的调整，来让两个堆中的数据满足上面的约定。\n\n于是，我们就可以利用两个堆，一个大顶堆、一个小顶堆，实现在动态数据集合中求中位数的操作。插入数据因为需要涉及堆化，所以时间复杂度变成了O(logn)，但是求中位数我们只需要返回大顶堆的堆顶元素就可以了，所以时间复杂度就是O(1)。\n实际上，利用两个堆不仅可以快速求出中位数，还可以快速求其他百分位的数据，原理是类似的。还记得我们在“为什么要学习数据结构与算法”里的这个问题吗？“如何快速求接口的99%响应时间？”我们现在就来看下，利用两个堆如何来实现。\n在开始这个问题的讲解之前，我先解释一下，什么是“99%响应时间”。\n中位数的概念就是将数据从小到大排列，处于中间位置，就叫中位数，这个数据会大于等于前面50%的数据。99百分位数的概念可以类比中位数，如果将一组数据从小到大排列，这个99百分位数就是大于前面99%数据的那个数据。\n如果你还是不太理解，我再举个例子。假设有100个数据，分别是1，2，3，……，100，那99百分位数就是99，因为小于等于99的数占总个数的99%。\n\n弄懂了这个概念，我们再来看99%响应时间。如果有100个接口访问请求，每个接口请求的响应时间都不同，比如55毫秒、100毫秒、23毫秒等，我们把这100个接口的响应时间按照从小到大排列，排在第99的那个数据就是99%响应时间，也叫99百分位响应时间。\n我们总结一下，如果有n个数据，将数据从小到大排列之后，99百分位数大约就是第n*99%个数据，同类，80百分位数大约就是第n*80%个数据。\n弄懂了这些，我们再来看如何求99%响应时间。\n我们维护两个堆，一个大顶堆，一个小顶堆。假设当前总数据的个数是n，大顶堆中保存n*99%个数据，小顶堆中保存n*1%个数据。大顶堆堆顶的数据就是我们要找的99%响应时间。\n每次插入一个数据的时候，我们要判断这个数据跟大顶堆和小顶堆堆顶数据的大小关系，然后决定插入到哪个堆中。如果这个新插入的数据比大顶堆的堆顶数据小，那就插入大顶堆；如果这个新插入的数据比小顶堆的堆顶数据大，那就插入小顶堆。\n但是，为了保持大顶堆中的数据占99%，小顶堆中的数据占1%，在每次新插入数据之后，我们都要重新计算，这个时候大顶堆和小顶堆中的数据个数，是否还符合99:1这个比例。如果不符合，我们就将一个堆中的数据移动到另一个堆，直到满足这个比例。移动的方法类似前面求中位数的方法，这里我就不啰嗦了。\n通过这样的方法，每次插入数据，可能会涉及几个数据的堆化操作，所以时间复杂度是O(logn)。每次求99%响应时间的时候，直接返回大顶堆中的堆顶数据即可，时间复杂度是O(1)。\n解答开篇学懂了上面的一些应用场景的处理思路，我想你应该能解决开篇的那个问题了吧。假设现在我们有一个包含10亿个搜索关键词的日志文件，如何快速获取到Top 10最热门的搜索关键词呢？\n处理这个问题，有很多高级的解决方法，比如使用MapReduce等。但是，如果我们将处理的场景限定为单机，可以使用的内存为1GB。那这个问题该如何解决呢？\n因为用户搜索的关键词，有很多可能都是重复的，所以我们首先要统计每个搜索关键词出现的频率。我们可以通过散列表、平衡二叉查找树或者其他一些支持快速查找、插入的数据结构，来记录关键词及其出现的次数。\n假设我们选用散列表。我们就顺序扫描这10亿个搜索关键词。当扫描到某个关键词时，我们去散列表中查询。如果存在，我们就将对应的次数加一；如果不存在，我们就将它插入到散列表，并记录次数为1。以此类推，等遍历完这10亿个搜索关键词之后，散列表中就存储了不重复的搜索关键词以及出现的次数。\n然后，我们再根据前面讲的用堆求Top K的方法，建立一个大小为10的小顶堆，遍历散列表，依次取出每个搜索关键词及对应出现的次数，然后与堆顶的搜索关键词对比。如果出现次数比堆顶搜索关键词的次数多，那就删除堆顶的关键词，将这个出现次数更多的关键词加入到堆中。\n以此类推，当遍历完整个散列表中的搜索关键词之后，堆中的搜索关键词就是出现次数最多的Top 10搜索关键词了。\n不知道你发现了没有，上面的解决思路其实存在漏洞。10亿的关键词还是很多的。我们假设10亿条搜索关键词中不重复的有1亿条，如果每个搜索关键词的平均长度是50个字节，那存储1亿个关键词起码需要5GB的内存空间，而散列表因为要避免频繁冲突，不会选择太大的装载因子，所以消耗的内存空间就更多了。而我们的机器只有1GB的可用内存空间，所以我们无法一次性将所有的搜索关键词加入到内存中。这个时候该怎么办呢？\n我们在哈希算法那一节讲过，相同数据经过哈希算法得到的哈希值是一样的。我们可以根据哈希算法的这个特点，将10亿条搜索关键词先通过哈希算法分片到10个文件中。\n具体可以这样做：我们创建10个空文件00，01，02，……，09。我们遍历这10亿个关键词，并且通过某个哈希算法对其求哈希值，然后哈希值同10取模，得到的结果就是这个搜索关键词应该被分到的文件编号。\n对这10亿个关键词分片之后，每个文件都只有1亿的关键词，去除掉重复的，可能就只有1000万个，每个关键词平均50个字节，所以总的大小就是500MB。1GB的内存完全可以放得下。\n我们针对每个包含1亿条搜索关键词的文件，利用散列表和堆，分别求出Top 10，然后把这个10个Top 10放在一块，然后取这100个关键词中，出现次数最多的10个关键词，这就是这10亿数据中的Top 10最频繁的搜索关键词了。\n内容小结我们今天主要讲了堆的几个重要的应用，它们分别是：优先级队列、求Top K问题和求中位数问题。\n优先级队列是一种特殊的队列，优先级高的数据先出队，而不再像普通的队列那样，先进先出。实际上，堆就可以看作优先级队列，只是称谓不一样罢了。求Top K问题又可以分为针对静态数据和针对动态数据，只需要利用一个堆，就可以做到非常高效率地查询Top K的数据。求中位数实际上还有很多变形，比如求99百分位数据、90百分位数据等，处理的思路都是一样的，即利用两个堆，一个大顶堆，一个小顶堆，随着数据的动态添加，动态调整两个堆中的数据，最后大顶堆的堆顶元素就是要求的数据。\n课后思考有一个访问量非常大的新闻网站，我们希望将点击量排名Top 10的新闻摘要，滚动显示在网站首页banner上，并且每隔1小时更新一次。如果你是负责开发这个功能的工程师，你会如何来实现呢？\n欢迎留言和我分享，我会第一时间给你反馈。\n","categories":["数据结构与算法","基础篇"]},{"title":"33 | 字符串匹配基础（中）：如何实现文本编辑器中的查找功能？","url":"/2020/08/07/33%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%9A%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%8C%B9%E9%85%8D%E5%9F%BA%E7%A1%80%EF%BC%88%E4%B8%AD%EF%BC%89/","content":"文本编辑器中的查找替换功能，我想你应该不陌生吧？比如，我们在Word中把一个单词统一替换成另一个，用的就是这个功能。你有没有想过，它是怎么实现的呢？\n当然，你用上一节讲的BF算法和RK算法，也可以实现这个功能，但是在某些极端情况下，BF算法性能会退化的比较严重，而RK算法需要用到哈希算法，而设计一个可以应对各种类型字符的哈希算法并不简单。\n对于工业级的软件开发来说，我们希望算法尽可能的高效，并且在极端情况下，性能也不要退化的太严重。那么，对于查找功能是重要功能的软件来说，比如一些文本编辑器，它们的查找功能都是用哪种算法来实现的呢？有没有比BF算法和RK算法更加高效的字符串匹配算法呢？\n今天，我们就来学习BM（Boyer-Moore）算法。它是一种非常高效的字符串匹配算法，有实验统计，它的性能是著名的KMP算法的3到4倍。BM算法的原理很复杂，比较难懂，学起来会比较烧脑，我会尽量给你讲清楚，同时也希望你做好打硬仗的准备。好，现在我们正式开始！\nBM算法的核心思想我们把模式串和主串的匹配过程，看作模式串在主串中不停地往后滑动。当遇到不匹配的字符时，BF算法和RK算法的做法是，模式串往后滑动一位，然后从模式串的第一个字符开始重新匹配。我举个例子解释一下，你可以看我画的这幅图。\n\n在这个例子里，主串中的c，在模式串中是不存在的，所以，模式串向后滑动的时候，只要c与模式串有重合，肯定无法匹配。所以，我们可以一次性把模式串往后多滑动几位，把模式串移动到c的后面。\n\n由现象找规律，你可以思考一下，当遇到不匹配的字符时，有什么固定的规律，可以将模式串往后多滑动几位呢？这样一次性往后滑动好几位，那匹配的效率岂不是就提高了？\n我们今天要讲的BM算法，本质上其实就是在寻找这种规律。借助这种规律，在模式串与主串匹配的过程中，当模式串和主串某个字符不匹配的时候，能够跳过一些肯定不会匹配的情况，将模式串往后多滑动几位。\nBM算法原理分析BM算法包含两部分，分别是坏字符规则（bad character rule）和好后缀规则（good suffix shift）。我们下面依次来看，这两个规则分别都是怎么工作的。\n1.坏字符规则前面两节讲的算法，在匹配的过程中，我们都是按模式串的下标从小到大的顺序，依次与主串中的字符进行匹配的。这种匹配顺序比较符合我们的思维习惯，而BM算法的匹配顺序比较特别，它是按照模式串下标从大到小的顺序，倒着匹配的。我画了一张图，你可以看下。\n\n我们从模式串的末尾往前倒着匹配，当我们发现某个字符没法匹配的时候。我们把这个没有匹配的字符叫作坏字符（主串中的字符）。\n\n我们拿坏字符c在模式串中查找，发现模式串中并不存在这个字符，也就是说，字符c与模式串中的任何字符都不可能匹配。这个时候，我们可以将模式串直接往后滑动三位，将模式串滑动到c后面的位置，再从模式串的末尾字符开始比较。\n\n这个时候，我们发现，模式串中最后一个字符d，还是无法跟主串中的a匹配，这个时候，还能将模式串往后滑动三位吗？答案是不行的。因为这个时候，坏字符a在模式串中是存在的，模式串中下标是0的位置也是字符a。这种情况下，我们可以将模式串往后滑动两位，让两个a上下对齐，然后再从模式串的末尾字符开始，重新匹配。\n\n第一次不匹配的时候，我们滑动了三位，第二次不匹配的时候，我们将模式串后移两位，那具体滑动多少位，到底有没有规律呢？\n当发生不匹配的时候，我们把坏字符对应的模式串中的字符下标记作si。如果坏字符在模式串中存在，我们把这个坏字符在模式串中的下标记作xi。如果不存在，我们把xi记作-1。那模式串往后移动的位数就等于si-xi。（注意，我这里说的下标，都是字符在模式串的下标）。\n\n这里我要特别说明一点，如果坏字符在模式串里多处出现，那我们在计算xi的时候，选择最靠后的那个，因为这样不会让模式串滑动过多，导致本来可能匹配的情况被滑动略过。\n利用坏字符规则，BM算法在最好情况下的时间复杂度非常低，是O(n/m)。比如，主串是aaabaaabaaabaaab，模式串是aaaa。每次比对，模式串都可以直接后移四位，所以，匹配具有类似特点的模式串和主串的时候，BM算法非常高效。\n不过，单纯使用坏字符规则还是不够的。因为根据si-xi计算出来的移动位数，有可能是负数，比如主串是aaaaaaaaaaaaaaaa，模式串是baaa。不但不会向后滑动模式串，还有可能倒退。所以，BM算法还需要用到“好后缀规则”。\n2.好后缀规则好后缀规则实际上跟坏字符规则的思路很类似。你看我下面这幅图。当模式串滑动到图中的位置的时候，模式串和主串有2个字符是匹配的，倒数第3个字符发生了不匹配的情况。\n\n这个时候该如何滑动模式串呢？当然，我们还可以利用坏字符规则来计算模式串的滑动位数，不过，我们也可以使用好后缀处理规则。两种规则到底如何选择，我稍后会讲。抛开这个问题，现在我们来看，好后缀规则是怎么工作的？\n我们把已经匹配的bc叫作好后缀，记作{u}。我们拿它在模式串中查找，如果找到了另一个跟{u}相匹配的子串{u*}，那我们就将模式串滑动到子串{u*}与主串中{u}对齐的位置。\n\n如果在模式串中找不到另一个等于{u}的子串，我们就直接将模式串，滑动到主串中{u}的后面，因为之前的任何一次往后滑动，都没有匹配主串中{u}的情况。\n\n不过，当模式串中不存在等于{u}的子串时，我们直接将模式串滑动到主串{u}的后面。这样做是否有点太过头呢？我们来看下面这个例子。这里面bc是好后缀，尽管在模式串中没有另外一个相匹配的子串{u*}，但是如果我们将模式串移动到好后缀的后面，如图所示，那就会错过模式串和主串可以匹配的情况。\n\n如果好后缀在模式串中不存在可匹配的子串，那在我们一步一步往后滑动模式串的过程中，只要主串中的{u}与模式串有重合，那肯定就无法完全匹配。但是当模式串滑动到前缀与主串中{u}的后缀有部分重合的时候，并且重合的部分相等的时候，就有可能会存在完全匹配的情况。\n\n所以，针对这种情况，我们不仅要看好后缀在模式串中，是否有另一个匹配的子串，我们还要考察好后缀的后缀子串，是否存在跟模式串的前缀子串匹配的。\n所谓某个字符串s的后缀子串，就是最后一个字符跟s对齐的子串，比如abc的后缀子串就包括c, bc。所谓前缀子串，就是起始字符跟s对齐的子串，比如abc的前缀子串有a，ab。我们从好后缀的后缀子串中，找一个最长的并且能跟模式串的前缀子串匹配的，假设是{v}，然后将模式串滑动到如图所示的位置。\n\n坏字符和好后缀的基本原理都讲完了，我现在回答一下前面那个问题。当模式串和主串中的某个字符不匹配的时候，如何选择用好后缀规则还是坏字符规则，来计算模式串往后滑动的位数？\n我们可以分别计算好后缀和坏字符往后滑动的位数，然后取两个数中最大的，作为模式串往后滑动的位数。这种处理方法还可以避免我们前面提到的，根据坏字符规则，计算得到的往后滑动的位数，有可能是负数的情况。\nBM算法代码实现学习完了基本原理，我们再来看，如何实现BM算法？\n“坏字符规则”本身不难理解。当遇到坏字符时，要计算往后移动的位数si-xi，其中xi的计算是重点，我们如何求得xi呢？或者说，如何查找坏字符在模式串中出现的位置呢？\n如果我们拿坏字符，在模式串中顺序遍历查找，这样就会比较低效，势必影响这个算法的性能。有没有更加高效的方式呢？我们之前学的散列表，这里可以派上用场了。我们可以将模式串中的每个字符及其下标都存到散列表中。这样就可以快速找到坏字符在模式串的位置下标了。\n关于这个散列表，我们只实现一种最简单的情况，假设字符串的字符集不是很大，每个字符长度是1字节，我们用大小为256的数组，来记录每个字符在模式串中出现的位置。数组的下标对应字符的ASCII码值，数组中存储这个字符在模式串中出现的位置。\n\n如果将上面的过程翻译成代码，就是下面这个样子。其中，变量b是模式串，m是模式串的长度，bc表示刚刚讲的散列表。\nprivate static final int SIZE = 256; // 全局变量或成员变量private void generateBC(char[] b, int m, int[] bc) &#123;    for (int i = 0; i &lt; SIZE; ++i) &#123;        bc[i] = -1; // 初始化bc    &#125;    for (int i = 0; i &lt; m; ++i) &#123;        int ascii = (int)b[i]; // 计算b[i]的ASCII值        bc[ascii] = i;    &#125;&#125;\n掌握了坏字符规则之后，我们先把BM算法代码的大框架写好，先不考虑好后缀规则，仅用坏字符规则，并且不考虑si-xi计算得到的移动位数可能会出现负数的情况。\npublic int bm(char[] a, int n, char[] b, int m) &#123;    int[] bc = new int[SIZE]; // 记录模式串中每个字符最后出现的位置    generateBC(b, m, bc); // 构建坏字符哈希表    int i = 0; // i表示主串与模式串对齐的第一个字符    while (i &lt;= n - m) &#123;        int j;        for (j = m - 1; j &gt;= 0; --j) &#123; // 模式串从后往前匹配            if (a[i+j] != b[j]) break; // 坏字符对应模式串中的下标是j        &#125;        if (j &lt; 0) &#123;            return i; // 匹配成功，返回主串与模式串第一个匹配的字符的位置        &#125;        // 这里等同于将模式串往后滑动j-bc[(int)a[i+j]]位        i = i + (j - bc[(int)a[i+j]]);     &#125;    return -1;&#125;\n代码里的注释已经很详细了，我就不再赘述了。不过，为了你方便理解，我画了一张图，将其中的一些关键变量标注在上面了，结合着图，代码应该更好理解。\n\n至此，我们已经实现了包含坏字符规则的框架代码，只剩下往框架代码中填充好后缀规则了。现在，我们就来看看，如何实现好后缀规则。它的实现要比坏字符规则复杂一些。\n在讲实现之前，我们先简单回顾一下，前面讲过好后缀的处理规则中最核心的内容：\n\n在模式串中，查找跟好后缀匹配的另一个子串；\n\n在好后缀的后缀子串中，查找最长的、能跟模式串前缀子串匹配的后缀子串；\n\n\n在不考虑效率的情况下，这两个操作都可以用很“暴力”的匹配查找方式解决。但是，如果想要BM算法的效率很高，这部分就不能太低效。如何来做呢？\n因为好后缀也是模式串本身的后缀子串，所以，我们可以在模式串和主串正式匹配之前，通过预处理模式串，预先计算好模式串的每个后缀子串，对应的另一个可匹配子串的位置。这个预处理过程比较有技巧，很不好懂，应该是这节最难懂的内容了，你要认真多读几遍。\n我们先来看，如何表示模式串中不同的后缀子串呢？因为后缀子串的最后一个字符的位置是固定的，下标为m-1，我们只需要记录长度就可以了。通过长度，我们可以确定一个唯一的后缀子串。\n\n现在，我们要引入最关键的变量suffix数组。suffix数组的下标k，表示后缀子串的长度，下标对应的数组值存储的是，在模式串中跟好后缀{u}相匹配的子串{u*}的起始下标值。这句话不好理解，我举一个例子。\n\n但是，如果模式串中有多个（大于1个）子串跟后缀子串{u}匹配，那suffix数组中该存储哪一个子串的起始位置呢？为了避免模式串往后滑动得过头了，我们肯定要存储模式串中最靠后的那个子串的起始位置，也就是下标最大的那个子串的起始位置。不过，这样处理就足够了吗？\n实际上，仅仅是选最靠后的子串片段来存储是不够的。我们再回忆一下好后缀规则。\n我们不仅要在模式串中，查找跟好后缀匹配的另一个子串，还要在好后缀的后缀子串中，查找最长的能跟模式串前缀子串匹配的后缀子串。\n如果我们只记录刚刚定义的suffix，实际上，只能处理规则的前半部分，也就是，在模式串中，查找跟好后缀匹配的另一个子串。所以，除了suffix数组之外，我们还需要另外一个boolean类型的prefix数组，来记录模式串的后缀子串是否能匹配模式串的前缀子串。\n\n现在，我们来看下，如何来计算并填充这两个数组的值？这个计算过程非常巧妙。\n我们拿下标从0到i的子串（i可以是0到m-2）与整个模式串，求公共后缀子串。如果公共后缀子串的长度是k，那我们就记录suffix[k]=j（j表示公共后缀子串的起始下标）。如果j等于0，也就是说，公共后缀子串也是模式串的前缀子串，我们就记录prefix[k]=true。\n\n我们把suffix数组和prefix数组的计算过程，用代码实现出来，就是下面这个样子：\n// b表示模式串，m表示长度，suffix，prefix数组事先申请好了private void generateGS(char[] b, int m, int[] suffix, boolean[] prefix) &#123;    for (int i = 0; i &lt; m; ++i) &#123; // 初始化        suffix[i] = -1;        prefix[i] = false;    &#125;    for (int i = 0; i &lt; m - 1; ++i) &#123; // b[0, i]        int j = i;        int k = 0; // 公共后缀子串长度        while (j &gt;= 0 &amp;&amp; b[j] == b[m-1-k]) &#123; // 与b[0, m-1]求公共后缀子串            --j;            ++k;            suffix[k] = j+1; //j+1表示公共后缀子串在b[0, i]中的起始下标        &#125;        if (j == -1) prefix[k] = true; //如果公共后缀子串也是模式串的前缀子串    &#125;&#125;\n有了这两个数组之后，我们现在来看，在模式串跟主串匹配的过程中，遇到不能匹配的字符时，如何根据好后缀规则，计算模式串往后滑动的位数？\n假设好后缀的长度是k。我们先拿好后缀，在suffix数组中查找其匹配的子串。如果suffix[k]不等于-1（-1表示不存在匹配的子串），那我们就将模式串往后移动j-suffix[k]+1位（j表示坏字符对应的模式串中的字符下标）。如果suffix[k]等于-1，表示模式串中不存在另一个跟好后缀匹配的子串片段。我们可以用下面这条规则来处理。\n\n好后缀的后缀子串b[r, m-1]（其中，r取值从j+2到m-1）的长度k=m-r，如果prefix[k]等于true，表示长度为k的后缀子串，有可匹配的前缀子串，这样我们可以把模式串后移r位。\n\n如果两条规则都没有找到可以匹配好后缀及其后缀子串的子串，我们就将整个模式串后移m位。\n\n至此，好后缀规则的代码实现我们也讲完了。我们把好后缀规则加到前面的代码框架里，就可以得到BM算法的完整版代码实现。\n// a,b表示主串和模式串；n，m表示主串和模式串的长度。public int bm(char[] a, int n, char[] b, int m) &#123;    int[] bc = new int[SIZE]; // 记录模式串中每个字符最后出现的位置    generateBC(b, m, bc); // 构建坏字符哈希表    int[] suffix = new int[m];    boolean[] prefix = new boolean[m];    generateGS(b, m, suffix, prefix);    int i = 0; // j表示主串与模式串匹配的第一个字符    while (i &lt;= n - m) &#123;        int j;        for (j = m - 1; j &gt;= 0; --j) &#123; // 模式串从后往前匹配            if (a[i+j] != b[j]) break; // 坏字符对应模式串中的下标是j        &#125;        if (j &lt; 0) &#123;            return i; // 匹配成功，返回主串与模式串第一个匹配的字符的位置        &#125;        int x = j - bc[(int)a[i+j]];        int y = 0;        if (j &lt; m-1) &#123; // 如果有好后缀的话            y = moveByGS(j, m, suffix, prefix);        &#125;        i = i + Math.max(x, y);    &#125;    return -1;&#125;// j表示坏字符对应的模式串中的字符下标; m表示模式串长度private int moveByGS(int j, int m, int[] suffix, boolean[] prefix) &#123;    int k = m - 1 - j; // 好后缀长度    if (suffix[k] != -1) return j - suffix[k] +1;    for (int r = j+2; r &lt;= m-1; ++r) &#123;        if (prefix[m-r] == true) &#123;            return r;        &#125;    &#125;    return m;&#125;\nBM算法的性能分析及优化我们先来分析BM算法的内存消耗。整个算法用到了额外的3个数组，其中bc数组的大小跟字符集大小有关，suffix数组和prefix数组的大小跟模式串长度m有关。\n如果我们处理字符集很大的字符串匹配问题，bc数组对内存的消耗就会比较多。因为好后缀和坏字符规则是独立的，如果我们运行的环境对内存要求苛刻，可以只使用好后缀规则，不使用坏字符规则，这样就可以避免bc数组过多的内存消耗。不过，单纯使用好后缀规则的BM算法效率就会下降一些了。\n对于执行效率来说，我们可以先从时间复杂度的角度来分析。\n实际上，我前面讲的BM算法是个初级版本。为了让你能更容易理解，有些复杂的优化我没有讲。基于我目前讲的这个版本，在极端情况下，预处理计算suffix数组、prefix数组的性能会比较差。\n比如模式串是aaaaaaa这种包含很多重复的字符的模式串，预处理的时间复杂度就是O(m^2)。当然，大部分情况下，时间复杂度不会这么差。关于如何优化这种极端情况下的时间复杂度退化，如果感兴趣，你可以自己研究一下。\n实际上，BM算法的时间复杂度分析起来是非常复杂，这篇论文“A new proof of the linearity of the Boyer-Moore string searching algorithm”证明了在最坏情况下，BM算法的比较次数上限是5n。这篇论文“Tight bounds on the complexity of the Boyer-Moore string matching algorithm”证明了在最坏情况下，BM算法的比较次数上限是3n。你可以自己阅读看看。\n解答开篇&amp;内容小结今天，我们讲了一种比较复杂的字符串匹配算法，BM算法。尽管复杂、难懂，但匹配的效率却很高，在实际的软件开发中，特别是一些文本编辑器中，应用比较多。如果一遍看不懂的话，你就多看几遍。\nBM算法核心思想是，利用模式串本身的特点，在模式串中某个字符与主串不能匹配的时候，将模式串往后多滑动几位，以此来减少不必要的字符比较，提高匹配的效率。BM算法构建的规则有两类，坏字符规则和好后缀规则。好后缀规则可以独立于坏字符规则使用。因为坏字符规则的实现比较耗内存，为了节省内存，我们可以只用好后缀规则来实现BM算法。\n课后思考你熟悉的编程语言中的查找函数，或者工具、软件中的查找功能，都是用了哪种字符串匹配算法呢？\n欢迎留言和我分享，也欢迎点击“请朋友读”，把今天的内容分享给你的好友，和他一起讨论、学习。\n","categories":["数据结构与算法","基础篇"]},{"title":"35 | Trie树：如何实现搜索引擎的搜索关键词提示功能？","url":"/2020/08/07/35%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%9ATrie%E6%A0%91/","content":"搜索引擎的搜索关键词提示功能，我想你应该不陌生吧？为了方便快速输入，当你在搜索引擎的搜索框中，输入要搜索的文字的某一部分的时候，搜索引擎就会自动弹出下拉框，里面是各种关键词提示。你可以直接从下拉框中选择你要搜索的东西，而不用把所有内容都输入进去，一定程度上节省了我们的搜索时间。\n\n尽管这个功能我们几乎天天在用，作为一名工程师，你是否思考过，它是怎么实现的呢？它底层使用的是哪种数据结构和算法呢？\n像Google、百度这样的搜索引擎，它们的关键词提示功能非常全面和精准，肯定做了很多优化，但万变不离其宗，底层最基本的原理就是今天要讲的这种数据结构：Trie树。\n什么是“Trie树”？Trie树，也叫“字典树”。顾名思义，它是一个树形结构。它是一种专门处理字符串匹配的数据结构，用来解决在一组字符串集合中快速查找某个字符串的问题。\n当然，这样一个问题可以有多种解决方法，比如散列表、红黑树，或者我们前面几节讲到的一些字符串匹配算法，但是，Trie树在这个问题的解决上，有它特有的优点。不仅如此，Trie树能解决的问题也不限于此，我们一会儿慢慢分析。\n现在，我们先来看下，Trie树到底长什么样子。\n我举个简单的例子来说明一下。我们有6个字符串，它们分别是：how，hi，her，hello，so，see。我们希望在里面多次查找某个字符串是否存在。如果每次查找，都是拿要查找的字符串跟这6个字符串依次进行字符串匹配，那效率就比较低，有没有更高效的方法呢？\n这个时候，我们就可以先对这6个字符串做一下预处理，组织成Trie树的结构，之后每次查找，都是在Trie树中进行匹配查找。Trie树的本质，就是利用字符串之间的公共前缀，将重复的前缀合并在一起。最后构造出来的就是下面这个图中的样子。\n\n其中，根节点不包含任何信息。每个节点表示一个字符串中的字符，从根节点到红色节点的一条路径表示一个字符串（注意：红色节点并不都是叶子节点）。\n为了让你更容易理解Trie树是怎么构造出来的，我画了一个Trie树构造的分解过程。构造过程的每一步，都相当于往Trie树中插入一个字符串。当所有字符串都插入完成之后，Trie树就构造好了。\n)\n当我们在Trie树中查找一个字符串的时候，比如查找字符串“her”，那我们将要查找的字符串分割成单个的字符h，e，r，然后从Trie树的根节点开始匹配。如图所示，绿色的路径就是在Trie树中匹配的路径。\n\n如果我们要查找的是字符串“he”呢？我们还用上面同样的方法，从根节点开始，沿着某条路径来匹配，如图所示，绿色的路径，是字符串“he”匹配的路径。但是，路径的最后一个节点“e”并不是红色的。也就是说，“he”是某个字符串的前缀子串，但并不能完全匹配任何字符串。\n\n如何实现一棵Trie树？知道了Trie树长什么样子，我们现在来看下，如何用代码来实现一个Trie树。\n从刚刚Trie树的介绍来看，Trie树主要有两个操作，一个是将字符串集合构造成Trie树。这个过程分解开来的话，就是一个将字符串插入到Trie树的过程。另一个是在Trie树中查询一个字符串。\n了解了Trie树的两个主要操作之后，我们再来看下，如何存储一个Trie树？\n从前面的图中，我们可以看出，Trie树是一个多叉树。我们知道，二叉树中，一个节点的左右子节点是通过两个指针来存储的，如下所示Java代码。那对于多叉树来说，我们怎么存储一个节点的所有子节点的指针呢？\nclass BinaryTreeNode &#123;    char data;    BinaryTreeNode left;    BinaryTreeNode right;  &#125;\n我先介绍其中一种存储方式，也是经典的存储方式，大部分数据结构和算法书籍中都是这么讲的。还记得我们前面讲到的散列表吗？借助散列表的思想，我们通过一个下标与字符一一映射的数组，来存储子节点的指针。这句话稍微有点抽象，不怎么好懂，我画了一张图你可以看看。\n\n假设我们的字符串中只有从a到z这26个小写字母，我们在数组中下标为0的位置，存储指向子节点a的指针，下标为1的位置存储指向子节点b的指针，以此类推，下标为25的位置，存储的是指向的子节点z的指针。如果某个字符的子节点不存在，我们就在对应的下标的位置存储null。\nclass TrieNode &#123;    char data;    TrieNode children[26];&#125;\n当我们在Trie树中查找字符串的时候，我们就可以通过字符的ASCII码减去“a”的ASCII码，迅速找到匹配的子节点的指针。比如，d的ASCII码减去a的ASCII码就是3，那子节点d的指针就存储在数组中下标为3的位置中。\n描述了这么多，有可能你还是有点懵，我把上面的描述翻译成了代码，你可以结合着一块看下，应该有助于你理解。\npublic class Trie &#123;    private TrieNode root = new TrieNode('/'); // 存储无意义字符    // 往Trie树中插入一个字符串    public void insert(char[] text) &#123;        TrieNode p = root;        for (int i = 0; i &lt; text.length; ++i) &#123;            int index = text[i] - 'a';            if (p.children[index] == null) &#123;                TrieNode newNode = new TrieNode(text[i]);                p.children[index] = newNode;            &#125;            p = p.children[index];        &#125;        p.isEndingChar = true;    &#125;    // 在Trie树中查找一个字符串    public boolean find(char[] pattern) &#123;        TrieNode p = root;        for (int i = 0; i &lt; pattern.length; ++i) &#123;            int index = pattern[i] - 'a';            if (p.children[index] == null) &#123;                return false; // 不存在pattern            &#125;            p = p.children[index];        &#125;        if (p.isEndingChar == false) return false; // 不能完全匹配，只是前缀        else return true; // 找到pattern    &#125;    public class TrieNode &#123;        public char data;        public TrieNode[] children = new TrieNode[26];        public boolean isEndingChar = false;        public TrieNode(char data) &#123;            this.data = data;        &#125;    &#125;&#125;\nTrie树的实现，你现在应该搞懂了。现在，我们来看下，在Trie树中，查找某个字符串的时间复杂度是多少？\n如果要在一组字符串中，频繁地查询某些字符串，用Trie树会非常高效。构建Trie树的过程，需要扫描所有的字符串，时间复杂度是O(n)（n表示所有字符串的长度和）。但是一旦构建成功之后，后续的查询操作会非常高效。\n每次查询时，如果要查询的字符串长度是k，那我们只需要比对大约k个节点，就能完成查询操作。跟原本那组字符串的长度和个数没有任何关系。所以说，构建好Trie树后，在其中查找字符串的时间复杂度是O(k)，k表示要查找的字符串的长度。\nTrie树真的很耗内存吗？前面我们讲了Trie树的实现，也分析了时间复杂度。现在你应该知道，Trie树是一种非常独特的、高效的字符串匹配方法。但是，关于Trie树，你有没有听过这样一种说法：“Trie树是非常耗内存的，用的是一种空间换时间的思路”。这是什么原因呢？\n刚刚我们在讲Trie树的实现的时候，讲到用数组来存储一个节点的子节点的指针。如果字符串中包含从a到z这26个字符，那每个节点都要存储一个长度为26的数组，并且每个数组元素要存储一个8字节指针（或者是4字节，这个大小跟CPU、操作系统、编译器等有关）。而且，即便一个节点只有很少的子节点，远小于26个，比如3、4个，我们也要维护一个长度为26的数组。\n我们前面讲过，Trie树的本质是避免重复存储一组字符串的相同前缀子串，但是现在每个字符（对应一个节点）的存储远远大于1个字节。按照我们上面举的例子，数组长度为26，每个元素是8字节，那每个节点就会额外需要26*8=208个字节。而且这还是只包含26个字符的情况。\n如果字符串中不仅包含小写字母，还包含大写字母、数字、甚至是中文，那需要的存储空间就更多了。所以，也就是说，在某些情况下，Trie树不一定会节省存储空间。在重复的前缀并不多的情况下，Trie树不但不能节省内存，还有可能会浪费更多的内存。\n当然，我们不可否认，Trie树尽管有可能很浪费内存，但是确实非常高效。那为了解决这个内存问题，我们是否有其他办法呢？\n我们可以稍微牺牲一点查询的效率，将每个节点中的数组换成其他数据结构，来存储一个节点的子节点指针。用哪种数据结构呢？我们的选择其实有很多，比如有序数组、跳表、散列表、红黑树等。\n假设我们用有序数组，数组中的指针按照所指向的子节点中的字符的大小顺序排列。查询的时候，我们可以通过二分查找的方法，快速查找到某个字符应该匹配的子节点的指针。但是，在往Trie树中插入一个字符串的时候，我们为了维护数组中数据的有序性，就会稍微慢了点。\n替换成其他数据结构的思路是类似的，这里我就不一一分析了，你可以结合前面学过的内容，自己分析一下。\n实际上，Trie树的变体有很多，都可以在一定程度上解决内存消耗的问题。比如，缩点优化，就是对只有一个子节点的节点，而且此节点不是一个串的结束节点，可以将此节点与子节点合并。这样可以节省空间，但却增加了编码难度。这里我就不展开详细讲解了，你如果感兴趣，可以自行研究下。\n\nTrie树与散列表、红黑树的比较实际上，字符串的匹配问题，笼统上讲，其实就是数据的查找问题。对于支持动态数据高效操作的数据结构，我们前面已经讲过好多了，比如散列表、红黑树、跳表等等。实际上，这些数据结构也可以实现在一组字符串中查找字符串的功能。我们选了两种数据结构，散列表和红黑树，跟Trie树比较一下，看看它们各自的优缺点和应用场景。\n在刚刚讲的这个场景，在一组字符串中查找字符串，Trie树实际上表现得并不好。它对要处理的字符串有及其严苛的要求。\n第一，字符串中包含的字符集不能太大。我们前面讲到，如果字符集太大，那存储空间可能就会浪费很多。即便可以优化，但也要付出牺牲查询、插入效率的代价。\n第二，要求字符串的前缀重合比较多，不然空间消耗会变大很多。\n第三，如果要用Trie树解决问题，那我们就要自己从零开始实现一个Trie树，还要保证没有bug，这个在工程上是将简单问题复杂化，除非必须，一般不建议这样做。\n第四，我们知道，通过指针串起来的数据块是不连续的，而Trie树中用到了指针，所以，对缓存并不友好，性能上会打个折扣。\n综合这几点，针对在一组字符串中查找字符串的问题，我们在工程中，更倾向于用散列表或者红黑树。因为这两种数据结构，我们都不需要自己去实现，直接利用编程语言中提供的现成类库就行了。\n讲到这里，你可能要疑惑了，讲了半天，我对Trie树一通否定，还让你用红黑树或者散列表，那Trie树是不是就没用了呢？是不是今天的内容就白学了呢？\n实际上，Trie树只是不适合精确匹配查找，这种问题更适合用散列表或者红黑树来解决。Trie树比较适合的是查找前缀匹配的字符串，也就是类似开篇问题的那种场景。\n解答开篇Trie树就讲完了，我们来看下开篇提到的问题：如何利用Trie树，实现搜索关键词的提示功能？\n我们假设关键词库由用户的热门搜索关键词组成。我们将这个词库构建成一个Trie树。当用户输入其中某个单词的时候，把这个词作为一个前缀子串在Trie树中匹配。为了讲解方便，我们假设词库里只有hello、her、hi、how、so、see这6个关键词。当用户输入了字母h的时候，我们就把以h为前缀的hello、her、hi、how展示在搜索提示框内。当用户继续键入字母e的时候，我们就把以he为前缀的hello、her展示在搜索提示框内。这就是搜索关键词提示的最基本的算法原理。\n\n不过，我讲的只是最基本的实现原理，实际上，搜索引擎的搜索关键词提示功能远非我讲的这么简单。如果再稍微深入一点，你就会想到，上面的解决办法遇到下面几个问题：\n\n我刚讲的思路是针对英文的搜索关键词提示，对于更加复杂的中文来说，词库中的数据又该如何构建成Trie树呢？\n\n如果词库中有很多关键词，在搜索提示的时候，用户输入关键词，作为前缀在Trie树中可以匹配的关键词也有很多，如何选择展示哪些内容呢？\n\n像Google这样的搜索引擎，用户单词拼写错误的情况下，Google还是可以使用正确的拼写来做关键词提示，这个又是怎么做到的呢？\n\n\n你可以先思考一下如何来解决，如果不会也没关系，这些问题，我们会在实战篇里具体来讲解。\n实际上，Trie树的这个应用可以扩展到更加广泛的一个应用上，就是自动输入补全，比如输入法自动补全功能、IDE代码编辑器自动补全功能、浏览器网址输入的自动补全功能等等。\n内容小结今天我们讲了一种特殊的树，Trie树。Trie树是一种解决字符串快速匹配问题的数据结构。如果用来构建Trie树的这一组字符串中，前缀重复的情况不是很多，那Trie树这种数据结构总体上来讲是比较费内存的，是一种空间换时间的解决问题思路。\n尽管比较耗费内存，但是对内存不敏感或者内存消耗在接受范围内的情况下，在Trie树中做字符串匹配还是非常高效的，时间复杂度是O(k)，k表示要匹配的字符串的长度。\n但是，Trie树的优势并不在于，用它来做动态集合数据的查找，因为，这个工作完全可以用更加合适的散列表或者红黑树来替代。Trie树最有优势的是查找前缀匹配的字符串，比如搜索引擎中的关键词提示功能这个场景，就比较适合用它来解决，也是Trie树比较经典的应用场景。\n课后思考我们今天有讲到，Trie树应用场合对数据要求比较苛刻，比如字符串的字符集不能太大，前缀重合比较多等。如果现在给你一个很大的字符串集合，比如包含1万条记录，如何通过编程量化分析这组字符串集合是否比较适合用Trie树解决呢？也就是如何统计字符串的字符集大小，以及前缀重合的程度呢？\n欢迎留言和我分享，也欢迎点击“请朋友读”，把今天的内容分享给你的好友，和他一起讨论、学习。\n","categories":["数据结构与算法","基础篇"]},{"title":"40 | 初识动态规划：如何巧妙解决“双十一”购物时的凑单问题？","url":"/2020/08/07/40%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%9A%E5%88%9D%E8%AF%86%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/","content":"淘宝的“双十一”购物节有各种促销活动，比如“满200元减50元”。假设你女朋友的购物车中有n个（n&gt;100）想买的商品，她希望从里面选几个，在凑够满减条件的前提下，让选出来的商品价格总和最大程度地接近满减条件（200元），这样就可以极大限度地“薅羊毛”。作为程序员的你，能不能编个代码来帮她搞定呢？\n要想高效地解决这个问题，就要用到我们今天讲的动态规划（Dynamic Programming）。\n动态规划学习路线动态规划比较适合用来求解最优问题，比如求最大值、最小值等等。它可以非常显著地降低时间复杂度，提高代码的执行效率。不过，它也是出了名的难学。它的主要学习难点跟递归类似，那就是，求解问题的过程不太符合人类常规的思维方式。对于新手来说，要想入门确实不容易。不过，等你掌握了之后，你会发现，实际上并没有想象中那么难。\n为了让你更容易理解动态规划，我分了三节给你讲解。这三节分别是，初识动态规划、动态规划理论、动态规划实战。\n第一节，我会通过两个非常经典的动态规划问题模型，向你展示我们为什么需要动态规划，以及动态规划解题方法是如何演化出来的。实际上，你只要掌握了这两个例子的解决思路，对于其他很多动态规划问题，你都可以套用类似的思路来解决。\n第二节，我会总结动态规划适合解决的问题的特征，以及动态规划解题思路。除此之外，我还会将贪心、分治、回溯、动态规划这四种算法思想放在一起，对比分析它们各自的特点以及适用的场景。\n第三节，我会教你应用第二节讲的动态规划理论知识，实战解决三个非常经典的动态规划问题，加深你对理论的理解。弄懂了这三节中的例子，对于动态规划这个知识点，你就算是入门了。\n0-1背包问题我在讲贪心算法、回溯算法的时候，多次讲到背包问题。今天，我们依旧拿这个问题来举例。\n对于一组不同重量、不可分割的物品，我们需要选择一些装入背包，在满足背包最大重量限制的前提下，背包中物品总重量的最大值是多少呢？\n关于这个问题，我们上一节讲了回溯的解决方法，也就是穷举搜索所有可能的装法，然后找出满足条件的最大值。不过，回溯算法的复杂度比较高，是指数级别的。那有没有什么规律，可以有效降低时间复杂度呢？我们一起来看看。\n// 回溯算法实现。注意：我把输入的变量都定义成了成员变量。private int maxW = Integer.MIN_VALUE; // 结果放到maxW中private int[] weight = &#123;2, 2, 4, 6, 3&#125;;  // 物品重量private int n = 5; // 物品个数private int w = 9; // 背包承受的最大重量public void f(int i, int cw) &#123; // 调用f(0, 0)    if (cw == w || i == n) &#123; // cw==w表示装满了，i==n表示物品都考察完了        if (cw &gt; maxW) maxW = cw;        return;    &#125;    f(i+1, cw); // 选择不装第i个物品    if (cw + weight[i] &lt;= w) &#123;        f(i+1,cw + weight[i]); // 选择装第i个物品    &#125;&#125;\n规律是不是不好找？那我们就举个例子、画个图看看。我们假设背包的最大承载重量是9。我们有5个不同的物品，每个物品的重量分别是2，2，4，6，3。如果我们把这个例子的回溯求解过程，用递归树画出来，就是下面这个样子：\n\n递归树中的每个节点表示一种状态，我们用（i, cw）来表示。其中，i表示将要决策第几个物品是否装入背包，cw表示当前背包中物品的总重量。比如，（2，2）表示我们将要决策第3个物品是否装入背包，在决策前，背包中物品的总重量是2。\n从递归树中，你应该能会发现，有些子问题的求解是重复的，比如图中f(2, 2)和f(3,4)都被重复计算了两次。我们可以借助递归那一节讲的“备忘录”的解决方式，记录已经计算好的f(i, cw)，当再次计算到重复的f(i, cw)的时候，可以直接从备忘录中取出来用，就不用再递归计算了，这样就可以避免冗余计算。\nprivate int maxW = Integer.MIN_VALUE; // 结果放到maxW中private int[] weight = &#123;2, 2, 4, 6, 3&#125;;  // 物品重量private int n = 5; // 物品个数private int w = 9; // 背包承受的最大重量private boolean[][] mem = new boolean[5][10]; // 备忘录，默认值falsepublic void f(int i, int cw) &#123; // 调用f(0, 0)    if (cw == w || i == n) &#123; // cw==w表示装满了，i==n表示物品都考察完了        if (cw &gt; maxW) maxW = cw;        return;    &#125;    if (mem[i][cw]) return; // 重复状态    mem[i][cw] = true; // 记录(i, cw)这个状态    f(i+1, cw); // 选择不装第i个物品    if (cw + weight[i] &lt;= w) &#123;        f(i+1,cw + weight[i]); // 选择装第i个物品    &#125;&#125;\n这种解决方法非常好。实际上，它已经跟动态规划的执行效率基本上没有差别。但是，多一种方法就多一种解决思路，我们现在来看看动态规划是怎么做的。\n我们把整个求解过程分为n个阶段，每个阶段会决策一个物品是否放到背包中。每个物品决策（放入或者不放入背包）完之后，背包中的物品的重量会有多种情况，也就是说，会达到多种不同的状态，对应到递归树中，就是有很多不同的节点。\n我们把每一层重复的状态（节点）合并，只记录不同的状态，然后基于上一层的状态集合，来推导下一层的状态集合。我们可以通过合并每一层重复的状态，这样就保证每一层不同状态的个数都不会超过w个（w表示背包的承载重量），也就是例子中的9。于是，我们就成功避免了每层状态个数的指数级增长。\n我们用一个二维数组states[n][w+1]，来记录每层可以达到的不同状态。\n第0个（下标从0开始编号）物品的重量是2，要么装入背包，要么不装入背包，决策完之后，会对应背包的两种状态，背包中物品的总重量是0或者2。我们用states[0][0]=true和states[0][2]=true来表示这两种状态。\n第1个物品的重量也是2，基于之前的背包状态，在这个物品决策完之后，不同的状态有3个，背包中物品总重量分别是0(0+0)，2(0+2 or 2+0)，4(2+2)。我们用states[1][0]=true，states[1][2]=true，states[1][4]=true来表示这三种状态。\n以此类推，直到考察完所有的物品后，整个states状态数组就都计算好了。我把整个计算的过程画了出来，你可以看看。图中0表示false，1表示true。我们只需要在最后一层，找一个值为true的最接近w（这里是9）的值，就是背包中物品总重量的最大值。\n\n文字描述可能还不够清楚。我把上面的过程，翻译成代码，你可以结合着一块看下。\n//weight:物品重量，n:物品个数，w:背包可承载重量public int knapsack(int[] weight, int n, int w) &#123;    boolean[][] states = new boolean[n][w+1]; // 默认值false    states[0][0] = true;  // 第一行的数据要特殊处理，可以利用哨兵优化    if (weight[0] &lt;= w) &#123;        states[0][weight[0]] = true;    &#125;    for (int i = 1; i &lt; n; ++i) &#123; // 动态规划状态转移        for (int j = 0; j &lt;= w; ++j) &#123;// 不把第i个物品放入背包            if (states[i-1][j] == true) states[i][j] = states[i-1][j];        &#125;        for (int j = 0; j &lt;= w-weight[i]; ++j) &#123;//把第i个物品放入背包            if (states[i-1][j]==true) states[i][j+weight[i]] = true;        &#125;    &#125;    for (int i = w; i &gt;= 0; --i) &#123; // 输出结果        if (states[n-1][i] == true) return i;    &#125;    return 0;&#125;\n实际上，这就是一种用动态规划解决问题的思路。我们把问题分解为多个阶段，每个阶段对应一个决策。我们记录每一个阶段可达的状态集合（去掉重复的），然后通过当前阶段的状态集合，来推导下一个阶段的状态集合，动态地往前推进。这也是动态规划这个名字的由来，你可以自己体会一下，是不是还挺形象的？\n前面我们讲到，用回溯算法解决这个问题的时间复杂度O(2^n)，是指数级的。那动态规划解决方案的时间复杂度是多少呢？我来分析一下。\n这个代码的时间复杂度非常好分析，耗时最多的部分就是代码中的两层for循环，所以时间复杂度是O(n*w)。n表示物品个数，w表示背包可以承载的总重量。\n从理论上讲，指数级的时间复杂度肯定要比O(n*w)高很多，但是为了让你有更加深刻的感受，我来举一个例子给你比较一下。\n我们假设有10000个物品，重量分布在1到15000之间，背包可以承载的总重量是30000。如果我们用回溯算法解决，用具体的数值表示出时间复杂度，就是2^10000，这是一个相当大的一个数字。如果我们用动态规划解决，用具体的数值表示出时间复杂度，就是10000*30000。虽然看起来也很大，但是和2^10000比起来，要小太多了。\n尽管动态规划的执行效率比较高，但是就刚刚的代码实现来说，我们需要额外申请一个n乘以w+1的二维数组，对空间的消耗比较多。所以，有时候，我们会说，动态规划是一种空间换时间的解决思路。你可能要问了，有什么办法可以降低空间消耗吗？\n实际上，我们只需要一个大小为w+1的一维数组就可以解决这个问题。动态规划状态转移的过程，都可以基于这个一维数组来操作。具体的代码实现我贴在这里，你可以仔细看下。\npublic static int knapsack2(int[] items, int n, int w) &#123;    boolean[] states = new boolean[w+1]; // 默认值false    states[0] = true;  // 第一行的数据要特殊处理，可以利用哨兵优化    if (items[0] &lt;= w) &#123;        states[items[0]] = true;    &#125;    for (int i = 1; i &lt; n; ++i) &#123; // 动态规划        for (int j = w-items[i]; j &gt;= 0; --j) &#123;//把第i个物品放入背包            if (states[j]==true) states[j+items[i]] = true;        &#125;    &#125;    for (int i = w; i &gt;= 0; --i) &#123; // 输出结果        if (states[i] == true) return i;    &#125;    return 0;&#125;\n这里我特别强调一下代码中的第8行，j需要从大到小来处理。如果我们按照j从小到大处理的话，会出现for循环重复计算的问题。你可以自己想一想，这里我就不详细说了。\n0-1背包问题升级版我们继续升级难度。我改造了一下刚刚的背包问题。你看这个问题又该如何用动态规划解决？\n我们刚刚讲的背包问题，只涉及背包重量和物品重量。我们现在引入物品价值这一变量。对于一组不同重量、不同价值、不可分割的物品，我们选择将某些物品装入背包，在满足背包最大重量限制的前提下，背包中可装入物品的总价值最大是多少呢？\n这个问题依旧可以用回溯算法来解决。这个问题并不复杂，所以具体的实现思路，我就不用文字描述了，直接给你看代码。\nprivate int maxV = Integer.MIN_VALUE; // 结果放到maxV中private int[] items = &#123;2, 2, 4, 6, 3&#125;;  // 物品的重量private int[] value = &#123;3, 4, 8, 9, 6&#125;; // 物品的价值private int n = 5; // 物品个数private int w = 9; // 背包承受的最大重量public void f(int i, int cw, int cv) &#123; // 调用f(0, 0, 0)    if (cw == w || i == n) &#123; // cw==w表示装满了，i==n表示物品都考察完了        if (cv &gt; maxV) maxV = cv;        return;    &#125;    f(i+1, cw, cv); // 选择不装第i个物品    if (cw + weight[i] &lt;= w) &#123;        f(i+1,cw+weight[i], cv+value[i]); // 选择装第i个物品    &#125;&#125;\n针对上面的代码，我们还是照例画出递归树。在递归树中，每个节点表示一个状态。现在我们需要3个变量（i, cw, cv）来表示一个状态。其中，i表示即将要决策第i个物品是否装入背包，cw表示当前背包中物品的总重量，cv表示当前背包中物品的总价值。\n\n我们发现，在递归树中，有几个节点的i和cw是完全相同的，比如f(2,2,4)和f(2,2,3)。在背包中物品总重量一样的情况下，f(2,2,4)这种状态对应的物品总价值更大，我们可以舍弃f(2,2,3)这种状态，只需要沿着f(2,2,4)这条决策路线继续往下决策就可以。\n也就是说，对于(i, cw)相同的不同状态，那我们只需要保留cv值最大的那个，继续递归处理，其他状态不予考虑。\n思路说完了，但是代码如何实现呢？如果用回溯算法，这个问题就没法再用“备忘录”解决了。所以，我们就需要换一种思路，看看动态规划是不是更容易解决这个问题？\n我们还是把整个求解过程分为n个阶段，每个阶段会决策一个物品是否放到背包中。每个阶段决策完之后，背包中的物品的总重量以及总价值，会有多种情况，也就是会达到多种不同的状态。\n我们用一个二维数组states[n][w+1]，来记录每层可以达到的不同状态。不过这里数组存储的值不再是boolean类型的了，而是当前状态对应的最大总价值。我们把每一层中(i, cw)重复的状态（节点）合并，只记录cv值最大的那个状态，然后基于这些状态来推导下一层的状态。\n我们把这个动态规划的过程翻译成代码，就是下面这个样子：\npublic static int knapsack3(int[] weight, int[] value, int n, int w) &#123;    int[][] states = new int[n][w+1];    for (int i = 0; i &lt; n; ++i) &#123; // 初始化states        for (int j = 0; j &lt; w+1; ++j) &#123;            states[i][j] = -1;        &#125;    &#125;    states[0][0] = 0;    if (weight[0] &lt;= w) &#123;        states[0][weight[0]] = value[0];    &#125;    for (int i = 1; i &lt; n; ++i) &#123; //动态规划，状态转移        for (int j = 0; j &lt;= w; ++j) &#123; // 不选择第i个物品            if (states[i-1][j] &gt;= 0) states[i][j] = states[i-1][j];        &#125;        for (int j = 0; j &lt;= w-weight[i]; ++j) &#123; // 选择第i个物品            if (states[i-1][j] &gt;= 0) &#123;                int v = states[i-1][j] + value[i];                if (v &gt; states[i][j+weight[i]]) &#123;                    states[i][j+weight[i]] = v;                &#125;            &#125;        &#125;    &#125;    // 找出最大值    int maxvalue = -1;    for (int j = 0; j &lt;= w; ++j) &#123;        if (states[n-1][j] &gt; maxvalue) maxvalue = states[n-1][j];    &#125;    return maxvalue;&#125;\n关于这个问题的时间、空间复杂度的分析，跟上一个例子大同小异，所以我就不赘述了。我直接给出答案，时间复杂度是O(n*w)，空间复杂度也是O(n*w)。跟上一个例子类似，空间复杂度也是可以优化的，你可以自己写一下。\n解答开篇掌握了今天讲的两个问题之后，你是不是觉得，开篇的问题很简单？\n对于这个问题，你当然可以利用回溯算法，穷举所有的排列组合，看大于等于200并且最接近200的组合是哪一个？但是，这样效率太低了点，时间复杂度非常高，是指数级的。当n很大的时候，可能“双十一”已经结束了，你的代码还没有运行出结果，这显然会让你在女朋友心中的形象大大减分。\n实际上，它跟第一个例子中讲的0-1背包问题很像，只不过是把“重量”换成了“价格”而已。购物车中有n个商品。我们针对每个商品都决策是否购买。每次决策之后，对应不同的状态集合。我们还是用一个二维数组states[n][x]，来记录每次决策之后所有可达的状态。不过，这里的x值是多少呢？\n0-1背包问题中，我们找的是小于等于w的最大值，x就是背包的最大承载重量w+1。对于这个问题来说，我们要找的是大于等于200（满减条件）的值中最小的，所以就不能设置为200加1了。就这个实际的问题而言，如果要购买的物品的总价格超过200太多，比如1000，那这个羊毛“薅”得就没有太大意义了。所以，我们可以限定x值为1001。\n不过，这个问题不仅要求大于等于200的总价格中的最小的，我们还要找出这个最小总价格对应都要购买哪些商品。实际上，我们可以利用states数组，倒推出这个被选择的商品序列。我先把代码写出来，待会再照着代码给你解释。\n// items商品价格，n商品个数, w表示满减条件，比如200public static void double11advance(int[] items, int n, int w) &#123;    boolean[][] states = new boolean[n][3*w+1];//超过3倍就没有薅羊毛的价值了    states[0][0] = true;  // 第一行的数据要特殊处理    if (items[0] &lt;= 3*w) &#123;        states[0][items[0]] = true;    &#125;    for (int i = 1; i &lt; n; ++i) &#123; // 动态规划        for (int j = 0; j &lt;= 3*w; ++j) &#123;// 不购买第i个商品            if (states[i-1][j] == true) states[i][j] = states[i-1][j];        &#125;        for (int j = 0; j &lt;= 3*w-items[i]; ++j) &#123;//购买第i个商品            if (states[i-1][j]==true) states[i][j+items[i]] = true;        &#125;    &#125;    int j;    for (j = w; j &lt; 3*w+1; ++j) &#123;         if (states[n-1][j] == true) break; // 输出结果大于等于w的最小值    &#125;    if (j == 3*w+1) return; // 没有可行解    for (int i = n-1; i &gt;= 1; --i) &#123; // i表示二维数组中的行，j表示列        if(j-items[i] &gt;= 0 &amp;&amp; states[i-1][j-items[i]] == true) &#123;            System.out.print(items[i] + \" \"); // 购买这个商品            j = j - items[i];        &#125; // else 没有购买这个商品，j不变。    &#125;    if (j != 0) System.out.print(items[0]);&#125;\n代码的前半部分跟0-1背包问题没有什么不同，我们着重看后半部分，看它是如何打印出选择购买哪些商品的。\n状态(i, j)只有可能从(i-1, j)或者(i-1, j-value[i])两个状态推导过来。所以，我们就检查这两个状态是否是可达的，也就是states[i-1][j]或者states[i-1][j-item[i]]是否是true。\n如果states[i-1][j]可达，就说明我们没有选择购买第i个商品，如果states[i-1][j-value[i]]可达，那就说明我们选择了购买第i个商品。我们从中选择一个可达的状态（如果两个都可达，就随意选择一个），然后，继续迭代地考察其他商品是否有选择购买。\n内容小结动态规划的第一节到此就讲完了。内容比较多，你可能需要多一点时间来消化。为了帮助你有的放矢地学习，我来强调一下，今天你应该掌握的重点内容。\n今天的内容不涉及动态规划的理论，我通过两个例子，给你展示了动态规划是如何解决问题的，并且一点一点详细给你讲解了动态规划解决问题的思路。这两个例子都是非常经典的动态规划问题，只要你真正搞懂这两个问题，基本上动态规划已经入门一半了。所以，你要多花点时间，真正弄懂这两个问题。\n从例子中，你应该能发现，大部分动态规划能解决的问题，都可以通过回溯算法来解决，只不过回溯算法解决起来效率比较低，时间复杂度是指数级的。动态规划算法，在执行效率方面，要高很多。尽管执行效率提高了，但是动态规划的空间复杂度也提高了，所以，很多时候，我们会说，动态规划是一种空间换时间的算法思想。\n我前面也说了，今天的内容并不涉及理论的知识。这两个例子的分析过程，我并没有涉及任何高深的理论方面的东西。而且，我个人觉得，贪心、分治、回溯、动态规划，这四个算法思想有关的理论知识，大部分都是“后验性”的，也就是说，在解决问题的过程中，我们往往是先想到如何用某个算法思想解决问题，然后才用算法理论知识，去验证这个算法思想解决问题的正确性。所以，你大可不必过于急于寻求动态规划的理论知识。\n课后思考“杨辉三角”不知道你听说过吗？我们现在对它进行一些改造。每个位置的数字可以随意填写，经过某个数字只能到达下面一层相邻的两个数字。\n假设你站在第一层，往下移动，我们把移动到最底层所经过的所有数字之和，定义为路径的长度。请你编程求出从最高层移动到最底层的最短路径长度。\n\n欢迎留言和我分享，也欢迎点击“请朋友读”，把今天的内容分享给你的好友，和他一起讨论、学习。\n","categories":["数据结构与算法","基础篇"]},{"title":"42 | 动态规划实战：如何实现搜索引擎中的拼写纠错功能？","url":"/2020/08/07/42%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%9A%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E5%AE%9E%E6%88%98/","content":"在Trie树那节我们讲过，利用Trie树，可以实现搜索引擎的关键词提示功能，这样可以节省用户输入搜索关键词的时间。实际上，搜索引擎在用户体验方面的优化还有很多，比如你可能经常会用的拼写纠错功能。\n当你在搜索框中，一不小心输错单词时，搜索引擎会非常智能地检测出你的拼写错误，并且用对应的正确单词来进行搜索。作为一名软件开发工程师，你是否想过，这个功能是怎么实现的呢？\n\n如何量化两个字符串的相似度？计算机只认识数字，所以要解答开篇的问题，我们就要先来看，如何量化两个字符串之间的相似程度呢？有一个非常著名的量化方法，那就是编辑距离（Edit Distance）。\n顾名思义，编辑距离指的就是，将一个字符串转化成另一个字符串，需要的最少编辑操作次数（比如增加一个字符、删除一个字符、替换一个字符）。编辑距离越大，说明两个字符串的相似程度越小；相反，编辑距离就越小，说明两个字符串的相似程度越大。对于两个完全相同的字符串来说，编辑距离就是0。\n根据所包含的编辑操作种类的不同，编辑距离有多种不同的计算方式，比较著名的有莱文斯坦距离（Levenshtein distance）和最长公共子串长度（Longest common substring length）。其中，莱文斯坦距离允许增加、删除、替换字符这三个编辑操作，最长公共子串长度只允许增加、删除字符这两个编辑操作。\n而且，莱文斯坦距离和最长公共子串长度，从两个截然相反的角度，分析字符串的相似程度。莱文斯坦距离的大小，表示两个字符串差异的大小；而最长公共子串的大小，表示两个字符串相似程度的大小。\n关于这两个计算方法，我举个例子给你说明一下。这里面，两个字符串mitcmu和mtacnu的莱文斯坦距离是3，最长公共子串长度是4。\n\n了解了编辑距离的概念之后，我们来看，如何快速计算两个字符串之间的编辑距离？\n如何编程计算莱文斯坦距离？之前我反复强调过，思考过程比结论更重要，所以，我现在就给你展示一下，解决这个问题，我的完整的思考过程。\n这个问题是求把一个字符串变成另一个字符串，需要的最少编辑次数。整个求解过程，涉及多个决策阶段，我们需要依次考察一个字符串中的每个字符，跟另一个字符串中的字符是否匹配，匹配的话如何处理，不匹配的话又如何处理。所以，这个问题符合多阶段决策最优解模型。\n我们前面讲了，贪心、回溯、动态规划可以解决的问题，都可以抽象成这样一个模型。要解决这个问题，我们可以先看一看，用最简单的回溯算法，该如何来解决。\n回溯是一个递归处理的过程。如果a[i]与b[j]匹配，我们递归考察a[i+1]和b[j+1]。如果a[i]与b[j]不匹配，那我们有多种处理方式可选：\n\n可以删除a[i]，然后递归考察a[i+1]和b[j]；\n\n可以删除b[j]，然后递归考察a[i]和b[j+1]；\n\n可以在a[i]前面添加一个跟b[j]相同的字符，然后递归考察a[i]和b[j+1];\n\n可以在b[j]前面添加一个跟a[i]相同的字符，然后递归考察a[i+1]和b[j]；\n\n可以将a[i]替换成b[j]，或者将b[j]替换成a[i]，然后递归考察a[i+1]和b[j+1]。\n\n\n我们将上面的回溯算法的处理思路，翻译成代码，就是下面这个样子：\nprivate char[] a = \"mitcmu\".toCharArray();private char[] b = \"mtacnu\".toCharArray();private int n = 6;private int m = 6;private int minDist = Integer.MAX_VALUE; // 存储结果// 调用方式 lwstBT(0, 0, 0);public lwstBT(int i, int j, int edist) &#123;    if (i == n || j == m) &#123;        if (i &lt; n) edist += (n-i);        if (j &lt; m) edist += (m - j);        if (edist &lt; minDist) minDist = edist;        return;    &#125;    if (a[i] == b[j]) &#123; // 两个字符匹配        lwstBT(i+1, j+1, edist);    &#125; else &#123; // 两个字符不匹配        lwstBT(i + 1, j, edist + 1); // 删除a[i]或者b[j]前添加一个字符        lwstBT(i, j + 1, edist + 1); // 删除b[j]或者a[i]前添加一个字符        lwstBT(i + 1, j + 1, edist + 1); // 将a[i]和b[j]替换为相同字符    &#125;&#125;\n根据回溯算法的代码实现，我们可以画出递归树，看是否存在重复子问题。如果存在重复子问题，那我们就可以考虑能否用动态规划来解决；如果不存在重复子问题，那回溯就是最好的解决方法。\n\n在递归树中，每个节点代表一个状态，状态包含三个变量(i, j, edist)，其中，edist表示处理到a[i]和b[j]时，已经执行的编辑操作的次数。\n在递归树中，(i, j)两个变量重复的节点很多，比如(3, 2)和(2, 3)。对于(i, j)相同的节点，我们只需要保留edist最小的，继续递归处理就可以了，剩下的节点都可以舍弃。所以，状态就从(i, j, edist)变成了(i, j, min_edist)，其中min_edist表示处理到a[i]和b[j]，已经执行的最少编辑次数。\n看到这里，你有没有觉得，这个问题跟上两节讲的动态规划例子非常相似？不过，这个问题的状态转移方式，要比之前两节课中讲到的例子都要复杂很多。上一节我们讲的矩阵最短路径问题中，到达状态(i, j)只能通过(i-1, j)或(i, j-1)两个状态转移过来，而今天这个问题，状态(i, j)可能从(i-1, j)，(i, j-1)，(i-1, j-1)三个状态中的任意一个转移过来。\n\n基于刚刚的分析，我们可以尝试着将把状态转移的过程，用公式写出来。这就是我们前面讲的状态转移方程。\n如果：a[i]!&#x3D;b[j]，那么：min_edist(i, j)就等于：min(min_edist(i-1,j)+1, min_edist(i,j-1)+1, min_edist(i-1,j-1)+1)如果：a[i]&#x3D;&#x3D;b[j]，那么：min_edist(i, j)就等于：min(min_edist(i-1,j)+1, min_edist(i,j-1)+1，min_edist(i-1,j-1))其中，min表示求三数中的最小值。\n了解了状态与状态之间的递推关系，我们画出一个二维的状态表，按行依次来填充状态表中的每个值。\n\n我们现在既有状态转移方程，又理清了完整的填表过程，代码实现就非常简单了。我将代码贴在下面，你可以对比着文字解释，一起看下。\npublic int lwstDP(char[] a, int n, char[] b, int m) &#123;    int[][] minDist = new int[n][m];    for (int j = 0; j &lt; m; ++j) &#123; // 初始化第0行:a[0..0]与b[0..j]的编辑距离        if (a[0] == b[j]) minDist[0][j] = j;        else if (j != 0) minDist[0][j] = minDist[0][j-1]+1;        else minDist[0][j] = 1;    &#125;    for (int i = 0; i &lt; n; ++i) &#123; // 初始化第0列:a[0..i]与b[0..0]的编辑距离        if (a[i] == b[0]) minDist[i][0] = i;        else if (i != 0) minDist[i][0] = minDist[i-1][0]+1;        else minDist[i][0] = 1;    &#125;    for (int i = 1; i &lt; n; ++i) &#123; // 按行填表        for (int j = 1; j &lt; m; ++j) &#123;            if (a[i] == b[j]) minDist[i][j] = min(                minDist[i-1][j]+1, minDist[i][j-1]+1, minDist[i-1][j-1]);            else minDist[i][j] = min(                minDist[i-1][j]+1, minDist[i][j-1]+1, minDist[i-1][j-1]+1);        &#125;    &#125;    return minDist[n-1][m-1];&#125;private int min(int x, int y, int z) &#123;    int minv = Integer.MAX_VALUE;    if (x &lt; minv) minv = x;    if (y &lt; minv) minv = y;    if (z &lt; minv) minv = z;    return minv;&#125;\n你可能会说，我虽然能看懂你讲的思路，但是遇到新的问题的时候，我还是会感觉到无从下手。这种感觉是非常正常的。关于复杂算法问题的解决思路，我还有一些经验、小技巧，可以分享给你。\n当我们拿到一个问题的时候，我们可以先不思考，计算机会如何实现这个问题，而是单纯考虑“人脑”会如何去解决这个问题。人脑比较倾向于思考具象化的、摸得着看得见的东西，不适合思考过于抽象的问题。所以，我们需要把抽象问题具象化。那如何具象化呢？我们可以实例化几个测试数据，通过人脑去分析具体实例的解，然后总结规律，再尝试套用学过的算法，看是否能够解决。\n除此之外，我还有一个非常有效、但也算不上技巧的东西，我也反复强调过，那就是多练。实际上，等你做多了题目之后，自然就会有感觉，看到问题，立马就能想到能否用动态规划解决，然后直接就可以寻找最优子结构，写出动态规划方程，然后将状态转移方程翻译成代码。\n如何编程计算最长公共子串长度？前面我们讲到，最长公共子串作为编辑距离中的一种，只允许增加、删除字符两种编辑操作。从名字上，你可能觉得它看起来跟编辑距离没什么关系。实际上，从本质上来说，它表征的也是两个字符串之间的相似程度。\n这个问题的解决思路，跟莱文斯坦距离的解决思路非常相似，也可以用动态规划解决。我刚刚已经详细讲解了莱文斯坦距离的动态规划解决思路，所以，针对这个问题，我直接定义状态，然后写状态转移方程。\n每个状态还是包括三个变量(i, j, max_lcs)，max_lcs表示a[0…i]和b[0…j]的最长公共子串长度。那(i, j)这个状态都是由哪些状态转移过来的呢？\n我们先来看回溯的处理思路。我们从a[0]和b[0]开始，依次考察两个字符串中的字符是否匹配。\n\n如果a[i]与b[j]互相匹配，我们将最大公共子串长度加一，并且继续考察a[i+1]和b[j+1]。\n\n如果a[i]与b[j]不匹配，最长公共子串长度不变，这个时候，有两个不同的决策路线：\n\n删除a[i]，或者在b[j]前面加上一个字符a[i]，然后继续考察a[i+1]和b[j]；\n\n删除b[j]，或者在a[i]前面加上一个字符b[j]，然后继续考察a[i]和b[j+1]。\n\n\n反过来也就是说，如果我们要求a[0…i]和b[0…j]的最长公共长度max_lcs(i, j)，我们只有可能通过下面三个状态转移过来：\n\n(i-1, j-1, max_lcs)，其中max_lcs表示a[0…i-1]和b[0…j-1]的最长公共子串长度；\n\n(i-1, j, max_lcs)，其中max_lcs表示a[0…i-1]和b[0…j]的最长公共子串长度；\n\n(i, j-1, max_lcs)，其中max_lcs表示a[0…i]和b[0…j-1]的最长公共子串长度。\n\n\n如果我们把这个转移过程，用状态转移方程写出来，就是下面这个样子：\n如果：a[i]&#x3D;&#x3D;b[j]，那么：max_lcs(i, j)就等于：max(max_lcs(i-1,j-1)+1, max_lcs(i-1, j), max_lcs(i, j-1))；如果：a[i]!&#x3D;b[j]，那么：max_lcs(i, j)就等于：max(max_lcs(i-1,j-1), max_lcs(i-1, j), max_lcs(i, j-1))；其中max表示求三数中的最大值。\n有了状态转移方程，代码实现就简单多了。我把代码贴到了下面，你可以对比着文字一块儿看。\npublic int lcs(char[] a, int n, char[] b, int m) &#123;    int[][] maxlcs = new int[n][m];    for (int j = 0; j &lt; m; ++j) &#123;//初始化第0行：a[0..0]与b[0..j]的maxlcs        if (a[0] == b[j]) maxlcs[0][j] = 1;        else if (j != 0) maxlcs[0][j] = maxlcs[0][j-1];        else maxlcs[0][j] = 0;    &#125;    for (int i = 0; i &lt; n; ++i) &#123;//初始化第0列：a[0..i]与b[0..0]的maxlcs        if (a[i] == b[0]) maxlcs[i][0] = 1;        else if (i != 0) maxlcs[i][0] = maxlcs[i-1][0];        else maxlcs[i][0] = 0;    &#125;    for (int i = 1; i &lt; n; ++i) &#123; // 填表        for (int j = 1; j &lt; m; ++j) &#123;            if (a[i] == b[j]) maxlcs[i][j] = max(                maxlcs[i-1][j], maxlcs[i][j-1], maxlcs[i-1][j-1]+1);            else maxlcs[i][j] = max(                maxlcs[i-1][j], maxlcs[i][j-1], maxlcs[i-1][j-1]);        &#125;    &#125;    return maxlcs[n-1][m-1];&#125;private int max(int x, int y, int z) &#123;    int maxv = Integer.MIN_VALUE;    if (x &gt; maxv) maxv = x;    if (y &gt; maxv) maxv = y;    if (z &gt; maxv) maxv = z;    return maxv;&#125;\n解答开篇今天的内容到此就讲完了，我们来看下开篇的问题。\n当用户在搜索框内，输入一个拼写错误的单词时，我们就拿这个单词跟词库中的单词一一进行比较，计算编辑距离，将编辑距离最小的单词，作为纠正之后的单词，提示给用户。\n这就是拼写纠错最基本的原理。不过，真正用于商用的搜索引擎，拼写纠错功能显然不会就这么简单。一方面，单纯利用编辑距离来纠错，效果并不一定好；另一方面，词库中的数据量可能很大，搜索引擎每天要支持海量的搜索，所以对纠错的性能要求很高。\n针对纠错效果不好的问题，我们有很多种优化思路，我这里介绍几种。\n\n我们并不仅仅取出编辑距离最小的那个单词，而是取出编辑距离最小的TOP 10，然后根据其他参数，决策选择哪个单词作为拼写纠错单词。比如使用搜索热门程度来决定哪个单词作为拼写纠错单词。\n\n我们还可以用多种编辑距离计算方法，比如今天讲到的两种，然后分别编辑距离最小的TOP 10，然后求交集，用交集的结果，再继续优化处理。\n\n我们还可以通过统计用户的搜索日志，得到最常被拼错的单词列表，以及对应的拼写正确的单词。搜索引擎在拼写纠错的时候，首先在这个最常被拼错单词列表中查找。如果一旦找到，直接返回对应的正确的单词。这样纠错的效果非常好。\n\n我们还有更加高级一点的做法，引入个性化因素。针对每个用户，维护这个用户特有的搜索喜好，也就是常用的搜索关键词。当用户输入错误的单词的时候，我们首先在这个用户常用的搜索关键词中，计算编辑距离，查找编辑距离最小的单词。\n\n\n针对纠错性能方面，我们也有相应的优化方式。我讲两种分治的优化思路。\n\n如果纠错功能的TPS不高，我们可以部署多台机器，每台机器运行一个独立的纠错功能。当有一个纠错请求的时候，我们通过负载均衡，分配到其中一台机器，来计算编辑距离，得到纠错单词。\n\n如果纠错系统的响应时间太长，也就是，每个纠错请求处理时间过长，我们可以将纠错的词库，分割到很多台机器。当有一个纠错请求的时候，我们就将这个拼写错误的单词，同时发送到这多台机器，让多台机器并行处理，分别得到编辑距离最小的单词，然后再比对合并，最终决定出一个最优的纠错单词。\n\n\n真正的搜索引擎的拼写纠错优化，肯定不止我讲的这么简单，但是万变不离其宗。掌握了核心原理，就是掌握了解决问题的方法，剩下就靠你自己的灵活运用和实战操练了。\n内容小结动态规划的三节内容到此就全部讲完了，不知道你掌握得如何呢？\n动态规划的理论尽管并不复杂，总结起来就是“一个模型三个特征”。但是，要想灵活应用并不简单。要想能真正理解、掌握动态规划，你只有多练习。\n这三节中，加上课后思考题，总共有8个动态规划问题。这8个问题都非常经典，是我精心筛选出来的。很多动态规划问题其实都可以抽象成这几个问题模型，所以，你一定要多看几遍，多思考一下，争取真正搞懂它们。\n只要弄懂了这几个问题，一般的动态规划问题，你应该都可以应付。对于动态规划这个知识点，你就算是入门了，再学习更加复杂的就会简单很多。\n课后思考我们有一个数字序列包含n个不同的数字，如何求出这个序列中的最长递增子序列长度？比如2, 9, 3, 6, 5, 1, 7这样一组数字序列，它的最长递增子序列就是2, 3, 5, 7，所以最长递增子序列的长度是4。\n欢迎留言和我分享，也欢迎点击“请朋友读”，把今天的内容分享给你的好友，和他一起讨论、学习。\n","categories":["数据结构与算法","基础篇"]},{"title":"41 | 动态规划理论：一篇文章带你彻底搞懂最优子结构、无后效性和重复子问题","url":"/2020/08/07/41%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%9A%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E7%90%86%E8%AE%BA/","content":"上一节，我通过两个非常经典的问题，向你展示了用动态规划解决问题的过程。现在你对动态规划应该有了一个初步的认识。\n今天，我主要讲动态规划的一些理论知识。学完这节内容，可以帮你解决这样几个问题：什么样的问题可以用动态规划解决？解决动态规划问题的一般思考过程是什么样的？贪心、分治、回溯、动态规划这四种算法思想又有什么区别和联系？\n理论的东西都比较抽象，不过你不用担心，我会结合具体的例子来讲解，争取让你这次就能真正理解这些知识点，也为后面的应用和实战做好准备。\n“一个模型三个特征”理论讲解什么样的问题适合用动态规划来解决呢？换句话说，动态规划能解决的问题有什么规律可循呢？实际上，动态规划作为一个非常成熟的算法思想，很多人对此已经做了非常全面的总结。我把这部分理论总结为“一个模型三个特征”。\n首先，我们来看，什么是“一个模型”？它指的是动态规划适合解决的问题的模型。我把这个模型定义为“多阶段决策最优解模型”。下面我具体来给你讲讲。\n我们一般是用动态规划来解决最优问题。而解决问题的过程，需要经历多个决策阶段。每个决策阶段都对应着一组状态。然后我们寻找一组决策序列，经过这组决策序列，能够产生最终期望求解的最优值。\n现在，我们再来看，什么是“三个特征”？它们分别是最优子结构、无后效性和重复子问题。这三个概念比较抽象，我来逐一详细解释一下。\n1.最优子结构最优子结构指的是，问题的最优解包含子问题的最优解。反过来说就是，我们可以通过子问题的最优解，推导出问题的最优解。如果我们把最优子结构，对应到我们前面定义的动态规划问题模型上，那我们也可以理解为，后面阶段的状态可以通过前面阶段的状态推导出来。\n2.无后效性无后效性有两层含义，第一层含义是，在推导后面阶段的状态的时候，我们只关心前面阶段的状态值，不关心这个状态是怎么一步一步推导出来的。第二层含义是，某阶段状态一旦确定，就不受之后阶段的决策影响。无后效性是一个非常“宽松”的要求。只要满足前面提到的动态规划问题模型，其实基本上都会满足无后效性。\n3.重复子问题这个概念比较好理解。前面一节，我已经多次提过。如果用一句话概括一下，那就是，不同的决策序列，到达某个相同的阶段时，可能会产生重复的状态。\n“一个模型三个特征”实例剖析“一个模型三个特征”这部分是理论知识，比较抽象，你看了之后可能还是有点懵，有种似懂非懂的感觉，没关系，这个很正常。接下来，我结合一个具体的动态规划问题，来给你详细解释。\n假设我们有一个n乘以n的矩阵w[n][n]。矩阵存储的都是正整数。棋子起始位置在左上角，终止位置在右下角。我们将棋子从左上角移动到右下角。每次只能向右或者向下移动一位。从左上角到右下角，会有很多不同的路径可以走。我们把每条路径经过的数字加起来看作路径的长度。那从左上角移动到右下角的最短路径长度是多少呢？\n\n我们先看看，这个问题是否符合“一个模型”？\n从(0, 0)走到(n-1, n-1)，总共要走2(n-1)步，也就对应着2(n-1)个阶段。每个阶段都有向右走或者向下走两种决策，并且每个阶段都会对应一个状态集合。\n我们把状态定义为min_dist(i, j)，其中i表示行，j表示列。min_dist表达式的值表示从(0, 0)到达(i, j)的最短路径长度。所以，这个问题是一个多阶段决策最优解问题，符合动态规划的模型。\n\n我们再来看，这个问题是否符合“三个特征”？\n我们可以用回溯算法来解决这个问题。如果你自己写一下代码，画一下递归树，就会发现，递归树中有重复的节点。重复的节点表示，从左上角到节点对应的位置，有多种路线，这也能说明这个问题中存在重复子问题。\n\n如果我们走到(i, j)这个位置，我们只能通过(i-1, j)，(i, j-1)这两个位置移动过来，也就是说，我们想要计算(i, j)位置对应的状态，只需要关心(i-1, j)，(i, j-1)两个位置对应的状态，并不关心棋子是通过什么样的路线到达这两个位置的。而且，我们仅仅允许往下和往右移动，不允许后退，所以，前面阶段的状态确定之后，不会被后面阶段的决策所改变，所以，这个问题符合“无后效性”这一特征。\n刚刚定义状态的时候，我们把从起始位置(0, 0)到(i, j)的最小路径，记作min_dist(i, j)。因为我们只能往右或往下移动，所以，我们只有可能从(i, j-1)或者(i-1, j)两个位置到达(i, j)。也就是说，到达(i, j)的最短路径要么经过(i, j-1)，要么经过(i-1, j)，而且到达(i, j)的最短路径肯定包含到达这两个位置的最短路径之一。换句话说就是，min_dist(i, j)可以通过min_dist(i, j-1)和min_dist(i-1, j)两个状态推导出来。这就说明，这个问题符合“最优子结构”。\nmin_dist(i, j) &#x3D; w[i][j] + min(min_dist(i, j-1), min_dist(i-1, j))\n两种动态规划解题思路总结刚刚我讲了，如何鉴别一个问题是否可以用动态规划来解决。现在，我再总结一下，动态规划解题的一般思路，让你面对动态规划问题的时候，能够有章可循，不至于束手无策。\n我个人觉得，解决动态规划问题，一般有两种思路。我把它们分别叫作，状态转移表法和状态转移方程法。\n1.状态转移表法一般能用动态规划解决的问题，都可以使用回溯算法的暴力搜索解决。所以，当我们拿到问题的时候，我们可以先用简单的回溯算法解决，然后定义状态，每个状态表示一个节点，然后对应画出递归树。从递归树中，我们很容易可以看出来，是否存在重复子问题，以及重复子问题是如何产生的。以此来寻找规律，看是否能用动态规划解决。\n找到重复子问题之后，接下来，我们有两种处理思路，第一种是直接用回溯加“备忘录”的方法，来避免重复子问题。从执行效率上来讲，这跟动态规划的解决思路没有差别。第二种是使用动态规划的解决方法，状态转移表法。第一种思路，我就不讲了，你可以看看上一节的两个例子。我们重点来看状态转移表法是如何工作的。\n我们先画出一个状态表。状态表一般都是二维的，所以你可以把它想象成二维数组。其中，每个状态包含三个变量，行、列、数组值。我们根据决策的先后过程，从前往后，根据递推关系，分阶段填充状态表中的每个状态。最后，我们将这个递推填表的过程，翻译成代码，就是动态规划代码了。\n尽管大部分状态表都是二维的，但是如果问题的状态比较复杂，需要很多变量来表示，那对应的状态表可能就是高维的，比如三维、四维。那这个时候，我们就不适合用状态转移表法来解决了。一方面是因为高维状态转移表不好画图表示，另一方面是因为人脑确实很不擅长思考高维的东西。\n现在，我们来看一下，如何套用这个状态转移表法，来解决之前那个矩阵最短路径的问题？\n从起点到终点，我们有很多种不同的走法。我们可以穷举所有走法，然后对比找出一个最短走法。不过如何才能无重复又不遗漏地穷举出所有走法呢？我们可以用回溯算法这个比较有规律的穷举算法。\n回溯算法的代码实现如下所示。代码很短，而且我前面也分析过很多回溯算法的例题，这里我就不多做解释了，你自己来看看。\nprivate int minDist = Integer.MAX_VALUE; // 全局变量或者成员变量// 调用方式：minDistBacktracing(0, 0, 0, w, n);public void minDistBT(int i, int j, int dist, int[][] w, int n) &#123;    // 到达了n-1, n-1这个位置了，这里看着有点奇怪哈，你自己举个例子看下    if (i == n &amp;&amp; j == n) &#123;        if (dist &lt; minDist) minDist = dist;        return;    &#125;    if (i &lt; n) &#123; // 往下走，更新i=i+1, j=j        minDistBT(i + 1, j, dist+w[i][j], w, n);    &#125;    if (j &lt; n) &#123; // 往右走，更新i=i, j=j+1        minDistBT(i, j+1, dist+w[i][j], w, n);    &#125;&#125;\n有了回溯代码之后，接下来，我们要画出递归树，以此来寻找重复子问题。在递归树中，一个状态（也就是一个节点）包含三个变量(i, j, dist)，其中i，j分别表示行和列，dist表示从起点到达(i, j)的路径长度。从图中，我们看出，尽管(i, j, dist)不存在重复的，但是(i, j)重复的有很多。对于(i, j)重复的节点，我们只需要选择dist最小的节点，继续递归求解，其他节点就可以舍弃了。\n\n既然存在重复子问题，我们就可以尝试看下，是否可以用动态规划来解决呢？\n我们画出一个二维状态表，表中的行、列表示棋子所在的位置，表中的数值表示从起点到这个位置的最短路径。我们按照决策过程，通过不断状态递推演进，将状态表填好。为了方便代码实现，我们按行来进行依次填充。\n)\n弄懂了填表的过程，代码实现就简单多了。我们将上面的过程，翻译成代码，就是下面这个样子。结合着代码、图和文字描述，应该更容易理解我讲的内容。\npublic int minDistDP(int[][] matrix, int n) &#123;    int[][] states = new int[n][n];    int sum = 0;    for (int j = 0; j &lt; n; ++j) &#123; // 初始化states的第一行数据        sum += matrix[0][j];        states[0][j] = sum;    &#125;    sum = 0;    for (int i = 0; i &lt; n; ++i) &#123; // 初始化states的第一列数据        sum += matrix[i][0];        states[i][0] = sum;    &#125;    for (int i = 1; i &lt; n; ++i) &#123;        for (int j = 1; j &lt; n; ++j) &#123;            states[i][j] =                 matrix[i][j] + Math.min(states[i][j-1], states[i-1][j]);        &#125;    &#125;    return states[n-1][n-1];&#125;\n2.状态转移方程法状态转移方程法有点类似递归的解题思路。我们需要分析，某个问题如何通过子问题来递归求解，也就是所谓的最优子结构。根据最优子结构，写出递归公式，也就是所谓的状态转移方程。有了状态转移方程，代码实现就非常简单了。一般情况下，我们有两种代码实现方法，一种是递归加“备忘录”，另一种是迭代递推。\n我们还是拿刚才的例子来举例。最优子结构前面已经分析过了，你可以回过头去再看下。为了方便你查看，我把状态转移方程放到这里。\nmin_dist(i, j) &#x3D; w[i][j] + min(min_dist(i, j-1), min_dist(i-1, j))\n这里我强调一下，状态转移方程是解决动态规划的关键。如果我们能写出状态转移方程，那动态规划问题基本上就解决一大半了，而翻译成代码非常简单。但是很多动态规划问题的状态本身就不好定义，状态转移方程也就更不好想到。\n下面我用递归加“备忘录”的方式，将状态转移方程翻译成来代码，你可以看看。对于另一种实现方式，跟状态转移表法的代码实现是一样的，只是思路不同。\nprivate int[][] matrix = &#123;&#123;1, 3, 5, 9&#125;, &#123;2, 1, 3, 4&#125;, &#123;5, 2, 6, 7&#125;, &#123;6, 8, 4, 3&#125;&#125;;private int n = 4;private int[][] mem = new int[4][4];public int minDist(int i, int j) &#123; // 调用minDist(n-1, n-1);    if (i == 0 &amp;&amp; j == 0) return matrix[0][0];    if (mem[i][j] &gt; 0) return mem[i][j];    int minLeft = Integer.MAX_VALUE;    if (j-1 &gt;= 0) &#123;        minLeft = minDist(i, j-1);    &#125;    int minUp = Integer.MAX_VALUE;    if (i-1 &gt;= 0) &#123;        minUp = minDist(i-1, j);    &#125;    int currMinDist = matrix[i][j] + Math.min(minLeft, minUp);    mem[i][j] = currMinDist;    return currMinDist;&#125;\n两种动态规划解题思路到这里就讲完了。我要强调一点，不是每个问题都同时适合这两种解题思路。有的问题可能用第一种思路更清晰，而有的问题可能用第二种思路更清晰，所以，你要结合具体的题目来看，到底选择用哪种解题思路。\n四种算法思想比较分析到今天为止，我们已经学习了四种算法思想，贪心、分治、回溯和动态规划。今天的内容主要讲些理论知识，我正好一块儿也分析一下这四种算法，看看它们之间有什么区别和联系。\n如果我们将这四种算法思想分一下类，那贪心、回溯、动态规划可以归为一类，而分治单独可以作为一类，因为它跟其他三个都不大一样。为什么这么说呢？前三个算法解决问题的模型，都可以抽象成我们今天讲的那个多阶段决策最优解模型，而分治算法解决的问题尽管大部分也是最优解问题，但是，大部分都不能抽象成多阶段决策模型。\n回溯算法是个“万金油”。基本上能用的动态规划、贪心解决的问题，我们都可以用回溯算法解决。回溯算法相当于穷举搜索。穷举所有的情况，然后对比得到最优解。不过，回溯算法的时间复杂度非常高，是指数级别的，只能用来解决小规模数据的问题。对于大规模数据的问题，用回溯算法解决的执行效率就很低了。\n尽管动态规划比回溯算法高效，但是，并不是所有问题，都可以用动态规划来解决。能用动态规划解决的问题，需要满足三个特征，最优子结构、无后效性和重复子问题。在重复子问题这一点上，动态规划和分治算法的区分非常明显。分治算法要求分割成的子问题，不能有重复子问题，而动态规划正好相反，动态规划之所以高效，就是因为回溯算法实现中存在大量的重复子问题。\n贪心算法实际上是动态规划算法的一种特殊情况。它解决问题起来更加高效，代码实现也更加简洁。不过，它可以解决的问题也更加有限。它能解决的问题需要满足三个条件，最优子结构、无后效性和贪心选择性（这里我们不怎么强调重复子问题）。\n其中，最优子结构、无后效性跟动态规划中的无异。“贪心选择性”的意思是，通过局部最优的选择，能产生全局的最优选择。每一个阶段，我们都选择当前看起来最优的决策，所有阶段的决策完成之后，最终由这些局部最优解构成全局最优解。\n内容小结今天的内容到此就讲完了，我带你来复习一下。\n我首先讲了什么样的问题适合用动态规划解决。这些问题可以总结概括为“一个模型三个特征”。其中，“一个模型”指的是，问题可以抽象成分阶段决策最优解模型。“三个特征”指的是最优子结构、无后效性和重复子问题。\n然后，我讲了两种动态规划的解题思路。它们分别是状态转移表法和状态转移方程法。其中，状态转移表法解题思路大致可以概括为，回溯算法实现-定义状态-画递归树-找重复子问题-画状态转移表-根据递推关系填表-将填表过程翻译成代码。状态转移方程法的大致思路可以概括为，找最优子结构-写状态转移方程-将状态转移方程翻译成代码。\n最后，我们对比了之前讲过的四种算法思想。贪心、回溯、动态规划可以解决的问题模型类似，都可以抽象成多阶段决策最优解模型。尽管分治算法也能解决最优问题，但是大部分问题的背景都不适合抽象成多阶段决策模型。\n今天的内容比较偏理论，可能会不好理解。很多理论知识的学习，单纯的填鸭式讲给你听，实际上效果并不好。要想真的把这些理论知识理解透，化为己用，还是需要你自己多思考，多练习。等你做了足够多的题目之后，自然就能自己悟出一些东西，这样再回过头来看理论，就会非常容易看懂。\n所以，在今天的内容中，如果有哪些地方你还不能理解，那也没关系，先放一放。下一节，我会运用今天讲到的理论，再解决几个动态规划的问题。等你学完下一节，可以再回过头来看下今天的理论知识，可能就会有一种顿悟的感觉。\n课后思考硬币找零问题，我们在贪心算法那一节中讲过一次。我们今天来看一个新的硬币找零问题。假设我们有几种不同币值的硬币v1，v2，……，vn（单位是元）。如果我们要支付w元，求最少需要多少个硬币。比如，我们有3种不同的硬币，1元、3元、5元，我们要支付9元，最少需要3个硬币（3个3元的硬币）。\n欢迎留言和我分享，也欢迎点击“请朋友读”，把今天的内容分享给你的好友，和他一起讨论、学习。\n","categories":["数据结构与算法","基础篇"]},{"title":"44 | 最短路径：地图软件是如何计算出最优出行路径的？","url":"/2020/08/07/44%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%9A%E6%9C%80%E7%9F%AD%E8%B7%AF%E5%BE%84/","content":"基础篇的时候，我们学习了图的两种搜索算法，深度优先搜索和广度优先搜索。这两种算法主要是针对无权图的搜索算法。针对有权图，也就是图中的每条边都有一个权重，我们该如何计算两点之间的最短路径（经过的边的权重和最小）呢？今天，我就从地图软件的路线规划问题讲起，带你看看常用的最短路径算法（Shortest Path Algorithm）。\n像Google地图、百度地图、高德地图这样的地图软件，我想你应该经常使用吧？如果想从家开车到公司，你只需要输入起始、结束地址，地图就会给你规划一条最优出行路线。这里的最优，有很多种定义，比如最短路线、最少用时路线、最少红绿灯路线等等。作为一名软件开发工程师，你是否思考过，地图软件的最优路线是如何计算出来的吗？底层依赖了什么算法呢？\n算法解析我们刚提到的最优问题包含三个：最短路线、最少用时和最少红绿灯。我们先解决最简单的，最短路线。\n解决软件开发中的实际问题，最重要的一点就是建模，也就是将复杂的场景抽象成具体的数据结构。针对这个问题，我们该如何抽象成数据结构呢？\n我们之前也提到过，图这种数据结构的表达能力很强，显然，把地图抽象成图最合适不过了。我们把每个岔路口看作一个顶点，岔路口与岔路口之间的路看作一条边，路的长度就是边的权重。如果路是单行道，我们就在两个顶点之间画一条有向边；如果路是双行道，我们就在两个顶点之间画两条方向不同的边。这样，整个地图就被抽象成一个有向有权图。\n具体的代码实现，我放在下面了。于是，我们要求解的问题就转化为，在一个有向有权图中，求两个顶点间的最短路径。\npublic class Graph &#123; // 有向有权图的邻接表表示    private LinkedList&lt;Edge&gt; adj[]; // 邻接表    private int v; // 顶点个数    public Graph(int v) &#123;        this.v = v;        this.adj = new LinkedList[v];        for (int i = 0; i &lt; v; ++i) &#123;            this.adj[i] = new LinkedList&lt;&gt;();        &#125;    &#125;    public void addEdge(int s, int t, int w) &#123; // 添加一条边        this.adj[s].add(new Edge(s, t, w));    &#125;    private class Edge &#123;        public int sid; // 边的起始顶点编号        public int tid; // 边的终止顶点编号        public int w; // 权重        public Edge(int sid, int tid, int w) &#123;            this.sid = sid;            this.tid = tid;            this.w = w;        &#125;    &#125;    // 下面这个类是为了dijkstra实现用的    private class Vertex &#123;        public int id; // 顶点编号ID        public int dist; // 从起始顶点到这个顶点的距离        public Vertex(int id, int dist) &#123;            this.id = id;            this.dist = dist;        &#125;    &#125;&#125;\n想要解决这个问题，有一个非常经典的算法，最短路径算法，更加准确地说，是单源最短路径算法（一个顶点到一个顶点）。提到最短路径算法，最出名的莫过于Dijkstra算法了。所以，我们现在来看，Dijkstra算法是怎么工作的。\n这个算法的原理稍微有点儿复杂，单纯的文字描述，不是很好懂。所以，我还是结合代码来讲解。\n// 因为Java提供的优先级队列，没有暴露更新数据的接口，所以我们需要重新实现一个private class PriorityQueue &#123; // 根据vertex.dist构建小顶堆    private Vertex[] nodes;    private int count;    public PriorityQueue(int v) &#123;        this.nodes = new Vertex[v+1];        this.count = v;    &#125;    public Vertex poll() &#123; // TODO: 留给读者实现...     &#125;    public void add(Vertex vertex) &#123; // TODO: 留给读者实现...    &#125;    // 更新结点的值，并且从下往上堆化，重新符合堆的定义。时间复杂度O(logn)。    public void update(Vertex vertex) &#123; // TODO: 留给读者实现...    &#125;     public boolean isEmpty() &#123; // TODO: 留给读者实现...    &#125;&#125;public void dijkstra(int s, int t) &#123; // 从顶点s到顶点t的最短路径    int[] predecessor = new int[this.v]; // 用来还原最短路径    Vertex[] vertexes = new Vertex[this.v];    for (int i = 0; i &lt; this.v; ++i) &#123;        vertexes[i] = new Vertex(i, Integer.MAX_VALUE);    &#125;    PriorityQueue queue = new PriorityQueue(this.v);// 小顶堆    boolean[] inqueue = new boolean[this.v]; // 标记是否进入过队列    vertexes[s].dist = 0;    queue.add(vertexes[s]);    inqueue[s] = true;    while (!queue.isEmpty()) &#123;        Vertex minVertex= queue.poll(); // 取堆顶元素并删除        if (minVertex.id == t) break; // 最短路径产生了        for (int i = 0; i &lt; adj[minVertex.id].size(); ++i) &#123;            Edge e = adj[minVertex.id].get(i); // 取出一条minVetex相连的边            Vertex nextVertex = vertexes[e.tid]; // minVertex--&gt;nextVertex            if (minVertex.dist + e.w &lt; nextVertex.dist) &#123; // 更新next的dist                nextVertex.dist = minVertex.dist + e.w;                predecessor[nextVertex.id] = minVertex.id;                if (inqueue[nextVertex.id] == true) &#123;                    queue.update(nextVertex); // 更新队列中的dist值                &#125; else &#123;                    queue.add(nextVertex);                    inqueue[nextVertex.id] = true;                &#125;            &#125;        &#125;    &#125;    // 输出最短路径    System.out.print(s);    print(s, t, predecessor);&#125;private void print(int s, int t, int[] predecessor) &#123;    if (s == t) return;    print(s, predecessor[t], predecessor);    System.out.print(\"-&gt;\" + t);&#125;\n我们用vertexes数组，记录从起始顶点到每个顶点的距离（dist）。起初，我们把所有顶点的dist都初始化为无穷大（也就是代码中的Integer.MAX_VALUE）。我们把起始顶点的dist值初始化为0，然后将其放到优先级队列中。\n我们从优先级队列中取出dist最小的顶点minVertex，然后考察这个顶点可达的所有顶点（代码中的nextVertex）。如果minVertex的dist值加上minVertex与nextVertex之间边的权重w小于nextVertex当前的dist值，也就是说，存在另一条更短的路径，它经过minVertex到达nextVertex。那我们就把nextVertex的dist更新为minVertex的dist值加上w。然后，我们把nextVertex加入到优先级队列中。重复这个过程，直到找到终止顶点t或者队列为空。\n以上就是Dijkstra算法的核心逻辑。除此之外，代码中还有两个额外的变量，predecessor数组和inqueue数组。\npredecessor数组的作用是为了还原最短路径，它记录每个顶点的前驱顶点。最后，我们通过递归的方式，将这个路径打印出来。打印路径的print递归代码我就不详细讲了，这个跟我们在图的搜索中讲的打印路径方法一样。如果不理解的话，你可以回过头去看下那一节。\ninqueue数组是为了避免将一个顶点多次添加到优先级队列中。我们更新了某个顶点的dist值之后，如果这个顶点已经在优先级队列中了，就不要再将它重复添加进去了。\n看完了代码和文字解释，你可能还是有点懵，那我就举个例子，再给你解释一下。\n\n理解了Dijkstra的原理和代码实现，我们来看下，Dijkstra算法的时间复杂度是多少？\n在刚刚的代码实现中，最复杂就是while循环嵌套for循环那部分代码了。while循环最多会执行V次（V表示顶点的个数），而内部的for循环的执行次数不确定，跟每个顶点的相邻边的个数有关，我们分别记作E0，E1，E2，……，E(V-1)。如果我们把这V个顶点的边都加起来，最大也不会超过图中所有边的个数E（E表示边的个数）。\nfor循环内部的代码涉及从优先级队列取数据、往优先级队列中添加数据、更新优先级队列中的数据，这样三个主要的操作。我们知道，优先级队列是用堆来实现的，堆中的这几个操作，时间复杂度都是O(logV)（堆中的元素个数不会超过顶点的个数V）。\n所以，综合这两部分，再利用乘法原则，整个代码的时间复杂度就是O(E*logV)。\n弄懂了Dijkstra算法，我们再来回答之前的问题，如何计算最优出行路线？\n从理论上讲，用Dijkstra算法可以计算出两点之间的最短路径。但是，你有没有想过，对于一个超级大地图来说，岔路口、道路都非常多，对应到图这种数据结构上来说，就有非常多的顶点和边。如果为了计算两点之间的最短路径，在一个超级大图上动用Dijkstra算法，遍历所有的顶点和边，显然会非常耗时。那我们有没有什么优化的方法呢？\n做工程不像做理论，一定要给出个最优解。理论上算法再好，如果执行效率太低，也无法应用到实际的工程中。对于软件开发工程师来说，我们经常要根据问题的实际背景，对解决方案权衡取舍。类似出行路线这种工程上的问题，我们没有必要非得求出个绝对最优解。很多时候，为了兼顾执行效率，我们只需要计算出一个可行的次优解就可以了。\n有了这个原则，你能想出刚刚那个问题的优化方案吗？\n虽然地图很大，但是两点之间的最短路径或者说较好的出行路径，并不会很“发散”，只会出现在两点之间和两点附近的区块内。所以我们可以在整个大地图上，划出一个小的区块，这个小区块恰好可以覆盖住两个点，但又不会很大。我们只需要在这个小区块内部运行Dijkstra算法，这样就可以避免遍历整个大图，也就大大提高了执行效率。\n不过你可能会说了，如果两点距离比较远，从北京海淀区某个地点，到上海黄浦区某个地点，那上面的这种处理方法，显然就不工作了，毕竟覆盖北京和上海的区块并不小。\n我给你点提示，你可以现在打开地图App，缩小放大一下地图，看下地图上的路线有什么变化，然后再思考，这个问题该怎么解决。\n对于这样两点之间距离较远的路线规划，我们可以把北京海淀区或者北京看作一个顶点，把上海黄浦区或者上海看作一个顶点，先规划大的出行路线。比如，如何从北京到上海，必须要经过某几个顶点，或者某几条干道，然后再细化每个阶段的小路线。\n这样，最短路径问题就解决了。我们再来看另外两个问题，最少时间和最少红绿灯。\n前面讲最短路径的时候，每条边的权重是路的长度。在计算最少时间的时候，算法还是不变，我们只需要把边的权重，从路的长度变成经过这段路所需要的时间。不过，这个时间会根据拥堵情况时刻变化。如何计算车通过一段路的时间呢？这是一个蛮有意思的问题，你可以自己思考下。\n每经过一条边，就要经过一个红绿灯。关于最少红绿灯的出行方案，实际上，我们只需要把每条边的权值改为1即可，算法还是不变，可以继续使用前面讲的Dijkstra算法。不过，边的权值为1，也就相当于无权图了，我们还可以使用之前讲过的广度优先搜索算法。因为我们前面讲过，广度优先搜索算法计算出来的两点之间的路径，就是两点的最短路径。\n不过，这里给出的所有方案都非常粗糙，只是为了给你展示，如何结合实际的场景，灵活地应用算法，让算法为我们所用，真实的地图软件的路径规划，要比这个复杂很多。而且，比起Dijkstra算法，地图软件用的更多的是类似A*的启发式搜索算法，不过也是在Dijkstra算法上的优化罢了，我们后面会讲到，这里暂且不展开。\n总结引申今天，我们学习了一种非常重要的图算法，Dijkstra最短路径算法。实际上，最短路径算法还有很多，比如Bellford算法、Floyd算法等等。如果感兴趣，你可以自己去研究。\n关于Dijkstra算法，我只讲了原理和代码实现。对于正确性，我没有去证明。之所以这么做，是因为证明过程会涉及比较复杂的数学推导。这个并不是我们的重点，你只要掌握这个算法的思路就可以了。\n这些算法实现思路非常经典，掌握了这些思路，我们可以拿来指导、解决其他问题。比如Dijkstra这个算法的核心思想，就可以拿来解决下面这个看似完全不相关的问题。这个问题是我之前工作中遇到的真实的问题，为了在较短的篇幅里把问题介绍清楚，我对背景做了一些简化。\n我们有一个翻译系统，只能针对单个词来做翻译。如果要翻译一整个句子，我们需要将句子拆成一个一个的单词，再丢给翻译系统。针对每个单词，翻译系统会返回一组可选的翻译列表，并且针对每个翻译打一个分，表示这个翻译的可信程度。\n\n针对每个单词，我们从可选列表中，选择其中一个翻译，组合起来就是整个句子的翻译。每个单词的翻译的得分之和，就是整个句子的翻译得分。随意搭配单词的翻译，会得到一个句子的不同翻译。针对整个句子，我们希望计算出得分最高的前k个翻译结果，你会怎么编程来实现呢？\n\n当然，最简单的办法还是借助回溯算法，穷举所有的排列组合情况，然后选出得分最高的前k个翻译结果。但是，这样做的时间复杂度会比较高，是O(m^n)，其中，m表示平均每个单词的可选翻译个数，n表示一个句子中包含多少个单词。这个解决方案，你可以当作回溯算法的练习题，自己编程实现一下，我就不多说了。\n实际上，这个问题可以借助Dijkstra算法的核心思想，非常高效地解决。每个单词的可选翻译是按照分数从大到小排列的，所以$a_{0}b_{0}c_{0}$肯定是得分最高组合结果。我们把$a_{0}b_{0}c_{0}$及得分作为一个对象，放入到优先级队列中。\n我们每次从优先级队列中取出一个得分最高的组合，并基于这个组合进行扩展。扩展的策略是每个单词的翻译分别替换成下一个单词的翻译。比如$a_{0}b_{0}c_{0}$扩展后，会得到三个组合，$a_{1}b_{0}c_{0}$、$a_{0}b_{1}c_{0}$、$a_{0}b_{0}c_{1}$。我们把扩展之后的组合，加到优先级队列中。重复这个过程，直到获取到k个翻译组合或者队列为空。\n\n我们来看，这种实现思路的时间复杂度是多少？\n假设句子包含n个单词，每个单词平均有m个可选的翻译，我们求得分最高的前k个组合结果。每次一个组合出队列，就对应着一个组合结果，我们希望得到k个，那就对应着k次出队操作。每次有一个组合出队列，就有n个组合入队列。优先级队列中出队和入队操作的时间复杂度都是O(logX)，X表示队列中的组合个数。所以，总的时间复杂度就是O(k*n*logX)。那X到底是多少呢？\nk次出入队列，队列中的总数据不会超过k*n，也就是说，出队、入队操作的时间复杂度是O(log(k*n))。所以，总的时间复杂度就是O(k*n*log(k*n))，比之前的指数级时间复杂度降低了很多。\n课后思考\n在计算最短时间的出行路线中，如何获得通过某条路的时间呢？这个题目很有意思，我之前面试的时候也被问到过，你可以思考看看。\n\n今天讲的出行路线问题，我假设的是开车出行，那如果是公交出行呢？如果混合地铁、公交、步行，又该如何规划路线呢？\n\n\n欢迎留言和我分享，也欢迎点击“请朋友读”，把今天的内容分享给你的好友，和他一起讨论、学习。\n","categories":["数据结构与算法","高级篇"]},{"title":"53 | 算法实战（二）：剖析搜索引擎背后的经典数据结构和算法","url":"/2020/08/07/53%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%9A%E7%AE%97%E6%B3%95%E5%AE%9E%E6%88%98%EF%BC%88%E4%BA%8C%EF%BC%89/","content":"像百度、Google这样的搜索引擎，在我们平时的工作、生活中，几乎天天都会用到。如果我们把搜索引擎也当作一个互联网产品的话，那它跟社交、电商这些类型的产品相比，有一个非常大的区别，那就是，它是一个技术驱动的产品。所谓技术驱动是指，搜索引擎实现起来，技术难度非常大，技术的好坏直接决定了这个产品的核心竞争力。\n在搜索引擎的设计与实现中，会用到大量的算法。有很多针对特定问题的算法，也有很多我们专栏中讲到的基础算法。所以，百度、Google这样的搜索引擎公司，在面试的时候，会格外重视考察候选人的算法能力。\n今天我就借助搜索引擎，这样一个非常有技术含量的产品，来给你展示一下，数据结构和算法是如何应用在其中的。\n整体系统介绍像Google这样的大型商用搜索引擎，有成千上万的工程师，十年如一日地对它进行优化改进，所以，它所包含的技术细节非常多。我很难、也没有这个能力，通过一篇文章把所有细节都讲清楚，当然这也不是我们专栏所专注的内容。\n所以，接下来的讲解，我主要给你展示，如何在一台机器上（假设这台机器的内存是8GB， 硬盘是100多GB），通过少量的代码，实现一个小型搜索引擎。不过，麻雀虽小，五脏俱全。跟大型搜索引擎相比，实现这样一个小型搜索引擎所用到的理论基础是相通的。\n搜索引擎大致可以分为四个部分：搜集、分析、索引、查询。其中，搜集，就是我们常说的利用爬虫爬取网页。分析，主要负责网页内容抽取、分词，构建临时索引，计算PageRank值这几部分工作。索引，主要负责通过分析阶段得到的临时索引，构建倒排索引。查询，主要负责响应用户的请求，根据倒排索引获取相关网页，计算网页排名，返回查询结果给用户。\n接下来，我就按照网页处理的生命周期，从这四个阶段，依次来给你讲解，一个网页从被爬取到最终展示给用户，这样一个完整的过程。与此同时，我会穿插讲解，这个过程中需要用到哪些数据结构和算法。\n搜集现在，互联网越来越发达，网站越来越多，对应的网页也就越来越多。对于搜索引擎来说，它事先并不知道网页都在哪里。打个比方来说就是，我们只知道海里面有很多鱼，但却并不知道鱼在哪里。那搜索引擎是如何爬取网页的呢？\n搜索引擎把整个互联网看作数据结构中的有向图，把每个页面看作一个顶点。如果某个页面中包含另外一个页面的链接，那我们就在两个顶点之间连一条有向边。我们可以利用图的遍历搜索算法，来遍历整个互联网中的网页。\n我们前面介绍过两种图的遍历方法，深度优先和广度优先。搜索引擎采用的是广度优先搜索策略。具体点讲的话，那就是，我们先找一些比较知名的网页（专业的叫法是权重比较高）的链接（比如新浪主页网址、腾讯主页网址等），作为种子网页链接，放入到队列中。爬虫按照广度优先的策略，不停地从队列中取出链接，然后去爬取对应的网页，解析出网页里包含的其他网页链接，再将解析出来的链接添加到队列中。\n基本的原理就是这么简单。但落实到实现层面，还有很多技术细节。我下面借助搜集阶段涉及的几个重要文件，来给你解释一下搜集工程都有哪些关键技术细节。\n1.待爬取网页链接文件：links.bin在广度优先搜索爬取页面的过程中，爬虫会不停地解析页面链接，将其放到队列中。于是，队列中的链接就会越来越多，可能会多到内存放不下。所以，我们用一个存储在磁盘中的文件（links.bin）来作为广度优先搜索中的队列。爬虫从links.bin文件中，取出链接去爬取对应的页面。等爬取到网页之后，将解析出来的链接，直接存储到links.bin文件中。\n这样用文件来存储网页链接的方式，还有其他好处。比如，支持断点续爬。也就是说，当机器断电之后，网页链接不会丢失；当机器重启之后，还可以从之前爬取到的位置继续爬取。\n关于如何解析页面获取链接，我额外多说几句。我们可以把整个页面看作一个大的字符串，然后利用字符串匹配算法，在这个大字符串中，搜索&lt;link&gt;这样一个网页标签，然后顺序读取&lt;link&gt;&lt;/link&gt;之间的字符串。这其实就是网页链接。\n2.网页判重文件：bloom_filter.bin如何避免重复爬取相同的网页呢？这个问题我们在位图那一节已经讲过了。使用布隆过滤器，我们就可以快速并且非常节省内存地实现网页的判重。\n不过，还是刚刚那个问题，如果我们把布隆过滤器存储在内存中，那机器宕机重启之后，布隆过滤器就被清空了。这样就可能导致大量已经爬取的网页会被重复爬取。\n这个问题该怎么解决呢？我们可以定期地（比如每隔半小时）将布隆过滤器持久化到磁盘中，存储在bloom_filter.bin文件中。这样，即便出现机器宕机，也只会丢失布隆过滤器中的部分数据。当机器重启之后，我们就可以重新读取磁盘中的bloom_filter.bin文件，将其恢复到内存中。\n3.原始网页存储文件：doc_raw.bin爬取到网页之后，我们需要将其存储下来，以备后面离线分析、索引之用。那如何存储海量的原始网页数据呢？\n如果我们把每个网页都存储为一个独立的文件，那磁盘中的文件就会非常多，数量可能会有几千万，甚至上亿。常用的文件系统显然不适合存储如此多的文件。所以，我们可以把多个网页存储在一个文件中。每个网页之间，通过一定的标识进行分隔，方便后续读取。具体的存储格式，如下图所示。其中，doc_id这个字段是网页的编号，我们待会儿再解释。\n\n当然，这样的一个文件也不能太大，因为文件系统对文件的大小也有一定的限制。所以，我们可以设置每个文件的大小不能超过一定的值（比如1GB）。随着越来越多的网页被添加到文件中，文件的大小就会越来越大，当超过1GB的时候，我们就创建一个新的文件，用来存储新爬取的网页。\n假设一台机器的硬盘大小是100GB左右，一个网页的平均大小是64KB。那在一台机器上，我们可以存储100万到200万左右的网页。假设我们的机器的带宽是10MB，那下载100GB的网页，大约需要10000秒。也就是说，爬取100多万的网页，也就是只需要花费几小时的时间。\n4.网页链接及其编号的对应文件：doc_id.bin刚刚我们提到了网页编号这个概念，我现在解释一下。网页编号实际上就是给每个网页分配一个唯一的ID，方便我们后续对网页进行分析、索引。那如何给网页编号呢？\n我们可以按照网页被爬取的先后顺序，从小到大依次编号。具体是这样做的：我们维护一个中心的计数器，每爬取到一个网页之后，就从计数器中拿一个号码，分配给这个网页，然后计数器加一。在存储网页的同时，我们将网页链接跟编号之间的对应关系，存储在另一个doc_id.bin文件中。\n爬虫在爬取网页的过程中，涉及的四个重要的文件，我就介绍完了。其中，links.bin和bloom_filter.bin这两个文件是爬虫自身所用的。另外的两个（doc_raw.bin、doc_id.bin）是作为搜集阶段的成果，供后面的分析、索引、查询用的。\n分析网页爬取下来之后，我们需要对网页进行离线分析。分析阶段主要包括两个步骤，第一个是抽取网页文本信息，第二个是分词并创建临时索引。我们逐一来讲解。\n1.抽取网页文本信息网页是半结构化数据，里面夹杂着各种标签、JavaScript代码、CSS样式。对于搜索引擎来说，它只关心网页中的文本信息，也就是，网页显示在浏览器中时，能被用户肉眼看到的那部分信息。我们如何从半结构化的网页中，抽取出搜索引擎关系的文本信息呢？\n我们之所以把网页叫作半结构化数据，是因为它本身是按照一定的规则来书写的。这个规则就是HTML语法规范。我们依靠HTML标签来抽取网页中的文本信息。这个抽取的过程，大体可以分为两步。\n第一步是去掉JavaScript代码、CSS格式以及下拉框中的内容（因为下拉框在用户不操作的情况下，也是看不到的）。也就是&lt;style&gt;&lt;/style&gt;，&lt;script&gt;&lt;/script&gt;，&lt;option&gt;&lt;/option&gt;这三组标签之间的内容。我们可以利用AC自动机这种多模式串匹配算法，在网页这个大字符串中，一次性查找&lt;style&gt;, &lt;script&gt;, &lt;option&gt;这三个关键词。当找到某个关键词出现的位置之后，我们只需要依次往后遍历，直到对应结束标签（&lt;/style&gt;, &lt;/script&gt;, &lt;/option&gt;）为止。而这期间遍历到的字符串连带着标签就应该从网页中删除。\n第二步是去掉所有HTML标签。这一步也是通过字符串匹配算法来实现的。过程跟第一步类似，我就不重复讲了。\n2.分词并创建临时索引经过上面的处理之后，我们就从网页中抽取出了我们关心的文本信息。接下来，我们要对文本信息进行分词，并且创建临时索引。\n对于英文网页来说，分词非常简单。我们只需要通过空格、标点符号等分隔符，将每个单词分割开来就可以了。但是，对于中文来说，分词就复杂太多了。我这里介绍一种比较简单的思路，基于字典和规则的分词方法。\n其中，字典也叫词库，里面包含大量常用的词语（我们可以直接从网上下载别人整理好的）。我们借助词库并采用最长匹配规则，来对文本进行分词。所谓最长匹配，也就是匹配尽可能长的词语。我举个例子解释一下。\n比如要分词的文本是“中国人民解放了”，我们词库中有“中国”“中国人”“中国人民”“中国人民解放军”这几个词，那我们就取最长匹配，也就是“中国人民”划为一个词，而不是把“中国”、“中国人”划为一个词。具体到实现层面，我们可以将词库中的单词，构建成Trie树结构，然后拿网页文本在Trie树中匹配。\n每个网页的文本信息在分词完成之后，我们都得到一组单词列表。我们把单词与网页之间的对应关系，写入到一个临时索引文件中（tmp_Index.bin），这个临时索引文件用来构建倒排索引文件。临时索引文件的格式如下：\n\n在临时索引文件中，我们存储的是单词编号，也就是图中的term_id，而非单词本身。这样做的目的主要是为了节省存储的空间。那这些单词的编号是怎么来的呢？\n给单词编号的方式，跟给网页编号类似。我们维护一个计数器，每当从网页文本信息中分割出一个新的单词的时候，我们就从计数器中取一个编号，分配给它，然后计数器加一。\n在这个过程中，我们还需要使用散列表，记录已经编过号的单词。在对网页文本信息分词的过程中，我们拿分割出来的单词，先到散列表中查找，如果找到，那就直接使用已有的编号；如果没有找到，我们再去计数器中拿号码，并且将这个新单词以及编号添加到散列表中。\n当所有的网页处理（分词及写入临时索引）完成之后，我们再将这个单词跟编号之间的对应关系，写入到磁盘文件中，并命名为term_id.bin。\n经过分析阶段，我们得到了两个重要的文件。它们分别是临时索引文件（tmp_index.bin）和单词编号文件（term_id.bin）。\n索引索引阶段主要负责将分析阶段产生的临时索引，构建成倒排索引。倒排索引（ Inverted index）中记录了每个单词以及包含它的网页列表。文字描述比较难理解，我画了一张倒排索引的结构图，你一看就明白。\n\n我们刚刚讲到，在临时索引文件中，记录的是单词跟每个包含它的文档之间的对应关系。那如何通过临时索引文件，构建出倒排索引文件呢？这是一个非常典型的算法问题，你可以先自己思考一下，再看我下面的讲解。\n解决这个问题的方法有很多。考虑到临时索引文件很大，无法一次性加载到内存中，搜索引擎一般会选择使用多路归并排序的方法来实现。\n我们先对临时索引文件，按照单词编号的大小进行排序。因为临时索引很大，所以一般基于内存的排序算法就没法处理这个问题了。我们可以用之前讲到的归并排序的处理思想，将其分割成多个小文件，先对每个小文件独立排序，最后再合并在一起。当然，实际的软件开发中，我们其实可以直接利用MapReduce来处理。\n临时索引文件排序完成之后，相同的单词就被排列到了一起。我们只需要顺序地遍历排好序的临时索引文件，就能将每个单词对应的网页编号列表找出来，然后把它们存储在倒排索引文件中。具体的处理过程，我画成了一张图。通过图，你应该更容易理解。\n\n除了倒排文件之外，我们还需要一个文件，来记录每个单词编号在倒排索引文件中的偏移位置。我们把这个文件命名为term_offset.bin。这个文件的作用是，帮助我们快速地查找某个单词编号在倒排索引中存储的位置，进而快速地从倒排索引中读取单词编号对应的网页编号列表。\n\n经过索引阶段的处理，我们得到了两个有价值的文件，它们分别是倒排索引文件（index.bin）和记录单词编号在索引文件中的偏移位置的文件（term_offset.bin）。\n查询前面三个阶段的处理，只是为了最后的查询做铺垫。因此，现在我们就要利用之前产生的几个文件，来实现最终的用户搜索功能。\n\ndoc_id.bin：记录网页链接和编号之间的对应关系。\n\nterm_id.bin：记录单词和编号之间的对应关系。\n\nindex.bin：倒排索引文件，记录每个单词编号以及对应包含它的网页编号列表。\n\nterm_offsert.bin：记录每个单词编号在倒排索引文件中的偏移位置。\n\n\n这四个文件中，除了倒排索引文件（index.bin）比较大之外，其他的都比较小。为了方便快速查找数据，我们将其他三个文件都加载到内存中，并且组织成散列表这种数据结构。\n当用户在搜索框中，输入某个查询文本的时候，我们先对用户输入的文本进行分词处理。假设分词之后，我们得到k个单词。\n我们拿这k个单词，去term_id.bin对应的散列表中，查找对应的单词编号。经过这个查询之后，我们得到了这k个单词对应的单词编号。\n我们拿这k个单词编号，去term_offset.bin对应的散列表中，查找每个单词编号在倒排索引文件中的偏移位置。经过这个查询之后，我们得到了k个偏移位置。\n我们拿这k个偏移位置，去倒排索引（index.bin）中，查找k个单词对应的包含它的网页编号列表。经过这一步查询之后，我们得到了k个网页编号列表。\n我们针对这k个网页编号列表，统计每个网页编号出现的次数。具体到实现层面，我们可以借助散列表来进行统计。统计得到的结果，我们按照出现次数的多少，从小到大排序。出现次数越多，说明包含越多的用户查询单词（用户输入的搜索文本，经过分词之后的单词）。\n经过这一系列查询，我们就得到了一组排好序的网页编号。我们拿着网页编号，去doc_id.bin文件中查找对应的网页链接，分页显示给用户就可以了。\n总结引申今天，我给你展示了一个小型搜索引擎的设计思路。这只是一个搜索引擎设计的基本原理，有很多优化、细节我们并未涉及，比如计算网页权重的PageRank算法、计算查询结果排名的tf-idf模型等等。\n在讲解的过程中，我们涉及的数据结构和算法有：图、散列表、Trie树、布隆过滤器、单模式字符串匹配算法、AC自动机、广度优先遍历、归并排序等。如果对其中哪些内容不清楚，你可以回到对应的章节进行复习。\n最后，如果有时间的话，我强烈建议你，按照我的思路，自己写代码实现一个简单的搜索引擎。这样写出来的，即便只是一个demo，但对于你深入理解数据结构和算法，也是很有帮助的。\n课后思考\n图的遍历方法有两种，深度优先和广度优先。我们讲到，搜索引擎中的爬虫是通过广度优先策略来爬取网页的。搜索引擎为什么选择广度优先策略，而不是深度优先策略呢？\n\n大部分搜索引擎在结果显示的时候，都支持摘要信息和网页快照。实际上，你只需要对我今天讲的设计思路，稍加改造，就可以支持这两项功能。你知道如何改造吗？\n\n\n欢迎留言和我分享，也欢迎点击“请朋友读”，把今天的内容分享给你的好友，和他一起讨论、学习。\n","categories":["数据结构与算法","实战篇"]},{"title":"JavaScript组件化开发","url":"/2020/03/21/JavaScript%E7%BB%84%E4%BB%B6%E5%8C%96%E5%BC%80%E5%8F%91/","content":"文章出自：https://blog.csdn.net/Prince_fmx/article/details/77926189\n下面我们来谈谈，在现有的知识体系下，如何很好的写组件。\n比如我们要实现这样一个组件，就是一个输入框里面字数的计数。这个应该是个很简单的需求。\n\n我们来看看，下面的各种写法。\n\n为了更清楚的演示，下面全部使用jQuery作为基础语言库。\n\n最简陋的写法嗯 所谓的入门级写法呢，就是完完全全的全局函数全局变量的写法。（就我所知，现在好多外包还是这种写法）\n代码如下：\n&lt;!DOCTYPE html&gt;&lt;html&gt;  &lt;head&gt;    &lt;meta charset=\"utf-8\"&gt;    &lt;title&gt;test&lt;/title&gt;    &lt;script src=\"http://code.jquery.com/jquery-1.9.1.min.js\"&gt;&lt;/script&gt;    &lt;script&gt;      $(function() &#123;        var input = $('#J_input');        //用来获取字数        function getNum()&#123;          return input.val().length;        &#125;        //渲染元素        function render()&#123;          var num = getNum();          //没有字数的容器就新建一个          if ($('#J_input_count').length == 0) &#123;            input.after('&lt;span id=\"J_input_count\"&gt;&lt;/span&gt;');          &#125;;          $('#J_input_count').html(num+'个字');        &#125;        //监听事件        input.on('keyup',function()&#123;          render();        &#125;);        //初始化，第一次渲染        render();      &#125;)    &lt;/script&gt;  &lt;/head&gt;  &lt;body&gt;    &lt;input type=\"text\" id=\"J_input\"/&gt;  &lt;/body&gt;&lt;/html&gt;\n\n这段代码跑也是可以跑的，但是呢，各种变量混乱，没有很好的隔离作用域,当页面变的复杂的时候,会很难去维护。目前这种代码基本是用不了的。当然少数的活动页面可以简单用用。\n作用域隔离让我们对上面的代码作些改动，使用单个变量模拟命名空间。\nvar textCount = &#123;  input:null,  init:function(config)&#123;    this.input = $(config.id);    this.bind();    //这边范围对应的对象，可以实现链式调用    return this;  &#125;,  bind:function()&#123;    var self = this;    this.input.on('keyup',function()&#123;      self.render();    &#125;);  &#125;,  getNum:function()&#123;    return this.input.val().length;  &#125;,  //渲染元素  render:function()&#123;    var num = this.getNum();    if ($('#J_input_count').length == 0) &#123;      this.input.after('&lt;span id=\"J_input_count\"&gt;&lt;/span&gt;');    &#125;;    $('#J_input_count').html(num+'个字');  &#125;&#125;$(function() &#123;  //在domready后调用  textCount.init(&#123;id:'#J_input'&#125;).render();&#125;)\n\n这样一改造，立马变的清晰了很多，所有的功能都在一个变量下面。代码更清晰，并且有统一的入口调用方法。\n但是还是有些瑕疵，这种写法没有私有的概念，比如上面的getNum,bind应该都是私有的方法。但是其他代码可以很随意的改动这些。当代码量特别特别多的时候，很容易出现变量重复，或被修改的问题。\n于是又出现了一种函数闭包的写法：\nvar TextCount = (function()&#123;  //私有方法，外面将访问不到  var _bind = function(that)&#123;    that.input.on('keyup',function()&#123;      that.render();    &#125;);  &#125;  var _getNum = function(that)&#123;    return that.input.val().length;  &#125;  var TextCountFun = function(config)&#123;  &#125;  TextCountFun.prototype.init = function(config) &#123;    this.input = $(config.id);    _bind(this);    return this;  &#125;;  TextCountFun.prototype.render = function() &#123;    var num = _getNum(this);    if ($('#J_input_count').length == 0) &#123;      this.input.after('&lt;span id=\"J_input_count\"&gt;&lt;/span&gt;');    &#125;;    $('#J_input_count').html(num+'个字');  &#125;;  //返回构造函数  return TextCountFun;&#125;)();$(function() &#123;  new TextCount().init(&#123;id:'#J_input'&#125;).render();&#125;)\n\n这种写法，把所有的东西都包在了一个自动执行的闭包里面，所以不会受到外面的影响，并且只对外公开了TextCountFun构造函数，生成的对象只能访问到init,render方法。这种写法已经满足绝大多数的需求了。事实上大部分的jQuery插件都是这种写法。\n面向对象上面的写法已经可以满足绝大多数需求了。\n但是呢，当一个页面特别复杂，当我们需要的组件越来越多，当我们需要做一套组件。仅仅用这个就不行了。首先的问题就是，这种写法太灵活了，写单个组件还可以。如果我们需要做一套风格相近的组件，而且是多个人同时在写。那真的是噩梦。\n在编程的圈子里，面向对象一直是被认为最佳的编写代码方式。比如java，就是因为把面向对象发挥到了极致，所以多个人写出来的代码都很接近，维护也很方便。但是很不幸的是，javascript不支持class类的定义。但是我们可以模拟。\n下面我们先实现个简单的javascript类：\nvar Class = (function() &#123;  var _mix = function(r, s) &#123;    for (var p in s) &#123;      if (s.hasOwnProperty(p)) &#123;        r[p] = s[p]      &#125;    &#125;  &#125;  var _extend = function() &#123;    //开关 用来使生成原型时,不调用真正的构成流程init    this.initPrototype = true    var prototype = new this()    this.initPrototype = false    var items = Array.prototype.slice.call(arguments) || []    var item    //支持混入多个属性，并且支持&#123;&#125;也支持 Function    while (item = items.shift()) &#123;      _mix(prototype, item.prototype || item)    &#125;    // 这边是返回的类，其实就是我们返回的子类    function SubClass() &#123;      if (!SubClass.initPrototype &amp;&amp; this.init)        this.init.apply(this, arguments)//调用init真正的构造函数    &#125;    // 赋值原型链，完成继承    SubClass.prototype = prototype    // 改变constructor引用    SubClass.prototype.constructor = SubClass    // 为子类也添加extend方法    SubClass.extend = _extend    return SubClass  &#125;  //超级父类  var Class = function() &#123;&#125;  //为超级父类添加extend方法  Class.extend = _extend  return Class&#125;)()\n\n这是拿John Resig的class简单修改了下。\n\n这边只是很简陋的实现了类的继承机制。如果对类的实现有兴趣可以参考我另一篇文章javascript oo实现\n\n我们看下使用方法：\n//继承超级父类，生成个子类Animal，并且混入一些方法。这些方法会到Animal的原型上。//另外这边不仅支持混入&#123;&#125;，还支持混入Functionvar Animal = Class.extend(&#123;  init:function(opts)&#123;    this.msg = opts.msg    this.type = \"animal\"  &#125;,  say:function()&#123;    alert(this.msg+\":i am a \"+this.type)  &#125;&#125;)//继承Animal，并且混入一些方法var Dog = Animal.extend(&#123;  init:function(opts)&#123;    //并未实现super方法，直接简单使用父类原型调用即可    Animal.prototype.init.call(this,opts)    //修改了type类型    this.type = \"dog\"  &#125;&#125;)//new Animal(&#123;msg:'hello'&#125;).say()new Dog(&#123;msg:'hi'&#125;).say()\n\n使用很简单，超级父类具有extend方法，可以继承出一个子类。子类也具有extend方法。\n这边要强调的是，继承的父类都是一个也就是单继承。但是可以通过extend实现多重混入。详见下面用法。\n有了这个类的扩展，我们可以这么编写代码了：\nvar TextCount = Class.extend(&#123;  init:function(config)&#123;    this.input = $(config.id);    this._bind();    this.render();  &#125;,  render:function() &#123;    var num = this._getNum();    if ($('#J_input_count').length == 0) &#123;      this.input.after('&lt;span id=\"J_input_count\"&gt;&lt;/span&gt;');    &#125;;    $('#J_input_count').html(num+'个字');  &#125;,  _getNum:function()&#123;    return this.input.val().length;  &#125;,  _bind:function()&#123;    var self = this;    self.input.on('keyup',function()&#123;      self.render();    &#125;);  &#125;&#125;)$(function() &#123;  new TextCount(&#123;    id:\"#J_input\"  &#125;);&#125;)\n\n这边可能还没看见class的真正好处，不急我们继续往下。\n抽象出base可以看到，我们的组件有些方法，是大部分组件都会有的。\n\n比如init用来初始化属性。\n比如render用来处理渲染的逻辑。\n比如bind用来处理事件的绑定。\n\n当然这也是一种约定俗成的规范了。如果大家全部按照这种风格来写代码，开发大规模组件库就变得更加规范，相互之间配合也更容易。\n这个时候面向对象的好处就来了，我们抽象出一个Base类。其他组件编写时都继承它。\nvar Base = Class.extend(&#123;  init:function(config)&#123;    //自动保存配置项    this.__config = config    this.bind()    this.render()  &#125;,  //可以使用get来获取配置项  get:function(key)&#123;    return this.__config[key]  &#125;,  //可以使用set来设置配置项  set:function(key,value)&#123;    this.__config[key] = value  &#125;,  bind:function()&#123;  &#125;,  render:function() &#123;  &#125;,  //定义销毁的方法，一些收尾工作都应该在这里  destroy:function()&#123;  &#125;&#125;)\n\nbase类主要把组件的一般性内容都提取了出来，这样我们编写组件时可以直接继承base类，覆盖里面的bind和render方法。\n于是我们可以这么写代码：\nvar TextCount = Base.extend(&#123;  _getNum:function()&#123;    return this.get('input').val().length;  &#125;,  bind:function()&#123;    var self = this;    self.get('input').on('keyup',function()&#123;      self.render();    &#125;);  &#125;,  render:function() &#123;    var num = this._getNum();    if ($('#J_input_count').length == 0) &#123;      this.get('input').after('&lt;span id=\"J_input_count\"&gt;&lt;/span&gt;');    &#125;;    $('#J_input_count').html(num+'个字');  &#125;&#125;)$(function() &#123;  new TextCount(&#123;    //这边直接传input的节点了，因为属性的赋值都是自动的。    input:$(\"#J_input\")  &#125;);&#125;)\n\n可以看到我们直接实现一些固定的方法，bind，render就行了。其他的base会自动处理（这里只是简单处理了配置属性的赋值）。\n事实上，这边的init，bind，render就已经有了点生命周期的影子，但凡是组件都会具有这几个阶段，初始化，绑定事件，以及渲染。当然这边还可以加一个destroy销毁的方法，用来清理现场。\n此外为了方便，这边直接变成了传递input的节点。因为属性赋值自动化了，一般来说这种情况下都是使用getter，setter来处理。这边就不详细展开了。\n引入事件机制（观察者模式）有了base应该说我们编写组件更加的规范化，体系化了。下面我们继续深挖。\n还是上面的那个例子，如果我们希望输入字的时候超过5个字就弹出警告。该怎么办呢。\n小白可能会说，那简单啊直接改下bind方法：\nvar TextCount = Base.extend(&#123;  ...  bind:function()&#123;    var self = this;    self.get('input').on('keyup',function()&#123;      if(self._getNum() &gt; 5)&#123;        alert('超过了5个字了。。。')      &#125;      self.render();    &#125;);  &#125;,  ...&#125;)\n\n的确也是一种方法，但是太low了，代码严重耦合。当这种需求特别特别多，代码会越来越乱。\n这个时候就要引入事件机制，也就是经常说的观察者模式。\n\n注意这边的事件机制跟平时的浏览器那些事件不是一回事，要分开来看。\n\n什么是观察者模式呢，官方的解释就不说了，直接拿这个例子来说。\n想象一下base是个机器人会说话，他会一直监听输入的字数并且汇报出去（通知）。而你可以把耳朵凑上去，听着他的汇报（监听）。发现字数超过5个字了，你就做些操作。\n所以这分为两个部分，一个是通知，一个是监听。\n假设通知是 fire方法，监听是on。于是我们可以这么写代码：\nvar TextCount = Base.extend(&#123;  ...  bind:function()&#123;    var self = this;    self.get('input').on('keyup',function()&#123;      //通知,每当有输入的时候，就报告出去。      self.fire('Text.input',self._getNum())      self.render();    &#125;);  &#125;,  ...&#125;)  $(function() &#123;  var t = new TextCount(&#123;    input:$(\"#J_input\")  &#125;);  //监听这个输入事件  t.on('Text.input',function(num)&#123;    //可以获取到传递过来的值    if(num&gt;5)&#123;      alert('超过了5个字了。。。')    &#125;  &#125;)&#125;)\n\nfire用来触发一个事件，可以传递数据。而on用来添加一个监听。这样组件里面只负责把一些关键的事件抛出来，至于具体的业务逻辑都可以添加监听来实现。没有事件的组件是不完整的。\n下面我们看看怎么实现这套事件机制。\n我们首先抛开base，想想怎么实现一个具有这套机制的类。\n//辅组函数，获取数组里某个元素的索引 indexvar _indexOf = function(array,key)&#123;  if (array === null) return -1  var i = 0, length = array.length  for (; i &lt; length; i++) if (array[i] === item) return i  return -1&#125;var Event = Class.extend(&#123;  //添加监听  on:function(key,listener)&#123;    //this.__events存储所有的处理函数    if (!this.__events) &#123;      this.__events = &#123;&#125;    &#125;    if (!this.__events[key]) &#123;      this.__events[key] = []    &#125;    if (_indexOf(this.__events,listener) === -1 &amp;&amp; typeof listener === 'function') &#123;      this.__events[key].push(listener)    &#125;    return this  &#125;,  //触发一个事件，也就是通知  fire:function(key)&#123;    if (!this.__events || !this.__events[key]) return    var args = Array.prototype.slice.call(arguments, 1) || []    var listeners = this.__events[key]    var i = 0    var l = listeners.length    for (i; i &lt; l; i++) &#123;      listeners[i].apply(this,args)    &#125;    return this  &#125;,  //取消监听  off:function(key,listener)&#123;    if (!key &amp;&amp; !listener) &#123;      this.__events = &#123;&#125;    &#125;    //不传监听函数，就去掉当前key下面的所有的监听函数    if (key &amp;&amp; !listener) &#123;      delete this.__events[key]    &#125;    if (key &amp;&amp; listener) &#123;      var listeners = this.__events[key]      var index = _indexOf(listeners, listener)      (index &gt; -1) &amp;&amp; listeners.splice(index, 1)      &#125;    return this;  &#125;&#125;)var a = new Event()//添加监听 test事件a.on('test',function(msg)&#123;  alert(msg)&#125;)//触发 test事件a.fire('test','我是第一次触发')a.fire('test','我又触发了')a.off('test')a.fire('test','你应该看不到我了')\n\n实现起来并不复杂，只要使用this.__events存下所有的监听函数。在fire的时候去找到并且执行就行了。\n这个时候面向对象的好处就来了，如果我们希望base拥有事件机制。只需要这么写:\nvar Base = Class.extend(Event,&#123;  ...  destroy:function()&#123;    //去掉所有的事件监听    this.off()  &#125;&#125;)//于是可以//var a  = new Base()// a.on(xxx,fn)//// a.fire()\n\n是的只要extend的时候多混入一个Event，这样Base或者它的子类生成的对象都会自动具有事件机制。\n有了事件机制我们可以把组件内部很多状态暴露出来，比如我们可以在set方法中抛出一个事件，这样每次属性变更的时候我们都可以监听到。\n到这里为止，我们的base类已经像模像样了，具有了init，bind，render，destroy方法来表示组件的各个关键过程，并且具有了事件机制。基本上已经可以很好的来开发组件了。\n更进一步，richbase我们还可以继续深挖。看看我们的base，还差些什么。首先浏览器的事件监听还很落后，需要用户自己在bind里面绑定，再然后现在的TextCount里面还存在dom操作，也没有自己的模板机制。这都是需要扩展的，于是我们在base的基础上再继承出一个richbase用来实现更完备的组件基类。\n主要实现这些功能：\n\n事件代理：不需要用户自己去找dom元素绑定监听，也不需要用户去关心什么时候销毁。\n模板渲染：用户不需要覆盖render方法，而是覆盖实现setUp方法。可以通过在setUp里面调用render来达到渲染对应html的目的。\n单向绑定：通过setChuckdata方法，更新数据，同时会更新html内容，不再需要dom操作。\n\n我们看下我们实现richbase后怎么写组件：\nvar TextCount = RichBase.extend(&#123;  //事件直接在这里注册，会代理到parentNode节点，parentNode节点在下面指定  EVENTS:&#123;    //选择器字符串，支持所有jQuery风格的选择器    'input':&#123;      //注册keyup事件      keyup:function(self,e)&#123;        //单向绑定，修改数据直接更新对应模板        self.setChuckdata('count',self._getNum())      &#125;    &#125;  &#125;,  //指定当前组件的模板  template:'&lt;span id=\"J_input_count\"&gt;&lt;%= count %&gt;个字&lt;/span&gt;',  //私有方法  _getNum:function()&#123;    return this.get('input').val().length || 0  &#125;,  //覆盖实现setUp方法，所有逻辑写在这里。最后可以使用render来决定需不需要渲染模板  //模板渲染后会append到parentNode节点下面，如果未指定，会append到document.body  setUp:function()&#123;    var self = this;    var input = this.get('parentNode').find('#J_input')    self.set('input',input)    var num = this._getNum()    //赋值数据，渲染模板，选用。有的组件没有对应的模板就可以不调用这步。    self.render(&#123;      count:num    &#125;)  &#125;&#125;)$(function() &#123;  //传入parentNode节点，组件会挂载到这个节点上。所有事件都会代理到这个上面。  new TextCount(&#123;    parentNode:$(\"#J_test_container\")  &#125;);&#125;)/**对应的html,做了些修改，主要为了加上parentNode，这边就是J_test_container&lt;div id=\"J_test_container\"&gt;  &lt;input type=\"text\" id=\"J_input\"/&gt;&lt;/div&gt;*/\n\n看下上面的用法，可以看到变得更简单清晰了：\n\n事件不需要自己绑定，直接注册在EVENTS属性上。程序会自动将事件代理到parentNode上。\n引入了模板机制，使用template规定组件的模板，然后在setUp里面使用render(data)的方式渲染模板，程序会自动帮你append到parentNode下面。\n单向绑定，无需操作dom，后面要改动内容，不需要操作dom，只需要调用setChuckdata(key,新的值)，选择性的更新某个数据，相应的html会自动重新渲染。\n\n下面我们看下richebase的实现：\nvar RichBase = Base.extend(&#123;  EVENTS:&#123;&#125;,  template:'',  init:function(config)&#123;    //存储配置项    this.__config = config    //解析代理事件    this._delegateEvent()    this.setUp()  &#125;,  //循环遍历EVENTS，使用jQuery的delegate代理到parentNode  _delegateEvent:function()&#123;    var self = this    var events = this.EVENTS || &#123;&#125;    var eventObjs,fn,select,type    var parentNode = this.get('parentNode') || $(document.body)    for (select in events) &#123;      eventObjs = events[select]      for (type in eventObjs) &#123;        fn = eventObjs[type]        parentNode.delegate(select,type,function(e)&#123;          fn.call(null,self,e)        &#125;)      &#125;    &#125;  &#125;,  //支持underscore的极简模板语法  //用来渲染模板，这边是抄的underscore的。非常简单的模板引擎，支持原生的js语法  _parseTemplate:function(str,data)&#123;    /**       * http://ejohn.org/blog/javascript-micro-templating/       * https://github.com/jashkenas/underscore/blob/0.1.0/underscore.js#L399       */    var fn = new Function('obj',                          'var p=[],print=function()&#123;p.push.apply(p,arguments);&#125;;' +                          'with(obj)&#123;p.push(\\'' + str                          .replace(/[\\r\\t\\n]/g, \" \")                          .split(\"&lt;%\").join(\"\\t\")                          .replace(/((^|%&gt;)[^\\t]*)'/g, \"$1\\r\")                          .replace(/\\t=(.*?)%&gt;/g, \"',$1,'\")                          .split(\"\\t\").join(\"');\")                          .split(\"%&gt;\").join(\"p.push('\")                          .split(\"\\r\").join(\"\\\\'\") +                          \"');&#125;return p.join('');\")    return data ? fn(data) : fn  &#125;,  //提供给子类覆盖实现  setUp:function()&#123;    this.render()  &#125;,  //用来实现刷新，只需要传入之前render时的数据里的key还有更新值，就可以自动刷新模板  setChuckdata:function(key,value)&#123;    var self = this    var data = self.get('__renderData')    //更新对应的值    data[key] = value    if (!this.template) return;    //重新渲染    var newHtmlNode = $(self._parseTemplate(this.template,data))    //拿到存储的渲染后的节点    var currentNode = self.get('__currentNode')    if (!currentNode) return;    //替换内容    currentNode.replaceWith(newHtmlNode)    self.set('__currentNode',newHtmlNode)  &#125;,  //使用data来渲染模板并且append到parentNode下面  render:function(data)&#123;    var self = this    //先存储起来渲染的data,方便后面setChuckdata获取使用    self.set('__renderData',data)    if (!this.template) return;    //使用_parseTemplate解析渲染模板生成html    //子类可以覆盖这个方法使用其他的模板引擎解析    var html = self._parseTemplate(this.template,data)    var parentNode = this.get('parentNode') || $(document.body)    var currentNode = $(html)    //保存下来留待后面的区域刷新    //存储起来，方便后面setChuckdata获取使用    self.set('__currentNode',currentNode)    parentNode.append(currentNode)  &#125;,  destroy:function()&#123;    var self = this    //去掉自身的事件监听    self.off()    //删除渲染好的dom节点    self.get('__currentNode').remove()    //去掉绑定的代理事件    var events = self.EVENTS || &#123;&#125;    var eventObjs,fn,select,type    var parentNode = self.get('parentNode')    for (select in events) &#123;      eventObjs = events[select]      for (type in eventObjs) &#123;        fn = eventObjs[type]        parentNode.undelegate(select,type,fn)      &#125;    &#125;  &#125;&#125;)\n\n主要做了两件事，一个就是事件的解析跟代理，全部代理到parentNode上面。另外就是把render抽出来，用户只需要实现setUp方法。如果需要模板支持就在setUp里面调用render来渲染模板，并且可以通过setChuckdata来刷新模板，实现单向绑定。\n结语有了richbase，基本上组件开发就没啥问题了。但是我们还是可以继续深挖下去。\n比如组件自动化加载渲染，局部刷新，比如父子组件的嵌套，再比如双向绑定，再比如实现ng-click这种风格的事件机制。\n当然这些东西已经不属于组件里面的内容了。再进一步其实已经是一个框架了。实际上最近比较流行的react，ploymer还有我们的brix等等都是实现了这套东西。受限于篇幅，这个以后有空再写篇文章详细分析下。\n","tags":["JavaScript","前端开发"]},{"title":"Java应用性能调优实践","url":"/2020/07/21/Java%E5%BA%94%E7%94%A8%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98%E5%AE%9E%E8%B7%B5/","content":"概述Java 应用性能优化是一个老生常谈的话题，典型的性能问题如页面响应慢、接口超时，服务器负载高、并发数低，数据库频繁死锁等。尤其是在”糙快猛”的互联网开发模式大行其道的今天，随着系统访问量的日益增加和代码的臃肿，各种性能问题开始纷至沓来。Java 应用性能的瓶颈点非常多，比如磁盘、内存、网络 I/O 等系统因素，Java 应用代码，JVM GC，数据库，缓存等。笔者根据个人经验，将 Java 性能优化分为 4 个层级：应用层、数据库层、框架层、JVM 层，如图 1 所示。\n\n图 1.Java 性能优化分层模型\n\n每层优化难度逐级增加，涉及的知识和解决的问题也会不同。比如应用层需要理解代码逻辑，通过 Java 线程栈定位有问题代码行等；数据库层面需要分析 SQL、定位死锁等；框架层需要懂源代码，理解框架机制；JVM 层需要对 GC 的类型和工作机制有深入了解，对各种 JVM 参数作用了然于胸。\n围绕 Java 性能优化，有两种最基本的分析方法：现场分析法和事后分析法。现场分析法通过保留现场，再采用诊断工具分析定位。现场分析对线上影响较大，部分场景（特别是涉及到用户关键的在线业务时）不太合适。事后分析法需要尽可能多收集现场数据，然后立即恢复服务，同时针对收集的现场数据进行事后分析和复现。下面我们从性能诊断工具出发，分享搜狗商业平台在其中的一些案例与实践。\n性能诊断工具性能诊断一种是针对已经确定有性能问题的系统和代码进行诊断，还有一种是对预上线系统提前性能测试，确定性能是否符合上线要求。本文主要针对前者，后者可以用各种性能压测工具（例如 JMeter）进行测试，不在本文讨论范围内。针对 Java 应用，性能诊断工具主要分为两层：OS 层面和 Java 应用层面（包括应用代码诊断和 GC 诊断）。\nOS 诊断OS 的诊断主要关注的是 CPU、Memory、I/O 三个方面。\nCPU 诊断对于 CPU 主要关注平均负载（Load Average），CPU 使用率，上下文切换次数（Context Switch）。\n通过 top 命令可以查看系统平均负载和 CPU 使用率，图 2 为通过 top 命令查看某系统的状态。\n\n图 2.top 命令示例\n\n平均负载有三个数字：63.66，58.39，57.18，分别表示过去 1 分钟、5 分钟、15 分钟机器的负载。按照经验，若数值小于 0.7*CPU 个数，则系统工作正常；若超过这个值，甚至达到 CPU 核数的四五倍，则系统的负载就明显偏高。图 2 中 15 分钟负载已经高达 57.18，1 分钟负载是 63.66（系统为 16 核），说明系统出现负载问题，且存在进一步升高趋势，需要定位具体原因了。\n通过 vmstat 命令可以查看 CPU 的上下文切换次数，如图 3 所示：\n\n图 3.vmstat 命令示例\n\n上下文切换次数发生的场景主要有如下几种：1）时间片用完，CPU 正常调度下一个任务；2）被其它优先级更高的任务抢占；3）执行任务碰到 I/O 阻塞，挂起当前任务，切换到下一个任务；4）用户代码主动挂起当前任务让出 CPU；5）多任务抢占资源，由于没有抢到被挂起；6）硬件中断。Java 线程上下文切换主要来自共享资源的竞争。一般单个对象加锁很少成为系统瓶颈，除非锁粒度过大。但在一个访问频度高，对多个对象连续加锁的代码块中就可能出现大量上下文切换，成为系统瓶颈。比如在我们系统中就曾出现 log4j 1.x 在较大并发下大量打印日志，出现频繁上下文切换，大量线程阻塞，导致系统吞吐量大降的情况，其相关代码如清单 1 所示，升级到 log4j 2.x 才解决这个问题。\n\n清单 1. log4j 1.x 同步代码片段\n\nfor(Category c = this; c != null; c=c.parent) &#123;    // Protected against simultaneous call to addAppender, removeAppender,...    synchronized(c) &#123;        if (c.aai != null) &#123;            write += c.aai.appendLoopAppenders(event);        &#125;        ...    &#125;&#125;\n\nMemory从操作系统角度，内存关注应用进程是否足够，可以使用 free –m 命令查看内存的使用情况。通过 top 命令可以查看进程使用的虚拟内存 VIRT 和物理内存 RES，根据公式 VIRT = SWAP + RES 可以推算出具体应用使用的交换分区（Swap）情况，使用交换分区过大会影响 Java 应用性能，可以将 swappiness 值调到尽可能小。因为对于 Java 应用来说，占用太多交换分区可能会影响性能，毕竟磁盘性能比内存慢太多。\nI/OI/O 包括磁盘 I/O 和网络 I/O，一般情况下磁盘更容易出现 I/O 瓶颈。通过 iostat 可以查看磁盘的读写情况，通过 CPU 的 I/O wait 可以看出磁盘 I/O 是否正常。如果磁盘 I/O 一直处于很高的状态，说明磁盘太慢或故障，成为了性能瓶颈，需要进行应用优化或者磁盘更换。\n除了常用的 top、 ps、vmstat、iostat 等命令，还有其他 Linux 工具可以诊断系统问题，如 mpstat、tcpdump、netstat、pidstat、sar 等。Brendan 总结列出了 Linux 不同设备类型的性能诊断工具，如图 4 所示，可供参考。\n\n图 4.Linux 性能观测工具\n\n\nJava 应用诊断工具应用代码诊断应用代码性能问题是相对好解决的一类性能问题。通过一些应用层面监控报警，如果确定有问题的功能和代码，直接通过代码就可以定位；或者通过 top+jstack，找出有问题的线程栈，定位到问题线程的代码上，也可以发现问题。对于更复杂，逻辑更多的代码段，通过 Stopwatch 打印性能日志往往也可以定位大多数应用代码性能问题。\n常用的 Java 应用诊断包括线程、堆栈、GC 等方面的诊断。\njstackjstack 命令通常配合 top 使用，通过 top -H -p pid 定位 Java 进程和线程，再利用 jstack -l pid 导出线程栈。由于线程栈是瞬态的，因此需要多次 dump，一般 3 次 dump，一般每次隔 5s 就行。将 top 定位的 Java 线程 pid 转成 16 进制，得到 Java 线程栈中的 nid，可以找到对应的问题线程栈。\n\n图 5. 通过 top –H -p 查看运行时间较长 Java 线程\n\n如图 5 所示，其中的线程 24985 运行时间较长，可能存在问题，转成 16 进制后，通过 Java 线程栈找到对应线程 0x6199 的栈如下，从而定位问题点，如图 6 所示。\n\n图 6.jstack 查看线程堆栈\n\n\nJProfilerJProfiler 可对 CPU、堆、内存进行分析，功能强大，如图 7 所示。同时结合压测工具，可以对代码耗时采样统计。\n\n图 7. 通过 JProfiler 进行内存分析\n\n\nGC 诊断Java GC 解决了程序员管理内存的风险，但 GC 引起的应用暂停成了另一个需要解决的问题。JDK 提供了一系列工具来定位 GC 问题，比较常用的有 jstat、jmap，还有第三方工具 MAT 等。\njstatjstat 命令可打印 GC 详细信息，Young GC 和 Full GC 次数，堆信息等。其命令格式为\njstat –gcxxx -t pid ，如图 8 所示。\n\n图 8.jstat 命令示例\n\n\njmapjmap 打印 Java 进程堆信息 jmap –heap pid。通过 jmap –dump:file=xxx pid 可 dump 堆到文件，然后通过其它工具进一步分析其堆使用情况\nMATMAT 是 Java 堆的分析利器，提供了直观的诊断报告，内置的 OQL 允许对堆进行类 SQL 查询，功能强大，outgoing reference 和 incoming reference 可以对对象引用追根溯源。\n\n图 9.MAT 示例\n\n图 9 是 MAT 使用示例，MAT 有两列显示对象大小，分别是 Shallow size 和 Retained size，前者表示对象本身占用内存的大小，不包含其引用的对象，后者是对象自己及其直接或间接引用的对象的 Shallow size 之和，即该对象被回收后 GC 释放的内存大小，一般说来关注后者大小即可。对于有些大堆 (几十 G) 的 Java 应用，需要较大内存才能打开 MAT。通常本地开发机内存过小，是无法打开的，建议在线下服务器端安装图形环境和 MAT，远程打开查看。或者执行 mat 命令生成堆索引，拷贝索引到本地，不过这种方式看到的堆信息有限。\n为了诊断 GC 问题，建议在 JVM 参数中加上-XX:+PrintGCDateStamps。常用的 GC 参数如图 10 所示。\n\n图 10. 常用 GC 参数\n\n对于 Java 应用，通过 top+jstack+jmap+MAT 可以定位大多数应用和内存问题，可谓必备工具。有些时候，Java 应用诊断需要参考 OS 相关信息，可使用一些更全面的诊断工具，比如 Zabbix（整合了 OS 和 JVM 监控）等。在分布式环境中，分布式跟踪系统等基础设施也对应用性能诊断提供了有力支持。\n性能优化实践在介绍了一些常用的性能诊断工具后，下面将结合我们在 Java 应用调优中的一些实践，从 JVM 层、应用代码层以及数据库层进行案例分享。\nJVM 调优：GC 之痛搜狗商业平台某系统重构时选择 RMI 作为内部远程调用协议，系统上线后开始出现周期性的服务停止响应，暂停时间由数秒到数十秒不等。通过观察 GC 日志，发现服务自启动后每小时会出现一次 Full GC。由于系统堆设置较大，Full GC 一次暂停应用时间会较长，这对线上实时服务影响较大。经过分析，在重构前系统没有出现定期 Full GC 的情况，因此怀疑是 RMI 框架层面的问题。通过公开资料，发现 RMI 的 GDC（Distributed Garbage Collection，分布式垃圾收集）会启动守护线程定期执行 Full GC 来回收远程对象，清单 2 中展示了其守护线程代码。\n\n清单 2.DGC 守护线程源代码\n\nprivate static class Daemon extends Thread &#123;    public void run() &#123;        for (;;) &#123;            //...            long d = maxObjectInspectionAge();            if (d &gt;= l) &#123;                System.gc();                d = 0;            &#125;            //...        &#125;    &#125;&#125;\n\n显示更多\n定位问题后解决起来就比较容易了。一种是通过增加-XX:+DisableExplicitGC 参数，直接禁用系统 GC 的显示调用，但对使用 NIO 的系统，会有堆外内存溢出的风险。另一种方式是通过调大 -Dsun.rmi.dgc.server.gcInterval 和-Dsun.rmi.dgc.client.gcInterval 参数，增加 Full GC 间隔，同时增加参数-XX:+ExplicitGCInvokesConcurrent，将一次完全 Stop-The-World 的 Full GC 调整为一次并发 GC 周期，减少应用暂停时间，同时对 NIO 应用也不会造成影响。从图 11 可知，调整之后的 Full GC 次数 在 3 月之后明显减少。\n\n图 11.Full GC 监控统计\n\nGC 调优对高并发大数据量交互的应用还是很有必要的，尤其是默认 JVM 参数通常不满足业务需求，需要进行专门调优。GC 日志的解读有很多公开的资料，本文不再赘述。GC 调优目标基本有三个思路：降低 GC 频率，可以通过增大堆空间，减少不必要对象生成；降低 GC 暂停时间，可以通过减少堆空间，使用 CMS GC 算法实现；避免 Full GC，调整 CMS 触发比例，避免 Promotion Failure 和 Concurrent mode failure（老年代分配更多空间，增加 GC 线程数加快回收速度），减少大对象生成等。\n应用层调优：嗅到代码的坏味道从应用层代码调优入手，剖析代码效率下降的根源，无疑是提高 Java 应用性能的很好的手段之一。\n某商业广告系统（采用 Nginx 进行负载均衡）某次日常上线后，其中有几台机器负载急剧升高，CPU 使用率迅速打满。我们对线上进行了紧急回滚，并通过 jmap 和 jstack 对其中某台服务器的现场进行保存。\n\n图 12. 通过 MAT 分析堆栈现场\n\n堆栈现场如图 12 所示，根据 MAT 对 dump 数据的分析，发现最多的内存对象为 byte[] 和 java.util.HashMap $Entry，且 java.util.HashMap $Entry 对象存在循环引用。初步定位在该 HashMap 的 put 过程中有可能出现了死循环问题（图中 java.util.HashMap $Entry 0x2add6d992cb8 和 0x2add6d992ce8 的 next 引用形成循环）。查阅相关文档定位这属于典型的并发使用的场景错误 (http://bugs.java.com/bugdatabase/view_bug.do?bug_id=6423457) ，简要的说就是 HashMap 本身并不具备多线程并发的特性，在多个线程同时 put 操作的情况下，内部数组进行扩容时会导致 HashMap 的内部链表形成环形结构，从而出现死循环。\n针对此次上线，最大的改动在于通过内存缓存网站数据来提升系统性能，同时使用了懒加载机制，如清单 3 所示。\n\n清单 3. 网站数据懒加载代码\n\nprivate static Map&lt;Long, UnionDomain&gt; domainMap = new HashMap&lt;Long, UnionDomain&gt;();private boolean isResetDomains() &#123;    if (CollectionUtils.isEmpty(domainMap)) &#123;        // 从远端 http 接口获取网站详情        List&lt;UnionDomain&gt; newDomains = unionDomainHttpClient            .queryAllUnionDomain();        if (CollectionUtils.isEmpty(domainMap)) &#123;            domainMap = new HashMap&lt;Long, UnionDomain&gt;();            for (UnionDomain domain : newDomains) &#123;                if (domain != null) &#123;                    domainMap.put(domain.getSubdomainId(), domain);                &#125;            &#125;        &#125;        return true;    &#125;    return false;&#125;\n\n显示更多\n可以看到此处的 domainMap 为静态共享资源，它是 HashMap 类型，在多线程情况下会导致其内部链表形成环形结构，出现死循环。\n通过对前端 Nginx 的连接和访问日志可以看到，由于在系统重启后 Nginx 积攒了大量的用户请求，在 Resin 容器启动，大量用户请求涌入应用系统，多个用户同时进行网站数据的请求和初始化工作，导致 HashMap 出现并发问题。在定位故障原因后解决方法则比较简单，主要的解决方法有：\n（1）采用 ConcurrentHashMap 或者同步块的方式解决上述并发问题;\n（2）在系统启动前完成网站缓存加载，去除懒加载等；\n（3）采用分布式缓存替换本地缓存等。\n对于坏代码的定位，除了常规意义上的代码审查外，借助诸如 MAT 之类的工具也可以在一定程度对系统性能瓶颈点进行快速定位。但是一些与特定场景绑定或者业务数据绑定的情况，却需要辅助代码走查、性能检测工具、数据模拟甚至线上引流等方式才能最终确认性能问题的出处。以下是我们总结的一些坏代码可能的一些特征，供大家参考：\n（1）代码可读性差，无基本编程规范；\n（2）对象生成过多或生成大对象，内存泄露等；\n（3）IO 流操作过多，或者忘记关闭；\n（4）数据库操作过多，事务过长;\n（5）同步使用的场景错误;\n（6）循环迭代耗时操作等。\n数据库层调优：死锁噩梦对于大部分 Java 应用来说，与数据库进行交互的场景非常普遍，尤其是 OLTP 这种对于数据一致性要求较高的应用，数据库的性能会直接影响到整个应用的性能。搜狗商业平台系统作为广告主的广告发布和投放平台，对其物料的实时性和一致性都有极高的要求，我们在关系型数据库优化方面也积累了一定的经验。\n对于广告物料库来说，较高的操作频繁度（特别是通过批量物料工具操作）很极易造成数据库的死锁情况发生，其中一个比较典型的场景是广告物料调价。客户往往会频繁的对物料的出价进行调整，从而间接给数据库系统造成较大的负载压力，也加剧了死锁发生的可能性。下面以搜狗商业平台某广告系统广告物料调价的案例进行说明。\n某商业广告系统某天访问量突增，造成系统负载升高以及数据库频繁死锁，死锁语句如图 13 所示。\n\n图 13. 死锁语句\n\n其中，groupdomain 表上索引为 idx_groupdomain_accountid (accountid)，idx_groupdomain_groupid(groupid)，primary(groupdomainid) 三个单索引结构，采用 Mysql innodb 引擎。\n此场景发生在更新组出价时，场景中存在着组、组行业（groupindus 表）和组网站（groupdomain 表）。当更新组出价时，若组行业出价使用组出价（通过 isusegroupprice 标示，若为 1 则使用组出价）。同时若组网站出价使用组行业出价（通过 isuseindusprice 标示，若为 1 则使用组行业出价）时，也需要同时更新其组网站出价。由于每个组下面最大可以有 3000 个网站，因此在更新组出价时会长时间的对相关记录进行锁定。从上面发生死锁的问题可以看到，事务 1 和事务 2 均选择了 idx_groupdomain_accountid 的单列索引。根据 Mysql innodb 引擎加锁的特点，在一次事务中只会选择一个索引使用，而且如果一旦使用二级索引进行加锁后，会尝试将主键索引进行加锁。进一步分析可知事务 1 在请求事务 2 持有的idx_groupdomain_accountid二级索引加锁（加锁范围”space id 5726 page no 8658 n bits 824 index”），但是事务 2 已获得该二级索引 (“space id 5726 page no 8658 n bits 824 index”) 上所加的锁，在等待请求锁定主键索引 PRIMARY 索引上的锁。由于事务 2 等待执行时间过长或长时间不释放锁，导致事务 1 最终发生回滚。\n通过对当天访问日志跟踪可以看到，当天有客户通过脚本方式发起大量的修改推广组出价的操作，导致有大量事务在循环等待前一个事务释放锁定的主键 PRIMARY 索引。该问题的根源实际上在于 Mysql innodb 引擎对于索引利用有限，在 Oracle 数据库中此问题并不突出。解决的方式自然是希望单个事务锁定的记录数越少越好，这样产生死锁的概率也会大大降低。最终使用了（accountid, groupid）的复合索引，缩小了单个事务锁定的记录条数，也实现了不同计划下的推广组数据记录的隔离，从而减少该类死锁的发生几率。\n通常来说，对于数据库层的调优我们基本上会从以下几个方面出发：\n（1）在 SQL 语句层面进行优化：慢 SQL 分析、索引分析和调优、事务拆分等；\n（2）在数据库配置层面进行优化：比如字段设计、调整缓存大小、磁盘 I/O 等数据库参数优化、数据碎片整理等；\n（3）从数据库结构层面进行优化：考虑数据库的垂直拆分和水平拆分等；\n（4）选择合适的数据库引擎或者类型适应不同场景，比如考虑引入 NoSQL 等。\n总结与建议性能调优同样遵循 2-8 原则，80%的性能问题是由 20%的代码产生的，因此优化关键代码事半功倍。同时，对性能的优化要做到按需优化，过度优化可能引入更多问题。对于 Java 性能优化，不仅要理解系统架构、应用代码，同样需要关注 JVM 层甚至操作系统底层。总结起来主要可以从以下几点进行考虑：\n1）基础性能的调优\n这里的基础性能指的是硬件层级或者操作系统层级的升级优化，比如网络调优，操作系统版本升级，硬件设备优化等。比如 F5 的使用和 SDD 硬盘的引入，包括新版本 Linux 在 NIO 方面的升级，都可以极大的促进应用的性能提升；\n2）数据库性能优化\n包括常见的事务拆分，索引调优，SQL 优化，NoSQL 引入等，比如在事务拆分时引入异步化处理，最终达到一致性等做法的引入，包括在针对具体场景引入的各类 NoSQL 数据库，都可以大大缓解传统数据库在高并发下的不足；\n3）应用架构优化\n引入一些新的计算或者存储框架，利用新特性解决原有集群计算性能瓶颈等；或者引入分布式策略，在计算和存储进行水平化，包括提前计算预处理等，利用典型的空间换时间的做法等；都可以在一定程度上降低系统负载；\n4）业务层面的优化\n技术并不是提升系统性能的唯一手段，在很多出现性能问题的场景中，其实可以看到很大一部分都是因为特殊的业务场景引起的，如果能在业务上进行规避或者调整，其实往往是最有效的。\n转自：https://developer.ibm.com/zh/articles/j-lo-performance-tuning-practice/\n","tags":["Java"]},{"title":"Java中Unsafe详细介绍","url":"/2020/06/03/Java%E4%B8%ADUnsafe%E8%AF%A6%E7%BB%86%E4%BB%8B%E7%BB%8D/","content":"前言Unsafe是位于sun.misc包下的一个类，主要提供一些用于执行低级别、不安全操作的方法，如直接访问系统内存资源、自主管理内存资源等，这些方法在提升Java运行效率、增强Java语言底层资源操作能力方面起到了很大的作用。但由于Unsafe类使Java语言拥有了类似C语言指针一样操作内存空间的能力，这无疑也增加了程序发生相关指针问题的风险。在程序中过度、不正确使用Unsafe类会使得程序出错的概率变大，使得Java这种安全的语言变得不再“安全”，因此对Unsafe的使用一定要慎重。\n注：本文对sun.misc.Unsafe公共API功能及相关应用场景进行介绍。\n基本介绍如下Unsafe源码所示，Unsafe类为一单例实现，提供静态方法getUnsafe获取Unsafe实例，当且仅当调用getUnsafe方法的类为引导类加载器所加载时才合法，否则抛出SecurityException异常。\npublic final class Unsafe &#123;  // 单例对象  private static final Unsafe theUnsafe;  private Unsafe() &#123;  &#125;  @CallerSensitive  public static Unsafe getUnsafe() &#123;    Class var0 = Reflection.getCallerClass();    // 仅在引导类加载器`BootstrapClassLoader`加载时才合法    if(!VM.isSystemDomainLoader(var0.getClassLoader())) &#123;      throw new SecurityException(\"Unsafe\");    &#125; else &#123;      return theUnsafe;    &#125;  &#125;&#125;\n\n那如若想使用这个类，该如何获取其实例？有如下两个可行方案。\n其一，从getUnsafe方法的使用限制条件出发，通过Java命令行命令-Xbootclasspath/a把调用Unsafe相关方法的类A所在jar包路径追加到默认的bootstrap路径中，使得A被引导类加载器加载，从而通过Unsafe.getUnsafe方法安全的获取Unsafe实例。\njava -Xbootclasspath/a: $&#123;path&#125;   // 其中path为调用Unsafe相关方法的类所在jar包路径\n\n其二，通过反射获取单例对象theUnsafe。\nprivate static Unsafe reflectGetUnsafe() &#123;    try &#123;      Field field = Unsafe.class.getDeclaredField(\"theUnsafe\");      field.setAccessible(true);      return (Unsafe) field.get(null);    &#125; catch (Exception e) &#123;      log.error(e.getMessage(), e);      return null;    &#125;&#125;\n\n功能介绍\n如上图所示，Unsafe提供的API大致可分为内存操作、CAS、Class相关、对象操作、线程调度、系统信息获取、内存屏障、数组操作等几类，下面将对其相关方法和应用场景进行详细介绍。\n内存操作这部分主要包含堆外内存的分配、拷贝、释放、给定地址值操作等方法。\n//分配内存, 相当于C++的malloc函数public native long allocateMemory(long bytes);//扩充内存public native long reallocateMemory(long address, long bytes);//释放内存public native void freeMemory(long address);//在给定的内存块中设置值public native void setMemory(Object o, long offset, long bytes, byte value);//内存拷贝public native void copyMemory(Object srcBase, long srcOffset, Object destBase, long destOffset, long bytes);//获取给定地址值，忽略修饰限定符的访问限制。与此类似操作还有: getInt，getDouble，getLong，getChar等public native Object getObject(Object o, long offset);//为给定地址设置值，忽略修饰限定符的访问限制，与此类似操作还有: putInt,putDouble，putLong，putChar等public native void putObject(Object o, long offset, Object x);//获取给定地址的byte类型的值（当且仅当该内存地址为allocateMemory分配时，此方法结果为确定的）public native byte getByte(long address);//为给定地址设置byte类型的值（当且仅当该内存地址为allocateMemory分配时，此方法结果才是确定的）public native void putByte(long address, byte x);\n\n通常，我们在Java中创建的对象都处于堆内内存（heap）中，堆内内存是由JVM所管控的Java进程内存，并且它们遵循JVM的内存管理机制，JVM会采用垃圾回收机制统一管理堆内存。与之相对的是堆外内存，存在于JVM管控之外的内存区域，Java中对堆外内存的操作，依赖于Unsafe提供的操作堆外内存的native方法。\n使用堆外内存的原因\n对垃圾回收停顿的改善。由于堆外内存是直接受操作系统管理而不是JVM，所以当我们使用堆外内存时，即可保持较小的堆内内存规模。从而在GC时减少回收停顿对于应用的影响。\n提升程序I/O操作的性能。通常在I/O通信过程中，会存在堆内内存到堆外内存的数据拷贝操作，对于需要频繁进行内存间数据拷贝且生命周期较短的暂存数据，都建议存储到堆外内存。\n\n典型应用DirectByteBuffer是Java用于实现堆外内存的一个重要类，通常用在通信过程中做缓冲池，如在Netty、MINA等NIO框架中应用广泛。DirectByteBuffer对于堆外内存的创建、使用、销毁等逻辑均由Unsafe提供的堆外内存API来实现。\n下图为DirectByteBuffer构造函数，创建DirectByteBuffer的时候，通过Unsafe.allocateMemory分配内存、Unsafe.setMemory进行内存初始化，而后构建Cleaner对象用于跟踪DirectByteBuffer对象的垃圾回收，以实现当DirectByteBuffer被垃圾回收时，分配的堆外内存一起被释放。\n\n那么如何通过构建垃圾回收追踪对象Cleaner实现堆外内存释放呢？\nCleaner继承自Java四大引用类型之一的虚引用PhantomReference（众所周知，无法通过虚引用获取与之关联的对象实例，且当对象仅被虚引用引用时，在任何发生GC的时候，其均可被回收），通常PhantomReference与引用队列ReferenceQueue结合使用，可以实现虚引用关联对象被垃圾回收时能够进行系统通知、资源清理等功能。如下图所示，当某个被Cleaner引用的对象将被回收时，JVM垃圾收集器会将此对象的引用放入到对象引用中的pending链表中，等待Reference-Handler进行相关处理。其中，Reference-Handler为一个拥有最高优先级的守护线程，会循环不断的处理pending链表中的对象引用，执行Cleaner的clean方法进行相关清理工作。\n\n所以当DirectByteBuffer仅被Cleaner引用（即为虚引用）时，其可以在任意GC时段被回收。当DirectByteBuffer实例对象被回收时，在Reference-Handler线程操作中，会调用Cleaner的clean方法根据创建Cleaner时传入的Deallocator来进行堆外内存的释放。\n\nCAS相关如下源代码释义所示，这部分主要为CAS相关操作的方法。\n/**\t*  CAS  * @param o         包含要修改field的对象  * @param offset    对象中某field的偏移量  * @param expected  期望值  * @param update    更新值  * @return          true | false  */public final native boolean compareAndSwapObject(Object o, long offset,  Object expected, Object update);public final native boolean compareAndSwapInt(Object o, long offset, int expected,int update);public final native boolean compareAndSwapLong(Object o, long offset, long expected, long update);\n\n什么是CAS? 即比较并替换，实现并发算法时常用到的一种技术。CAS操作包含三个操作数——内存位置、预期原值及新值。执行CAS操作的时候，将内存位置的值与预期原值比较，如果相匹配，那么处理器会自动将该位置值更新为新值，否则，处理器不做任何操作。我们都知道，CAS是一条CPU的原子指令（cmpxchg指令），不会造成所谓的数据不一致问题，Unsafe提供的CAS方法（如compareAndSwapXXX）底层实现即为CPU指令cmpxchg。\n典型应用CAS在java.util.concurrent.atomic相关类、Java AQS、CurrentHashMap等实现上有非常广泛的应用。如下图所示，AtomicInteger的实现中，静态字段valueOffset即为字段value的内存偏移地址，valueOffset的值在AtomicInteger初始化时，在静态代码块中通过Unsafe的objectFieldOffset方法获取。在AtomicInteger中提供的线程安全方法中，通过字段valueOffset的值可以定位到AtomicInteger对象中value的内存地址，从而可以根据CAS实现对value字段的原子操作。\n\n下图为某个AtomicInteger对象自增操作前后的内存示意图，对象的基地址baseAddress=“0x110000”，通过baseAddress+valueOffset得到value的内存地址valueAddress=“0x11000c”；然后通过CAS进行原子性的更新操作，成功则返回，否则继续重试，直到更新成功为止。\n\n线程调度这部分，包括线程挂起、恢复、锁机制等方法。\n//取消阻塞线程public native void unpark(Object thread);//阻塞线程public native void park(boolean isAbsolute, long time);//获得对象锁（可重入锁）@Deprecatedpublic native void monitorEnter(Object o);//释放对象锁@Deprecatedpublic native void monitorExit(Object o);//尝试获取对象锁@Deprecatedpublic native boolean tryMonitorEnter(Object o);\n\n如上源码说明中，方法park、unpark即可实现线程的挂起与恢复，将一个线程进行挂起是通过park方法实现的，调用park方法后，线程将一直阻塞直到超时或者中断等条件出现；unpark可以终止一个挂起的线程，使其恢复正常。\n典型应用Java锁和同步器框架的核心类AbstractQueuedSynchronizer，就是通过调用LockSupport.park()和LockSupport.unpark()实现线程的阻塞和唤醒的，而LockSupport的park、unpark方法实际是调用Unsafe的park、unpark方式来实现。\nClass相关此部分主要提供Class和它的静态字段的操作相关方法，包含静态字段内存定位、定义类、定义匿名类、检验&amp;确保初始化等。\n//获取给定静态字段的内存地址偏移量，这个值对于给定的字段是唯一且固定不变的public native long staticFieldOffset(Field f);//获取一个静态类中给定字段的对象指针public native Object staticFieldBase(Field f);//判断是否需要初始化一个类，通常在获取一个类的静态属性的时候（因为一个类如果没初始化，它的静态属性也不会初始化）使用。 当且仅当ensureClassInitialized方法不生效时返回false。public native boolean shouldBeInitialized(Class&lt;?&gt; c);//检测给定的类是否已经初始化。通常在获取一个类的静态属性的时候（因为一个类如果没初始化，它的静态属性也不会初始化）使用。public native void ensureClassInitialized(Class&lt;?&gt; c);//定义一个类，此方法会跳过JVM的所有安全检查，默认情况下，ClassLoader（类加载器）和ProtectionDomain（保护域）实例来源于调用者public native Class&lt;?&gt; defineClass(String name, byte[] b, int off, int len, ClassLoader loader, ProtectionDomain protectionDomain);//定义一个匿名类public native Class&lt;?&gt; defineAnonymousClass(Class&lt;?&gt; hostClass, byte[] data, Object[] cpPatches);\n\n典型应用从Java 8开始，JDK使用invokedynamic及VM Anonymous Class结合来实现Java语言层面上的Lambda表达式。\n\ninvokedynamic： invokedynamic是Java 7为了实现在JVM上运行动态语言而引入的一条新的虚拟机指令，它可以实现在运行期动态解析出调用点限定符所引用的方法，然后再执行该方法，invokedynamic指令的分派逻辑是由用户设定的引导方法决定。\nVM Anonymous Class：可以看做是一种模板机制，针对于程序动态生成很多结构相同、仅若干常量不同的类时，可以先创建包含常量占位符的模板类，而后通过Unsafe.defineAnonymousClass方法定义具体类时填充模板的占位符生成具体的匿名类。生成的匿名类不显式挂在任何ClassLoader下面，只要当该类没有存在的实例对象、且没有强引用来引用该类的Class对象时，该类就会被GC回收。故而VM Anonymous Class相比于Java语言层面的匿名内部类无需通过ClassClassLoader进行类加载且更易回收。\n\n在Lambda表达式实现中，通过invokedynamic指令调用引导方法生成调用点，在此过程中，会通过ASM动态生成字节码，而后利用Unsafe的defineAnonymousClass方法定义实现相应的函数式接口的匿名类，然后再实例化此匿名类，并返回与此匿名类中函数式方法的方法句柄关联的调用点；而后可以通过此调用点实现调用相应Lambda表达式定义逻辑的功能。下面以如下图所示的Test类来举例说明。\n\nTest类编译后的class文件反编译后的结果如下图一所示（删除了对本文说明无意义的部分），我们可以从中看到main方法的指令实现、invokedynamic指令调用的引导方法BootstrapMethods、及静态方法lambda$main$0（实现了Lambda表达式中字符串打印逻辑）等。在引导方法执行过程中，会通过Unsafe.defineAnonymousClass生成如下图二所示的实现Consumer接口的匿名类。其中，accept方法通过调用Test类中的静态方法lambda$main$0来实现Lambda表达式中定义的逻辑。而后执行语句consumer.accept（&quot;lambda&quot;）其实就是调用下图二所示的匿名类的accept方法。\n\n对象操作此部分主要包含对象成员属性相关操作及非常规的对象实例化方式等相关方法。\n//返回对象成员属性在内存地址相对于此对象的内存地址的偏移量public native long objectFieldOffset(Field f);//获得给定对象的指定地址偏移量的值，与此类似操作还有：getInt，getDouble，getLong，getChar等public native Object getObject(Object o, long offset);//给定对象的指定地址偏移量设值，与此类似操作还有：putInt，putDouble，putLong，putChar等public native void putObject(Object o, long offset, Object x);//从对象的指定偏移量处获取变量的引用，使用volatile的加载语义public native Object getObjectVolatile(Object o, long offset);//存储变量的引用到对象的指定的偏移量处，使用volatile的存储语义public native void putObjectVolatile(Object o, long offset, Object x);//有序、延迟版本的putObjectVolatile方法，不保证值的改变被其他线程立即看到。只有在field被volatile修饰符修饰时有效public native void putOrderedObject(Object o, long offset, Object x);//绕过构造方法、初始化代码来创建对象public native Object allocateInstance(Class&lt;?&gt; cls) throws InstantiationException;\n\n典型应用\n常规对象实例化方式：我们通常所用到的创建对象的方式，从本质上来讲，都是通过new机制来实现对象的创建。但是，new机制有个特点就是当类只提供有参的构造函数且无显示声明无参构造函数时，则必须使用有参构造函数进行对象构造，而使用有参构造函数时，必须传递相应个数的参数才能完成对象实例化。\n非常规的实例化方式：而Unsafe中提供allocateInstance方法，仅通过Class对象就可以创建此类的实例对象，而且不需要调用其构造函数、初始化代码、JVM安全检查等。它抑制修饰符检测，也就是即使构造器是private修饰的也能通过此方法实例化，只需提类对象即可创建相应的对象。由于这种特性，allocateInstance在java.lang.invoke、Objenesis（提供绕过类构造器的对象生成方式）、Gson（反序列化时用到）中都有相应的应用。\n\n如下图所示，在Gson反序列化时，如果类有默认构造函数，则通过反射调用默认构造函数创建实例，否则通过UnsafeAllocator来实现对象实例的构造，UnsafeAllocator通过调用Unsafe的allocateInstance实现对象的实例化，保证在目标类无默认构造函数时，反序列化不够影响。\n\n数组相关这部分主要介绍与数据操作相关的arrayBaseOffset与arrayIndexScale这两个方法，两者配合起来使用，即可定位数组中每个元素在内存中的位置。\n//返回数组中第一个元素的偏移地址public native int arrayBaseOffset(Class&lt;?&gt; arrayClass);//返回数组中一个元素占用的大小public native int arrayIndexScale(Class&lt;?&gt; arrayClass);\n\n典型应用这两个与数据操作相关的方法，在java.util.concurrent.atomic 包下的AtomicIntegerArray（可以实现对Integer数组中每个元素的原子性操作）中有典型的应用，如下图AtomicIntegerArray源码所示，通过Unsafe的arrayBaseOffset、arrayIndexScale分别获取数组首元素的偏移地址base及单个元素大小因子scale。后续相关原子性操作，均依赖于这两个值进行数组中元素的定位，如下图二所示的getAndAdd方法即通过checkedByteOffset方法获取某数组元素的偏移地址，而后通过CAS实现原子性操作。\n\n内存屏障在Java 8中引入，用于定义内存屏障（也称内存栅栏，内存栅障，屏障指令等，是一类同步屏障指令，是CPU或编译器在对内存随机访问的操作中的一个同步点，使得此点之前的所有读写操作都执行后才可以开始执行此点之后的操作），避免代码重排序。\n//内存屏障，禁止load操作重排序。屏障前的load操作不能被重排序到屏障后，屏障后的load操作不能被重排序到屏障前public native void loadFence();//内存屏障，禁止store操作重排序。屏障前的store操作不能被重排序到屏障后，屏障后的store操作不能被重排序到屏障前public native void storeFence();//内存屏障，禁止load、store操作重排序public native void fullFence();\n\n典型应用在Java 8中引入了一种锁的新机制——StampedLock，它可以看成是读写锁的一个改进版本。StampedLock提供了一种乐观读锁的实现，这种乐观读锁类似于无锁的操作，完全不会阻塞写线程获取写锁，从而缓解读多写少时写线程“饥饿”现象。由于StampedLock提供的乐观读锁不阻塞写线程获取读锁，当线程共享变量从主内存load到线程工作内存时，会存在数据不一致问题，所以当使用StampedLock的乐观读锁时，需要遵从如下图用例中使用的模式来确保数据的一致性。\n\n如上图用例所示计算坐标点Point对象，包含点移动方法move及计算此点到原点的距离的方法distanceFromOrigin。在方法distanceFromOrigin中，首先，通过tryOptimisticRead方法获取乐观读标记；然后从主内存中加载点的坐标值 (x,y)；而后通过StampedLock的validate方法校验锁状态，判断坐标点(x,y)从主内存加载到线程工作内存过程中，主内存的值是否已被其他线程通过move方法修改，如果validate返回值为true，证明(x, y)的值未被修改，可参与后续计算；否则，需加悲观读锁，再次从主内存加载(x,y)的最新值，然后再进行距离计算。其中，校验锁状态这步操作至关重要，需要判断锁状态是否发生改变，从而判断之前copy到线程工作内存中的值是否与主内存的值存在不一致。\n下图为StampedLock.validate方法的源码实现，通过锁标记与相关常量进行位运算、比较来校验锁状态，在校验逻辑之前，会通过Unsafe的loadFence方法加入一个load内存屏障，目的是避免上图用例中步骤②和StampedLock.validate中锁状态校验运算发生重排序导致锁状态校验不准确的问题。\n\n系统相关这部分包含两个获取系统相关信息的方法。\n//返回系统指针的大小。返回值为4（32位系统）或 8（64位系统）。public native int addressSize();//内存页的大小，此值为2的幂次方。public native int pageSize();\n\n典型应用如下图所示的代码片段，为java.nio下的工具类Bits中计算待申请内存所需内存页数量的静态方法，其依赖于Unsafe中pageSize方法获取系统内存页大小实现后续计算逻辑。\n\n结语本文对Java中的sun.misc.Unsafe的用法及应用场景进行了基本介绍，我们可以看到Unsafe提供了很多便捷、有趣的API方法。即便如此，由于Unsafe中包含大量自主操作内存的方法，如若使用不当，会对程序带来许多不可控的灾难。因此对它的使用我们需要慎之又慎。\n","tags":["Java"]},{"title":"Java命名规范参考","url":"/2020/03/23/Java%E5%91%BD%E5%90%8D%E8%A7%84%E8%8C%83%E5%8F%82%E8%80%83/","content":"\n转自：https://mp.weixin.qq.com/s/O7U4XU-4QysmvL6cRfVfvw\n\n一，Java中的命名规范好的命名能体现出代码的特征，含义或者是用途，让阅读者可以根据名称的含义快速厘清程序的脉络。不同语言中采用的命名形式大相径庭，Java中常用到的命名形式共有三种，既首字母大写的UpperCamelCase，首字母小写的lowerCamelCase以及全部大写的并用下划线分割单词的UPPERCAMELUNSER_SCORE。通常约定，类一般采用大驼峰命名，方法和局部变量使用小驼峰命名，而大写下划线命名通常是常量和枚举中使用。\n\n\n\n类型(名)\n约束\n例\n\n\n\n项目\n全部小写 多个单词用中划线分隔‘-’\nspring-cloud\n\n\n包\n全部小写\ncom.alibaba.fastjson\n\n\n类\n单词首字母大写\nFeature, FieldDeserializer\n\n\n变量\n首字母小写 多个单词组成时， 除首个单词 其他单词首字母都要大写\npassword,  userName\n\n\n常量\n全部大写，多个单词，用’_’分隔\nCACHEEXPIREDTIME\n\n\n方法\n同变量\nread(),  getById(Long id)\n\n\n二，包命名包名统一使用小写，点分隔符之间有且仅有一个自然语义的英文单词或者多个单词自然连接到一块（如 springframework，deepspace不需要使用任何分割）。包名统一使用单数形式，如果类命有复数含义，则可以使用复数形式。\n包名的构成可以分为以下几四部分【前缀】 【发起者名】【项目名】【模块名】。常见的前缀可以分为以下几种：\n\n\n\n前缀\n例\n含义\n\n\n\nindi 或 onem\nindi.发起者名.项目名.模块名.……\n个体项目 个人发起，但非自己独自完成 可公开或私有项目， copyright主要属于发起者。\n\n\npers\npers.个人名.项目名.模块名.……\n个人项目 指个人发起，独自完成， 可分享的项目 copyright主要属于个人\n\n\npriv\npriv.个人名.项目名.模块名.……\n私有项目，指个人发起，独自完成 非公开的私人使用的项目， copyright属于个人。\n\n\nteam\nteam.团队名.项目名.模块名.……\n团队项目，指由团队发起 并由该团队开发的项目 copyright属于该团队所有\n\n\n顶级域名\ncom.公司名.项目名.模块名.……\n公司项目 copyright由项目发起的公司所有\n\n\n三，类命名类名使用大驼峰命名形式，类命通常时名词或名词短语，接口名除了用名词和名词短语以外，还可以使用形容词或形容词短语，如Cloneable，Callable等，表示实现该接口的类有某种功能或能力。对于测试类则以它要测试的类开头，以Test结尾，如HashMapTest。\n对于一些特殊特有名词缩写也可以使用全大写命名，比如XMLHttpRequest，不过笔者认为缩写三个字母以内都大写，超过三个字母则按照要给单词算。这个没有标准如阿里巴巴中fastjson用JSONObject作为类命，而google则使用JsonObjectRequest命名，对于这种特殊的缩写，原则是统一就好。\n\n\n\n属性(类)\n约束\n例\n\n\n\n抽象\nAbstract  或 Base 开头\nBaseUserService\n\n\n枚举\nEnum 作为后缀\nOSType\n\n\n工具\nUtils作为后缀\nStringUtils\n\n\n异常\nException结尾\nRuntimeException\n\n\n接口实现\n接口名+ Impl\nUserServiceImpl\n\n\n领域模型相\n/DO/DTO/VO/DAO\n正例：UserDAO 反例：UserDao\n\n\n设计模式相关\nBuilder，Factory等\n当使用到设计模式时 要使用对应的设计模式作为后缀 如ThreadFactory\n\n\n处理特定功能\nHandler，Predicate Validator\n表示处理器，校验器，断言 这些类工厂还有配套的方法名 如handle，predicate，validate\n\n\n测试\nTest后缀\nUserServiceTest 表示用来测试UserService类的\n\n\nMVC分层\nController，Service ServiceImpl，DAO 后缀\nUserManageController UserManageDAO\n\n\n四，方法方法命名采用小驼峰的形式，首字小写，往后的每个单词首字母都要大写。和类名不同的是，方法命名一般为动词或动词短语，与参数或参数名共同组成动宾短语，即动词 + 名词。一个好的函数名一般能通过名字直接获知该函数实现什么样的功能。\n4.1 返回真伪值的方法注：pre- prefix前缀，suf- suffix后缀，alo-alone 单独使用\n\n\n\n位置\n单词\n意义\n例\n\n\n\npre\nis\n对象是否符合期待的状态\nisValid\n\n\npre\ncan\n对象能否执行所期待的动作\ncanRemove\n\n\npre\nshould\n调用方执行某个命令 或方法是好还是不好 应不应该， 或者说推荐还是不推荐\nshouldMigrate\n\n\npre\nhas\n对象是否持有 所期待的数据和属性\nhasObservers\n\n\npre\nneeds\n调用方是否需要 执行某个命令或方法\nneedsMigrate\n\n\n4.2 用来检查的方法\n\n\n单词\n意义\n例\n\n\n\nensure\n检查是否为期待的状态 不是则抛出异常或返回error code\nensureCapacity\n\n\nvalidate\n检查是否为正确的状态 不是则抛出异常或返回error code\nvalidateInputs\n\n\n4.3 按需求才执行的方法\n\n\n位置\n单词\n意义\n例\n\n\n\nsuf\nIfNeeded\n需要的时候执行 不需要则什么都不做\ndrawIfNeeded\n\n\npre\nmight\n同上\nmightCreate\n\n\npre\ntry\n尝试执行 失败时抛出异常 或是返回errorcode\ntryCreate\n\n\nsuf\nOrDefault\n尝试执行 失败时返回默认值\ngetOrDefault\n\n\nsuf\nOrElse\n尝试执行 失败时返回 实际参数中指定的值\ngetOrElse\n\n\npre\nforce\n强制尝试执行 error抛出异常或是返回值\nforceCreate,  forceStop\n\n\n4.4 异步相关方法\n\n\n位置\n单词\n意义\n例\n\n\n\npre\nblocking\n线程阻塞方法\nblockingGetUser\n\n\nsuf\nInBackground\n执行在后台线程\ndoInBackground\n\n\nsuf\nAsync\n异步方法\nsendAsync\n\n\nsuf\nSync\n同步方法\nsendSync\n\n\npre / alo\nschedule\nJob和Tas k放入队列\nschedule,  scheduleJob\n\n\npre / alo\npost\n同上\npostJob\n\n\npre / alo\nexecute\n执行异步 或同步方法\nexecute, executeTask\n\n\npre / alo\nstart\n同上\nstar, tstartJob\n\n\npre / alo\ncancel\n停止异步方法\ncance, cancelJob\n\n\npre / alo\nstop\n同上\nstop,stopJob\n\n\n4.5 回调方法\n\n\n位置\n单词\n意义\n例\n\n\n\npre\non\n事件发生时执行\nonCompleted\n\n\npre\nbefore\n事件发生前执行\nbeforeUpdate\n\n\npre\npre\n同上\npreUpdate\n\n\npre\nwill\n同上\nwillUpdate\n\n\npre\nafter\n事件发生后执行\nafterUpdate\n\n\npre\npost\n同上\npostUpdate\n\n\npre\ndid\n同上\ndidUpdate\n\n\npre\nshould\n确认事件 是否可以执行\nshouldUpdate\n\n\n4.6 操作对象生命周期的方法\n\n\n单词\n意义\n例\n\n\n\ninitialize\n初始化或延迟初始化使用\ninitialize\n\n\npause\n暂停\nonPause , pause\n\n\nstop\n停止\nonStop, stop\n\n\nabandon\n销毁的替代\nabandon\n\n\ndestroy\n同上\ndestroy\n\n\ndispose\n同上\ndispose\n\n\n4.7 与集合操作相关的方法\n\n\n单词\n意义\n例\n\n\n\ncontains\n是包含指定对象相同的对象\ncontains\n\n\nadd\n添加\naddJob\n\n\nappend\n添加\nappendJob\n\n\ninsert\n插入到下标n\ninsertJob\n\n\nput\n添加与key对应的元素\nputJob\n\n\nremove\n移除元素\nremoveJob\n\n\nenqueue\n添加到队列的最末位\nenqueueJob\n\n\ndequeue\n从队列中头部取出并移除\ndequeueJob\n\n\npush\n添加到栈头\npushJob\n\n\npop\n从栈头取出并移除\npopJob\n\n\npeek\n从栈头取出但不移除\npeekJob\n\n\nfind\n寻找符合条件的某物\nfindById\n\n\n4.8 与数据相关的方法\n\n\n单词\n意义\n例\n\n\n\ncreate\n新创建\ncreateAccount\n\n\nnew\n新创建\nnewAccount\n\n\nfrom\n从既有的某物新建 或是从其他的数据新建\nfromConfig\n\n\nto\n转换\ntoString\n\n\nupdate\n更新既有某物\nupdateAccount\n\n\nload\n读取\nloadAccount\n\n\nfetch\n远程读取\nfetchAccount\n\n\ndelete\n删除\ndeleteAccount\n\n\nremove\n删除\nremoveAccount\n\n\nsave\n保存\nsaveAccount\n\n\nstore\n保存\nstoreAccount\n\n\ncommit\n保存\ncommitChange\n\n\napply\n保存或应用\napplyChange\n\n\nclear\n清除或是恢复到初始状态\nclearAll\n\n\nreset\n清除或是恢复到初始状态\nresetAll\n\n\n4.9 成对出现的动词\n\n\n单词\n意义\n\n\n\nget获取\nset 设置\n\n\nadd 增加\nremove 删除\n\n\ncreate 创建\ndestory 移除\n\n\nstart 启动\nstop 停止\n\n\nopen 打开\nclose 关闭\n\n\nread 读取\nwrite 写入\n\n\nload 载入\nsave 保存\n\n\ncreate 创建\ndestroy 销毁\n\n\nbegin 开始\nend 结束\n\n\nbackup 备份\nrestore 恢复\n\n\nimport 导入\nexport 导出\n\n\nsplit 分割\nmerge 合并\n\n\ninject 注入\nextract 提取\n\n\nattach 附着\ndetach 脱离\n\n\nbind 绑定\nseparate 分离\n\n\nview 查看\nbrowse 浏览\n\n\nedit 编辑\nmodify 修改\n\n\nselect 选取\nmark 标记\n\n\ncopy 复制\npaste 粘贴\n\n\nundo 撤销\nredo 重做\n\n\ninsert 插入\ndelete 移除\n\n\nadd 加入\nappend 添加\n\n\nclean 清理\nclear 清除\n\n\nindex 索引\nsort 排序\n\n\nfind 查找\nsearch 搜索\n\n\nincrease 增加\ndecrease 减少\n\n\nplay 播放\npause 暂停\n\n\nlaunch 启动\nrun 运行\n\n\ncompile 编译\nexecute 执行\n\n\ndebug 调试\ntrace 跟踪\n\n\nobserve 观察\nlisten 监听\n\n\nbuild 构建\npublish 发布\n\n\ninput 输入\noutput 输出\n\n\nencode 编码\ndecode 解码\n\n\nencrypt 加密\ndecrypt 解密\n\n\ncompress 压缩\ndecompress 解压缩\n\n\npack 打包\nunpack 解包\n\n\nparse 解析\nemit 生成\n\n\nconnect 连接\ndisconnect 断开\n\n\nsend 发送\nreceive 接收\n\n\ndownload 下载\nupload 上传\n\n\nrefresh 刷新\nsynchronize 同步\n\n\nupdate 更新\nrevert 复原\n\n\nlock 锁定\nunlock 解锁\n\n\ncheck out 签出\ncheck in 签入\n\n\nsubmit 提交\ncommit 交付\n\n\npush 推\npull 拉\n\n\nexpand 展开\ncollapse 折叠\n\n\nbegin 起始\nend 结束\n\n\nstart 开始\nfinish 完成\n\n\nenter 进入\nexit 退出\n\n\nabort 放弃\nquit 离开\n\n\nobsolete 废弃\ndepreciate 废旧\n\n\ncollect 收集\naggregate 聚集\n\n\n五，变量&amp;常量命名5.1 变量命名变量是指在程序运行中可以改变其值的量，包括成员变量和局部变量。变量名由多单词组成时，第一个单词的首字母小写，其后单词的首字母大写，俗称骆驼式命名法（也称驼峰命名法），如 computedValues，index，变量命名时，尽量简短且能清楚的表达变量的作用，命名体现具体的业务含义即可。\n变量名不应以下划线或美元符号开头，尽管这在语法上是允许的。变量名应简短且富于描述。变量名的选用应该易于记忆，即，能够指出其用途。尽量避免单个字符的变量名，除非是一次性的临时变量。pojo中的布尔变量，都不要加is(数据库中的布尔字段全都要加 is_ 前缀)。\n5.2 常量命名常量命名CONSTANT_CASE，一般采用全部大写（作为方法参数时除外），单词间用下划线分割。那么什么是常量呢？\n常量是在作用域内保持不变的值，一般使用final进行修饰。一般分为三种，全局常量（public static final修饰），类内常量（private static final 修饰）以及局部常量（方法内，或者参数中的常量），局部常量比较特殊，通常采用小驼峰命名即可。\n/** * 一个demo * @author Jann Lee * @date 2019-12-07 00:25 **/public class HelloWorld &#123;    /**     * 局部常量(正例)     */    public static final long USER_MESSAGE_CACHE_EXPIRE_TIME = 3600;    /**     * 局部常量(反例，命名不清晰）     */    public static final long MESSAGE_CACHE_TIME = 3600;    /**     * 全局常量     */    private static final String ERROR_MESSAGE = \" error message\";    /**     * 成员变量     */    private int currentUserId;    /**     * 控制台打印 &#123;@code message&#125; 信息     * @param message 消息体，局部常量     */    public void sayHello(final String message) &#123;        System.out.println(\"Hello world!\");    &#125;&#125;\n\n常量一般都有自己的业务含义,不要害怕长度过长而进行省略或者缩写。如，用户消息缓存过期时间的表示，那种方式更佳清晰，交给你来评判。更多BAT经验文章，可以在订阅号“码匠笔记”后台回复“经验”，N+1篇热文免费获取。\n通用命名规则\n尽量不要使用拼音；杜绝拼音和英文混用。对于一些通用的表示或者难以用英文描述的可以采用拼音，一旦采用拼音就坚决不能和英文混用。正例：BeiJing，HangZhou 反例：validateCanShu\n命名过程中尽量不要出现特殊的字符，常量除外。\n尽量不要和jdk或者框架中已存在的类重名，也不能使用java中的关键字命名。\n妙用介词，如for(可以用同音的4代替), to(可用同音的2代替)，from，with，of等。如类名采用User4RedisDO，方法名getUserInfoFromRedis，convertJson2Map等。\n\n六，代码注解6.1 注解的原则好的命名增加代码阅读性，代码的命名往往有严格的限制。而注解不同，程序员往往可以自由发挥，单并不意味着可以为所欲为之胡作非为。优雅的注解通常要满足三要素。\n\nNothing is strange 没有注解的代码对于阅读者非常不友好，哪怕代码写的在清除，阅读者至少从心理上会有抵触，更何况代码中往往有许多复杂的逻辑，所以一定要写注解，不仅要记录代码的逻辑，还有说清楚修改的逻辑。\nLess is more 从代码维护角度来讲，代码中的注解一定是精华中的精华。合理清晰的命名能让代码易于理解，对于逻辑简单且命名规范，能够清楚表达代码功能的代码不需要注解。滥用注解会增加额外的负担，更何况大部分都是废话。\n\n// 根据id获取信息【废话注解】getMessageById(id)\n\n\nAdvance with the time 注解应该随着代码的变动而改变，注解表达的信息要与代码中完全一致。通常情况下修改代码后一定要修改注解。\n\n6.2 注解格式注解大体上可以分为两种，一种是javadoc注解，另一种是简单注解。javadoc注解可以生成JavaAPI为外部用户提供有效的支持javadoc注解通常在使用IDEA，或者Eclipse等开发工具时都可以自动生成，也支持自定义的注解模板，仅需要对对应的字段进行解释。参与同一项目开发的同学，尽量设置成相同的注解模板。\na. 包注解包注解在工作中往往比较特殊，通过包注解可以快速知悉当前包下代码是用来实现哪些功能，强烈建议工作中加上，尤其是对于一些比较复杂的包，包注解一般在包的根目录下，名称统一为package-info.java。\n/** * 落地也质量检测 * 1. 用来解决什么问题对广告主投放的广告落地页进行性能检测，模拟不同的系统，如Android，IOS等; 模拟不同的网络：2G，3G，4G，wifi等 * 2. 如何实现基于chrome浏览器，用chromedriver驱动浏览器，设置对应的网络，OS参数，获取到浏览器返回结果。 * 注意：网络环境配置信息&#123;@link cn.mycookies.landingpagecheck.meta.NetWorkSpeedEnum&#125;目前使用是常规速度，可以根据实际情况进行调整 * * @author cruder * @time 2019/12/7 20:3 下午 */package cn.mycookies.landingpagecheck;\n\nb. 类注接javadoc注解中，每个类都必须有注解。\n/*** Copyright (C), 2019-2020, Jann  balabala...** 类的介绍：这是一个用来做什么事情的类，有哪些功能，用到的技术.....** @author   类创建者姓名 保持对齐* @date     创建日期 保持对齐* @version  版本号 保持对齐*/\n\nc. 属性注解在每个属性前面必须加上属性注释，通常有一下两种形式，至于怎么选择，你高兴就好，不过一个项目中要保持统一。\n/** 提示信息 */private String userName;/** * 密码 */private String password;\n\nd. 方法注释在每个方法前面必须加上方法注释，对于方法中的每个参数，以及返回值都要有说明。\n/**  * 方法的详细说明，能干嘛，怎么实现的，注意事项...  *  * @param xxx      参数1的使用说明， 能否为null  * @return 返回结果的说明， 不同情况下会返回怎样的结果  * @throws 异常类型   注明从此类方法中抛出异常的说明  */\n\ne. 构造方法注释在每个构造方法前面必须加上注释，注释模板如下：\n/**  * 构造方法的详细说明  *  * @param xxx      参数1的使用说明， 能否为null  * @throws 异常类型   注明从此类方法中抛出异常的说明  */\n\n而简单注解往往是需要工程师字节定义，在使用注解时应该注意一下几点：\n\n枚举类的各个属性值都要使用注解，枚举可以理解为是常量，通常不会发生改变，通常会被在多个地方引用，对枚举的修改和添加属性通常会带来很大的影响。\n保持排版整洁，不要使用行尾注释；双斜杠和星号之后要用1个空格分隔。\n\nint id = 1; // 反例：不要使用行尾注释//反例：换行符与注释之间没有缩进int age = 18;// 正例：姓名String name;/**  * 1. 多行注释  *  * 2. 对于不同的逻辑说明，可以用空行分隔  */\n\n总结无论是命名和注解，他们的目的都是为了让代码和工程师进行对话，增强代码的可读性，可维护性。优秀的代码往往能够见名知意，注解往往是对命名的补充和完善。命名太难了！\n","tags":["Java"]},{"title":"Linux系统中bash的四种模式","url":"/2019/12/09/Linux%E7%B3%BB%E7%BB%9F%E4%B8%ADbash%E7%9A%84%E5%9B%9B%E7%A7%8D%E6%A8%A1%E5%BC%8F/","content":"一、前言今天在配置jenkins的执行节点，但是执行节点shell的PATH变量始终不对，无法找到git命令。我先前已经在/etc/profile中配置了git的PATH,通过putty连接的shell中也检查PATH变量是正确的，且git命令也能正常执行。后来查阅资料才知道这个问题是由于我没有很好的理解bash的四种模式而造成的。\n解决办法：\n#!&#x2F;bin&#x2F;bash -ilex......对于e参数表示一旦出错,就退出当前的shell，x参数表示可以显示所执行的每一条命令\n\nLinux的bash的其实分为四种模式，bash会依据这四种模式而选择加载不同的配置文件，而且加载的顺序也有所不同.\n这四种bash模式分别是：\n1、interactive + login\n2、non-interactive + login\n3、interactive + non-login\n4、non-interactive + non-login\n本文在整理前人资料的基础上，着重介绍这四种bash模式在初始化时如何进行配置文件加载的。\n二、bash四种模式的shell（一）、interactive + login模式的shell第一种模式是交互式的登陆shell，这里面有两个概念需要解释：interactive和login：\nlogin故名思义，即登陆，login shell是指用户以非图形化界面或者以ssh登陆到机器上时获得的第一个shell，简单些说就是需要输入用户名和密码的shell。因此通常不管以何种方式登陆机器后用户获得的第一个shell就是login shell。\ninteractive意为交互式，这也很好理解，interactive  shell会有一个输入提示符，并且它的标准输入、输出和错误输出都会显示在控制台上。所以一般来说只要是需要用户交互的，即一个命令一个命令的输入的shell都是interactive  shell。而如果无需用户交互，它便是non-interactive shell。通常来说如bash script.sh此类执行脚本的命令就会启动一个non-interactive shell，它不需要与用户进行交互，执行完后它便会退出创建的shell。\n那么此模式最简单的两个例子为：\n\n用户直接登陆到机器获得的第一个shell\n用户使用ssh user@remote获得的shell\n\n加载配置文件\n这种模式下，shell首先加载/etc/profile，然后再尝试依次去加载下列三个配置文件之一，一旦找到其中一个便不再接着寻找：\n\n~/.bash_profile\n~/.bash_login\n~/.profile\n\n下面给出这个加载过程的伪代码：\nexecute /etc/profileIF ~/.bash_profile exists THENexecute ~/.bash_profileELSEIF ~/.bash_login exist THENexecute ~/.bash_loginELSEIF ~/.profile exist THENexecute ~/.profileEND IFEND IFEND IF\n\n为了验证这个过程，我们来做一些测试。首先设计每个配置文件的内容如下：\nuser@remote &gt; cat &#x2F;etc&#x2F;profileecho @ &#x2F;etc&#x2F;profileuser@remote &gt; cat ~&#x2F;.bash_profileecho @ ~&#x2F;.bash_profileuser@remote &gt; cat ~&#x2F;.bash_loginecho @ ~&#x2F;.bash_loginuser@remote &gt; cat ~&#x2F;.profileecho @ ~&#x2F;.profile\n\n然后打开一个login shell，注意，为方便起见，这里使用bash -l命令，它会打开一个login shell，在man bash中可以看到此参数的解释：\n\n-l Make bash act as if it had been invoked as a login shell\n\n进入这个新的login shell，便会得到以下输出：\n@ &#x2F;etc&#x2F;profile@ &#x2F;home&#x2F;user&#x2F;.bash_profile\n\n因为没有了~/.bash_profile的屏蔽，所以~/.bash_login被加载，但最后一个~/.profile仍被忽略。\n再次移除~/.bash_login，启动login shell的输出结果为：\n@ &#x2F;etc&#x2F;profile@ &#x2F;home&#x2F;user&#x2F;.profile\n\n~/.profile终于熬出头，得见天日。通过以上三个实验，配置文件的加载过程得到了验证，除去/etc/profile首先被加载外，其余三个文件的加载顺序为：~/.bash_profile&gt; ~/.bash_login &gt; ~/.profile，只要找到一个便终止查找。\n前面说过，使用ssh也会得到一个login shell，所以如果在另外一台机器上运行ssh user@remote时，也会得到上面一样的结论。\n配置文件的意义\n那么，为什么bash要弄得这么复杂？每个配置文件存在的意义是什么？\n/etc/profile很好理解，它是一个全局的配置文件。后面三个位于用户主目录中的配置文件都针对用户个人，也许你会问为什么要有这么多，只用一个~/.profile不好么？究竟每个文件有什么意义呢？这是个好问题。\nCameron Newham和Bill Rosenblatt在他们的著作《Learning the bash Shell, 2nd Edition》的59页解释了原因：\n\nbash allows two synonyms for .bash_profile: .bash_login, derived from  the C shell’s file named .login, and .profile, derived from the Bourne  shell and Korn shell files named .profile. Only one of these three is  read when you log in. If .bash_profile doesn’t exist in your home  directory, then bash will look for .bash_login. If that doesn’t exist it  will look for .profile.\nOne advantage of bash’s ability to look for either synonym is that  you can retain your .profile if you have been using the Bourne shell. If  you need to add bash-specific commands, you can put them in  .bash_profile followed by the command source .profile. When you log in,  all the bash-specific commands will be executed and bash will source  .profile, executing the remaining commands. If you decide to switch to  using the Bourne shell you don’t have to modify your existing files. A  similar approach was intended for .bash_login and the C shell .login,  but due to differences in the basic syntax of the shells, this is not a  good idea.\n\n原来一切都是为了兼容，这么设计是为了更好的应付在不同shell之间切换的场景。因为bash完全兼容Bourne shell，所以.bash_profile和.profile可以很好的处理bash和Bourne shell之间的切换。但是由于C shell和bash之间的基本语法存在着差异，作者认为引入.bash_login并不是个好主意。所以由此我们可以得出这样的最佳实践：\n\n应该尽量杜绝使用.bash_login，如果已经创建，那么需要创建.bash_profile来屏蔽它被调用\n.bash_profile适合放置bash的专属命令，可以在其最后读取.profile，如此一来，便可以很好的在Bourne shell和bash之间切换了\n\n（二）、non-interactive + login模式的shell第二种模式的shell为non-interactive login shell，即非交互式的登陆shell，这种是不太常见的情况。一种创建此shell的方法为：bash -l script.sh，前面提到过-l参数是将shell作为一个login shell启动，而执行脚本又使它为non-interactive shell。\n对于这种类型的shell，配置文件的加载与第一种完全一样，在此不再赘述。\n（三）、interactive + non-login模式的shell第三种模式为交互式的非登陆shell，这种模式最常见的情况为在一个已有shell中运行bash，此时会打开一个交互式的shell，而因为不再需要登陆，因此不是login shell。\n加载配置文件\n对于此种情况，启动shell时会去查找并加载/etc/bash.bashrc和~/.bashrc文件。\n为了进行验证，与第一种模式一样，设计各配置文件内容如下：\nuser@remote &gt; cat &#x2F;etc&#x2F;bash.bashrcecho @ &#x2F;etc&#x2F;bash.bashrcuser@remote &gt; cat ~&#x2F;.bashrcecho @ ~&#x2F;.bashrc\n\n然后我们启动一个交互式的非登陆shell，直接运行bash即可，可以得到以下结果：\n@ &#x2F;etc&#x2F;bash.bashrc@ &#x2F;home&#x2F;user&#x2F;.bashrc\n\nbashrc VS profile\n从刚引入的两个配置文件的存放路径可以很容易的判断，第一个文件是全局性的，第二个文件属于当前用户。在前面的模式当中，已经出现了几种配置文件，多数是以profile命名的，那么为什么这里又增加两个文件呢？这样不会增加复杂度么？我们来看看此处的文件和前面模式中的文件的区别。\n首先看第一种模式中的profile类型文件，它是某个用户唯一的用来设置全局环境变量的地方, 因为用户可以有多个shell比如bash,  sh, zsh等, 但像环境变量这种其实只需要在统一的一个地方初始化就可以, 而这个地方就是profile，所以启动一个login  shell会加载此文件，后面由此shell中启动的新shell进程如bash，sh，zsh等都可以由login shell中继承环境变量等配置。\n接下来看bashrc，其后缀rc的意思为Run Commands，由名字可以推断出，此处存放bash需要运行的命令，但注意，这些命令一般只用于交互式的shell，通常在这里会设置交互所需要的所有信息，比如bash的补全、alias、颜色、提示符等等。\n所以可以看出，引入多种配置文件完全是为了更好的管理配置，每个文件各司其职，只做好自己的事情。\n下面给出这个加载过程的伪代码：\n（四）、non-interactive + non-login模式的shell最后一种模式为非交互非登陆的shell，创建这种shell典型有两种方式：\n\nbash script.sh\nssh user@remote command\n\n这两种都是创建一个shell，执行完脚本之后便退出，不再需要与用户交互。\n加载配置文件\n对于这种模式而言，它会去寻找环境变量BASH_ENV，将变量的值作为文件名进行查找，如果找到便加载它。\n同样，我们对其进行验证。首先，测试该环境变量未定义时配置文件的加载情况，这里需要一个测试脚本：\nuser@remote &gt; cat ~&#x2F;script.shecho Hello World\n\n再次执行bash script.sh，结果为：\n@ &#x2F;home&#x2F;user&#x2F;.bashrcHello World\n\n果然，~/.bashrc被加载，而它是由环境变量BASH_ENV设定的。\n三、关于bash四种模式的直观示图至此，四种模式下配置文件如何加载已经讲完，因为涉及的配置文件有些多，我们再以两个图来更为直观的进行描述：\n第一张图来自这篇文章，bash的每种模式会读取其所在列的内容，首先执行A，然后是B，C。而B1，B2和B3表示只会执行第一个存在的文件：\n+----------------+--------+-----------+---------------+|                | login  |interactive|non-interactive||                |        |non-login  |non-login      |+----------------+--------+-----------+---------------+|/etc/profile    |   A    |           |               |+----------------+--------+-----------+---------------+|/etc/bash.bashrc|        |    A      |               |+----------------+--------+-----------+---------------+|~/.bashrc       |        |    B      |               |+----------------+--------+-----------+---------------+|~/.bash_profile |   B1   |           |               |+----------------+--------+-----------+---------------+|~/.bash_login   |   B2   |           |               |+----------------+--------+-----------+---------------+|~/.profile      |   B3   |           |               |+----------------+--------+-----------+---------------+|BASH_ENV        |        |           |       A       |+----------------+--------+-----------+---------------+\n\n上图只给出了三种模式，原因是第一种login实际上已经包含了两种，因为这两种模式下对配置文件的加载是一致的。\n另外一篇文章给出了一个更直观的图：\n\n上图的情况稍稍复杂一些，因为它使用了几个关于配置文件的参数：--login，--rcfile，--noprofile，--norc，这些参数的引入会使配置文件的加载稍稍发生改变，不过总体来说，不影响我们前面的讨论，相信这张图不会给你带来更多的疑惑。\n四、bash与sh陷阱ssh user@remote ~/myscript.sh属于哪一种模式？相信此时你可以非常轻松的回答出来：non-login + non-interactive。对于这种模式，bash会选择加载$BASH_ENV的值所对应的文件，所以为了让它加载/etc/profile，可以设定：\nuser@local &gt; export BASH_ENV&#x3D;&#x2F;etc&#x2F;profile\n\n然后执行上面的命令，依旧会出现如下错误：\n~/myscript.sh: line n: app: command not found\n\n这是怎么回事？这看起来像是环境变量引起的问题，为了证实这一猜想，我在这条命令之前加了一句：which app，来查看app的安装路径。在remote本机上执行脚本时，它会打印出app正确的安装路径。但再次用ssh来执行时，却遇到下面的错误：\nwhich: no app in (/usr/bin:/bin:/usr/sbin:/sbin)\n\n这很奇怪，怎么括号中的环境变量没有了app程序的安装路径？不是已通过/etc/profile设置到PATH中了？再次在脚本中加入echo $PATH并以ssh执行，这才发现，环境变量仍是系统初始化时的结果：\n/usr/bin:/bin:/usr/sbin:/sbin\n\n这证明/etc/profile根本没有被调用。为什么？我们已经将BASH_ENV的值设置成了/etc/profile。但是似乎并没有加载/etc/profile呢？\n仔细查看之后才发现脚本myscript.sh的第一行为#!/usr/bin/env sh，注意看，它和前面提到的#!/usr/bin/env bash不一样，可能就是这里出了问题。我们先尝试把它改成#!/usr/bin/env bash，再次执行，错误果然消失了，这与我们前面的分析结果一致。\n第一行的这个语句有什么用？设置成sh和bash有什么区别？带着这些疑问，再来查看man bash：\n\nIf the program is a file beginning with #!, the remainder of the first line specifies an interpreter for the program.\n\n它表示这个文件的解释器，即用什么程序来打开此文件，就好比Windows上双击一个文件时会以什么程序打开一样。因为这里不是bash，而是sh，那么我们前面讨论的都不复有效了，真糟糕。我们来看看这个sh的路径：\nuser@remote &gt; ll &#96;which sh&#96;lrwxrwxrwx 1 root root 9 Apr 25  2014 &#x2F;usr&#x2F;bin&#x2F;sh -&gt; &#x2F;bin&#x2F;bash\n\n原来sh只是bash的一个软链接，既然如此，BASH_ENV应该是有效的啊，为何此处无效？还是回到man bash，同样在INVOCATION一节的下部看到了这样的说明：\n\nIf bash is invoked with the name sh, it tries to mimic the startup  behavior of historical versions of sh as closely as possible, while  conforming to the POSIX standard as well. When invoked as an interactive  login shell, or a non-interactive shell with the –login option, it  first attempts to read and execute commands from /etc/profile and  ~/.profile, in that order. The –noprofile option may be used to inhibit  this behavior. When invoked as an interactive shell with the name sh,  bash looks for the variable ENV, expands its value if it is defined, and  uses the expanded value as the name of a file to read and execute.  Since a shell invoked as sh does not attempt to read and execute  commands from any other startup files, the –rcfile option has no effect.  A non-interactive shell invoked with the name sh does not attempt to  read any other startup files. When invoked as sh, bash enters posix mode  after the startup files are read.\n\n简而言之，当bash以是sh命启动时，即我们此处的情况，bash会尽可能的模仿sh，所以配置文件的加载变成了下面这样：\n\ninteractive + login: 读取/etc/profile和~/.profile\nnon-interactive + login: 同上\ninteractive + non-login: 读取ENV环境变量对应的文件\nnon-interactive + non-login: 不读取任何文件\n\n这样便可以解释为什么出错了，因为这里属于non-interactive + non-login，所以bash不会读取任何文件，故而即使设置了BASH_ENV也不会起作用。所以为了解决问题，只需要把sh换成bash，再设置环境变量BASH_ENV即可。\n另外，其实我们还可以设置参数到第一行的解释器中，如#!/bin/bash --login，如此一来，bash便会强制为login shell，所以/etc/profile也会被加载。相比上面那种方法，这种更为简单。\n五、总结与建议（一）、总结为了更好的理清这几种模式，下面我们对一些典型的启动方式各属于什么模式进行一个总结：\n\n登陆机器后的第一个shell：login + interactive\n新启动一个shell进程，如运行bash：non-login + interactive\n执行脚本，如bash script.sh：non-login + non-interactive\n运行头部有如#!/usr/bin/env bash的可执行文件，如./executable：non-login + non-interactive\n通过ssh登陆到远程主机：login + interactive\n远程执行脚本，如ssh user@remote script.sh：non-login + non-interactive\n远程执行脚本，同时请求控制台，如ssh user@remote -t &#39;echo $PWD&#39;：non-login + interactive\n在图形化界面中打开terminal：\nLinux上: non-login + interactive\nMac OS X上: login + interactive\n\n（二）、建议回顾一下前面提到的所有配置文件，总共有以下几种：\n\n/etc/profile\n~/.bash_profile\n~/.bash_login\n~/.profile\n/etc/bash.bashrc\n~/.bashrc\n$BASH_ENV\n$ENV\n\n不知你是否会有疑问，这么多的配置文件，究竟每个文件里面应该包含哪些配置，比如PATH应该在哪？提示符应该在哪配置？启动的程序应该在哪？等等。所以在文章的最后，我搜罗了一些最佳实践供各位参考。（这里只讨论属于用户个人的配置文件）\n\n~/.bash_profile：应该尽可能的简单，通常会在最后加载.profile和.bashrc(注意顺序)\n~/.bash_login：在前面讨论过，别用它\n~/.profile：此文件用于login shell，所有你想在整个用户会话期间都有效的内容都应该放置于此，比如启动进程，环境变量等\n~/.bashrc：只放置与bash有关的命令，所有与交互有关的命令都应该出现在此，比如bash的补全、alias、颜色、提示符等等。\n\n的。\n转自：https://blog.csdn.net/hudashi/article/details/82464995\n","tags":["Linux"]},{"title":"Linux文件路径介绍","url":"/2020/06/01/Linux%E6%96%87%E4%BB%B6%E8%B7%AF%E5%BE%84%E4%BB%8B%E7%BB%8D/","content":"大纲\n\n\n目录\n\n\n\n\n/bin\n存放二进制可执行文件(ls,cat,mkdir等)，常用命令一般都在这里。\n\n\n/etc\n存放系统管理和配置文件\n\n\n/home\n存放所有用户文件的根目录，是用户主目录的基点，比如用户user的主目录就是/home/user，可以用~user表示\n\n\n/usr\n用于存放系统应用程序，比较重要的目录/usr/local 本地系统管理员软件安装目录（安装系统级的应用）。这是最庞大的目录，要用到的应用程序和文件几乎都在这个目录。/usr/x11r6 存放x window的目录/usr/bin 众多的应用程序  /usr/sbin 超级用户的一些管理程序  /usr/doc linux文档  /usr/include linux下开发和编译应用程序所需要的头文件  /usr/lib 常用的动态链接库和软件包的配置文件  /usr/man 帮助文档  /usr/src 源代码，linux内核的源代码就放在/usr/src/linux里  /usr/local/bin 本地增加的命令  /usr/local/lib 本地增加的库\n\n\n/opt\n额外安装的可选应用程序包所放置的位置。一般情况下，我们可以把tomcat等都安装到这里。\n\n\n/proc\n虚拟文件系统目录，是系统内存的映射。可直接访问这个目录来获取系统信息。\n\n\n/root\n超级用户（系统管理员）的主目录（特权阶级^o^）\n\n\n/sbin\n存放二进制可执行文件，只有root才能访问。这里存放的是系统管理员使用的系统级别的管理命令和程序。如ifconfig等。\n\n\n/dev\n用于存放设备文件。\n\n\n/mnt\n系统管理员安装临时文件系统的安装点，系统提供这个目录是让用户临时挂载其他的文件系统。\n\n\n/boot\n存放用于系统引导时使用的各种文件\n\n\n/lib\n存放跟文件系统中的程序运行所需要的共享库及内核模块。共享库又叫动态链接共享库，作用类似windows里的.dll文件，存放了根文件系统程序运行所需的共享文件。\n\n\n/tmp\n用于存放各种临时文件，是公用的临时文件存储点。\n\n\n/var\n用于存放运行时需要改变数据的文件，也是某些大文件的溢出区，比方说各种服务的日志文件（系统启动日志等。）等。\n\n\n/lost+found\n这个目录平时是空的，系统非正常关机而留下“无家可归”的文件（windows下叫什么.chk）就在这里\n\n\n\n/bin 二进制可执行命令\n/dev 设备特殊文件\n/etc 系统管理和配置文件\n/etc/rc.d 启动的配置文件和脚本\n/home 用户主目录的基点，比如用户user的主目录就是/home/user，可以用~user表示\n/lib 标准程序设计库，又叫动态链接共享库，作用类似windows里的.dll文件\n/sbin 超级管理命令，这里存放的是系统管理员使用的管理程序\n/tmp 公共的临时文件存储点\n/root 系统管理员的主目录\n/mnt 系统提供这个目录是让用户临时挂载其他的文件系统\n/lost+found 这个目录平时是空的，系统非正常关机而留下“无家可归”的文件（windows下叫什么.chk）就在这里\n/proc 虚拟的目录，是系统内存的映射。可直接访问这个目录来获取系统信息。\n/var 某些大文件的溢出区，比方说各种服务的日志文件\n/usr 最庞大的目录，要用到的应用程序和文件几乎都在这个目录，其中包含：\n/usr/x11R6 存放x window的目录\n/usr/bin 众多的应用程序\n/usr/sbin 超级用户的一些管理程序\n/usr/doc linux文档\n/usr/include linux下开发和编译应用程序所需要的头文件\n/usr/lib 常用的动态链接库和软件包的配置文件\n/usr/man 帮助文档\n/usr/src 源代码，linux内核的源代码就放在/usr/src/linux里\n/usr/local/bin 本地增加的命令\n/usr/local/lib 本地增加的库根文件系统\n\n\n\n通常情况下，根文件系统所占空间一般应该比较小，因为其中的绝大部分文件都不需要经常改动，而且包括严格的文件和一个小的不经常改变的文件系统不容易损坏。除了可能的一个叫/ vmlinuz标准的系统引导映像之外，根目录一般不含任何文件。所有其他文件在根文件系统的子目录中。\n\n/bin目录/bin目录包含了引导启动所需的命令或普通用户可能用的命令(可能在引导启动后)。这些命令都是二进制文件的可执行程序(bin是binary–二进制的简称)，多是系统中重要的系统文件。\n/sbin目录/sbin目录类似/bin，也用于存储二进制文件。因为其中的大部分文件多是系统管理员使用的基本的系统程序，所以虽然普通用户必要且允许时可以使用，但一般不给普通用户使用。\n/etc目录/etc目录存放着各种系统配置文件，其中包括了用户信息文件/etc/passwd，系统初始化文件/etc/rc等。linux正是*这些文件才得以正常地运行。\n/root目录/root目录是超级用户的目录。\n/lib目录/lib目录是根文件系统上的程序所需的共享库，存放了根文件系统程序运行所需的共享文件。这些文件包含了可被许多程序共享的代码，以避免每个程序都包含有相同的子程序的副本，故可以使得可执行文件变得更小，节省空间。\n/lib/modules目录/lib/modules目录包含系统核心可加载各种模块，尤其是那些在恢复损坏的系统时重新引导系统所需的模块(例如网络和文件系统驱动)。\n/dev目录/dev目录存放了设备文件，即设备驱动程序，用户通过这些文件访问外部设备。比如，用户可以通过访问/dev/mouse来访问鼠标的输入，就像访问其他文件一样。\n/tmp目录/tmp目录存放程序在运行时产生的信息和数据。但在引导启动后，运行的程序最好使用/var/tmp来代替/tmp，因为前者可能拥有一个更大的磁盘空间。\n/boot目录/boot目录存放引导加载器(bootstraploader)使用的文件，如lilo，核心映像也经常放在这里，而不是放在根目录中。但是如果有许多核心映像，这个目录就可能变得很大，这时使用单独的文件系统会更好一些。还有一点要注意的是，要确保核心映像必须在ide硬盘的前1024柱面内。\n/mnt目录/mnt目录是系统管理员临时安装(mount)文件系统的安装点。程序并不自动支持安装到/mnt。/mnt下面可以分为许多子目录，例如/mnt/dosa可能是使用msdos文件系统的软驱，而/mnt/exta可能是使用ext2文件系统的软驱，/mnt/cdrom光驱等等。\n/proc,/usr,/var,/home目录其他文件系统的安装点。\n\n下面详细介绍；\n/etc文件系统/etc目录包含各种系统配置文件，下面说明其中的一些。其他的你应该知道它们属于哪个程序，并阅读该程序的man页。许多网络配置文件也在/etc中。\n\n/etc/rc或/etc/rc. d或/etc/rc?. d启动、或改变运行级时运行的脚本或脚本的目录。\n/etc/passwd用户数据库，其中的域给出了用户名、真实姓名、用户起始目录、加密口令和用户的其他信息。\n/etc/fdprm软盘参数表，用以说明不同的软盘格式。可用setfdprm进行设置。更多的信息见setfdprm的帮助页。\n/etc/fstab指定启动时需要自动安装的文件系统列表。也包括用swapon-a启用的swap区的信息。\n/etc/group类似/etc/passwd，但说明的不是用户信息而是组的信息。包括组的各种数据。\n/etc/inittabinit的配置文件。\n/etc/issue包括用户在登录提示符前的输出信息。通常包括系统的一段短说明或欢迎信息。具体内容由系统管理员确定。\n/etc/magic“file”的配置文件。包含不同文件格式的说明，“file”基于它猜测文件类型。\n/etc/motdmotd是messageoftheday的缩写，用户成功登录后自动输出。内容由系统管理员确定。常用于通告信息，如计划关机时间的警告等。\n/etc/mtab当前安装的文件系统列表。由脚本(scritp)初始化，并由mount命令自动更新。当需要一个当前安装的文件系统的列表时使用(例如df命令)。\n/etc/shadow在安装了影子(shadow)口令软件的系统上的影子口令文件。影子口令文件将/etc/passwd文件中的加密口令移动到/etc/shadow中，而后者只对超级用户(root)可读。这使破译口令更困难，以此增加系统的安全性。\n/etc/login. defslogin命令的配置文件。\n/etc/printcap类似/etc/termcap，但针对打印机。语法不同。\n/etc/profile、/etc/csh. login、/etc/csh. cshrc登录或启动时bourne或cshells执行的文件。这允许系统管理员为所有用户建立全局缺省环境。\n/etc/securetty确认安全终端，即哪个终端允许超级用户(root)登录。一般只列出虚拟控制台，这样就不可能(至少很困难)通过调制解调器(modem)或网络闯入系统并得到超级用户特权。\n/etc/shells列出可以使用的shell。chsh命令允许用户在本文件指定范围内改变登录的shell。提供一台机器ftp服务的服务进程ftpd检查用户shell是否列在/etc/shells文件中，如果不是，将不允许该用户登录。\n/etc/termcap终端性能数据库。说明不同的终端用什么“转义序列”控制。写程序时不直接输出转义序列(这样只能工作于特定品牌的终端)，而是从/etc/termcap中查找要做的工作的正确序列。这样，多数的程序可以在多数终端上运行。\n\n/dev文件系统/dev目录包括所有设备的设备文件。设备文件用特定的约定命名，这在设备列表中说明。设备文件在安装时由系统产生，以后可以用/dev/makedev描述。/dev/makedev. local是系统管理员为本地设备文件(或连接)写的描述文稿(即如一些非标准设备驱动不是标准makedev的一部分)。下面简要介绍/dev下一些常用文件。\n\n/dev/console系统控制台，也就是直接和系统连接的监视器。\n/dev/hdide硬盘驱动程序接口。如：/dev/hda指的是第一个硬盘，had1则是指/dev/hda的第一个分区。如系统中有其他的硬盘，则依次为/dev/hdb、/dev/hdc、. . . . . . ；如有多个分区则依次为hda1、hda2. . . . . .\n/dev/sdscsi磁盘驱动程序接口。如有系统有scsi硬盘，就不会访问/dev/had，而会访问/dev/sda。\n/dev/fd软驱设备驱动程序。如：/dev/fd0指系统的第一个软盘，也就是通常所说的a：盘，/dev/fd1指第二个软盘，. . . . . . 而/dev/fd1h1440则表示访问驱动器1中的4. 5高密盘。\n/dev/stscsi磁带驱动器驱动程序。\n/dev/tty提供虚拟控制台支持。如：/dev/tty1指的是系统的第一个虚拟控制台，/dev/tty2则是系统的第二个虚拟控制台。\n/dev/pty提供远程登陆伪终端支持。在进行telnet登录时就要用到/dev/pty设备。\n/dev/ttys计算机串行接口，对于dos来说就是“com1”口。\n/dev/cua计算机串行接口，与调制解调器一起使用的设备。\n/dev/null“黑洞”，所有写入该设备的信息都将消失。例如：当想要将屏幕上的输出信息隐藏起来时，只要将输出信息输入到/dev/null中即可。\n\n/usr文件系统/usr是个很重要的目录，通常这一文件系统很大，因为所有程序安装在这里。/usr里的所有文件一般来自linux发行版(distribution)；本地安装的程序和其他东西在/usr/local下，因为这样可以在升级新版系统或新发行版时无须重新安装全部程序。/usr目录下的许多内容是可选的，但这些功能会使用户使用系统更加有效。/usr可容纳许多大型的软件包和它们的配置文件。下面列出一些重要的目录(一些不太重要的目录被省略了)。\n\n/usr/x11r6包含xwindow系统的所有可执行程序、配置文件和支持文件。为简化x的开发和安装，x的文件没有集成到系统中。xwindow系统是一个功能强大的图形环境，提供了大量的图形工具程序。用户如果对microsoftwindows或machintosh比较熟悉的话，就不会对xwindow系统感到束手无策了。\n/usr/x386类似/usr/x11r6，但是是专门给x11release5的。\n/usr/bin集中了几乎所有用户命令，是系统的软件库。另有些命令在/bin或/usr/local/bin中。\n/usr/sbin包括了根文件系统不必要的系统管理命令，例如多数服务程序。\n/usr/man、/usr/info、/usr/doc这些目录包含所有手册页、gnu信息文档和各种其他文档文件。每个联机手册的“节”都有两个子目录。例如：/usr/man/man1中包含联机手册第一节的源码(没有格式化的原始文件)，/usr/man/cat1包含第一节已格式化的内容。l联机手册分为以下九节：内部命令、系统调用、库函数、设备、文件格式、游戏、宏软件包、系统管理和核心程序。\n/usr/include包含了c语言的头文件，这些文件多以. h结尾，用来描述c语言程序中用到的数据结构、子过程和常量。为了保持一致性，这实际上应该放在/usr/lib下，但习惯上一直沿用了这个名字。\n/usr/lib包含了程序或子系统的不变的数据文件，包括一些site-wide配置文件。名字lib来源于库(library);编程的原始库也存在/usr/lib里。当编译程序时，程序便会和其中的库进行连接。也有许多程序把配置文件存入其中。\n/usr/local本地安装的软件和其他文件放在这里。这与/usr很相似。用户可能会在这发现一些比较大的软件包，如tex、emacs等。\n\n/var文件系统/var包含系统一般运行时要改变的数据。通常这些数据所在的目录的大小是要经常变化或扩充的。原来/var目录中有些内容是在/usr中的，但为了保持/usr目录的相对稳定，就把那些需要经常改变的目录放到/var中了。每个系统是特定的，即不通过网络与其他计算机共享。下面列出一些重要的目录(一些不太重要的目录省略了)。\n\n/var/catman包括了格式化过的帮助(man)页。帮助页的源文件一般存在/usr/man/man中；有些man页可能有预格式化的版本，存在/usr/man/cat中。而其他的man页在第一次看时都需要格式化，格式化完的版本存在/var/man中，这样其他人再看相同的页时就无须等待格式化了。(/var/catman经常被清除，就像清除临时目录一样。)\n/var/lib存放系统正常运行时要改变的文件。\n/var/local存放/usr/local中安装的程序的可变数据(即系统管理员安装的程序)。注意，如果必要，即使本地安装的程序也会使用其他/var目录，例如/var/lock。\n/var/lock锁定文件。许多程序遵循在/var/lock中产生一个锁定文件的约定，以用来支持他们正在使用某个特定的设备或文件。其他程序注意到这个锁定文件时，就不会再使用这个设备或文件。\n/var/log各种程序的日志(log)文件，尤其是login(/var/log/wtmplog纪录所有到系统的登录和注销)和syslog(/var/log/messages纪录存储所有核心和系统程序信息)。/var/log里的文件经常不确定地增长，应该定期清除。\n/var/run保存在下一次系统引导前有效的关于系统的信息文件。例如，/var/run/utmp包含当前登录的用户的信息。\n/var/spool放置“假脱机(spool)”程序的目录，如mail、news、打印队列和其他队列工作的目录。每个不同的spool在/var/spool下有自己的子目录，例如，用户的邮箱就存放在/var/spool/mail中。\n/var/tmp比/tmp允许更大的或需要存在较长时间的临时文件。注意系统管理员可能不允许/var/tmp有很旧的文件。\n\n/proc文件系统/proc文件系统是一个伪的文件系统，就是说它是一个实际上不存在的目录，因而这是一个非常特殊的目录。它并不存在于某个磁盘上，而是由核心在内存中产生。这个目录用于提供关于系统的信。下面说明一些最重要的文件和目录(/proc文件系统在procman页中有更详细的说明)。\n\n/proc/x关于进程x的信息目录，这一x是这一进程的标识号。每个进程在/proc下有一个名为自己进程号的目录。\n/proc/cpuinfo存放处理器(cpu)的信息，如cpu的类型、制造商、型号和性能等。\n/proc/devices当前运行的核心配置的设备驱动的列表。\n/proc/dma显示当前使用的dma通道。\n/proc/filesystems核心配置的文件系统信息。\n/proc/interrupts显示被占用的中断信息和占用者的信息，以及被占用的数量。\n/proc/ioports当前使用的i/o端口。\n/proc/kcore系统物理内存映像。与物理内存大小完全一样，然而实际上没有占用这么多内存；它仅仅是在程序访问它时才被创建。(注意：除非你把它拷贝到什么地方，否则/proc下没有任何东西占用任何磁盘空间。)\n/proc/kmsg核心输出的消息。也会被送到syslog。\n/proc/ksyms核心符号表。\n/proc/loadavg系统“平均负载”；3个没有意义的指示器指出系统当前的工作量。\n/proc/meminfo各种存储器使用信息，包括物理内存和交换分区(swap)。\n/proc/modules存放当前加载了哪些核心模块信息。\n/proc/net网络协议状态信息。\n/proc/self存放到查看/proc的程序的进程目录的符号连接。当2个进程查看/proc时，这将会是不同的连接。这主要便于程序得到它自己的进程目录。\n/proc/stat系统的不同状态，例如，系统启动后页面发生错误的次数。\n/proc/uptime系统启动的时间长度。\n/proc/version核心版本\n\n","tags":["Linux"]},{"title":"图解ReentrantReadWriteLock实现分析","url":"/2020/06/03/%E5%9B%BE%E8%A7%A3ReentrantReadWriteLock%E5%AE%9E%E7%8E%B0%E5%88%86%E6%9E%90/","content":"概述本文主要分析JCU包中读写锁接口(ReadWriteLock)的重要实现类ReentrantReadWriteLock。主要实现读共享，写互斥功能，对比单纯的互斥锁在共享资源使用场景为频繁读取及少量修改的情况下可以较好的提高性能。\nReadWriteLock接口简单说明ReadWriteLock接口只定义了两个方法：\npublic interface ReadWriteLock &#123;    /**     * Returns the lock used for reading.     *     * @return the lock used for reading     */    Lock readLock();    /**     * Returns the lock used for writing.     *     * @return the lock used for writing     */    Lock writeLock();&#125;\n\n通过调用相应方法获取读锁或写锁，获取的读锁及写锁都是Lock接口的实现，可以如同使用Lock接口一样使用（其实也有一些特性是不支持的）。\nReentrantReadWriteLock使用示例读写锁的使用并不复杂，可以参考以下使用示例：\nclass RWDictionary &#123;    private final Map&lt;String, Data&gt; m = new TreeMap&lt;String, Data&gt;();    private final ReentrantReadWriteLock rwl = new ReentrantReadWriteLock();    private final Lock r = rwl.readLock();    private final Lock w = rwl.writeLock();    public Data get(String key) &#123;        r.lock();        try &#123; return m.get(key); &#125;        finally &#123; r.unlock(); &#125;    &#125;    public String[] allKeys() &#123;        r.lock();        try &#123; return m.keySet().toArray(); &#125;        finally &#123; r.unlock(); &#125;    &#125;    public Data put(String key, Data value) &#123;        w.lock();        try &#123; return m.put(key, value); &#125;        finally &#123; w.unlock(); &#125;    &#125;    public void clear() &#123;        w.lock();        try &#123; m.clear(); &#125;        finally &#123; w.unlock(); &#125;    &#125;&#125;\n\n与普通重入锁使用的主要区别在于需要使用不同的锁对象引用读写锁，并且在读写时分别调用对应的锁。\nReentrantReadWriteLock锁实现分析本节通过学习源码分析可重入读写锁的实现。\n图解重要函数及对象关系从图中可见读写锁的加锁解锁操作最终都是调用ReentrantReadWriteLock类的内部类Sync提供的方法。与一文中描述相似，Sync对象通过继承AbstractQueuedSynchronizer进行实现，故后续分析主要基于Sync类进行。\n读写锁Sync结构分析Sync继承于AbstractQueuedSynchronizer，其中主要功能均在AbstractQueuedSynchronizer中完成，其中最重要功能为控制线程获取锁失败后转换为等待状态及在满足一定条件后唤醒等待状态的线程。先对AbstractQueuedSynchronizer进行观察。\nAbstractQueuedSynchronizer图解图中展示AQS类较为重要的数据结构，包括int类型变量state用于记录锁的状态，继承自AbstractOwnableSynchronizer类的Thread类型变量exclusiveOwnerThread用于指向当前排他的获取锁的线程，AbstractQueuedSynchronizer.Node类型的变量head及tail。其中Node对象表示当前等待锁的节点，Node中thread变量指向等待的线程，waitStatus表示当前等待节点状态，mode为节点类型。多个节点之间使用prev及next组成双向链表，参考CLH锁队列的方式进行锁的获取，但其中与CLH队列的重要区别在于CLH队列中后续节点需要自旋轮询前节点状态以确定前置节点是否已经释放锁，期间不释放CPU资源，而AQS中Node节点指向的线程在获取锁失败后调用LockSupport.park函数使其进入阻塞状态，让出CPU资源，故在前置节点释放锁时需要调用unparkSuccessor函数唤醒后继节点。根据以上说明可得知此上图图主要表现当前thread0线程获取了锁，thread1线程正在等待。\n读写锁Sync对于AQS使用读写锁中Sync类是继承于AQS，并且主要使用上文介绍的数据结构中的state及waitStatus变量进行实现。实现读写锁与实现普通互斥锁的主要区别在于需要分别记录读锁状态及写锁状态，并且等待队列中需要区别处理两种加锁操作。Sync使用不同的mode描述等待队列中的节点以区分读锁等待节点和写锁等待节点。mode取值包括SHARED及EXCLUSIVE两种，分别代表当前等待节点为读锁和写锁。\n读写锁Sync代码过程分析写锁加锁通过对于重要函数关系的分析，写锁加锁最终调用Sync类的acquire函数（继承自AQS）\npublic final void acquire(int arg) &#123;    if (!tryAcquire(arg) &amp;&amp;        acquireQueued(addWaiter(Node.EXCLUSIVE), arg))        selfInterrupt();&#125;\n\n现在分情况图解分析\n无锁状态其中state变量为0，表示高位地位地位均为0，没有任何锁，且等待节点的首尾均指向空（此处特指head节点没有初始化时），锁的所有者线程也为空。\n有锁状态在加写锁时如果当前AQS已经是有锁状态，则需要进一步处理。有锁状态主要分为已有写锁和已有读锁状态，并且根据最终当前线程是否可直接获取锁分为两种情况：\n\n非重入：如果满足一下两个条件之一，当前线程必须加入等待队列（暂不考虑非公平锁抢占情况）a. 已有读锁；b. 有写锁且获取写锁的线程不为当前请求锁的线程。\n重入：有写锁且当前获取写锁的线程与当前请求锁的线程为同一线程，则直接获取锁并将写锁状态值加1。\n\n在非重入状态，当前线程创建等待节点追加到等待队列队尾，如果当前头结点为空，则需要创建一个默认的头结点。之后再当前获取锁的线程释放锁后，会唤醒等待中的节点，即为thread1。如果当前等待队列存在多个等待节点，由于thread1等待节点为EXCLUSIVE模式，则只会唤醒当前一个节点，不会传播唤醒信号。\n读锁加锁通过对于重要函数关系的分析，写锁加锁最终调用Sync类的acquireShared函数（继承自AQS）：\npublic final void acquireShared(int arg) &#123;    if (tryAcquireShared(arg) &lt; 0)        doAcquireShared(arg);&#125;\n\n同上文，现在分情况图解分析\n无锁状态其中有两个新的变量：firstReader及firstReaderHoldCount。firstReader指向在无锁状态下第一个获取读锁的线程，firstReaderHoldCount记录第一个获取读锁的线程持有当前锁的计数（主要用于重入）。\n有锁状态无锁状态获取读锁比较简单，在有锁状态则需要分情况讨论。其中需要分当前被持有的锁是读锁还是写锁，并且每种情况需要区分等待队列中是否有等待节点。\n已有读锁且等待队列为空由于本节的前提是等待队列为空的情况，故readerShouldBlock函数一定返回false，则当前线程使用CAS对读锁计数进行增加（同上文，如果同时多个线程在读锁进行竞争，则只有一个线程能够直接获取读锁，其他线程需要进入fullTryAcquireShared函数继续进行锁的获取）。在成功对读锁计数器进行增加后，当前线程需要继续对当前线程持有读锁的计数进行增加。此时分为两种情况：\n\n当前线程是第一个获取读锁的线程，此时由于第一个获取读锁的线程已经通过firstReader及firstReaderHoldCount两个变量进行存储，则仅仅需要将firstReaderHoldCount加1即可;\n当前线程不是第一个获取读锁的线程，则需要使用readHolds进行存储，readHolds是ThreadLocal的子类，通过readHolds可获取当前线程对应的HoldCounter类的对象，该对象保存了当前线程获取读锁的计数。考虑程序的局部性原理，又使用cachedHoldCounter缓存最近使用的HoldCounter类的对象，如在一段时间内只有一个线程请求读锁则可加速对读锁获取的计数。\n\n根据上图所示，thread0为首节点，thread1线程继续申请读锁，获取成功后使用ThreadLocal链接的方式进行存储计数对象，并且由于其为最近获取读锁的线程，则cachedHoldCounter对象设置指向thread1对应的计数对象。\n已有读锁且等待队列不为空上图展示当前thread0与thread1线程获取读锁，thread0为首个获取读锁的节点，并且thread2线程在等待获取写锁。如图所示，在当前锁被为读锁且有等待队列情况下，thread3及thread4线程申请读锁，则被封装为等待节点追加到当前等待队列后，节点模式为SHARED，线程使用LockSupport.park函数进入阻塞状态，让出CPU资源，直到前驱的等待节点完成锁的获取和释放后进行唤醒。\n已有写锁被获取在两种情况下，读锁获取都会进入等待队列等待前序节点唤醒，这里不再赘述。\n读等待节点被唤醒读写锁与单纯的排他锁主要区别在于读锁的共享性，在读写锁实现中保证读锁能够共享的其中一个机制就在于，如果一个读锁等待节点被唤醒后其会继续唤醒拍在当前唤醒节点之后的SHARED模式等待节点。查看源码：\nprivate void doAcquireShared(int arg) &#123;    final Node node = addWaiter(Node.SHARED);    boolean failed = true;    try &#123;        boolean interrupted = false;        for (;;) &#123;            final Node p = node.predecessor();            if (p == head) &#123;                int r = tryAcquireShared(arg);                if (r &gt;= 0) &#123;                    //注意看这里                    setHeadAndPropagate(node, r);                    p.next = null; // help GC                    if (interrupted)                        selfInterrupt();                    failed = false;                    return;                &#125;            &#125;            if (shouldParkAfterFailedAcquire(p, node) &amp;&amp;                parkAndCheckInterrupt())                interrupted = true;        &#125;    &#125; finally &#123;        if (failed)            cancelAcquire(node);    &#125;&#125;\n\n在for循环中，线程如果获取读锁成功后，需要调用setHeadAndPropagate方法。查看其源码：\nprivate void setHeadAndPropagate(Node node, int propagate) &#123;    Node h = head; // Record old head for check below    setHead(node);    if (propagate &gt; 0 || h == null || h.waitStatus &lt; 0 ||        (h = head) == null || h.waitStatus &lt; 0) &#123;        Node s = node.next;        if (s == null || s.isShared())            doReleaseShared();    &#125;&#125;\n\n在满足传播条件情况下，获取读锁后继续唤醒后续节点，所以如果当前锁是读锁状态则等待节点第一个节点一定是写锁等待节点。\n锁降级锁降级算是获取读锁的特例，如在t0线程已经获取写锁的情况下，再调取读锁加锁函数则可以直接获取读锁，但此时其他线程仍然无法获取读锁或写锁，在t0线程释放写锁后，如果有节点等待则会唤醒后续节点，后续节点可见的状态为目前有t0线程获取了读锁。所降级有什么应用场景呢？引用读写锁中使用示例代码\nclass CachedData &#123;    Object data;    volatile boolean cacheValid;    final ReentrantReadWriteLock rwl = new ReentrantReadWriteLock();    void processCachedData() &#123;        rwl.readLock().lock();        if (!cacheValid) &#123;            // Must release read lock before acquiring write lock            rwl.readLock().unlock();            rwl.writeLock().lock();            try &#123;                // Recheck state because another thread might have                // acquired write lock and changed state before we did.                if (!cacheValid) &#123;                    data = ...                        cacheValid = true;                &#125;                // Downgrade by acquiring read lock before releasing write lock                rwl.readLock().lock();            &#125; finally &#123;                rwl.writeLock().unlock(); // Unlock write, still hold read            &#125;        &#125;        try &#123;            use(data);        &#125; finally &#123;            rwl.readLock().unlock();        &#125;    &#125;&#125;\n\n其中针对变量cacheValid的使用主要过程为加读锁、读取、释放读锁、加写锁、修改值、加读锁、释放写锁、使用数据、释放读锁。其中后续几步（加写锁、修改值、加读锁、释放写锁、使用数据、释放读锁）为典型的锁降级。如果不使用锁降级，则过程可能有三种情况：\n\n第一种：加写锁、修改值、释放写锁、使用数据，即使用写锁修改数据后直接使用刚修改的数据，这样可能有数据的不一致，如当前线程释放写锁的同时其他线程（如t0）获取写锁准备修改（还没有改）cacheValid变量，而当前线程却继续运行，则当前线程读到的cacheValid变量的值为t0修改前的老数据；\n第二种：加写锁、修改值、使用数据、释放写锁，即将修改数据与再次使用数据合二为一，这样不会有数据的不一致，但是由于混用了读写两个过程，以排它锁的方式使用读写锁，减弱了读写锁读共享的优势，增加了写锁（独占锁）的占用时间；\n第三种：加写锁、修改值、释放写锁、加读锁、使用数据、释放读锁，即使用写锁修改数据后再请求读锁来使用数据，这是时数据的一致性是可以得到保证的，但是由于释放写锁和获取读锁之间存在时间差，则当前想成可能会需要进入等待队列进行等待，可能造成线程的阻塞降低吞吐量。\n\n因此针对以上情况提供了锁的降级功能，可以在完成数据修改后尽快读取最新的值，且能够减少写锁占用时间。最后注意，读写锁不支持锁升级，即获取读锁、读数据、获取写锁、释放读锁、释放写锁这个过程，因为读锁为共享锁，如同时有多个线程获取了读锁后有一个线程进行锁升级获取了写锁，这会造成同时有读锁（其他线程）和写锁的情况，造成其他线程可能无法感知新修改的数据（此为逻辑性错误），并且在JAVA读写锁实现上由于当前线程获取了读锁，再次请求写锁时必然会阻塞而导致后续释放读锁的方法无法执行，这回造成死锁（此为功能性错误）。\n写锁释放锁过程了解了加锁过程后解锁过程就非常简单，每次调用解锁方法都会减少重入计数次数，直到减为0则唤醒后续第一个等待节点，如唤醒的后续节点为读等待节点，则后续节点会继续传播唤醒状态。\n读锁释放过程读锁释放过比写锁稍微复杂，因为是共享锁，所以可能会有多个线程同时获取读锁，故在解锁时需要做两件事：\n\n获取当前线程对应的重入计数，并进行减1，此处天生为线程安全的，不需要特殊处理；\n当前读锁获取次数减1，此处由于可能存在多线程竞争，故使用自旋CAS进行设置。\n\n完成以上两步后，如读状态为0，则唤醒后续等待节点。\n总结根据以上分析，本文主要展示了读写锁的场景及方式，并分析读写锁核心功能（加解锁）的代码实现。Java读写锁同时附带了更多其他方法，包括锁状态监控和带超时机制的加锁方法等，本文不在赘述。并且读写锁中写锁可使用Conditon机制也不在详细说明。\n","tags":["Java"]},{"title":"排序算法","url":"/2020/03/23/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/","content":"\n来源：https://github.com/hustcc/JS-Sorting-Algorithm\n分享只为更多人受益，如有侵权请联系删除！\n\n总览\n一、冒泡排序冒泡排序（Bubble Sort）也是一种简单直观的排序算法。它重复地走访过要排序的数列，一次比较两个元素，如果他们的顺序错误就把他们交换过来。走访数列的工作是重复地进行直到没有再需要交换，也就是说该数列已经排序完成。这个算法的名字由来是因为越小的元素会经由交换慢慢“浮”到数列的顶端。\n作为最简单的排序算法之一，冒泡排序给我的感觉就像 Abandon 在单词书里出现的感觉一样，每次都在第一页第一位，所以最熟悉。冒泡排序还有一种优化算法，就是立一个 flag，当在一趟序列遍历中元素没有发生交换，则证明该序列已经有序。但这种改进对于提升性能来说并没有什么太大作用。\n1. 算法步骤\n比较相邻的元素。如果第一个比第二个大，就交换他们两个。\n\n对每一对相邻元素作同样的工作，从开始第一对到结尾的最后一对。这步做完后，最后的元素会是最大的数。\n\n针对所有的元素重复以上的步骤，除了最后一个。\n\n持续每次对越来越少的元素重复上面的步骤，直到没有任何一对数字需要比较。\n\n\n2. 动图演示\n3. 什么时候最快当输入的数据已经是正序时（都已经是正序了，我还要你冒泡排序有何用啊）。\n4. 什么时候最慢当输入的数据是反序时（写一个 for 循环反序输出数据不就行了，干嘛要用你冒泡排序呢，我是闲的吗）。\n5. Java代码实现public class BubbleSort implements IArraySort &#123;  @Override  public int[] sort(int[] sourceArray) throws Exception &#123;    // 对 arr 进行拷贝，不改变参数内容    int[] arr = Arrays.copyOf(sourceArray, sourceArray.length);    for (int i = 1; i &lt; arr.length; i++) &#123;      // 设定一个标记，若为true，则表示此次循环没有进行交换，也就是待排序列已经有序，排序已经完成。      boolean flag = true;      for (int j = 0; j &lt; arr.length - i; j++) &#123;        if (arr[j] &gt; arr[j + 1]) &#123;          int tmp = arr[j];          arr[j] = arr[j + 1];          arr[j + 1] = tmp;          flag = false;        &#125;      &#125;      if (flag) &#123;        break;      &#125;    &#125;    return arr;  &#125;&#125;\n\n二、选择排序选择排序是一种简单直观的排序算法，无论什么数据进去都是 O(n²) 的时间复杂度。所以用到它的时候，数据规模越小越好。唯一的好处可能就是不占用额外的内存空间了吧。\n1. 算法步骤\n首先在未排序序列中找到最小（大）元素，存放到排序序列的起始位置\n\n再从剩余未排序元素中继续寻找最小（大）元素，然后放到已排序序列的末尾。\n\n重复第二步，直到所有元素均排序完毕。\n\n\n2. 动图演示\n3.Java代码实现public class SelectionSort implements IArraySort &#123;  @Override  public int[] sort(int[] sourceArray) throws Exception &#123;    int[] arr = Arrays.copyOf(sourceArray, sourceArray.length);    // 总共要经过 N-1 轮比较    for (int i = 0; i &lt; arr.length - 1; i++) &#123;      int min = i;      // 每轮需要比较的次数 N-i      for (int j = i + 1; j &lt; arr.length; j++) &#123;        if (arr[j] &lt; arr[min]) &#123;          // 记录目前能找到的最小值元素的下标          min = j;        &#125;      &#125;      // 将找到的最小值和i位置所在的值进行交换      if (i != min) &#123;        int tmp = arr[i];        arr[i] = arr[min];        arr[min] = tmp;      &#125;    &#125;    return arr;  &#125;&#125;\n\n三、插入排序插入排序的代码实现虽然没有冒泡排序和选择排序那么简单粗暴，但它的原理应该是最容易理解的了，因为只要打过扑克牌的人都应该能够秒懂。插入排序是一种最简单直观的排序算法，它的工作原理是通过构建有序序列，对于未排序数据，在已排序序列中从后向前扫描，找到相应位置并插入。\n插入排序和冒泡排序一样，也有一种优化算法，叫做拆半插入。\n1. 算法步骤\n将第一待排序序列第一个元素看做一个有序序列，把第二个元素到最后一个元素当成是未排序序列。\n\n从头到尾依次扫描未排序序列，将扫描到的每个元素插入有序序列的适当位置。（如果待插入的元素与有序序列中的某个元素相等，则将待插入元素插入到相等元素的后面。）\n\n\n2. 动图演示\n3. Java 代码实现public class InsertSort implements IArraySort &#123;  @Override  public int[] sort(int[] sourceArray) throws Exception &#123;    // 对 arr 进行拷贝，不改变参数内容    int[] arr = Arrays.copyOf(sourceArray, sourceArray.length);    // 从下标为1的元素开始选择合适的位置插入，因为下标为0的只有一个元素，默认是有序的    for (int i = 1; i &lt; arr.length; i++) &#123;      // 记录要插入的数据      int tmp = arr[i];      // 从已经排序的序列最右边的开始比较，找到比其小的数      int j = i;      while (j &gt; 0 &amp;&amp; tmp &lt; arr[j - 1]) &#123;        arr[j] = arr[j - 1];        j--;      &#125;      // 存在比其小的数，插入      if (j != i) &#123;        arr[j] = tmp;      &#125;    &#125;    return arr;  &#125;&#125;\n\n四、希尔排序希尔排序，也称递减增量排序算法，是插入排序的一种更高效的改进版本。但希尔排序是非稳定排序算法。\n希尔排序是基于插入排序的以下两点性质而提出改进方法的：\n\n插入排序在对几乎已经排好序的数据操作时，效率高，即可以达到线性排序的效率；\n但插入排序一般来说是低效的，因为插入排序每次只能将数据移动一位；\n\n希尔排序的基本思想是：先将整个待排序的记录序列分割成为若干子序列分别进行直接插入排序，待整个序列中的记录“基本有序”时，再对全体记录进行依次直接插入排序。\n1. 算法步骤\n选择一个增量序列 t1，t2，……，tk，其中 ti &gt; tj, tk = 1；\n按增量序列个数 k，对序列进行 k 趟排序；\n每趟排序，根据对应的增量 ti，将待排序列分割成若干长度为 m 的子序列，分别对各子表进行直接插入排序。仅增量因子为 1 时，整个序列作为一个表来处理，表长度即为整个序列的长度。\n\n2. Java 代码实现public class ShellSort implements IArraySort &#123;  @Override  public int[] sort(int[] sourceArray) throws Exception &#123;    // 对 arr 进行拷贝，不改变参数内容    int[] arr = Arrays.copyOf(sourceArray, sourceArray.length);    int gap = 1;    while (gap &lt; arr.length/3) &#123;      gap = gap * 3 + 1;    &#125;    while (gap &gt; 0) &#123;      for (int i = gap; i &lt; arr.length; i++) &#123;        int tmp = arr[i];        int j = i - gap;        while (j &gt;= 0 &amp;&amp; arr[j] &gt; tmp) &#123;          arr[j + gap] = arr[j];          j -= gap;        &#125;        arr[j + gap] = tmp;      &#125;      gap = (int) Math.floor(gap / 3);    &#125;    return arr;  &#125;&#125;\n\n五、归并排序归并排序（Merge sort）是建立在归并操作上的一种有效的排序算法。该算法是采用分治法（Divide and Conquer）的一个非常典型的应用。\n作为一种典型的分而治之思想的算法应用，归并排序的实现由两种方法：\n\n自上而下的递归（所有递归的方法都可以用迭代重写，所以就有了第 2 种方法）；\n自下而上的迭代；\n\n和选择排序一样，归并排序的性能不受输入数据的影响，但表现比选择排序好的多，因为始终都是 O(nlogn) 的时间复杂度。代价是需要额外的内存空间。\n2. 算法步骤\n申请空间，使其大小为两个已经排序序列之和，该空间用来存放合并后的序列；\n\n设定两个指针，最初位置分别为两个已经排序序列的起始位置；\n\n比较两个指针所指向的元素，选择相对小的元素放入到合并空间，并移动指针到下一位置；\n\n重复步骤 3 直到某一指针达到序列尾；\n\n将另一序列剩下的所有元素直接复制到合并序列尾。\n\n\n3. 动图演示\n4. Java 代码实现public class MergeSort implements IArraySort &#123;  @Override  public int[] sort(int[] sourceArray) throws Exception &#123;    // 对 arr 进行拷贝，不改变参数内容    int[] arr = Arrays.copyOf(sourceArray, sourceArray.length);    if (arr.length &lt; 2) &#123;      return arr;    &#125;    int middle = (int) Math.floor(arr.length / 2);    int[] left = Arrays.copyOfRange(arr, 0, middle);    int[] right = Arrays.copyOfRange(arr, middle, arr.length);    return merge(sort(left), sort(right));  &#125;  protected int[] merge(int[] left, int[] right) &#123;    int[] result = new int[left.length + right.length];    int i = 0;    while (left.length &gt; 0 &amp;&amp; right.length &gt; 0) &#123;      if (left[0] &lt;= right[0]) &#123;        result[i++] = left[0];        left = Arrays.copyOfRange(left, 1, left.length);      &#125; else &#123;        result[i++] = right[0];        right = Arrays.copyOfRange(right, 1, right.length);      &#125;    &#125;    while (left.length &gt; 0) &#123;      result[i++] = left[0];      left = Arrays.copyOfRange(left, 1, left.length);    &#125;    while (right.length &gt; 0) &#123;      result[i++] = right[0];      right = Arrays.copyOfRange(right, 1, right.length);    &#125;    return result;  &#125;&#125;\n\n六、快速排序快速排序是由东尼·霍尔所发展的一种排序算法。在平均状况下，排序 n 个项目要 Ο(nlogn) 次比较。在最坏状况下则需要 Ο(n2) 次比较，但这种状况并不常见。事实上，快速排序通常明显比其他 Ο(nlogn) 算法更快，因为它的内部循环（inner loop）可以在大部分的架构上很有效率地被实现出来。\n快速排序使用分治法（Divide and conquer）策略来把一个串行（list）分为两个子串行（sub-lists）。\n快速排序又是一种分而治之思想在排序算法上的典型应用。本质上来看，快速排序应该算是在冒泡排序基础上的递归分治法。\n\n快速排序的最坏运行情况是 O(n²)，比如说顺序数列的快排。但它的平摊期望时间是 O(nlogn)，且 O(nlogn) 记号中隐含的常数因子很小，比复杂度稳定等于 O(nlogn) 的归并排序要小很多。所以，对绝大多数顺序性较弱的随机数列而言，快速排序总是优于归并排序。\n\n1. 算法步骤\n从数列中挑出一个元素，称为 “基准”（pivot）;\n\n重新排序数列，所有元素比基准值小的摆放在基准前面，所有元素比基准值大的摆在基准的后面（相同的数可以到任一边）。在这个分区退出之后，该基准就处于数列的中间位置。这个称为分区（partition）操作；\n\n递归地（recursive）把小于基准值元素的子数列和大于基准值元素的子数列排序；\n\n\n递归的最底部情形，是数列的大小是零或一，也就是永远都已经被排序好了。虽然一直递归下去，但是这个算法总会退出，因为在每次的迭代（iteration）中，它至少会把一个元素摆到它最后的位置去。\n2. 动图演示\n3. Java 代码实现public class QuickSort implements IArraySort &#123;  @Override  public int[] sort(int[] sourceArray) throws Exception &#123;    // 对 arr 进行拷贝，不改变参数内容    int[] arr = Arrays.copyOf(sourceArray, sourceArray.length);    return quickSort(arr, 0, arr.length - 1);  &#125;  private int[] quickSort(int[] arr, int left, int right) &#123;    if (left &lt; right) &#123;      int partitionIndex = partition(arr, left, right);      quickSort(arr, left, partitionIndex - 1);      quickSort(arr, partitionIndex + 1, right);    &#125;    return arr;  &#125;  private int partition(int[] arr, int left, int right) &#123;    // 设定基准值（pivot）    int pivot = left;    int index = pivot + 1;    for (int i = index; i &lt;= right; i++) &#123;      if (arr[i] &lt; arr[pivot]) &#123;        swap(arr, i, index);        index++;      &#125;    &#125;    swap(arr, pivot, index - 1);    return index - 1;  &#125;  private void swap(int[] arr, int i, int j) &#123;    int temp = arr[i];    arr[i] = arr[j];    arr[j] = temp;  &#125;&#125;\n\n七、堆排序堆排序（Heapsort）是指利用堆这种数据结构所设计的一种排序算法。堆积是一个近似完全二叉树的结构，并同时满足堆积的性质：即子结点的键值或索引总是小于（或者大于）它的父节点。堆排序可以说是一种利用堆的概念来排序的选择排序。分为两种方法：\n\n大顶堆：每个节点的值都大于或等于其子节点的值，在堆排序算法中用于升序排列；\n小顶堆：每个节点的值都小于或等于其子节点的值，在堆排序算法中用于降序排列；\n\n堆排序的平均时间复杂度为 Ο(nlogn)。\n1. 算法步骤\n将待排序序列构建成一个堆 H[0……n-1]，根据（升序降序需求）选择大顶堆或小顶堆；\n\n把堆首（最大值）和堆尾互换；\n\n把堆的尺寸缩小 1，并调用 shift_down(0)，目的是把新的数组顶端数据调整到相应位置；\n\n重复步骤 2，直到堆的尺寸为 1。\n\n\n2. 动图演示\n3. Java 代码实现public class HeapSort implements IArraySort &#123;  @Override  public int[] sort(int[] sourceArray) throws Exception &#123;    // 对 arr 进行拷贝，不改变参数内容    int[] arr = Arrays.copyOf(sourceArray, sourceArray.length);    int len = arr.length;    buildMaxHeap(arr, len);    for (int i = len - 1; i &gt; 0; i--) &#123;      swap(arr, 0, i);      len--;      heapify(arr, 0, len);    &#125;    return arr;  &#125;  private void buildMaxHeap(int[] arr, int len) &#123;    for (int i = (int) Math.floor(len / 2); i &gt;= 0; i--) &#123;      heapify(arr, i, len);    &#125;  &#125;  private void heapify(int[] arr, int i, int len) &#123;    int left = 2 * i + 1;    int right = 2 * i + 2;    int largest = i;    if (left &lt; len &amp;&amp; arr[left] &gt; arr[largest]) &#123;      largest = left;    &#125;    if (right &lt; len &amp;&amp; arr[right] &gt; arr[largest]) &#123;      largest = right;    &#125;    if (largest != i) &#123;      swap(arr, i, largest);      heapify(arr, largest, len);    &#125;  &#125;  private void swap(int[] arr, int i, int j) &#123;    int temp = arr[i];    arr[i] = arr[j];    arr[j] = temp;  &#125;&#125;\n\n八、计数排序计数排序的核心在于将输入的数据值转化为键存储在额外开辟的数组空间中。作为一种线性时间复杂度的排序，计数排序要求输入的数据必须是有确定范围的整数。\n1. 动图演示\n2. Java 代码实现public class CountingSort implements IArraySort &#123;  @Override  public int[] sort(int[] sourceArray) throws Exception &#123;    // 对 arr 进行拷贝，不改变参数内容    int[] arr = Arrays.copyOf(sourceArray, sourceArray.length);    int maxValue = getMaxValue(arr);    return countingSort(arr, maxValue);  &#125;  private int[] countingSort(int[] arr, int maxValue) &#123;    int bucketLen = maxValue + 1;    int[] bucket = new int[bucketLen];    for (int value : arr) &#123;      bucket[value]++;    &#125;    int sortedIndex = 0;    for (int j = 0; j &lt; bucketLen; j++) &#123;      while (bucket[j] &gt; 0) &#123;        arr[sortedIndex++] = j;        bucket[j]--;      &#125;    &#125;    return arr;  &#125;  private int getMaxValue(int[] arr) &#123;    int maxValue = arr[0];    for (int value : arr) &#123;      if (maxValue &lt; value) &#123;        maxValue = value;      &#125;    &#125;    return maxValue;  &#125;&#125;\n\n九、桶排序桶排序是计数排序的升级版。它利用了函数的映射关系，高效与否的关键就在于这个映射函数的确定。为了使桶排序更加高效，我们需要做到这两点：\n\n在额外空间充足的情况下，尽量增大桶的数量\n使用的映射函数能够将输入的 N 个数据均匀的分配到 K 个桶中\n\n同时，对于桶中元素的排序，选择何种比较排序算法对于性能的影响至关重要。\n1. 什么时候最快当输入的数据可以均匀的分配到每一个桶中。\n2. 什么时候最慢当输入的数据被分配到了同一个桶中。\n3. Java 代码实现public class BucketSort implements IArraySort &#123;  private static final InsertSort insertSort = new InsertSort();  @Override  public int[] sort(int[] sourceArray) throws Exception &#123;    // 对 arr 进行拷贝，不改变参数内容    int[] arr = Arrays.copyOf(sourceArray, sourceArray.length);    return bucketSort(arr, 5);  &#125;  private int[] bucketSort(int[] arr, int bucketSize) throws Exception &#123;    if (arr.length == 0) &#123;      return arr;    &#125;    int minValue = arr[0];    int maxValue = arr[0];    for (int value : arr) &#123;      if (value &lt; minValue) &#123;        minValue = value;      &#125; else if (value &gt; maxValue) &#123;        maxValue = value;      &#125;    &#125;    int bucketCount = (int) Math.floor((maxValue - minValue) / bucketSize) + 1;    int[][] buckets = new int[bucketCount][0];    // 利用映射函数将数据分配到各个桶中    for (int i = 0; i &lt; arr.length; i++) &#123;      int index = (int) Math.floor((arr[i] - minValue) / bucketSize);      buckets[index] = arrAppend(buckets[index], arr[i]);    &#125;    int arrIndex = 0;    for (int[] bucket : buckets) &#123;      if (bucket.length &lt;= 0) &#123;        continue;      &#125;      // 对每个桶进行排序，这里使用了插入排序      bucket = insertSort.sort(bucket);      for (int value : bucket) &#123;        arr[arrIndex++] = value;      &#125;    &#125;    return arr;  &#125;  /**     * 自动扩容，并保存数据     *     * @param arr     * @param value     */  private int[] arrAppend(int[] arr, int value) &#123;    arr = Arrays.copyOf(arr, arr.length + 1);    arr[arr.length - 1] = value;    return arr;  &#125;&#125;\n\n十、基数排序基数排序是一种非比较型整数排序算法，其原理是将整数按位数切割成不同的数字，然后按每个位数分别比较。由于整数也可以表达字符串（比如名字或日期）和特定格式的浮点数，所以基数排序也不是只能使用于整数。\n1. 基数排序 vs 计数排序 vs 桶排序基数排序有两种方法：\n这三种排序算法都利用了桶的概念，但对桶的使用方法上有明显差异案例看大家发的：\n\n基数排序：根据键值的每位数字来分配桶；\n计数排序：每个桶只存储单一键值；\n桶排序：每个桶存储一定范围的数值；\n\n2. LSD 基数排序动图演示\n3. Java 代码实现/** * 基数排序 * 考虑负数的情况还可以参考： https://code.i-harness.com/zh-CN/q/e98fa9 */public class RadixSort implements IArraySort &#123;  @Override  public int[] sort(int[] sourceArray) throws Exception &#123;    // 对 arr 进行拷贝，不改变参数内容    int[] arr = Arrays.copyOf(sourceArray, sourceArray.length);    int maxDigit = getMaxDigit(arr);    return radixSort(arr, maxDigit);  &#125;  /**     * 获取最高位数     */  private int getMaxDigit(int[] arr) &#123;    int maxValue = getMaxValue(arr);    return getNumLenght(maxValue);  &#125;  private int getMaxValue(int[] arr) &#123;    int maxValue = arr[0];    for (int value : arr) &#123;      if (maxValue &lt; value) &#123;        maxValue = value;      &#125;    &#125;    return maxValue;  &#125;  protected int getNumLenght(long num) &#123;    if (num == 0) &#123;      return 1;    &#125;    int lenght = 0;    for (long temp = num; temp != 0; temp /= 10) &#123;      lenght++;    &#125;    return lenght;  &#125;  private int[] radixSort(int[] arr, int maxDigit) &#123;    int mod = 10;    int dev = 1;    for (int i = 0; i &lt; maxDigit; i++, dev *= 10, mod *= 10) &#123;      // 考虑负数的情况，这里扩展一倍队列数，其中 [0-9]对应负数，[10-19]对应正数 (bucket + 10)      int[][] counter = new int[mod * 2][0];      for (int j = 0; j &lt; arr.length; j++) &#123;        int bucket = ((arr[j] % mod) / dev) + mod;        counter[bucket] = arrayAppend(counter[bucket], arr[j]);      &#125;      int pos = 0;      for (int[] bucket : counter) &#123;        for (int value : bucket) &#123;          arr[pos++] = value;        &#125;      &#125;    &#125;    return arr;  &#125;  /**     * 自动扩容，并保存数据     *     * @param arr     * @param value     */  private int[] arrayAppend(int[] arr, int value) &#123;    arr = Arrays.copyOf(arr, arr.length + 1);    arr[arr.length - 1] = value;    return arr;  &#125;&#125;\n","tags":["算法相关"]},{"title":"Java中各种锁详细介绍","url":"/2020/04/07/Java%E4%B8%AD%E5%90%84%E7%A7%8D%E9%94%81%E8%AF%A6%E7%BB%86%E4%BB%8B%E7%BB%8D/","content":"\n转自：https://www.cnblogs.com/jyroy/p/11365935.html\n相关文章：https://www.cnblogs.com/hustzzl/p/9343797.html\nhttps://www.jianshu.com/p/e674ee68fd3f\nhttps://mp.weixin.qq.com/s/-fDGn-AIYJ64Q1MlqpDiBg\n\nJava提供了种类丰富的锁，每种锁因其特性的不同，在适当的场景下能够展现出非常高的效率。本文旨在对锁相关源码（本文中的源码来自JDK 8）、使用场景进行举例，为读者介绍主流锁的知识点，以及不同的锁的适用场景。\nJava中往往是按照是否含有某一特性来定义锁，我们通过特性将锁进行分组归类，再使用对比的方式进行介绍，帮助大家更快捷的理解相关知识。下面给出本文内容的总体分类目录：\n\n1. 乐观锁 VS 悲观锁乐观锁与悲观锁是一种广义上的概念，体现了看待线程同步的不同角度。在Java和数据库中都有此概念对应的实际应用。\n先说概念。对于同一个数据的并发操作，悲观锁认为自己在使用数据的时候一定有别的线程来修改数据，因此在获取数据的时候会先加锁，确保数据不会被别的线程修改。Java中，synchronized关键字和Lock的实现类都是悲观锁。\n而乐观锁认为自己在使用数据时不会有别的线程修改数据，所以不会添加锁，只是在更新数据的时候去判断之前有没有别的线程更新了这个数据。如果这个数据没有被更新，当前线程将自己修改的数据成功写入。如果数据已经被其他线程更新，则根据不同的实现方式执行不同的操作（例如报错或者自动重试）。\n乐观锁在Java中是通过使用无锁编程来实现，最常采用的是CAS算法，Java原子类中的递增操作就通过CAS自旋实现的。\n\n根据从上面的概念描述我们可以发现：\n\n悲观锁适合写操作多的场景，先加锁可以保证写操作时数据正确。\n乐观锁适合读操作多的场景，不加锁的特点能够使其读操作的性能大幅提升。\n\n光说概念有些抽象，我们来看下乐观锁和悲观锁的调用方式示例：\n\n通过调用方式示例，我们可以发现悲观锁基本都是在显式的锁定之后再操作同步资源，而乐观锁则直接去操作同步资源。那么，为何乐观锁能够做到不锁定同步资源也可以正确的实现线程同步呢？我们通过介绍乐观锁的主要实现方式 “CAS” 的技术原理来为大家解惑。\nCAS全称 Compare And Swap（比较与交换），是一种无锁算法。在不使用锁（没有线程被阻塞）的情况下实现多线程之间的变量同步。java.util.concurrent包中的原子类就是通过CAS来实现了乐观锁。\nCAS算法涉及到三个操作数：\n\n需要读写的内存值 V。\n进行比较的值 A。\n要写入的新值 B。\n\n当且仅当 V 的值等于 A 时，CAS通过原子方式用新值B来更新V的值（“比较+更新”整体是一个原子操作），否则不会执行任何操作。一般情况下，“更新”是一个不断重试的操作。\n之前提到java.util.concurrent包中的原子类，就是通过CAS来实现了乐观锁，那么我们进入原子类AtomicInteger的源码，看一下AtomicInteger的定义：\n\n根据定义我们可以看出各属性的作用：\n\nunsafe： 获取并操作内存的数据。\nvalueOffset： 存储value在AtomicInteger中的偏移量。\nvalue： 存储AtomicInteger的int值，该属性需要借助volatile关键字保证其在线程间是可见的。\n\n接下来，我们查看AtomicInteger的自增函数incrementAndGet()的源码时，发现自增函数底层调用的是unsafe.getAndAddInt()。但是由于JDK本身只有Unsafe.class，只通过class文件中的参数名，并不能很好的了解方法的作用，所以我们通过OpenJDK 8 来查看Unsafe的源码：\n\n根据OpenJDK 8的源码我们可以看出，getAndAddInt()循环获取给定对象o中的偏移量处的值v，然后判断内存值是否等于v。如果相等则将内存值设置为 v + delta，否则返回false，继续循环进行重试，直到设置成功才能退出循环，并且将旧值返回。整个“比较+更新”操作封装在compareAndSwapInt()中，在JNI里是借助于一个CPU指令完成的，属于原子操作，可以保证多个线程都能够看到同一个变量的修改值。\n后续JDK通过CPU的cmpxchg指令，去比较寄存器中的 A 和 内存中的值 V。如果相等，就把要写入的新值 B 存入内存中。如果不相等，就将内存值 V 赋值给寄存器中的值 A。然后通过Java代码中的while循环再次调用cmpxchg指令进行重试，直到设置成功为止。\nCAS虽然很高效，但是它也存在三大问题，这里也简单说一下：\n\nABA问题。CAS需要在操作值的时候检查内存值是否发生变化，没有发生变化才会更新内存值。但是如果内存值原来是A，后来变成了B，然后又变成了A，那么CAS进行检查时会发现值没有发生变化，但是实际上是有变化的。ABA问题的解决思路就是在变量前面添加版本号，每次变量更新的时候都把版本号加一，这样变化过程就从“A－B－A”变成了“1A－2B－3A”。\n\nJDK从1.5开始提供了AtomicStampedReference类来解决ABA问题，具体操作封装在compareAndSet()中。compareAndSet()首先检查当前引用和当前标志与预期引用和预期标志是否相等，如果都相等，则以原子方式将引用值和标志的值设置为给定的更新值。\n\n循环时间长开销大。CAS操作如果长时间不成功，会导致其一直自旋，给CPU带来非常大的开销。\n\n只能保证一个共享变量的原子操作。对一个共享变量执行操作时，CAS能够保证原子操作，但是对多个共享变量操作时，CAS是无法保证操作的原子性的。\n\n\nJava从1.5开始JDK提供了AtomicReference类来保证引用对象之间的原子性，可以把多个变量放在一个对象里来进行CAS操作。\n2. 自旋锁 VS 适应性自旋锁在介绍自旋锁前，我们需要介绍一些前提知识来帮助大家明白自旋锁的概念。\n阻塞或唤醒一个Java线程需要操作系统切换CPU状态来完成，这种状态转换需要耗费处理器时间。如果同步代码块中的内容过于简单，状态转换消耗的时间有可能比用户代码执行的时间还要长。\n在许多场景中，同步资源的锁定时间很短，为了这一小段时间去切换线程，线程挂起和恢复现场的花费可能会让系统得不偿失。如果物理机器有多个处理器，能够让两个或以上的线程同时并行执行，我们就可以让后面那个请求锁的线程不放弃CPU的执行时间，看看持有锁的线程是否很快就会释放锁。\n而为了让当前线程“稍等一下”，我们需让当前线程进行自旋，如果在自旋完成后前面锁定同步资源的线程已经释放了锁，那么当前线程就可以不必阻塞而是直接获取同步资源，从而避免切换线程的开销。这就是自旋锁。\n\n自旋锁本身是有缺点的，它不能代替阻塞。自旋等待虽然避免了线程切换的开销，但它要占用处理器时间。如果锁被占用的时间很短，自旋等待的效果就会非常好。反之，如果锁被占用的时间很长，那么自旋的线程只会白浪费处理器资源。所以，自旋等待的时间必须要有一定的限度，如果自旋超过了限定次数（默认是10次，可以使用-XX:PreBLockSpin来更改）没有成功获得锁，就应当挂起线程。\n自旋锁的实现原理同样也是CAS，AtomicInteger中调用unsafe进行自增操作的源码中的do-while循环就是一个自旋操作，如果修改数值失败则通过循环来执行自旋，直至修改成功。\n\n自旋锁在JDK1.4.2中引入，使用-XX:+UseSpinning来开启。JDK 6中变为默认开启，并且引入了自适应的自旋锁（适应性自旋锁）。\n自适应意味着自旋的时间（次数）不再固定，而是由前一次在同一个锁上的自旋时间及锁的拥有者的状态来决定。如果在同一个锁对象上，自旋等待刚刚成功获得过锁，并且持有锁的线程正在运行中，那么虚拟机就会认为这次自旋也是很有可能再次成功，进而它将允许自旋等待持续相对更长的时间。如果对于某个锁，自旋很少成功获得过，那在以后尝试获取这个锁时将可能省略掉自旋过程，直接阻塞线程，避免浪费处理器资源。\n在自旋锁中 另有三种常见的锁形式:TicketLock、CLHLock和MCSLock，本文中仅做名词介绍，不做深入讲解，感兴趣的同学可以自行查阅相关资料。\n3. 无锁 VS 偏向锁 VS 轻量级锁 VS 重量级锁这四种锁是指锁的状态，专门针对synchronized的。在介绍这四种锁状态之前还需要介绍一些额外的知识。\n首先为什么synchronized能实现线程同步？\n在回答这个问题之前我们需要了解两个重要的概念：“Java对象头”、“Monitor”。\n\nJava对象头\n\nsynchronized是悲观锁，在操作同步资源之前需要给同步资源先加锁，这把锁就是存在Java对象头里的，而Java对象头又是什么呢？\n我们以Hotspot虚拟机为例，Hotspot的对象头主要包括两部分数据：Mark Word（标记字段）、Klass Pointer（类型指针）。\nMark Word：默认存储对象的HashCode，分代年龄和锁标志位信息。这些信息都是与对象自身定义无关的数据，所以Mark Word被设计成一个非固定的数据结构以便在极小的空间内存存储尽量多的数据。它会根据对象的状态复用自己的存储空间，也就是说在运行期间Mark Word里存储的数据会随着锁标志位的变化而变化。\nKlass Point：对象指向它的类元数据的指针，虚拟机通过这个指针来确定这个对象是哪个类的实例。\n\nMonitor\n\nMonitor可以理解为一个同步工具或一种同步机制，通常被描述为一个对象。每一个Java对象就有一把看不见的锁，称为内部锁或者Monitor锁。\nMonitor是线程私有的数据结构，每一个线程都有一个可用monitor record列表，同时还有一个全局的可用列表。每一个被锁住的对象都会和一个monitor关联，同时monitor中有一个Owner字段存放拥有该锁的线程的唯一标识，表示该锁被这个线程占用。\n现在话题回到synchronized，synchronized通过Monitor来实现线程同步，Monitor是依赖于底层的操作系统的Mutex Lock（互斥锁）来实现的线程同步。\n如同我们在自旋锁中提到的“阻塞或唤醒一个Java线程需要操作系统切换CPU状态来完成，这种状态转换需要耗费处理器时间。如果同步代码块中的内容过于简单，状态转换消耗的时间有可能比用户代码执行的时间还要长”。这种方式就是synchronized最初实现同步的方式，这就是JDK 6之前synchronized效率低的原因。这种依赖于操作系统Mutex Lock所实现的锁我们称之为“重量级锁”，JDK 6中为了减少获得锁和释放锁带来的性能消耗，引入了“偏向锁”和“轻量级锁”。\n所以目前锁一共有4种状态，级别从低到高依次是：无锁、偏向锁、轻量级锁和重量级锁。锁状态只能升级不能降级。\n通过上面的介绍，我们对synchronized的加锁机制以及相关知识有了一个了解，那么下面我们给出四种锁状态对应的的Mark Word内容，然后再分别讲解四种锁状态的思路以及特点：\n\n无锁无锁没有对资源进行锁定，所有的线程都能访问并修改同一个资源，但同时只有一个线程能修改成功。\n无锁的特点就是修改操作在循环内进行，线程会不断的尝试修改共享资源。如果没有冲突就修改成功并退出，否则就会继续循环尝试。如果有多个线程修改同一个值，必定会有一个线程能修改成功，而其他修改失败的线程会不断重试直到修改成功。上面我们介绍的CAS原理及应用即是无锁的实现。无锁无法全面代替有锁，但无锁在某些场合下的性能是非常高的。\n偏向锁偏向锁是指一段同步代码一直被一个线程所访问，那么该线程会自动获取锁，降低获取锁的代价。\n在大多数情况下，锁总是由同一线程多次获得，不存在多线程竞争，所以出现了偏向锁。其目标就是在只有一个线程执行同步代码块时能够提高性能。\n当一个线程访问同步代码块并获取锁时，会在Mark Word里存储锁偏向的线程ID。在线程进入和退出同步块时不再通过CAS操作来加锁和解锁，而是检测Mark Word里是否存储着指向当前线程的偏向锁。引入偏向锁是为了在无多线程竞争的情况下尽量减少不必要的轻量级锁执行路径，因为轻量级锁的获取及释放依赖多次CAS原子指令，而偏向锁只需要在置换ThreadID的时候依赖一次CAS原子指令即可。\n偏向锁只有遇到其他线程尝试竞争偏向锁时，持有偏向锁的线程才会释放锁，线程不会主动释放偏向锁。偏向锁的撤销，需要等待全局安全点（在这个时间点上没有字节码正在执行），它会首先暂停拥有偏向锁的线程，判断锁对象是否处于被锁定状态。撤销偏向锁后恢复到无锁（标志位为“01”）或轻量级锁（标志位为“00”）的状态。\n偏向锁在JDK 6及以后的JVM里是默认启用的。可以通过JVM参数关闭偏向锁：-XX:-UseBiasedLocking=false，关闭之后程序默认会进入轻量级锁状态。\n轻量级锁是指当锁是偏向锁的时候，被另外的线程所访问，偏向锁就会升级为轻量级锁，其他线程会通过自旋的形式尝试获取锁，不会阻塞，从而提高性能。\n在代码进入同步块的时候，如果同步对象锁状态为无锁状态（锁标志位为“01”状态，是否为偏向锁为“0”），虚拟机首先将在当前线程的栈帧中建立一个名为锁记录（Lock Record）的空间，用于存储锁对象目前的Mark Word的拷贝，然后拷贝对象头中的Mark Word复制到锁记录中。\n拷贝成功后，虚拟机将使用CAS操作尝试将对象的Mark Word更新为指向Lock Record的指针，并将Lock Record里的owner指针指向对象的Mark Word。\n如果这个更新动作成功了，那么这个线程就拥有了该对象的锁，并且对象Mark Word的锁标志位设置为“00”，表示此对象处于轻量级锁定状态。\n如果轻量级锁的更新操作失败了，虚拟机首先会检查对象的Mark Word是否指向当前线程的栈帧，如果是就说明当前线程已经拥有了这个对象的锁，那就可以直接进入同步块继续执行，否则说明多个线程竞争锁。\n若当前只有一个等待线程，则该线程通过自旋进行等待。但是当自旋超过一定的次数，或者一个线程在持有锁，一个在自旋，又有第三个来访时，轻量级锁升级为重量级锁。\n重量级锁升级为重量级锁时，锁标志的状态值变为“10”，此时Mark Word中存储的是指向重量级锁的指针，此时等待锁的线程都会进入阻塞状态。\n整体的锁状态升级流程如下：\n\n综上，偏向锁通过对比Mark Word解决加锁问题，避免执行CAS操作。而轻量级锁是通过用CAS操作和自旋来解决加锁问题，避免线程阻塞和唤醒而影响性能。重量级锁是将除了拥有锁的线程以外的线程都阻塞。\n4. 公平锁 VS 非公平锁公平锁是指多个线程按照申请锁的顺序来获取锁，线程直接进入队列中排队，队列中的第一个线程才能获得锁。公平锁的优点是等待锁的线程不会饿死。缺点是整体吞吐效率相对非公平锁要低，等待队列中除第一个线程以外的所有线程都会阻塞，CPU唤醒阻塞线程的开销比非公平锁大。\n非公平锁是多个线程加锁时直接尝试获取锁，获取不到才会到等待队列的队尾等待。但如果此时锁刚好可用，那么这个线程可以无需阻塞直接获取到锁，所以非公平锁有可能出现后申请锁的线程先获取锁的场景。非公平锁的优点是可以减少唤起线程的开销，整体的吞吐效率高，因为线程有几率不阻塞直接获得锁，CPU不必唤醒所有线程。缺点是处于等待队列中的线程可能会饿死，或者等很久才会获得锁。\n直接用语言描述可能有点抽象，这里作者用从别处看到的一个例子来讲述一下公平锁和非公平锁。\n\n如上图所示，假设有一口水井，有管理员看守，管理员有一把锁，只有拿到锁的人才能够打水，打完水要把锁还给管理员。每个过来打水的人都要管理员的允许并拿到锁之后才能去打水，如果前面有人正在打水，那么这个想要打水的人就必须排队。管理员会查看下一个要去打水的人是不是队伍里排最前面的人，如果是的话，才会给你锁让你去打水；如果你不是排第一的人，就必须去队尾排队，这就是公平锁。\n但是对于非公平锁，管理员对打水的人没有要求。即使等待队伍里有排队等待的人，但如果在上一个人刚打完水把锁还给管理员而且管理员还没有允许等待队伍里下一个人去打水时，刚好来了一个插队的人，这个插队的人是可以直接从管理员那里拿到锁去打水，不需要排队，原本排队等待的人只能继续等待。如下图所示：\n\n接下来我们通过ReentrantLock的源码来讲解公平锁和非公平锁。\n\n根据代码可知，ReentrantLock里面有一个内部类Sync，Sync继承AQS（AbstractQueuedSynchronizer），添加锁和释放锁的大部分操作实际上都是在Sync中实现的。它有公平锁FairSync和非公平锁NonfairSync两个子类。ReentrantLock默认使用非公平锁，也可以通过构造器来显示的指定使用公平锁。\n下面我们来看一下公平锁与非公平锁的加锁方法的源码:\n\n通过上图中的源代码对比，我们可以明显的看出公平锁与非公平锁的Lock()方法唯一的区别就在于公平锁在获取同步状态时多了一个限制条件：hasQueuedPredecessors()。\n\n再进入hasQueuedPredecessors()，可以看到该方法主要做一件事情：主要是判断当前线程是否位于同步队列中的第一个。如果是则返回true，否则返回false。\n综上，公平锁就是通过同步队列来实现多个线程按照申请锁的顺序来获取锁，从而实现公平的特性。非公平锁加锁时不考虑排队等待问题，直接尝试获取锁，所以存在后申请却先获得锁的情况。\n5. 可重入锁 VS 非可重入锁可重入锁又名递归锁，是指在同一个线程在外层方法获取锁的时候，再进入该线程的内层方法会自动获取锁（前提锁对象得是同一个对象或者class），不会因为之前已经获取过还没释放而阻塞。Java中ReentrantLock和synchronized都是可重入锁，可重入锁的一个优点是可一定程度避免死锁。下面用示例代码来进行分析：\n\n在上面的代码中，类中的两个方法都是被内置锁synchronized修饰的，doSomething()方法中调用doOthers()方法。因为内置锁是可重入的，所以同一个线程在调用doOthers()时可以直接获得当前对象的锁，进入doOthers()进行操作。\n如果是一个不可重入锁，那么当前线程在调用doOthers()之前需要将执行doSomething()时获取当前对象的锁释放掉，实际上该对象锁已被当前线程所持有，且无法释放。所以此时会出现死锁。\n而为什么可重入锁就可以在嵌套调用时可以自动获得锁呢？我们通过图示和源码来分别解析一下。\n还是打水的例子，有多个人在排队打水，此时管理员允许锁和同一个人的多个水桶绑定。这个人用多个水桶打水时，第一个水桶和锁绑定并打完水之后，第二个水桶也可以直接和锁绑定并开始打水，所有的水桶都打完水之后打水人才会将锁还给管理员。这个人的所有打水流程都能够成功执行，后续等待的人也能够打到水。这就是可重入锁。\n\n但如果是非可重入锁的话，此时管理员只允许锁和同一个人的一个水桶绑定。第一个水桶和锁绑定打完水之后并不会释放锁，导致第二个水桶不能和锁绑定也无法打水。当前线程出现死锁，整个等待队列中的所有线程都无法被唤醒。\n\n之前我们说过ReentrantLock和synchronized都是重入锁，那么我们通过重入锁ReentrantLock以及非可重入锁NonReentrantLock的源码来对比分析一下为什么非可重入锁在重复调用同步资源时会出现死锁。\n首先ReentrantLock和NonReentrantLock都继承父类AQS，其父类AQS中维护了一个同步状态status来计数重入次数，status初始值为0。\n当线程尝试获取锁时，可重入锁先尝试获取并更新status值，如果status == 0表示没有其他线程在执行同步代码，则把status置为1，当前线程开始执行。如果status != 0，则判断当前线程是否是获取到这个锁的线程，如果是的话执行status+1，且当前线程可以再次获取锁。而非可重入锁是直接去获取并尝试更新当前status的值，如果status != 0的话会导致其获取锁失败，当前线程阻塞。\n释放锁时，可重入锁同样先获取当前status的值，在当前线程是持有锁的线程的前提下。如果status-1 == 0，则表示当前线程所有重复获取锁的操作都已经执行完毕，然后该线程才会真正释放锁。而非可重入锁则是在确定当前线程是持有锁的线程之后，直接将status置为0，将锁释放。\n\n6. 独享锁 VS 共享锁独享锁和共享锁同样是一种概念。我们先介绍一下具体的概念，然后通过ReentrantLock和ReentrantReadWriteLock的源码来介绍独享锁和共享锁。\n独享锁也叫排他锁，是指该锁一次只能被一个线程所持有。如果线程T对数据A加上排它锁后，则其他线程不能再对A加任何类型的锁。获得排它锁的线程即能读数据又能修改数据。JDK中的synchronized和JUC中Lock的实现类就是互斥锁。\n共享锁是指该锁可被多个线程所持有。如果线程T对数据A加上共享锁后，则其他线程只能对A再加共享锁，不能加排它锁。获得共享锁的线程只能读数据，不能修改数据。\n独享锁与共享锁也是通过AQS来实现的，通过实现不同的方法，来实现独享或者共享。\n下图为ReentrantReadWriteLock的部分源码：\n\n我们看到ReentrantReadWriteLock有两把锁：ReadLock和WriteLock，由词知意，一个读锁一个写锁，合称“读写锁”。再进一步观察可以发现ReadLock和WriteLock是靠内部类Sync实现的锁。Sync是AQS的一个子类，这种结构在CountDownLatch、ReentrantLock、Semaphore里面也都存在。\n在ReentrantReadWriteLock里面，读锁和写锁的锁主体都是Sync，但读锁和写锁的加锁方式不一样。读锁是共享锁，写锁是独享锁。读锁的共享锁可保证并发读非常高效，而读写、写读、写写的过程互斥，因为读锁和写锁是分离的。所以ReentrantReadWriteLock的并发性相比一般的互斥锁有了很大提升。\n那读锁和写锁的具体加锁方式有什么区别呢？在了解源码之前我们需要回顾一下其他知识。\n在最开始提及AQS的时候我们也提到了state字段（int类型，32位），该字段用来描述有多少线程获持有锁。\n在独享锁中这个值通常是0或者1（如果是重入锁的话state值就是重入的次数），在共享锁中state就是持有锁的数量。但是在ReentrantReadWriteLock中有读、写两把锁，所以需要在一个整型变量state上分别描述读锁和写锁的数量（或者也可以叫状态）。于是将state变量“按位切割”切分成了两个部分，高16位表示读锁状态（读锁个数），低16位表示写锁状态（写锁个数）。如下图所示：\n\n了解了概念之后我们再来看代码，先看写锁的加锁源码：\n\n\n这段代码首先取到当前锁的个数c，然后再通过c来获取写锁的个数w。因为写锁是低16位，所以取低16位的最大值与当前的c做与运算（ int w = exclusiveCount(c); ），高16位和0与运算后是0，剩下的就是低位运算的值，同时也是持有写锁的线程数目。\n在取到写锁线程的数目后，首先判断是否已经有线程持有了锁。如果已经有线程持有了锁（c!=0），则查看当前写锁线程的数目，如果写线程数为0（即此时存在读锁）或者持有锁的线程不是当前线程就返回失败（涉及到公平锁和非公平锁的实现）。\n如果写入锁的数量大于最大数（65535，2的16次方-1）就抛出一个Error。\n如果当且写线程数为0（那么读线程也应该为0，因为上面已经处理c!=0的情况），并且当前线程需要阻塞那么就返回失败；如果通过CAS增加写线程数失败也返回失败。\n如果c=0，w=0或者c&gt;0，w&gt;0（重入），则设置当前线程或锁的拥有者，返回成功！\n\ntryAcquire()除了重入条件（当前线程为获取了写锁的线程）之外，增加了一个读锁是否存在的判断。如果存在读锁，则写锁不能被获取，原因在于：必须确保写锁的操作对读锁可见，如果允许读锁在已被获取的情况下对写锁的获取，那么正在运行的其他读线程就无法感知到当前写线程的操作。\n因此，只有等待其他读线程都释放了读锁，写锁才能被当前线程获取，而写锁一旦被获取，则其他读写线程的后续访问均被阻塞。写锁的释放与ReentrantLock的释放过程基本类似，每次释放均减少写状态，当写状态为0时表示写锁已被释放，然后等待的读写线程才能够继续访问读写锁，同时前次写线程的修改对后续的读写线程可见。\n接着是读锁的代码：\n\n可以看到在tryAcquireShared(int unused)方法中，如果其他线程已经获取了写锁，则当前线程获取读锁失败，进入等待状态。如果当前线程获取了写锁或者写锁未被获取，则当前线程（线程安全，依靠CAS保证）增加读状态，成功获取读锁。读锁的每次释放（线程安全的，可能有多个读线程同时释放读锁）均减少读状态，减少的值是“1&lt;&lt;16”。所以读写锁才能实现读读的过程共享，而读写、写读、写写的过程互斥。\n此时，我们再回头看一下互斥锁ReentrantLock中公平锁和非公平锁的加锁源码：\n\n我们发现在ReentrantLock虽然有公平锁和非公平锁两种，但是它们添加的都是独享锁。根据源码所示，当某一个线程调用Lock方法获取锁时，如果同步资源没有被其他线程锁住，那么当前线程在使用CAS更新state成功后就会成功抢占该资源。而如果公共资源被占用且不是被当前线程占用，那么就会加锁失败。所以可以确定ReentrantLock无论读操作还是写操作，添加的锁都是都是独享锁。\n","tags":["Java"]},{"title":"NIO详细介绍","url":"/2020/07/06/NIO%E8%AF%A6%E7%BB%86%E4%BB%8B%E7%BB%8D/","content":"一、介绍Doug Lea写的一篇文章来阐述NIO\nNIO是一个双向的缓存通道，通道负责建立和缓冲区的链接。\nChannel负责传输，Buffer负责中转储存。\n二、要素1. 缓存区（Buffer）1.1 介绍缓存区（Buffer）：一个用于特定基本数据类型的容器。由java.nio包定义的，所有缓冲区都是Buffer抽象类的子类。\nJava NIO中的Buffer主要用于与NIO通道进行交互，数据是从通道读入缓冲区，从缓冲区写入通道中。\nBuffer就像一个数组，可以保存多个相同类型的数据。根据数据类型不同（boolean除外），有以下Buffer常用子类：\n\nByteBuffer\nCharBuffer\nShortBuffer\nIntBuffer\nLongBuffer\nFloatBuffer\nDoubleBuffer\n\n上述Buffer类都是采用相似的方法进行管理数据，只是各自管理的数据类型不同而已。都是通过如下方法获取一个Buffer对象：\nstatic XXXBufer allocate(int capacity)：创建一个容量为capacity的XXXBuffer对象。\n1.2 基本属性\n容量（capacity）：表示Buffer最大数据容量，缓冲区容量不能为负，并且创建后不能更改。\n\n限制（limit）：第一个不应该读取或写入的数据的索引，即位于limit后的数据不可读写。缓存区的限制不能为负，并且不能大于其容量。\n\n位置（position）：下一个要读取或写入的数据的索引。缓冲区的位置不能为负，并且不能大于其容量。\n\n标记（mark）与重置（reset）：标记是一个索引，通过Buffer中的mark()方法指定Buffer中一个特定的position，之后可以通过reset()方法恢复到这个position。\n\n\n遵守原则：0&lt;=mark&lt;=position&lt;=limit&lt;=capacity，管道将会从Buffer中读取position到limit的内容。\n1.3 常用方法\n\n\n方法\n描述\n\n\n\nBuffer clear()\n清空缓冲区并返回对缓冲区的引用\n\n\nBuffer flip()\n将缓冲区的界限设置为当前位置，并将当前位置重置为0\n\n\nint capacity()\n返回Buffer的capacity的大小\n\n\nboolean hasRemaining()\n判断缓冲区中是否还有元素\n\n\nint limit()\n返回Buffer的界限（limit）的位置\n\n\nBuffer limit(int newLimit)\n将设置缓冲区界限为newLimit，并返回一个具有新limit的缓冲区对象\n\n\nBuffer mark()\n对缓冲区设置标记\n\n\nint position()\n返回缓冲区的当前位置position\n\n\nBuffer position(int newPosition)\n将设置缓冲区的当前位置为newPosition，并返回修改后的Buffer对象\n\n\nint remaining()\n返回position和limit之间的元素个数\n\n\nBuffer reset()\n将位置position转到以前设置的mark所在的位置\n\n\nBuffer rewind()\n将位置设置为0，取消设置mark\n\n\n1.4 缓冲区的数据操作Buffer所有子类提供了两个用于数据操作的方法：get()与put()方法\n\n获取Buffer中的数据\nget()：读取单个字节\nget(int index)：读取指定索引位置的字节（不会移动position）\nget(byte[] dst, int offset, int length)：\nget(byte[] dst)：批量读取多个字节到dst中\n\n放入数据到Buffer\nput(byte b)：将给定单个字节写入缓冲区的当前位置\nput(byte[] src, int offset, int length)：\nput(byte[] src)：将src中的字节写入缓冲区的当前位置\nput(ByteBuffer src)：\nput(int index, byte b)：将指定字节写入缓冲区的索引位置（不会移动position）\n\n\n/** * 一、缓冲区（Buffer）:在java NIO中负责数据的存取。缓冲区就是数组。用于存储 * 不同数据类型的数据 * * 根据数据类型的不同（boolean除外），提供对应类型的缓冲区： * ByteBuffer - 最常用的 * CharBuffer * ShortBuffer * IntBuffer * LongBuffer * FloatBuffer * DoubleBuffer * * 上述的缓冲区的管理方式几乎是一致的 ，通过allocate()获取缓冲区 * * 二、缓冲区里面的存取数据的两个核心方法： * put()：存入数据到缓冲区中 * get()：获取缓冲区中的数据 * * 四 、缓冲区中的核心属性 * capacity：容量，表示缓冲区中最大存储数据的容量，一旦声明了则不能改变。 * limit：表示缓冲区中可以操作数据的大小。（limit后数据是不可以进行读写的） * position：位置，表示缓冲区正在操作数据的位置。 * mark：标记，表示记录当前position的位置，可以通过reset()恢复到mark的位置 * * 0 &lt;= mark &lt;= position &lt;= limit &lt;= capacity */public class TestBuffer &#123;    public void test2() &#123;        String str = \"abcde\";        ByteBuffer buf = ByteBuffer.allocate(1024);        buf.put(str.getBytes());        buf.flip();        byte[] dst = new byte[buf.limit()];        buf.get(dst,0,2);        System.out.println(new String(dst, 0, 2));        System.out.println(buf.position());        //mark()：标记        buf.mark();        buf.get(dst,2,2);        System.out.println(new String(dst,2,2));        System.out.println(buf.position());        //reset()        buf.reset();        System.out.println(buf.position());        //判断缓冲区中是否还有剩余数据        if (buf.hasRemaining()) &#123;            //如果有的话，那么获取缓冲区中可以操作的数量            System.out.println(buf.remaining());        &#125;    &#125;    public void test1() &#123;        String str = \"abcde\";        //1.分配一个指定大小的缓冲区        ByteBuffer buf = ByteBuffer.allocate(1024);        System.out.println(\"-------allocate()-------\");        System.out.println(\"正在操作的位置 \"+buf.position());        System.out.println(\"缓冲区中可操作数据的大小 \"+buf.limit());        System.out.println(\"容量 \"+buf.capacity());        //2.利用put()方法存入数据到缓冲区        buf.put(str.getBytes());        System.out.println(\"-------put()-------\");        System.out.println(\"正在操作的位置 \"+buf.position());        System.out.println(\"缓冲区中可操作数据的大小 \"+buf.limit());        System.out.println(\"容量 \"+buf.capacity());        //3.切换成读取数据的模式,利用flip()方法来进行读取数据        buf.flip();        System.out.println(\"-------flip()-------\");        System.out.println(\"正在操作的位置 \"+buf.position());        System.out.println(\"缓冲区中可操作数据的大小 \"+buf.limit());        System.out.println(\"容量 \"+buf.capacity());        //4.利用get()读取缓冲区中的数据        byte[] dst = new byte[buf.limit()];        buf.get(dst);        System.out.println(new String(dst,0,dst.length));        System.out.println(\"-------get()-------\");        System.out.println(\"正在操作的位置 \"+buf.position());        System.out.println(\"缓冲区中可操作数据的大小 \"+buf.limit());        System.out.println(\"容量 \"+buf.capacity());        //5.rewind():表示可以重复读取        buf.rewind();        System.out.println(\"-------rewind()-------\");        System.out.println(\"正在操作的位置 \"+buf.position());        System.out.println(\"缓冲区中可操作数据的大小 \"+buf.limit());        System.out.println(\"容量 \"+buf.capacity());        //6.清空缓冲区,但是缓冲区中的数据依然存在，只不过数据是处于被遗望的状态        buf.clear();        System.out.println(\"-------clear()-------\");        System.out.println(\"正在操作的位置 \"+buf.position());        System.out.println(\"缓冲区中可操作数据的大小 \"+buf.limit());        System.out.println(\"容量 \"+buf.capacity());        System.out.println((char)buf.get());    &#125;&#125;\n\n1.5 直接缓冲区与非直接缓冲区\n字节缓冲区要么是直接的，要么是非直接的。如果为直接字节缓冲区，则Java虚拟机会尽最大努力直接在此缓冲区上执行本机I/O操作。也就是说，在每次调用基础操作系统的一个本机I/O操作之前（或之后），虚拟机都会尽量避免将缓冲区的内容复制到中间缓冲区中（或从中间缓冲区中复制内容）。\n\n直接字节缓冲区可以通过调用此类的allocateDirect()工厂方法来创建。此方法返回的缓冲区进行分配和取消分配所需成本通常高于非直接缓冲区。直接缓冲区的内容可以驻留在常规的垃圾回收堆之外，因此，它们对应用程序的内存需求量造成的影响可能并不明显。所以，建议将直接缓冲区主要分配给那些易受基础系统的本机I/O操作影响的大型、持久的缓冲区。一般情况下，最好仅在直接缓冲区能在程序性能方面带来明显好处是分配它们。\n\n直接字节缓冲区还可以通过FileChannel的map()方法将文件区域直接映射到内存中来创建。该方法返回MappedByteBuffer。Java平台的实现有助于通过JNI从本机代码创建直接字节缓冲区。如果以上这些缓冲区中的某个缓冲区实例指的是不可访问的内存区域，则试图访问该区域不会更改该缓冲区的内容，并且将会在访问期间或稍后的某个时间导致抛出不确定的异常。\n\n字节缓冲区是直接缓冲区还是非直接缓冲区可通过调用其isDirect()方法来确定。提供此方法是为了能够在性能关键型代码中执行显示缓冲区管理。\n\n\n2. 通道（Channel）由java.nio.channels包定义。Channel表示IO源与目标打开的连接。Channel类似于传统的“流”。只不过Channel本身不能直接访问数据，Channel只能与Buffer进行交互。\n2.1 主要实现类2.1.1 FileChannel\n用于读取、写入、映射和操作文件的通道\n\n\nFileChannel的常用方法\n\n\n\n\n方法\n描述\n\n\n\nint read(ByteBuffer dst)\n从Channel中读取数据到ByteBuffer\n\n\nlong read(ByteBuffer[] dsts, int offset, int length)\n将Channel中的数据“分散”到ByteBuffer[]\n\n\nint write(ByteBuffer src)\n将ByteBuffer中的数据写入到Channel\n\n\nlong write(ByteBuffer[] srcs, int offset, int length)\n将ByteBuffer[]中的数据“聚集”到Channel\n\n\nlong position()\n返回此通道的文件位置\n\n\nFileChannel position(long newPosition)\n设置此通道的文件位置\n\n\nlong size()\n返回此通道的文件的当前大小\n\n\nFileChannel truncate(long size)\n将此通道的文件截取为给定大小\n\n\nvoid force(boolean metaData)\n强制将所有对此通道的文件更新写入到储存设备中\n\n\n2.1.2 DatagramChannel\n用过UDP读写网络中的数据通道\n操作步骤：打开DatagramChannel -&gt; 接收/发送数据\n\npublic class TestNonBlockingNIO2 &#123;    public void send() throws IOException&#123;        DatagramChannel dc = DatagramChannel.open();        dc.configureBlocking(false);        ByteBuffer buf = ByteBuffer.allocate(1024);        Scanner scan = new Scanner(System.in);        while(scan.hasNext())&#123;            String str = scan.next();            buf.put((new Date().toString() + \":\\n\" + str).getBytes());            buf.flip();            dc.send(buf, new InetSocketAddress(\"127.0.0.1\", 9898));            buf.clear();        &#125;        dc.close();    &#125;    public void receive() throws IOException&#123;        DatagramChannel dc = DatagramChannel.open();        dc.configureBlocking(false);        dc.bind(new InetSocketAddress(9898));        Selector selector = Selector.open();        dc.register(selector, SelectionKey.OP_READ);        while(selector.select() &gt; 0)&#123;            Iterator&lt;SelectionKey&gt; it = selector.selectedKeys().iterator();            while(it.hasNext())&#123;                SelectionKey sk = it.next();                if(sk.isReadable())&#123;                    ByteBuffer buf = ByteBuffer.allocate(1024);                    dc.receive(buf);                    buf.flip();                    System.out.println(new String(buf.array(), 0, buf.limit()));                    buf.clear();                &#125;            &#125;            it.remove();        &#125;    &#125;&#125;\n\n2.1.3 SocketChannel\nJava NIO中的SocketChannel是一个连接到TCP网络套接字的通道。通过TCP读写网络中的数据\n操作步骤：打开SocketChannel -&gt; 读写数据 -&gt; 关闭SocketChannel\n\npublic void client() throws IOException &#123;    //1. 获取通道    SocketChannel sChannel = SocketChannel.open(new InetSocketAddress(\"127.0.0.1\", 9898));    FileChannel inChannel = FileChannel.open(Paths.get(\"1.jpg\"), StandardOpenOption.READ);    //2. 分配指定大小的缓冲区    ByteBuffer buf = ByteBuffer.allocate(1024);    //3. 读取本地文件，并发送到服务端    while(inChannel.read(buf) != -1)&#123;        buf.flip();        sChannel.write(buf);        buf.clear();    &#125;    //4. 关闭通道    inChannel.close();    sChannel.close();&#125;\n\n2.1.4 ServerSocketChannel\nJava NIO中的ServerSocketChannel是一个可以监听新进来的TCP连接的通道，就像标准IO中的ServerSocket一样。可以监听新进来的TCP连接，对每一个新进来的连接都会创建一个SocketChannel。\n\npublic void server() throws IOException&#123;    //1. 获取通道    ServerSocketChannel ssChannel = ServerSocketChannel.open();    FileChannel outChannel = FileChannel.open(Paths.get(\"2.jpg\"), StandardOpenOption.WRITE, StandardOpenOption.CREATE);    //2. 绑定连接    ssChannel.bind(new InetSocketAddress(9898));    //3. 获取客户端连接的通道    SocketChannel sChannel = ssChannel.accept();    //4. 分配指定大小的缓冲区    ByteBuffer buf = ByteBuffer.allocate(1024);    //5. 接收客户端的数据，并保存到本地    while(sChannel.read(buf) != -1)&#123;        buf.flip();        outChannel.write(buf);        buf.clear();    &#125;    //6. 关闭通道    sChannel.close();    outChannel.close();    ssChannel.close();&#125;\n\n2.2 获取通道获取通道的一种方式是对支持通道的对象调用getChannel()方法。支持通道的类如下：\n\nFileInputStream\n\nFileOutputStream\n\nRandomAccessFile\n\nDatagramSocket\n\nSocket\n\nServerSocket\n\n\n获取通道的其他方式是使用Files类的静态方法newByteChannel()获取字节通道。或者通过通道的静态方法open()打开并返回指定通道。FileChannel.open(Path path, OpenOption... options)\n2.3 通道之间的数据传输2.3.1 缓冲区传输\n将Buffer中数据写入Channel\n\n// 将Buffer中数据写入Channel中int bytesWritten = inChannel.write(buf);\n\n\n从Channel读取数据到Buffer\n\n// 从Channel读取数据到Buffer中int bytesRead = inChannel.read(buf);\n\n\n使用示例\n\npublic class TestChannel &#123;    //1.利用通道来完成文件的复制    public void test1() &#123;        FileInputStream fis = null;        FileOutputStream fos = null;        FileChannel inChannel = null;        FileChannel outChannel = null;        try &#123;            fis = new FileInputStream(\"1.jpg\");            fos = new FileOutputStream(\"3.jpg\");            //2.获取通道            inChannel = fis.getChannel();            outChannel = fos.getChannel();            //3.分配一个指定大小的缓冲区            ByteBuffer buf = ByteBuffer.allocate(1024);            //4.将通道中的数据存入缓冲区中读取数据            while (inChannel.read(buf) != -1) &#123;                buf.flip();//切换成读取数据的模式                //5.将缓冲区中的数据再写入到通道                outChannel.write(buf);                //清空缓冲区                buf.clear();            &#125;        &#125; catch (IOException e) &#123;            e.printStackTrace();        &#125;finally &#123;            if ( outChannel != null) &#123;                try &#123;                    outChannel.close();                &#125; catch (IOException e) &#123;                    e.printStackTrace();                &#125;            &#125;            if (inChannel!=null) &#123;                try &#123;                    inChannel.close();                &#125; catch (IOException e) &#123;                    e.printStackTrace();                &#125;            &#125;            if (fos!=null) &#123;                try &#123;                    fos.close();                &#125; catch (IOException e) &#123;                    e.printStackTrace();                &#125;            &#125;            if (fis!=null) &#123;                try &#123;                    fis.close();                &#125; catch (IOException e) &#123;                    e.printStackTrace();                &#125;            &#125;        &#125;    &#125;&#125;\n\n//2.使用直接缓冲区完成文件的复制(内存映射文件的方式)public void test2() throws IOException &#123;    long start = System.currentTimeMillis();    FileChannel inChannel = FileChannel.open(Paths.get(\"1.jpg\"), StandardOpenOption.READ);    FileChannel outChannel = FileChannel.open(Paths.get(\"8.jpg\"), StandardOpenOption.WRITE, StandardOpenOption.READ, StandardOpenOption.CREATE_NEW);    //内存映射文件    MappedByteBuffer inMappedBuf = inChannel.map(MapMode.READ_ONLY, 0, inChannel.size());    MappedByteBuffer outMappedBuf = outChannel.map(MapMode.READ_WRITE, 0, inChannel.size());    //直接对缓冲区进行数据的读写操作    byte[] dst = new byte[inMappedBuf.limit()];    inMappedBuf.get(dst);    outMappedBuf.put(dst);    inChannel.close();    outChannel.close();    long end = System.currentTimeMillis();    System.out.println(\"内存映射文件所花时间：\"+(end-start));&#125;\n\n2.3.2 直接传输\ntransferFrom()\ntransferTo()\n\nRandomAccessFile fromFile = new RandomAccessFile(\"fromFile.txt\", \"rw\");// 获取FileChannelFileChannel fromChannel = fromFile.getChannel();RandomAccessFile toFile = new RandomAccessFile(\"toFile.txt\", \"rw\");FileChannel toChannel = toFile.getChannel();// 定义传输位置long position = 0L;// 最多传输的字节数long count = fromChannel.size();// 将数据从源通道传输到另一个通道toChannel.transferFrom(fromChannel, position, count);\n\n/**  * 通道之间的数据传输（直接缓冲区）  * @throws IOException  */public void test3() throws IOException &#123;    FileChannel inChannel = FileChannel.open(Paths.get(\"1.jpg\"), StandardOpenOption.READ);    FileChannel outChannel = FileChannel.open(Paths.get(\"9.jpg\"), StandardOpenOption.WRITE,StandardOpenOption.READ,StandardOpenOption.CREATE_NEW);    inChannel.transferTo(0, inChannel.size(), outChannel);    //outChannel.transferFrom(inChannel,0,inChannel.size());    inChannel.close();    outChannel.close();&#125;\n\n2.4 通道之间的内存映射\nJava IO操作中通常采用BufferedReader，BufferedInputStream等带缓冲的IO类处理大文件，不过Java NIO中引入了一种基于MappedByteBuffer操作大文件的方式，其读写性能极高\n\npublic void test2() throws IOException &#123;    long start = System.currentTimeMillis();    FileChannel inChannel = FileChannel.open(Paths.get(\"1.jpg\"), StandardOpenOption.READ);    FileChannel outChannel = FileChannel.open(Paths.get(\"8.jpg\"), StandardOpenOption.WRITE,StandardOpenOption.READ,StandardOpenOption.CREATE_NEW);    //内存映射文件    MappedByteBuffer inMappedBuf = inChannel.map(MapMode.READ_ONLY, 0, inChannel.size());    MappedByteBuffer outMappedBuf = outChannel.map(MapMode.READ_WRITE, 0, inChannel.size());    //直接对缓冲区进行数据的读写操作    byte[] dst = new byte[inMappedBuf.limit()];    inMappedBuf.get(dst);    outMappedBuf.put(dst);    inChannel.close();    outChannel.close();    long end = System.currentTimeMillis();    System.out.println(\"内存映射文件所花时间：\"+(end-start));&#125;\n\n3. 分散（Scatter）和聚集（Gather）分散读取（Scattering Reads）是指从Channel中读取的数据“分散”到多个Buffer中。\n聚集写入（Gathering Writes）是指将多个Buffer中的数据“聚集”到Channel。\n//分散和聚集public void test4() throws IOException&#123;    RandomAccessFile raf1 = new RandomAccessFile(\"1.txt\", \"rw\");    //1. 获取通道    FileChannel channel1 = raf1.getChannel();    //2. 分配指定大小的缓冲区    ByteBuffer buf1 = ByteBuffer.allocate(100);    ByteBuffer buf2 = ByteBuffer.allocate(1024);    //3. 分散读取    ByteBuffer[] bufs = &#123;buf1, buf2&#125;;    channel1.read(bufs);    for (ByteBuffer byteBuffer : bufs) &#123;        byteBuffer.flip();    &#125;    System.out.println(new String(bufs[0].array(), 0, bufs[0].limit()));    System.out.println(\"-----------------\");    System.out.println(new String(bufs[1].array(), 0, bufs[1].limit()));    //4. 聚集写入    RandomAccessFile raf2 = new RandomAccessFile(\"2.txt\", \"rw\");    FileChannel channel2 = raf2.getChannel();    channel2.write(bufs);&#125;\n\n4. 阻塞与非阻塞\n传统的IO流都是阻塞式的。也就是说，当一个线程调用read()或write()时，该线程被阻塞，直到有一些数据被读取或写入，该线程在此期间不能执行其他任务。因此，在完成网络通信进行IO操作时，由于线程会阻塞，所以服务器端必须为每个客户端都提供一个独立的线程进行处理，当服务器端需要处理大量客户端时，性能急剧下降。\nJava NIO是非阻塞模式的。当线程从某通道进行读写数据时，若没有数据可用时，该线程可以进行其他任务。线程通常将非阻塞IO的空闲时间用于在其他通道上执行IO操作，所以单独的线程可以管理多个输入和输出通道。因此，NIO可以让服务器端使用一个或有限几个线程来同时处理连接到服务器端的所有客户端。\n\n5. 选择器（Selector）选择器（Selector）是SelectableChannel对象的多路复用器，Selector可以同时监控多个SelectableChannel的IO状况，也就是说，利用Selector可使一个单独的线程管理多个Channel。Selector是非阻塞IO的核心。\n\n5.1 选择器的使用\n创建Selector：通过调用Selector.open()方法创建一个Selector\n\n//创建一个选择器，并把SocketChannel交给selector对象Selector selector = Selector.open();\n\n\n向选择器注册通道\n\n// 创建一个Socket套接字Socket socket = new Socket(InetAddress.getByName(\"127.0.0.1\"), 8888);// 获取SocketChannelSocketChannel channel = socket.getChannel();// 创建选择器Selector selector = Selector.open();// 将SocketChannel切换到非阻塞模式channel.configureBlocking(false);// 向Selector注释Channelchannel.register(selector, SelectionKey.OP_ACCEPT);\n\n当调用register(Selector sel, int ops)给通道注册选择器时，需要设置选择监听的事件类型，通过第二个参数ops指定。\n可以监听的事件类型（可使用SelectionKey的四个常量表示）：\n\n读：SelectionKey.OP_READ(1 &lt;&lt; 0 : 1)\n\n写：SelectionKey.OP_WRITE(1 &lt;&lt; 2 : 4)\n\n连接：SelectionKey.OP_CONNECT(1 &lt;&lt; 3 : 8)\n\n接受：SelectionKey.OP_ACCEPT(1&lt;&lt;4 : 16)\n\n\n若注册时不止监听一个事件，则可以使用“位或”操作符连接。\nint interestSet = SelectionKey.OP_READ | SelectionKey.OP_WRITE;\n\n5.2 选择键（SelectionKey）\n表示SelectableChannel和Selector之间的注册关系。\n\n\n\n\n方法\n描述\n\n\n\nint interestOps()\n获取感兴趣事件集合\n\n\nint readyOps()\n获取通道已经准备就绪的操作的集合\n\n\nSelectableChannel channel()\n获取注册通道\n\n\nSelector selector()\n返回选择器\n\n\nboolean isReadable()\n检测Channel中读事件是否就绪\n\n\nboolean isWritable()\n检测Channel中写事件是否就绪\n\n\nboolean isConnectable()\n检测Channel中连接是否就绪\n\n\nboolean isConnectable()\n检测Channel中接收是否就绪\n\n\nObject attach(Object ob)\n将给定的对象附加到此键\n\n\nObject attachment()\n获取当前的附加对象\n\n\nselectionKey.attach(theObject);Object attachedObj = selectionKey.attachment();\n\n5.3 选择器（Selector）的常用方法\n\n\n方法\n描述\n\n\n\nSet&lt;SelectionKey&gt; keys()\n所有的SelectionKey集合。代表注册在该Selector上的Channel\n\n\nSet&lt;SelectionKey&gt; selectedKeys()\n被选择的SelectionKey集合。返回次Selector的已选择键集\n\n\nint select()\n监控所有注册的Channel，当它们中间有需要处理的IO操作时，根据设置SelectionKey的集合，返回符合匹配的Channel的数量\n\n\nint select(long timeout)\n可以设置超时时长的select()操作\n\n\nint selectNow()\n执行一个立即返回的select()操作，该方法不会阻塞线程\n\n\nSelector wakeup()\n使一个还未返回的select()操作方法立即返回\n\n\nvoid close()\n关闭该选择器\n\n\n6. 管道（Pipe）\nJava NIO管道是2个线程之间的单向数据连接。Pipe有一个source通道和一个sink通道。数据会被写到sink通道，从source通道读取。\n\n\n向管道写数据\n\npublic void write(String str) throws IOException &#123;    // 创建管道    Pipe pipe = Pipe.open();    // 向管道写输入    Pipe.SinkChannel sink = pipe.sink();    // 通过sink的write()方法写数据    ByteBuffer buf = ByteBuffer.allocate(1024);    buf.clear();    buf.put(str.getBytes());    buf.flip();    while (buf.hasRemaining()) &#123;        sink.write(buf);    &#125;&#125;\n\n\n从管道读数据\n\npublic void read(Pipe pipe) throws IOException &#123;    Pipe.SourceChannel source = pipe.source();    ByteBuffer buffer = ByteBuffer.allocate(1024);    source.read(buffer);&#125;\n\n7. Path与Paths\njava.nio.file.Path接口代表一个平台无关的平台路径，描述了目录结构中文件的位置\n\nPaths提供get()方法用来获取Path对象\nPath get(String first, String... more)：用于将多个字符串连接成路径\n\nPath常用方法：\nboolean endsWith(String other)：判断是否以other路径结束\nboolean startsWith(String other)：判断是否以other路径开始\nboolean isAbsolute()：判断是否是绝对路径\nPath getFileName()：返回与调用Path对象关联的文件名\nPath getName(int index)：返回的指定索引位置index的路径名称\nint getNameCount()：返回Path根目录后面元素的数量\nPath getParent()：返回Path对象包含整个路径，不包含Path对象指定的文件路径\nPath getRoot()：返回调用Path对象的根路径\nPath resolve(String other)：将相对路径解析为绝对路径\nPath toAbsolutePath()：作为绝对路径返回调用Path对象\nString toString()：返回调用Path对象的字符串表示形式\n\n\n8. Files类\njava.nio.file.Files用于操作文件或目录的工具类\n\n\nFiles常用方法：\n\ncopy(InputStream in, Path target, CopyOption... options)：文件的复制\nPath createDirectory(Path dir, FileAttribute&lt;?&gt;... attrs)：创建一个目录\nPath createFile(Path path, FileAttribute&lt;?&gt;... attrs)：创建一个文件\nvoid delete(Path path)：删除一个文件\nPath move(Path source, Path target, CopyOption... options)：将src移动到target位置\nlong size(Path path)：返回指定文件的大小\n\n用于判断\n\nboolean exists(Path path, LinkOption... options)：判断文件是否存在\nboolean isDirectory(Path path, LinkOption... options)：判断是否是目录\nboolean isExecutable(Path path)：判断是否是可执行文件\nboolean isHidden(Path path)：判断是否是隐藏文件\nboolean isReadable(Path path)：判断是否是可读\nboolean isWritable(Path path)：判断是否是可写\nboolean notExists(Path path, LinkOption... options)：判断文件是否不存在\n&lt;A extends BasicFileAttributes&gt; A readAttributes(Path path, Class&lt;A&gt; type, LinkOption... options)：获取与path指定的文件相关联的属性\n\n用于操作内容\n\nSeekableByteChannel newByteChannel(Path path, OpenOption... options)：获取与指定文件的连接，options指定打开方式\nDirectoryStream&lt;Path&gt; newDirectoryStream(Path dir)：打开path指定的目录\nInputStream newInputStream(Path path, OpenOption... options)：获取InputStream对象\nOutputStream newOutputStream(Path path, OpenOption... options)：获取OutputStream对象\n","tags":["Java","NIO"]},{"title":"MySQL-锁","url":"/2019/11/13/MySQL%E9%94%81/","content":"一、概述数据库锁定机制简单来说，就是数据库为了保证数据的一致性，而使各种共享资源在被并发访问变得有序所设计的一种规则。对于任何一种数据库来说都需要有相应的锁定机制，所以MySQL自然也不能例外。MySQL数据库由于其自身架构的特点，存在多种数据存储引擎，每种存储引擎所针对的应用场景特点都不太一样，为了满足各自特定应用场景的需求，每种存储引擎的锁定机制都是为各自所面对的特定场景而优化设计，所以各存储引擎的锁定机制也有较大区别。MySQL各存储引擎使用了三种类型（级别）的锁定机制：表级锁定，行级锁定和页级锁定。\n1. 表级锁定（table-level）表级别的锁定是MySQL各存储引擎中最大颗粒度的锁定机制。该锁定机制最大的特点是实现逻辑非常简单，带来的系统负面影响最小。所以获取锁和释放锁的速度很快。由于表级锁一次会将整个表锁定，所以可以很好的避免困扰我们的死锁问题。当然，锁定颗粒度大所带来最大的负面影响就是出现锁定资源争用的概率也会最高，致使并大度大打折扣。使用表级锁定的主要是MyISAM，MEMORY，CSV等一些非事务性存储引擎。\n2. 行级锁定（row-level）行级锁定最大的特点就是锁定对象的颗粒度很小，也是目前各大数据库管理软件所实现的锁定颗粒度最小的。由于锁定颗粒度很小，所以发生锁定资源争用的概率也最小，能够给予应用程序尽可能大的并发处理能力而提高一些需要高并发应用系统的整体性能。虽然能够在并发处理能力上面有较大的优势，但是行级锁定也因此带来了不少弊端。由于锁定资源的颗粒度很小，所以每次获取锁和释放锁需要做的事情也更多，带来的消耗自然也就更大了。此外，行级锁定也最容易发生死锁。使用行级锁定的主要是InnoDB存储引擎。\n3. 页级锁定（page-level）页级锁定是MySQL中比较独特的一种锁定级别，在其他数据库管理软件中也并不是太常见。页级锁定的特点是锁定颗粒度介于行级锁定与表级锁之间，所以获取锁定所需要的资源开销，以及所能提供的并发处理能力也同样是介于上面二者之间。另外，页级锁定和行级锁定一样，会发生死锁。在数据库实现资源锁定的过程中，随着锁定资源颗粒度的减小，锁定相同数据量的数据所需要消耗的内存数量是越来越多的，实现算法也会越来越复杂。不过，随着锁定资源颗粒度的减小，应用程序的访问请求遇到锁等待的可能性也会随之降低，系统整体并发度也随之提升。使用页级锁定的主要是BerkeleyDB存储引擎。\n4. 总结MySQL这3种锁的特性可大致归纳如下：\n\n表级锁：开销小，加锁快；不会出现死锁；锁定粒度大，发生锁冲突的概率最高，并发度最低；\n行级锁：开销大，加锁慢；会出现死锁；锁定粒度最小，发生锁冲突的概率最低，并发度也最高；\n页面锁：开销和加锁时间界于表锁和行锁之间；会出现死锁；锁定粒度界于表锁和行锁之间，并发度一般。\n\n适用：从锁的角度来说，表级锁更适合于以查询为主，只有少量按索引条件更新数据的应用，如Web应用；而行级锁则更适合于有大量按索引条件并发更新少量不同数据，同时又有并发查询的应用，如一些在线事务处理（OLTP）系统。\n二、表级锁定由于MyISAM存储引擎使用的锁定机制完全是由MySQL提供的表级锁定实现，所以下面我们将以MyISAM存储引擎作为示例存储引擎。\n1. MySQL表级锁的锁模式MySQL的表级锁有两种模式：表共享读锁（Table Read Lock）和表独占写锁（Table Write Lock）。锁模式的兼容性：\n\n对MyISAM表的读操作，不会阻塞其他用户对同一表的读请求，但会阻塞对同一表的写请求；\n对MyISAM表的写操作，则会阻塞其他用户对同一表的读和写操作；\nMyISAM表的读操作与写操作之间，以及写操作之间是串行的。当一个线程获得对一个表的写锁后，只有持有锁的线程可以对表进行更新操作。其他线程的读、写操作都会等待，直到锁被释放为止。\n\n2. 如何加表锁MyISAM在执行查询语句（SELECT）前，会自动给涉及的所有表加读锁，在执行更新操作（UPDATE、DELETE、INSERT等）前，会自动给涉及的表加写锁，这个过程并不需要用户干预，因此，用户一般不需要直接用LOCK  TABLE命令给MyISAM表显式加锁。\n3. MyISAM表锁优化建议对于MyISAM存储引擎，虽然使用表级锁定在锁定实现的过程中比实现行级锁定或者页级锁所带来的附加成本都要小，锁定本身所消耗的资源也是最少。但是由于锁定的颗粒度比较到，所以造成锁定资源的争用情况也会比其他的锁定级别都要多，从而在较大程度上会降低并发处理能力。所以，在优化MyISAM存储引擎锁定问题的时候，最关键的就是如何让其提高并发度。由于锁定级别是不可能改变的了，所以我们首先需要尽可能让锁定的时间变短，然后就是让可能并发进行的操作尽可能的并发。\n（1）查询表级锁争用情况MySQL内部有两组专门的状态变量记录系统内部锁资源争用情况：\nmysql&gt; show status like &#39;table%&#39;;+----------------------------+---------+| Variable_name              | Value   |+----------------------------+---------+| Table_locks_immediate      | 100     || Table_locks_waited         | 11      |+----------------------------+---------+\n\n这里有两个状态变量记录MySQL内部表级锁定的情况，两个变量说明如下：\n\nTable_locks_immediate：产生表级锁定的次数；\nTable_locks_waited：出现表级锁定争用而发生等待的次数；\n\n两个状态值都是从系统启动后开始记录，出现一次对应的事件则数量加1。如果这里的Table_locks_waited状态值比较高，那么说明系统中表级锁定争用现象比较严重，就需要进一步分析为什么会有较多的锁定资源争用了。\n（2）缩短锁定时间如何让锁定时间尽可能的短呢？唯一的办法就是让我们的Query执行时间尽可能的短。\n\na) 尽两减少大的复杂Query，将复杂Query分拆成几个小的Query分布进行；\nb) 尽可能的建立足够高效的索引，让数据检索更迅速；\nc) 尽量让MyISAM存储引擎的表只存放必要的信息，控制字段类型；\nd) 利用合适的机会优化MyISAM表数据文件。\n\n（3）分离能并行的操作说到MyISAM的表锁，而且是读写互相阻塞的表锁，可能有些人会认为在MyISAM存储引擎的表上就只能是完全的串行化，没办法再并行了。大家不要忘记了，MyISAM的存储引擎还有一个非常有用的特性，那就是ConcurrentInsert（并发插入）的特性。\nMyISAM存储引擎有一个控制是否打开Concurrent Insert功能的参数选项：concurrent_insert，可以设置为0，1或者2。三个值的具体说明如下：\n\nconcurrent_insert=2，无论MyISAM表中有没有空洞，都允许在表尾并发插入记录；\nconcurrent_insert=1，如果MyISAM表中没有空洞（即表的中间没有被删除的行），MyISAM允许在一个进程读表的同时，另一个进程从表尾插入记录。这也是MySQL的默认设置；\nconcurrent_insert=0，不允许并发插入。\n\n可以利用MyISAM存储引擎的并发插入特性，来解决应用中对同一表查询和插入的锁争用。例如，将concurrent_insert系统变量设为2，总是允许并发插入；同时，通过定期在系统空闲时段执行OPTIMIZE  TABLE语句来整理空间碎片，收回因删除记录而产生的中间空洞。\n（4）合理利用读写优先级MyISAM存储引擎的是读写互相阻塞的，那么，一个进程请求某个MyISAM表的读锁，同时另一个进程也请求同一表的写锁，MySQL如何处理呢？\n答案是写进程先获得锁。不仅如此，即使读请求先到锁等待队列，写请求后到，写锁也会插到读锁请求之前。\n这是因为MySQL的表级锁定对于读和写是有不同优先级设定的，默认情况下是写优先级要大于读优先级。\n所以，如果我们可以根据各自系统环境的差异决定读与写的优先级：\n通过执行命令SET LOW_PRIORITY_UPDATES=1，使该连接读比写的优先级高。如果我们的系统是一个以读为主，可以设置此参数，如果以写为主，则不用设置；\n通过指定INSERT、UPDATE、DELETE语句的LOW_PRIORITY属性，降低该语句的优先级。\n虽然上面方法都是要么更新优先，要么查询优先的方法，但还是可以用其来解决查询相对重要的应用（如用户登录系统）中，读锁等待严重的问题。\n另外，MySQL也提供了一种折中的办法来调节读写冲突，即给系统参数max_write_lock_count设置一个合适的值，当一个表的读锁达到这个值后，MySQL就暂时将写请求的优先级降低，给读进程一定获得锁的机会。\n这里还要强调一点：一些需要长时间运行的查询操作，也会使写进程“饿死”，因此，应用中应尽量避免出现长时间运行的查询操作，不要总想用一条SELECT语句来解决问题，因为这种看似巧妙的SQL语句，往往比较复杂，执行时间较长，在可能的情况下可以通过使用中间表等措施对SQL语句做一定的“分解”，使每一步查询都能在较短时间完成，从而减少锁冲突。如果复杂查询不可避免，应尽量安排在数据库空闲时段执行，比如一些定期统计可以安排在夜间执行。\n三、行级锁定行级锁定不是MySQL自己实现的锁定方式，而是由其他存储引擎自己所实现的，如广为大家所知的InnoDB存储引擎，以及MySQL的分布式存储引擎NDBCluster等都是实现了行级锁定。考虑到行级锁定君由各个存储引擎自行实现，而且具体实现也各有差别，而InnoDB是目前事务型存储引擎中使用最为广泛的存储引擎，所以这里我们就主要分析一下InnoDB的锁定特性。\n1. InnoDB锁定模式及实现机制考虑到行级锁定君由各个存储引擎自行实现，而且具体实现也各有差别，而InnoDB是目前事务型存储引擎中使用最为广泛的存储引擎，所以这里我们就主要分析一下InnoDB的锁定特性。\n总的来说，InnoDB的锁定机制和Oracle数据库有不少相似之处。InnoDB的行级锁定同样分为两种类型，共享锁和排他锁，而在锁定机制的实现过程中为了让行级锁定和表级锁定共存，InnoDB也同样使用了意向锁（表级锁定）的概念，也就有了意向共享锁和意向排他锁这两种。\n当一个事务需要给自己需要的某个资源加锁的时候，如果遇到一个共享锁正锁定着自己需要的资源的时候，自己可以再加一个共享锁，不过不能加排他锁。但是，如果遇到自己需要锁定的资源已经被一个排他锁占有之后，则只能等待该锁定释放资源之后自己才能获取锁定资源并添加自己的锁定。而意向锁的作用就是当一个事务在需要获取资源锁定的时候，如果遇到自己需要的资源已经被排他锁占用的时候，该事务可以需要锁定行的表上面添加一个合适的意向锁。如果自己需要一个共享锁，那么就在表上面添加一个意向共享锁。而如果自己需要的是某行（或者某些行）上面添加一个排他锁的话，则先在表上面添加一个意向排他锁。意向共享锁可以同时并存多个，但是意向排他锁同时只能有一个存在。所以，可以说InnoDB的锁定模式实际上可以分为四种：共享锁（S），排他锁（X），意向共享锁（IS）和意向排他锁（IX）\n如果一个事务请求的锁模式与当前的锁兼容，InnoDB就将请求的锁授予该事务；反之，如果两者不兼容，该事务就要等待锁释放。\n意向锁是InnoDB自动加的，不需用户干预。对于UPDATE、DELETE和INSERT语句，InnoDB会自动给涉及数据集加排他锁（X)；对于普通SELECT语句，InnoDB不会加任何锁；事务可以通过以下语句显示给记录集加共享锁或排他锁。\n共享锁（S）：SELECT * FROM table_name WHERE ... LOCK IN SHARE MODE排他锁（X)：SELECT * FROM table_name WHERE ... FOR UPDATE\n\n用SELECT … IN SHARE MODE获得共享锁，主要用在需要数据依存关系时来确认某行记录是否存在，并确保没有人对这个记录进行UPDATE或者DELETE操作。但是如果当前事务也需要对该记录进行更新操作，则很有可能造成死锁，对于锁定行记录后需要进行更新操作的应用，应该使用SELECT… FOR UPDATE方式获得排他锁。\n2. InnoDB行锁实现方式InnoDB行锁是通过给索引上的索引项加锁来实现的，只有通过索引条件检索数据，InnoDB才使用行级锁，否则，InnoDB将使用表锁\n在实际应用中，要特别注意InnoDB行锁的这一特性，不然的话，可能导致大量的锁冲突，从而影响并发性能。下面通过一些实际例子来加以说明。\n\n（1）在不通过索引条件查询的时候，InnoDB确实使用的是表锁，而不是行锁。\n（2）由于MySQL的行锁是针对索引加的锁，不是针对记录加的锁，所以虽然是访问不同行的记录，但是如果是使用相同的索引键，是会出现锁冲突的。\n（3）当表有多个索引的时候，不同的事务可以使用不同的索引锁定不同的行，另外，不论是使用主键索引、唯一索引或普通索引，InnoDB都会使用行锁来对数据加锁。\n（4）即便在条件中使用了索引字段，但是否使用索引来检索数据是由MySQL通过判断不同执行计划的代价来决定的，如果MySQL认为全表扫描效率更高，比如对一些很小的表，它就不会使用索引，这种情况下InnoDB将使用表锁，而不是行锁。因此，在分析锁冲突时，别忘了检查SQL的执行计划，以确认是否真正使用了索引。\n\n3. 间隙锁（Next-Key锁）当我们用范围条件而不是相等条件检索数据，并请求共享或排他锁时，InnoDB会给符合条件的已有数据记录的索引项加锁；对于键值在条件范围内但并不存在的记录，叫做“间隙（GAP)”，InnoDB也会对这个“间隙”加锁，这种锁机制就是所谓的间隙锁（Next-Key锁）。例：假如emp表中只有101条记录，其empid的值分别是 1,2,…,100,101，下面的SQL：\nmysql&gt; select * from emp where empid &gt; 100 for update;\n\n是一个范围条件的检索，InnoDB不仅会对符合条件的empid值为101的记录加锁，也会对empid大于101（这些记录并不存在）的“间隙”加锁。InnoDB使用间隙锁的目的：\n\n（1）防止幻读，以满足相关隔离级别的要求。对于上面的例子，要是不使用间隙锁，如果其他事务插入了empid大于100的任何记录，那么本事务如果再次执行上述语句，就会发生幻读；\n（2）为了满足其恢复和复制的需要。\n\n很显然，在使用范围条件检索并锁定记录时，即使某些不存在的键值也会被无辜的锁定，而造成在锁定的时候无法插入锁定键值范围内的任何数据。在某些场景下这可能会对性能造成很大的危害。除了间隙锁给InnoDB带来性能的负面影响之外，通过索引实现锁定的方式还存在其他几个较大的性能隐患：\n\n（1）当Query无法利用索引的时候，InnoDB会放弃使用行级别锁定而改用表级别的锁定，造成并发性能的降低；\n（2）当Query使用的索引并不包含所有过滤条件的时候，数据检索使用到的索引键所只想的数据可能有部分并不属于该Query的结果集的行列，但是也会被锁定，因为间隙锁锁定的是一个范围，而不是具体的索引键；\n（3）当Query在使用索引定位数据的时候，如果使用的索引键一样但访问的数据行不同的时候（索引只是过滤条件的一部分），一样会被锁定。\n\n因此，在实际应用开发中，尤其是并发插入比较多的应用，我们要尽量优化业务逻辑，尽量使用相等条件来访问更新数据，避免使用范围条件。\n还要特别说明的是，InnoDB除了通过范围条件加锁时使用间隙锁外，如果使用相等条件请求给一个不存在的记录加锁，InnoDB也会使用间隙锁。\n4. 死锁上文讲过，MyISAM表锁是deadlock   free的，这是因为MyISAM总是一次获得所需的全部锁，要么全部满足，要么等待，因此不会出现死锁。但在InnoDB中，除单个SQL组成的事务外，锁是逐步获得的，当两个事务都需要获得对方持有的排他锁才能继续完成事务，这种循环锁等待就是典型的死锁。\n在InnoDB的事务管理和锁定机制中，有专门检测死锁的机制，会在系统中产生死锁之后的很短时间内就检测到该死锁的存在。当InnoDB检测到系统中产生了死锁之后，InnoDB会通过相应的判断来选这产生死锁的两个事务中较小的事务来回滚，而让另外一个较大的事务成功完成。\n那InnoDB是以什么来为标准判定事务的大小的呢？MySQL官方手册中也提到了这个问题，实际上在InnoDB发现死锁之后，会计算出两个事务各自插入、更新或者删除的数据量来判定两个事务的大小。也就是说哪个事务所改变的记录条数越多，在死锁中就越不会被回滚掉。\n但是有一点需要注意的就是，当产生死锁的场景中涉及到不止InnoDB存储引擎的时候，InnoDB是没办法检测到该死锁的，这时候就只能通过锁定超时限制参数InnoDB_lock_wait_timeout来解决。需要说明的是，这个参数并不是只用来解决死锁问题，在并发访问比较高的情况下，如果大量事务因无法立即获得所需的锁而挂起，会占用大量计算机资源，造成严重性能问题，甚至拖跨数据库。我们通过设置合适的锁等待超时阈值，可以避免这种情况发生。\n通常来说，死锁都是应用设计的问题，通过调整业务流程、数据库对象设计、事务大小，以及访问数据库的SQL语句，绝大部分死锁都可以避免。下面就通过实例来介绍几种避免死锁的常用方法：\n\n（1）在应用中，如果不同的程序会并发存取多个表，应尽量约定以相同的顺序来访问表，这样可以大大降低产生死锁的机会。\n（2）在程序以批量方式处理数据的时候，如果事先对数据排序，保证每个线程按固定的顺序来处理记录，也可以大大降低出现死锁的可能。\n（3）在事务中，如果要更新记录，应该直接申请足够级别的锁，即排他锁，而不应先申请共享锁，更新时再申请排他锁，因为当用户申请排他锁时，其他事务可能又已经获得了相同记录的共享锁，从而造成锁冲突，甚至死锁。\n（4）在REPEATABLE-READ隔离级别下，如果两个线程同时对相同条件记录用SELECT…FOR   UPDATE加排他锁，在没有符合该条件记录情况下，两个线程都会加锁成功。程序发现记录尚不存在，就试图插入一条新记录，如果两个线程都这么做，就会出现死锁。这种情况下，将隔离级别改成READ  COMMITTED，就可避免问题。\n（5）当隔离级别为READ COMMITTED时，如果两个线程都先执行SELECT…FOR  UPDATE，判断是否存在符合条件的记录，如果没有，就插入记录。此时，只有一个线程能插入成功，另一个线程会出现锁等待，当第1个线程提交后，第2个线程会因主键重出错，但虽然这个线程出错了，却会获得一个排他锁。这时如果有第3个线程又来申请排他锁，也会出现死锁。对于这种情况，可以直接做插入操作，然后再捕获主键重异常，或者在遇到主键重错误时，总是执行ROLLBACK释放获得的排他锁。\n\n5. 什么时候使用表锁对于InnoDB表，在绝大部分情况下都应该使用行级锁，因为事务和行锁往往是我们之所以选择InnoDB表的理由。但在个别特殊事务中，也可以考虑使用表级锁：\n\n（1）事务需要更新大部分或全部数据，表又比较大，如果使用默认的行锁，不仅这个事务执行效率低，而且可能造成其他事务长时间锁等待和锁冲突，这种情况下可以考虑使用表锁来提高该事务的执行速度。\n（2）事务涉及多个表，比较复杂，很可能引起死锁，造成大量事务回滚。这种情况也可以考虑一次性锁定事务涉及的表，从而避免死锁、减少数据库因事务回滚带来的开销。当然，应用中这两种事务不能太多，否则，就应该考虑使用MyISAM表了。\n\n在InnoDB下，使用表锁要注意以下两点。\n\n（1）使用LOCK  TABLES虽然可以给InnoDB加表级锁，但必须说明的是，表锁不是由InnoDB存储引擎层管理的，而是由其上一层──MySQL  Server负责的，仅当autocommit=0、InnoDB_table_locks=1（默认设置）时，InnoDB层才能知道MySQL加的表锁，MySQL   Server也才能感知InnoDB加的行锁，这种情况下，InnoDB才能自动识别涉及表级锁的死锁，否则，InnoDB将无法自动检测并处理这种死锁。\n（2）在用  LOCK TABLES对InnoDB表加锁时要注意，要将AUTOCOMMIT设为0，否则MySQL不会给表加锁；事务结束前，不要用UNLOCK  TABLES释放表锁，因为UNLOCK TABLES会隐含地提交事务；COMMIT或ROLLBACK并不能释放用LOCK  TABLES加的表级锁，必须用UNLOCK TABLES释放表锁。正确的方式见如下语句：例如，如果需要写表t1并从表t读，可以按如下做：\n\nSET AUTOCOMMIT&#x3D;0;LOCK TABLES t1 WRITE, t2 READ, ...;[do something with tables t1 and t2 here];COMMIT;UNLOCK TABLES;\n\n6. InnoDB行锁优化建议InnoDB存储引擎由于实现了行级锁定，虽然在锁定机制的实现方面所带来的性能损耗可能比表级锁定会要更高一些，但是在整体并发处理能力方面要远远优于MyISAM的表级锁定的。当系统并发量较高的时候，InnoDB的整体性能和MyISAM相比就会有比较明显的优势了。但是，InnoDB的行级锁定同样也有其脆弱的一面，当我们使用不当的时候，可能会让InnoDB的整体性能表现不仅不能比MyISAM高，甚至可能会更差。\n\n（1）要想合理利用InnoDB的行级锁定，做到扬长避短，我们必须做好以下工作：\na) 尽可能让所有的数据检索都通过索引来完成，从而避免InnoDB因为无法通过索引键加锁而升级为表级锁定；\nb) 合理设计索引，让InnoDB在索引键上面加锁的时候尽可能准确，尽可能的缩小锁定范围，避免造成不必要的锁定而影响其他Query的执行；\nc) 尽可能减少基于范围的数据检索过滤条件，避免因为间隙锁带来的负面影响而锁定了不该锁定的记录；\nd) 尽量控制事务的大小，减少锁定的资源量和锁定时间长度；\ne) 在业务环境允许的情况下，尽量使用较低级别的事务隔离，以减少MySQL因为实现事务隔离级别所带来的附加成本。\n\n\n（2）由于InnoDB的行级锁定和事务性，所以肯定会产生死锁，下面是一些比较常用的减少死锁产生概率的小建议：\na) 类似业务模块中，尽可能按照相同的访问顺序来访问，防止产生死锁；\nb) 在同一个事务中，尽可能做到一次锁定所需要的所有资源，减少死锁产生概率；\nc) 对于非常容易产生死锁的业务部分，可以尝试使用升级锁定颗粒度，通过表级锁定来减少死锁产生的概率。\n\n\n（3）可以通过检查InnoDB_row_lock状态变量来分析系统上的行锁的争夺情况：mysql&gt; show status like &#39;InnoDB_row_lock%&#39;;+-------------------------------+-------+| Variable_name                 | Value |+-------------------------------+-------+| InnoDB_row_lock_current_waits | 0     || InnoDB_row_lock_time          | 0     || InnoDB_row_lock_time_avg      | 0     || InnoDB_row_lock_time_max      | 0     || InnoDB_row_lock_waits         | 0     |+-------------------------------+-------+\n\n\n\nInnoDB 的行级锁定状态变量不仅记录了锁定等待次数，还记录了锁定总时长，每次平均时长，以及最大时长，此外还有一个非累积状态量显示了当前正在等待锁定的等待数量。对各个状态量的说明如下：\n\nInnoDB_row_lock_current_waits：当前正在等待锁定的数量；\nInnoDB_row_lock_time：从系统启动到现在锁定总时间长度；\nInnoDB_row_lock_time_avg：每次等待所花平均时间；\nInnoDB_row_lock_time_max：从系统启动到现在等待最常的一次所花的时间；\nInnoDB_row_lock_waits：系统启动后到现在总共等待的次数；\n\n对于这5个状态变量，比较重要的主要是InnoDB_row_lock_time_avg（等待平均时长），InnoDB_row_lock_waits（等待总次数）以及InnoDB_row_lock_time（等待总时长）这三项。尤其是当等待次数很高，而且每次等待时长也不小的时候，我们就需要分析系统中为什么会有如此多的等待，然后根据分析结果着手指定优化计划。\n如果发现锁争用比较严重，如InnoDB_row_lock_waits和InnoDB_row_lock_time_avg的值比较高，还可以通过设置InnoDB Monitors 来进一步观察发生锁冲突的表、数据行等，并分析锁争用的原因。\n锁冲突的表、数据行等，并分析锁争用的原因。具体方法如下：\nmysql&gt; create table InnoDB_monitor(a INT) engine&#x3D;InnoDB;\n\n然后就可以用下面的语句来进行查看：\nmysql&gt; show engine InnoDB status;\n\n监视器可以通过发出下列语句来停止查看：\nmysql&gt; drop table InnoDB_monitor;\n\n设置监视器后，会有详细的当前锁等待的信息，包括表名、锁类型、锁定记录的情况等，便于进行进一步的分析和问题的确定。可能会有读者朋友问为什么要先创建一个叫InnoDB_monitor的表呢？因为创建该表实际上就是告诉InnoDB我们开始要监控他的细节状态了，然后InnoDB就会将比较详细的事务以及锁定信息记录进入MySQL的errorlog中，以便我们后面做进一步分析使用。打开监视器以后，默认情况下每15秒会向日志中记录监控的内容，如果长时间打开会导致.err文件变得非常的巨大，所以用户在确认问题原因之后，要记得删除监控表以关闭监视器，或者通过使用“–console”选项来启动服务器以关闭写日志文件。\n四、常用命令查询哪些线程运行show full processlist;\n\n查看哪些表可以打开show open tables WHERE In_use&gt;0;\n","tags":["MySQL","数据库"]},{"title":"Linux常用命令","url":"/2019/10/22/Linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/","content":"常用技巧修改YUM源为国内源修改为阿里yum源-mirrors.aliyun.com\n\n1、首先备份系统自带yum源配置文件/etc/yum.repos.d/CentOS-Base.repo\n\nmv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.backup\n\n\n2、查看CentOS系统版本\n\nlsb_release -a\n\n\n3、下载ailiyun的yum源配置文件到/etc/yum.repos.d/\n\n# CentOS 5wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-5.repo# CentOS 6wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-6.repo# CentOS 7wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo\n\n\n4、清理缓存\n\nyum clean all\n\n\n5、重新生成缓存\n\nyum makecache\n\n\n6、验证\n\nyum repolist\n\n查看磁盘占用du -sh *du -sh /var/lib/mysql\n\n查看Linux版本cat /etc/centos-releasecat /proc/versionuname -acat /etc/issue# 查看位数getconf LONG_BIT\n\n# 可以查看所有用户的列表cat /etc/passwd# 查看用户组cat /etc/group# 查看当前登录用户的组内成员groups# 查看gliethttp用户所在的组,以及组内成员groups gliethttp# 查看当前登录用户名whoami\n\n查看支持的Shell类型cat /etc/shells# 切换zsh模式chsh -s /bin/zsh\n\n修改IP# 通过修改配置文件vim /etc/sysconfig/network-scripts/ifcfg-eth0# 重启网络service network restart# 查看改动后的效果ip addr\n\n命令传参符号：``名称：反引号，上分隔符\n位置：反引号（`）这个字符一般在键盘的左上角，数字1的左边，不要将其同单引号（’）混淆\n作用：反引号括起来的字符串被shell解释为命令行，在执行时，shell首先执行该命令行，并以它的标准输出结果取代整个反引号（包括两个反引号）部分\n举例：\necho `rpm -qa|grep java`\n\n命令：xargsxargs是给命令传递参数的一个过滤器，也是组合多个命令的一个工具。它把一个数据流分割为一些足够小的块，以方便过滤器和命令进行处理。\n通常情况下，xargs从管道或者stdin中读取数据，但是它也能够从文件的输出中读取数据。\nxargs的默认命令是echo，这意味着通过管道传递给xargs的输入将会包含换行和空白，不过通过xargs的处理，换行和空白将被空格取代。\n举例：\nrpm -qa|grep java |xargs echo\n\n命令：awk就是把文件逐行的读入，以空格为默认分隔符将每行切片，切开的部分再进行各种分析处理。\nrpm -qa|grep java|awk '&#123;print $1&#125;'\n\n文件句柄数量查看Linux系统默认的最大文件句柄数，系统默认是1024\n查看Linux系统某个进程打开的文件句柄数量\nlsof -n | grep 5950 -c\n\n修改Linux系统的最大文件句柄数限制的方法：\n\nulimit -n 65535\n\n针对当前session有效，用户退出或者系统重新后恢复默认值\n\n修改profile文件：在profile文件中添加：ulimit -n 65535\n\n只对单个用户有效\n\n修改文件：/etc/security/limits.conf，在文件中添加：（立即生效-当前session中运行ulimit -a命令无法显示）\n\nsoft nofile 32768 #限制单个进程最大文件句柄数（到达此限制时系统报警）\n\nhard nofile 65536 #限制单个进程最大文件句柄数（到达此限制时系统报错）\n\n修改文件：/etc/sysctl.conf。在文件中添加：\n\nfs.file-max=655350 #限制整个系统最大文件句柄数\n\n\n运行命令：/sbin/sysctl -p 使配置生效\n某某人的心得#!/bin/bashsed -i 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/selinux/config  #关闭selinuxsed -i 's/#GSSAPIAuthentication yes/GSSAPIAuthentication no/g' /etc/ssh/sshd_configsed -i 's/#UseDNS yes/UseDNS no/g' /etc/ssh/sshd_configsed -i 's/BOOTPROTO=\"dhcp\"/BOOTPROTO=\"static\"/g' /etc/sysconfig/network-scripts/ifcfg-eth0echo \"IPADDR='192.168.99.2'\"  &gt;&gt; /etc/sysconfig/network-scripts/ifcfg-eth0echo \"PREFIX=21\" &gt;&gt; /etc/sysconfig/network-scripts/ifcfg-eth0echo \"GATEWAY='192.168.100.1'\" &gt;&gt; /etc/sysconfig/network-scripts/ifcfg-eth0echo \"DNS1='219.141.136.10'\" &gt;&gt; /etc/sysconfig/network-scripts/ifcfg-eth0systemctl set-default multi-user.targetsystemctl stop firewalld.servicesystemctl disable firewalld.service\n\n命令文档useradd用法：useradd [选项] 登录      useradd -D      useradd -D [选项]选项：  -b, --base-dir BASE_DIR       新账户的主目录的基目录  -c, --comment COMMENT         新账户的 GECOS 字段  -d, --home-dir HOME_DIR       新账户的主目录  -D, --defaults                显示或更改默认的 useradd 配置  -e, --expiredate EXPIRE_DATE  新账户的过期日期  -f, --inactive INACTIVE       新账户的密码不活动期  -g, --gid GROUP               新账户主组的名称或 ID  -G, --groups GROUPS   \t\t新账户的附加组列表  -h, --help                    显示此帮助信息并推出  -k, --skel SKEL_DIR   \t\t使用此目录作为骨架目录  -K, --key KEY=VALUE           不使用 /etc/login.defs 中的默认值  -l, --no-log-init     \t\t不要将此用户添加到最近登录和登录失败数据库  -m, --create-home     \t\t创建用户的主目录  -M, --no-create-home          不创建用户的主目录  -N, --no-user-group   \t\t不创建同名的组  -o, --non-unique              允许使用重复的 UID 创建用户  -p, --password PASSWORD       加密后的新账户密码  -r, --system                  创建一个系统账户  -R, --root CHROOT_DIR         chroot 到的目录  -P, --prefix PREFIX_DIR       prefix directory where are located the /etc/* files  -s, --shell SHELL             新账户的登录 shell  -u, --uid UID                 新账户的用户 ID  -U, --user-group              创建与用户同名的组  -Z, --selinux-user SEUSER     为 SELinux 用户映射使用指定 SEUSER\n\nuserdel用法：userdel [选项] 登录选项：  -f, --force                   force some actions that would fail otherwise                                e.g. removal of user still logged in                                or files, even if not owned by the user  -h, --help                    显示此帮助信息并推出  -r, --remove                  删除主目录和邮件池  -R, --root CHROOT_DIR         chroot 到的目录  -P, --prefix PREFIX_DIR       prefix directory where are located the /etc/* files  -Z, --selinux-user            为用户删除所有的 SELinux 用户映射\n\nvim\nset nu    显示行号gg     跳转到文件开头&#x2F;     向后搜索?    向前搜索n    查找下一处N    查找上一处|     光标所在行行首L    屏幕所显示的底行&#123;    段首&#125;    段尾-    前一行行首+    后一行行首(    句首)    下一句首$    行末M    屏幕中间行0    行首（零）hjkl    左下上右x    删除光标所在字符R    替换模式（可以替换任意字符）r    单个替换dd     删除光标所在的行D    删除至行末（从光标位置开始）s    删除字符并插入（单个字符删除，并进入插入模式）S    删除行并插入（整行删除）&gt;&gt;     缩进（相当于一个tab）&lt;&lt;     反缩进&#x3D;    自动格式化J    合并上下两行I    插入到行首i     插入C    从光标处开始修改至行位a    在光标后附件或追加A    在行末追加p    粘贴（后）P    粘贴（前）Esc     命令模式ZZ     保存退出编辑(vi，含保存)ZQ    不保存退出编辑\n\n\n指定IP\nIPADDR=192.168.0.230 #静态IPGATEWAY=192.168.0.1 #默认网关NETMASK=255.255.255.0 #子网掩码DNS1=192.168.0.1 #DNS 配置DNS2=8.8.8.8\n\n自动获取IP\nBOOTPROTO=dhcpONBOOT=yes\nchmod\n\nchmod 更改权限o 所有者 g 组 o 其他r--4 w--2 x--1-R 处理指定目录以及其子目录下的所有文件chmod u+x,g+x index.htmlchmod 755 index.htmlchmod -R o+x fa\n\nchownchown 将指定文件的拥有者改为指定的用户或组chown [选项]... [所有者][:[组]] 文件...-R 处理指定目录以及其子目录下的所有文件chown weblogic:bea index.htmlchown -R weblogic:bea test1\n\nlsof# 查看使用某端口的进程lsof -i:8090netstat -ap|grep 8090# 查看到进程id之后，使用netstat命令查看其占用的端口netstat -nap|grep 7779# 查看PIDps -aux|grep chat.js| grep -v grep | awk '&#123;print $2&#125;'\n\nnohupnohup 加 &amp;大家都知道是后台运行并把stdout输出到文件nohup.out中。其实&amp;是后台运行的命令。一般都是在linux下nohup格式：nohup command_line或者nohup command_line &amp;这之间的差别是带&amp;的命令行，即使terminal关闭，或者电脑死机程序依然运行（前提是你把程序递交到服务器上）；它把标准输出（STDOUT）和标准错误（STDERR）结果输出到nohup.txt文件这个看似很方便，但是当输出很大的时候，nohup.txt文件会非常大，或者多个后台命令的时候大家都会输出到nohup.txt文件，不利于查找结果和调试程序。所以能够重定向输出会非常方便。下面要介绍标准输出，标准输入 和标准错误了。其实我门一直都在用，只是没有注意到，比如\\&gt;.&#x2F;command.sh &gt; output\\#这其中的&gt;就是标准输出符号，其实是 1&gt;output 的缩写\\&gt;.&#x2F;command.sh 2&gt; output＃这里的2&gt;就是将标准错误输出到output文件里。而0&lt; 则是标准输入了。下面步入正题，重定向后台命令\\&gt;nohup .&#x2F;command.sh &gt; output 2&gt;&amp;1 &amp;ls xxx &gt;out.txt 2&gt;&amp;1, 实际上可换成 ls xxx 1&gt;out.txt 2&gt;&amp;1；重定向符号&gt;默认是1,错误和输出都传到out.txt了。解释：前面的nohup 和后面的&amp;我想大家都能明白了把。主要是中间的 2&gt;&amp;1的意思这个意思是把标准错误（2）重定向到标准输出中（1），而标准输出又导入文件output里面，所以结果是标准错误和标准输出都导入文件output里面了。至于为什么需要将标准错误重定向到标准输出的原因，那就归结为标准错误没有缓冲区，而stdout有。这就会导致 &gt;output 2&gt;output 文件output被两次打开，而stdout和stderr将会竞争覆盖，这肯定不是我门想要的.这就是为什么有人会写成：nohup .&#x2F;command.sh &gt;output 2&gt;output出错的原因了\\##########################最后谈一下&#x2F;dev&#x2F;null文件的作用这是一个无底洞，任何东西都可以定向到这里，但是却无法打开。所以一般很大的stdou和stderr当你不关心的时候可以利用stdout和stderr定向到这里&gt;.&#x2F;command.sh &gt;&#x2F;dev&#x2F;null 2&gt;&amp;1\n\nps\nps -aux\n\nUSER: 行程拥有者PID: pid%CPU: 占用的 CPU 使用率%MEM: 占用的内存使用率VSZ: 占用的虚拟内存大小，申请内存值RSS: 占用的内存大小 ，实际使用的物理内存TTY: 终端的次要装置号码 (minor device number of tty)STAT: 该行程的状态:D: 不可中断的静止 (通悸□□缜b进行 I/O 动作)R: 正在执行中S: 静止状态T: 暂停执行Z: 不存在但暂时无法消除W: 没有足够的内存分页可分配&lt;: 高优先序的行程N: 低优先序的行程L: 有内存分页分配并锁在内存内 (实时系统或捱A I/O)START: 行程开始时间TIME: 执行的时间COMMAND:所执行的指令\n\n\nps aux | sort -k3nr |head -n 10 按照消耗CPU前10排序的进程\n\nps aux | sort -k4nr |head -n 10 按照消耗内存前10排序的进程\n\n\nsort [-bcdfimMnr][-o&lt;输出文件&gt;][-t&lt;分隔字符&gt;][+&lt;起始栏位&gt;-&lt;结束栏位&gt;][--help][--verison][文件]\n\n参数说明：\n\n-b 忽略每行前面开始出的空格字符。\n\n-c 检查文件是否已经按照顺序排序。\n\n-d 排序时，处理英文字母、数字及空格字符外，忽略其他的字符。\n\n-f 排序时，将小写字母视为大写字母。\n\n-i 排序时，除了040至176之间的ASCII字符外，忽略其他的字符。\n\n-m 将几个排序好的文件进行合并。\n\n-M 将前面3个字母依照月份的缩写进行排序。\n\n-n 依照数值的大小排序。\n\n-o&lt;输出文件&gt; 将排序后的结果存入指定的文件。\n\n-r 以相反的顺序来排序。\n\n-t&lt;分隔字符&gt; 指定排序时所用的栏位分隔字符。\n\n+&lt;起始栏位&gt;-&lt;结束栏位&gt; 以指定的栏位来排序，范围由起始栏位到结束栏位的前一栏位。\n\n–help 显示帮助。\n\n–version 显示版本信息。\n\n\nrpm\n安装\n\n# 安装 example.rpm 包rpm -i example.rpm# 安装 example.rpm 包并在安装过程中显示正在安装的文件信息rpm -iv example.rpm# 安装 example.rpm 包并在安装过程中显示正在安装的文件信息及安装进度rpm -ivh example.rpm\n\n\n卸载\n\nrpm -e --nodeps 要卸载的软件包\n\n\n查询系统已安装的软件\n\nrpm -q 软件名\n\n\n查看系统中所有已经安装的包\n\nrpm -qarpm -qa |morerpm -qa |grep abc\n\nrpm -qpl mysql-community-server-5.7.26-1.el7.x86_64.rpm\n\n\n查看安装包时间\n\nrpm -qi\n\n\n查看安装包路径\n\nrpm -ql\n\n\n按时间顺序列出rpm包\n\nrpm -q --all --last\n\ntartar filename.tar Dirname-c 创建新文件,-f 通常必选-v 操作过程显示到显示器-x 还原备份文件-r 追加文件到备份文件中-z 打包压缩---调用gzip#打包当前目录下的所有文件tar -cvf 20161226.tar *#还原tar包tar -xvf 20161226.tar#把file文件追加到tar中tar -rvf 2016.1226.tar file#打包后以gizp压缩tar -zcvf 20161226.tar.gz#解压缩tar -zxvf 20161226.tar.gz\n\nyum\n全称为 Yellow dog Updater Modified，它是一个在线的软件安装命令。\n\n# yum源配置路径/etc/yum.repos.d# 重新配置后需要生效yum cleancacheyum makecacheyum listyum list installedyum searchyum remove\n\nfirewall#查看所有打开的端口firewall-cmd --zone=public --list-ports#添加（--permanent永久生效，没有此参数重启后失效）firewall-cmd --zone=public --add-port=80/tcp --permanent#重新载入firewall-cmd --reload#查看firewall-cmd --zone=public --query-port=80/tcp#删除firewall-cmd --zone=public --remove-port=80/tcp --permanent#批量开放端口firewall-cmd --permanent --zone=public --add-port=100-500/tcpfirewall-cmd --permanent --zone=public --add-port=100-500/udpfirewall-cmd --reload\n\n# 查看防火状态systemctl status firewalldservice  iptables status# 暂时关闭防火墙systemctl stop firewalldservice  iptables stop# 永久关闭防火墙systemctl disable firewalldchkconfig iptables off# 重启防火墙systemctl enable firewalldservice iptables restart# 永久关闭后重启chkconfig iptables on\n\nwhichwhich命令的作用是在PATH变量指定的路径中搜索某个系统命令的位置并且返回第一个搜索结果。\nUsage: /usr/bin/which [options] [--] COMMAND [...]Write the full path of COMMAND(s) to standard output.--version, -[vV] Print version and exit successfully.--help,          Print this help and exit successfully.--skip-dot       Skip directories in PATH that start with a dot.--skip-tilde     Skip directories in PATH that start with a tilde.--show-dot       Don't expand a dot to current directory in output.--show-tilde     Output a tilde for HOME directory for non-root.--tty-only       Stop processing options on the right if not on tty.--all, -a        Print all matches in PATH, not just the first--read-alias, -i Read list of aliases from stdin.--skip-alias     Ignore option --read-alias; don't read stdin.--read-functions Read bash functions from stdin.--skip-functions Ignore option --read-functions; don't read stdin.Recommended use is to write the output of (alias; declare -f) to standardinput, so that which can show aliases and bash functions. See which(1) forexamples.If the options --read-alias and/or --read-functions are specified then theoutput can be a full alias or function definition, optionally followed bythe full path of each command used inside of those.Report bugs to &lt;which-bugs@gnu.org&gt;.\n\nhostnamectl\ncentos7\n\nhostnamectl [OPTIONS...] COMMAND ...Query or change system hostname.-h --help              Show this help--version           Show package version--no-ask-password   Do not prompt for password-H --host=[USER@]HOST  Operate on remote host-M --machine=CONTAINER Operate on local container--transient         Only set transient hostname--static            Only set static hostname--pretty            Only set pretty hostnameCommands:status                 Show current hostname settingsset-hostname NAME      Set system hostnameset-icon-name NAME     Set icon name for hostset-chassis NAME       Set chassis type for hostset-deployment NAME    Set deployment environment for hostset-location NAME      Set location for host\n\n例如：\nhostnamectl set-hostname namehostnamectl status\n\nwgetGNU Wget 1.14，非交互式的网络文件下载工具。用法： wget [选项]... [URL]...长选项所必须的参数在使用短选项时也是必须的。启动：-V,  --version           显示 Wget 的版本信息并退出。-h,  --help              打印此帮助。-b,  --background        启动后转入后台。-e,  --execute=COMMAND   运行一个“.wgetrc”风格的命令。日志和输入文件：-o,  --output-file=FILE    将日志信息写入 FILE。-a,  --append-output=FILE  将信息添加至 FILE。-d,  --debug               打印大量调试信息。-q,  --quiet               安静模式 (无信息输出)。-v,  --verbose             详尽的输出 (此为默认值)。-nv, --no-verbose          关闭详尽输出，但不进入安静模式。--report-speed=TYPE   Output bandwidth as TYPE.  TYPE can be bits.-i,  --input-file=FILE     下载本地或外部 FILE 中的 URLs。-F,  --force-html          把输入文件当成 HTML 文件。-B,  --base=URL            解析与 URL 相关的HTML 输入文件 (由 -i -F 选项指定)。--config=FILE         Specify config file to use.下载：-t,  --tries=NUMBER            设置重试次数为 NUMBER (0 代表无限制)。--retry-connrefused       即使拒绝连接也是重试。-O,  --output-document=FILE    将文档写入 FILE。-nc, --no-clobber              skip downloads that would download toexisting files (overwriting them).-c,  --continue                断点续传下载文件。--progress=TYPE           选择进度条类型。-N,  --timestamping            只获取比本地文件新的文件。--no-use-server-timestamps     不用服务器上的时间戳来设置本地文件。-S,  --server-response         打印服务器响应。--spider                  不下载任何文件。-T,  --timeout=SECONDS         将所有超时设为 SECONDS 秒。--dns-timeout=SECS        设置 DNS 查寻超时为 SECS 秒。--connect-timeout=SECS    设置连接超时为 SECS 秒。--read-timeout=SECS       设置读取超时为 SECS 秒。-w,  --wait=SECONDS            等待间隔为 SECONDS 秒。--waitretry=SECONDS       在获取文件的重试期间等待 1..SECONDS 秒。--random-wait             获取多个文件时，每次随机等待间隔0.5*WAIT...1.5*WAIT 秒。--no-proxy                禁止使用代理。-Q,  --quota=NUMBER            设置获取配额为 NUMBER 字节。--bind-address=ADDRESS    绑定至本地主机上的 ADDRESS (主机名或是 IP)。--limit-rate=RATE         限制下载速率为 RATE。--no-dns-cache            关闭 DNS 查寻缓存。--restrict-file-names=OS  限定文件名中的字符为 OS 允许的字符。--ignore-case             匹配文件/目录时忽略大小写。-4,  --inet4-only              仅连接至 IPv4 地址。-6,  --inet6-only              仅连接至 IPv6 地址。--prefer-family=FAMILY    首先连接至指定协议的地址FAMILY 为 IPv6，IPv4 或是 none。--user=USER               将 ftp 和 http 的用户名均设置为 USER。--password=PASS           将 ftp 和 http 的密码均设置为 PASS。--ask-password            提示输入密码。--no-iri                  关闭 IRI 支持。--local-encoding=ENC      IRI (国际化资源标识符) 使用 ENC 作为本地编码。--remote-encoding=ENC     使用 ENC 作为默认远程编码。--unlink                  remove file before clobber.目录：-nd, --no-directories           不创建目录。-x,  --force-directories        强制创建目录。-nH, --no-host-directories      不要创建主目录。--protocol-directories     在目录中使用协议名称。-P,  --directory-prefix=PREFIX  以 PREFIX/... 保存文件--cut-dirs=NUMBER          忽略远程目录中 NUMBER 个目录层。HTTP 选项：--http-user=USER        设置 http 用户名为 USER。--http-password=PASS    设置 http 密码为 PASS。--no-cache              不在服务器上缓存数据。--default-page=NAME     改变默认页(默认页通常是“index.html”)。-E,  --adjust-extension      以合适的扩展名保存 HTML/CSS 文档。--ignore-length         忽略头部的‘Content-Length’区域。--header=STRING         在头部插入 STRING。--max-redirect          每页所允许的最大重定向。--proxy-user=USER       使用 USER 作为代理用户名。--proxy-password=PASS   使用 PASS 作为代理密码。--referer=URL           在 HTTP 请求头包含‘Referer: URL’。--save-headers          将 HTTP 头保存至文件。-U,  --user-agent=AGENT      标识为 AGENT 而不是 Wget/VERSION。--no-http-keep-alive    禁用 HTTP keep-alive (永久连接)。--no-cookies            不使用 cookies。--load-cookies=FILE     会话开始前从 FILE 中载入 cookies。--save-cookies=FILE     会话结束后保存 cookies 至 FILE。--keep-session-cookies  载入并保存会话 (非永久) cookies。--post-data=STRING      使用 POST 方式；把 STRING 作为数据发送。--post-file=FILE        使用 POST 方式；发送 FILE 内容。--content-disposition   当选中本地文件名时允许 Content-Disposition 头部 (尚在实验)。--content-on-error      output the received content on server errors.--auth-no-challenge     发送不含服务器询问的首次等待的基本 HTTP 验证信息。HTTPS (SSL/TLS) 选项：--secure-protocol=PR     choose secure protocol, one of auto, SSLv2,SSLv3, TLSv1, TLSv1_1 and TLSv1_2.--no-check-certificate   不要验证服务器的证书。--certificate=FILE       客户端证书文件。--certificate-type=TYPE  客户端证书类型，PEM 或 DER。--private-key=FILE       私钥文件。--private-key-type=TYPE  私钥文件类型，PEM 或 DER。--ca-certificate=FILE    带有一组 CA 认证的文件。--ca-directory=DIR       保存 CA 认证的哈希列表的目录。--random-file=FILE       带有生成 SSL PRNG 的随机数据的文件。--egd-file=FILE          用于命名带有随机数据的 EGD 套接字的文件。FTP 选项：--ftp-user=USER         设置 ftp 用户名为 USER。--ftp-password=PASS     设置 ftp 密码为 PASS。--no-remove-listing     不要删除‘.listing’文件。--no-glob               不在 FTP 文件名中使用通配符展开。--no-passive-ftp        禁用“passive”传输模式。--preserve-permissions  保留远程文件的权限。--retr-symlinks         递归目录时，获取链接的文件 (而非目录)。WARC options:--warc-file=FILENAME      save request/response data to a .warc.gz file.--warc-header=STRING      insert STRING into the warcinfo record.--warc-max-size=NUMBER    set maximum size of WARC files to NUMBER.--warc-cdx                write CDX index files.--warc-dedup=FILENAME     do not store records listed in this CDX file.--no-warc-compression     do not compress WARC files with GZIP.--no-warc-digests         do not calculate SHA1 digests.--no-warc-keep-log        do not store the log file in a WARC record.--warc-tempdir=DIRECTORY  location for temporary files created by theWARC writer.递归下载：-r,  --recursive          指定递归下载。-l,  --level=NUMBER       最大递归深度 (inf 或 0 代表无限制，即全部下载)。--delete-after       下载完成后删除本地文件。-k,  --convert-links      让下载得到的 HTML 或 CSS 中的链接指向本地文件。--backups=N   before writing file X, rotate up to N backup files.-K,  --backup-converted   在转换文件 X 前先将它备份为 X.orig。-m,  --mirror             -N -r -l inf --no-remove-listing 的缩写形式。-p,  --page-requisites    下载所有用于显示 HTML 页面的图片之类的元素。--strict-comments    用严格方式 (SGML) 处理 HTML 注释。递归接受/拒绝：-A,  --accept=LIST               逗号分隔的可接受的扩展名列表。-R,  --reject=LIST               逗号分隔的要拒绝的扩展名列表。--accept-regex=REGEX        regex matching accepted URLs.--reject-regex=REGEX        regex matching rejected URLs.--regex-type=TYPE           regex type (posix|pcre).-D,  --domains=LIST              逗号分隔的可接受的域列表。--exclude-domains=LIST      逗号分隔的要拒绝的域列表。--follow-ftp                跟踪 HTML 文档中的 FTP 链接。--follow-tags=LIST          逗号分隔的跟踪的 HTML 标识列表。--ignore-tags=LIST          逗号分隔的忽略的 HTML 标识列表。-H,  --span-hosts                递归时转向外部主机。-L,  --relative                  只跟踪有关系的链接。-I,  --include-directories=LIST  允许目录的列表。--trust-server-names             use the name specified by the redirectionurl last component.-X,  --exclude-directories=LIST  排除目录的列表。-np, --no-parent                 不追溯至父目录。\n\ncurlUsage: curl [options...] &lt;url&gt;Options: (H) means HTTP/HTTPS only, (F) means FTP only--anyauth       Pick \"any\" authentication method (H)-a, --append        Append to target file when uploading (F/SFTP)--basic         Use HTTP Basic Authentication (H)--cacert FILE   CA certificate to verify peer against (SSL)--capath DIR    CA directory to verify peer against (SSL)-E, --cert CERT[:PASSWD] Client certificate file and password (SSL)--cert-type TYPE Certificate file type (DER/PEM/ENG) (SSL)--ciphers LIST  SSL ciphers to use (SSL)--compressed    Request compressed response (using deflate or gzip)-K, --config FILE   Specify which config file to read--connect-timeout SECONDS  Maximum time allowed for connection-C, --continue-at OFFSET  Resumed transfer offset-b, --cookie STRING/FILE  String or file to read cookies from (H)-c, --cookie-jar FILE  Write cookies to this file after operation (H)--create-dirs   Create necessary local directory hierarchy--crlf          Convert LF to CRLF in upload--crlfile FILE  Get a CRL list in PEM format from the given file-d, --data DATA     HTTP POST data (H)--data-ascii DATA  HTTP POST ASCII data (H)--data-binary DATA  HTTP POST binary data (H)--data-urlencode DATA  HTTP POST data url encoded (H)--delegation STRING GSS-API delegation permission--digest        Use HTTP Digest Authentication (H)--disable-eprt  Inhibit using EPRT or LPRT (F)--disable-epsv  Inhibit using EPSV (F)-D, --dump-header FILE  Write the headers to this file--egd-file FILE  EGD socket path for random data (SSL)--engine ENGINGE  Crypto engine (SSL). \"--engine list\" for list-f, --fail          Fail silently (no output at all) on HTTP errors (H)-F, --form CONTENT  Specify HTTP multipart POST data (H)--form-string STRING  Specify HTTP multipart POST data (H)--ftp-account DATA  Account data string (F)--ftp-alternative-to-user COMMAND  String to replace \"USER [name]\" (F)--ftp-create-dirs  Create the remote dirs if not present (F)--ftp-method [MULTICWD/NOCWD/SINGLECWD] Control CWD usage (F)--ftp-pasv      Use PASV/EPSV instead of PORT (F)-P, --ftp-port ADR  Use PORT with given address instead of PASV (F)--ftp-skip-pasv-ip Skip the IP address for PASV (F)--ftp-pret      Send PRET before PASV (for drftpd) (F)--ftp-ssl-ccc   Send CCC after authenticating (F)--ftp-ssl-ccc-mode ACTIVE/PASSIVE  Set CCC mode (F)--ftp-ssl-control Require SSL/TLS for ftp login, clear for transfer (F)-G, --get           Send the -d data with a HTTP GET (H)-g, --globoff       Disable URL sequences and ranges using &#123;&#125; and []-H, --header LINE   Custom header to pass to server (H)-I, --head          Show document info only-h, --help          This help text--hostpubmd5 MD5  Hex encoded MD5 string of the host public key. (SSH)-0, --http1.0       Use HTTP 1.0 (H)--ignore-content-length  Ignore the HTTP Content-Length header-i, --include       Include protocol headers in the output (H/F)-k, --insecure      Allow connections to SSL sites without certs (H)--interface INTERFACE  Specify network interface/address to use-4, --ipv4          Resolve name to IPv4 address-6, --ipv6          Resolve name to IPv6 address-j, --junk-session-cookies Ignore session cookies read from file (H)--keepalive-time SECONDS  Interval between keepalive probes--key KEY       Private key file name (SSL/SSH)--key-type TYPE Private key file type (DER/PEM/ENG) (SSL)--krb LEVEL     Enable Kerberos with specified security level (F)--libcurl FILE  Dump libcurl equivalent code of this command line--limit-rate RATE  Limit transfer speed to this rate-l, --list-only     List only names of an FTP directory (F)--local-port RANGE  Force use of these local port numbers-L, --location      Follow redirects (H)--location-trusted like --location and send auth to other hosts (H)-M, --manual        Display the full manual--mail-from FROM  Mail from this address--mail-rcpt TO  Mail to this receiver(s)--mail-auth AUTH  Originator address of the original email--max-filesize BYTES  Maximum file size to download (H/F)--max-redirs NUM  Maximum number of redirects allowed (H)-m, --max-time SECONDS  Maximum time allowed for the transfer--metalink      Process given URLs as metalink XML file--negotiate     Use HTTP Negotiate Authentication (H)-n, --netrc         Must read .netrc for user name and password--netrc-optional Use either .netrc or URL; overrides -n--netrc-file FILE  Set up the netrc filename to use-N, --no-buffer     Disable buffering of the output stream--no-keepalive  Disable keepalive use on the connection--no-sessionid  Disable SSL session-ID reusing (SSL)--noproxy       List of hosts which do not use proxy--ntlm          Use HTTP NTLM authentication (H)-o, --output FILE   Write output to &lt;file&gt; instead of stdout--pass PASS     Pass phrase for the private key (SSL/SSH)--post301       Do not switch to GET after following a 301 redirect (H)--post302       Do not switch to GET after following a 302 redirect (H)--post303       Do not switch to GET after following a 303 redirect (H)-#, --progress-bar  Display transfer progress as a progress bar--proto PROTOCOLS  Enable/disable specified protocols--proto-redir PROTOCOLS  Enable/disable specified protocols on redirect-x, --proxy [PROTOCOL://]HOST[:PORT] Use proxy on given port--proxy-anyauth Pick \"any\" proxy authentication method (H)--proxy-basic   Use Basic authentication on the proxy (H)--proxy-digest  Use Digest authentication on the proxy (H)--proxy-negotiate Use Negotiate authentication on the proxy (H)--proxy-ntlm    Use NTLM authentication on the proxy (H)-U, --proxy-user USER[:PASSWORD]  Proxy user and password--proxy1.0 HOST[:PORT]  Use HTTP/1.0 proxy on given port-p, --proxytunnel   Operate through a HTTP proxy tunnel (using CONNECT)--pubkey KEY    Public key file name (SSH)-Q, --quote CMD     Send command(s) to server before transfer (F/SFTP)--random-file FILE  File for reading random data from (SSL)-r, --range RANGE   Retrieve only the bytes within a range--raw           Do HTTP \"raw\", without any transfer decoding (H)-e, --referer       Referer URL (H)-J, --remote-header-name Use the header-provided filename (H)-O, --remote-name   Write output to a file named as the remote file--remote-name-all Use the remote file name for all URLs-R, --remote-time   Set the remote file's time on the local output-X, --request COMMAND  Specify request command to use--resolve HOST:PORT:ADDRESS  Force resolve of HOST:PORT to ADDRESS--retry NUM   Retry request NUM times if transient problems occur--retry-delay SECONDS When retrying, wait this many seconds between each--retry-max-time SECONDS  Retry only within this period-S, --show-error    Show error. With -s, make curl show errors when they occur-s, --silent        Silent mode. Don't output anything--socks4 HOST[:PORT]  SOCKS4 proxy on given host + port--socks4a HOST[:PORT]  SOCKS4a proxy on given host + port--socks5 HOST[:PORT]  SOCKS5 proxy on given host + port--socks5-basic  Enable username/password auth for SOCKS5 proxies--socks5-gssapi Enable GSS-API auth for SOCKS5 proxies--socks5-hostname HOST[:PORT] SOCKS5 proxy, pass host name to proxy--socks5-gssapi-service NAME  SOCKS5 proxy service name for gssapi--socks5-gssapi-nec  Compatibility with NEC SOCKS5 server-Y, --speed-limit RATE  Stop transfers below speed-limit for 'speed-time' secs-y, --speed-time SECONDS  Time for trig speed-limit abort. Defaults to 30--ssl           Try SSL/TLS (FTP, IMAP, POP3, SMTP)--ssl-reqd      Require SSL/TLS (FTP, IMAP, POP3, SMTP)-2, --sslv2         Use SSLv2 (SSL)-3, --sslv3         Use SSLv3 (SSL)--ssl-allow-beast Allow security flaw to improve interop (SSL)--stderr FILE   Where to redirect stderr. - means stdout--tcp-nodelay   Use the TCP_NODELAY option-t, --telnet-option OPT=VAL  Set telnet option--tftp-blksize VALUE  Set TFTP BLKSIZE option (must be &gt;512)-z, --time-cond TIME  Transfer based on a time condition-1, --tlsv1         Use =&gt; TLSv1 (SSL)--tlsv1.0       Use TLSv1.0 (SSL)--tlsv1.1       Use TLSv1.1 (SSL)--tlsv1.2       Use TLSv1.2 (SSL)--tlsv1.3       Use TLSv1.3 (SSL)--tls-max VERSION  Use TLS up to VERSION (SSL)--trace FILE    Write a debug trace to the given file--trace-ascii FILE  Like --trace but without the hex output--trace-time    Add time stamps to trace/verbose output--tr-encoding   Request compressed transfer encoding (H)-T, --upload-file FILE  Transfer FILE to destination--url URL       URL to work with-B, --use-ascii     Use ASCII/text transfer-u, --user USER[:PASSWORD]  Server user and password--tlsuser USER  TLS username--tlspassword STRING TLS password--tlsauthtype STRING  TLS authentication type (default SRP)--unix-socket FILE    Connect through this UNIX domain socket-A, --user-agent STRING  User-Agent to send to server (H)-v, --verbose       Make the operation more talkative-V, --version       Show version number and quit-w, --write-out FORMAT  What to output after completion--xattr        Store metadata in extended file attributes-q                 If used as the first parameter disables .curlrc\n","tags":["Linux"]}]